Evi NemethGarth SnyderTrent R HeinBen WhaleyDan MackinUNIX AND LINUX SYSTEMADMINISTRATIONHANDBOOKFIFTH EDITIONwith James Garnett Fabrizio Branca and Adrian MouatLibrary of Congress Control Number Copyright Pearson Education IncISBN ISBN Tribute to Evi xlPreface xliiForeword xlivAcknowledgments xlviSECTION ONE BASIC ADMINISTRATIONChapter Where to Start Essential duties of a system administrator Controlling access Adding hardware Automating tasks Overseeing backups Installing and upgrading software Monitoring Troubleshooting Maintaining local documentation Vigilantly monitoring security Tuning performance Developing site policies Working with vendors Fire fighting ContentsSuggested background Linux distributions Example systems used in this book Example Linux distributions Example UNIX distribution Notation and typographical conventions Units Man pages and other online documentation Organization of the man pages man read man pages Storage of man pages Other authoritative documentation Systemspecific guides Packagespecific documentation Books RFC publications Other sources of information Keeping current HowTos and reference sites Conferences Ways to find and install software Determining if software is already installed Adding new software Building software from source code Installing from a web script Where to host Specialization and adjacent disciplines DevOps Site reliability engineers Security operations engineers Network administrators Database administrators Network operations center NOC engineers Data center technicians Architects Recommended reading System administration and DevOps Essential tools Chapter Booting and System Management Daemons Boot process overview System firmware BIOS vs UEFI Legacy BIOS UEFI Boot loaders GRUB the GRand Unified Boot loader GRUB configuration The GRUB command line Linux kernel options The FreeBSD boot process The BIOS path boot The UEFI path loader configuration loader commands System management daemons Responsibilities of init Implementations of init Traditional init systemd vs the world inits judged and assigned their proper punishments systemd in detail Units and unit files systemctl manage systemd Unit statuses Targets Dependencies among units Execution order A more complex unit file example Local services and customizations Service and startup control caveats systemd logging FreeBSD init and startup scripts Reboot and shutdown procedures Shutting down physical systems Shutting down cloud systems Stratagems for a nonbooting system Singleuser mode Singleuser mode on FreeBSD Singleuser mode with GRUB Recovery of cloud systems Chapter Access Control and Rootly Powers Standard UNIX access control Filesystem access control Process ownership The root account Setuid and setgid execution Management of the root account Root account login su substitute user identity sudo limited su Example configuration sudo pros and cons sudo vs advanced access control Typical setup Environment management sudo without passwords Precedence sudo without a control terminal Sitewide sudo configuration Disabling the root account System accounts other than root Extensions to the standard access control model Drawbacks of the standard model PAM Pluggable Authentication Modules Kerberos network cryptographic authentication Filesystem access control lists Linux capabilities Linux namespaces Modern access control Separate ecosystems Mandatory access control Rolebased access control SELinux SecurityEnhanced Linux AppArmor Recommended reading Chapter Process Control Components of a process PID process ID number PPID parent PID UID and EUID real and effective user ID GID and EGID real and effective group ID Niceness Control terminal The life cycle of a process Signals kill send signals Process and thread states ps monitor processes Interactive monitoring with top nice and renice influence scheduling priority The proc filesystem strace and truss trace signals and system calls Runaway processes Periodic processes cron schedule commands The format of crontab files Crontab management Other crontabs cron access control systemd timers Structure of systemd timers systemd timer example systemd time expressions Transient timers Common uses for scheduled tasks Sending mail Cleaning up a filesystem Rotating a log file Running batch jobs Backing up and mirroring Chapter The Filesystem Pathnames Filesystem mounting and unmounting Organization of the file tree File types Regular files Directories Hard links Character and block device files Local domain sockets Named pipes Symbolic links File attributes The permission bits The setuid and setgid bits The sticky bit ls list and inspect files chmod change permissions chown and chgrp change ownership and group umask assign default permissions Linux bonus flags Access control lists A cautionary note ACL types Implementation of ACLs Linux ACL support FreeBSD ACL support POSIX ACLs Interaction between traditional modes and ACLs POSIX access determination POSIX ACL inheritance NFSv ACLs NFSv entities for which permissions can be specified NFSv access determination ACL inheritance in NFSv NFSv ACL viewing Interactions between ACLs and modes NFSv ACL setup Chapter Software Installation and Management Operating system installation Installing from the network Setting up PXE Using kickstart the automated installer for Red Hat and CentOS Setting up a kickstart configuration file Building a kickstart server Pointing kickstart at your config file Automating installation for Debian and Ubuntu Netbooting with Cobbler the open source Linux provisioning server Automating FreeBSD installation Managing packages Linux package management systems rpm manage RPM packages dpkg manage deb packages Highlevel Linux package management systems Package repositories RHN the Red Hat Network APT the Advanced Package Tool Repository configuration An example etcaptsourceslist file Creation of a local repository mirror APT automation yum release management for RPM FreeBSD software management The base system pkg the FreeBSD package manager The ports collection Software localization and configuration Organizing your localization Structuring updates Limiting the field of play Testing Recommended reading Chapter Scripting and the Shell Scripting philosophy Write microscripts Learn a few tools well Automate all the things Dont optimize prematurely Pick the right scripting language Follow best practices Shell basics Command editing Pipes and redirection Variables and quoting Environment variables Common filter commands cut separate lines into fields sort sort lines uniq print unique lines wc count lines words and characters tee copy input to two places head and tail read the beginning or end of a file grep search text sh scripting Execution From commands to scripts Input and output Spaces in filenames Commandline arguments and functions Control flow Loops Arithmetic Regular expressions The matching process Literal characters Special characters Example regular expressions Captures Greediness laziness and catastrophic backtracking Python programming The passion of Python Python or Python Python quick start Objects strings numbers lists dictionaries tuples and files Input validation example Loops Ruby programming Installation Ruby quick start Blocks Symbols and option hashes Regular expressions in Ruby Ruby as a filter Library and environment management for Python and Ruby Finding and installing packages Creating reproducible environments Multiple environments virtualenv virtual environments for Python RVM the Ruby enVironment Manager Revision control with Git A simple Git example Git caveats Social coding with Git Recommended reading Shells and shell scripting Regular expressions Python Ruby Chapter User Management Account mechanics The etcpasswd file Login name Encrypted password UID user ID number Default GID group ID number GECOS field Home directory Login shell The Linux etcshadow file FreeBSDs etcmasterpasswd and etcloginconf files The etcmasterpasswd file The etcloginconf file The etcgroup file Manual steps for adding users Editing the passwd and group files Setting a password Creating the home directory and installing startup files Setting home directory permissions and ownerships Configuring roles and administrative privileges Finishing up Scripts for adding users useradd adduser and newusers useradd on Linux adduser on Debian and Ubuntu adduser on FreeBSD newusers on Linux adding in bulk Safe removal of a users account and files User login lockout Risk reduction with PAM Centralized account management LDAP and Active Directory Applicationlevel single signon systems Identity management systems Chapter Cloud Computing The cloud in context Cloud platform choices Public private and hybrid clouds Amazon Web Services Google Cloud Platform DigitalOcean Cloud service fundamentals Access to the cloud Regions and availability zones Virtual private servers Networking Storage Identity and authorization Automation Serverless functions Clouds VPS quick start by platform Amazon Web Services aws control AWS subsystems Creating an EC instance Viewing the console log Stopping and terminating instances Google Cloud Platform Setting up gcloud Running an instance on GCE DigitalOcean Cost control Recommended Reading Chapter Logging Log locations Files not to manage How to view logs in the systemd journal The systemd journal Configuring the systemd journal Adding more filtering options for journalctl Coexisting with syslog Syslog Reading syslog messages Rsyslog architecture Rsyslog versions Rsyslog configuration Modules sysklogd syntax Legacy directives RainerScript Config file examples Basic rsyslog configuration Network logging client Central logging host Syslog message security Syslog configuration debugging Kernel and boottime logging Management and rotation of log files logrotate crossplatform log management newsyslog log management on FreeBSD Management of logs at scale The ELK stack Graylog Logging as a service Logging policies Chapter Drivers and the Kernel Kernel chores for system administrators Kernel version numbering Linux kernel versions FreeBSD kernel versions Devices and their drivers Device files and device numbers Challenges of device file management Manual creation of device files Modern device file management Linux device management Sysfs a window into the souls of devices udevadm explore devices Rules and persistent names FreeBSD device management Devfs automatic device file configuration devd higherlevel device management Linux kernel configuration Tuning Linux kernel parameters Building a custom kernel If it aint broke dont fix it Setting up to build the Linux kernel Configuring kernel options Building the kernel binary Adding a Linux device driver FreeBSD kernel configuration Tuning FreeBSD kernel parameters Building a FreeBSD kernel Loadable kernel modules Loadable kernel modules in Linux Loadable kernel modules in FreeBSD Booting Linux boot messages FreeBSD boot messages Booting alternate kernels in the cloud Kernel errors Linux kernel errors FreeBSD kernel panics Recommended reading Chapter Printing CUPS printing Interfaces to the printing system The print queue Multiple printers and queues Printer instances Network printer browsing Filters CUPS server administration Network print server setup Printer autoconfiguration Network printer configuration Printer configuration examples Service shutoff Other configuration tasks Troubleshooting tips Print daemon restart Log files Direct printing connections Network printing problems Recommended reading SECTION TWO NETWORKINGChapter TCPIP Networking TCPIP and its relationship to the Internet Who runs the Internet Network standards and documentation Networking basics IPv and IPv Packets and encapsulation Ethernet framing Maximum transfer unit Packet addressing Hardware MAC addressing IP addressing Hostname addressing Ports Address types IP addresses the gory details IPv address classes IPv subnetting Tricks and tools for subnet arithmetic CIDR Classless InterDomain Routing Address allocation Private addresses and network address translation NAT IPv addressing IPv address notation IPv prefixes Automatic host numbering Stateless address autoconfiguration IPv tunneling IPv information sources Routing Routing tables ICMP redirects IPv ARP and IPv neighbor discovery DHCP the Dynamic Host Configuration Protocol DHCP software DHCP behavior ISCs DHCP software Security issues IP forwarding ICMP redirects Source routing Broadcast pings and other directed broadcasts IP spoofing Hostbased firewalls Virtual private networks Basic network configuration Hostname and IP address assignment Network interface and IP configuration Routing configuration DNS configuration Systemspecific network configuration Linux networking NetworkManager ip manually configure a network Debian and Ubuntu network configuration Red Hat and CentOS network configuration Linux network hardware options Linux TCPIP options Securityrelated kernel variables FreeBSD networking ifconfig configure network interfaces FreeBSD network hardware configuration FreeBSD boottime network configuration FreeBSD TCPIP configuration Network troubleshooting ping check to see if a host is alive traceroute trace IP packets Packet sniffers tcpdump commandline packet sniffer Wireshark and TShark tcpdump on steroids Network monitoring SmokePing gather ping statistics over time iPerf track network performance Cacti collect and graph data Firewalls and NAT Linux iptables rules chains and tables iptables rule targets iptables firewall setup A complete example Linux NAT and packet filtering IPFilter for UNIX systems Cloud networking AWSs virtual private cloud VPC Subnets and routing tables Security groups and NACLs A sample VPC architecture Creating a VPC with Terraform Google Cloud Platform networking DigitalOcean networking Recommended reading History Classics and bibles Protocols Chapter Physical Networking Ethernet the Swiss Army knife of networking Ethernet signaling Ethernet topology Unshielded twistedpair cabling Optical fiber Ethernet connection and expansion Hubs Switches VLANcapable switches Routers Autonegotiation Power over Ethernet Jumbo frames Wireless Ethernet for nomads Wireless standards Wireless client access Wireless infrastructure and WAPs Wireless topology Small money wireless Big money wireless Wireless security SDN softwaredefined networking Network testing and debugging Building wiring UTP cabling options Connections to offices Wiring standards Network design issues Network architecture vs building architecture Expansion Congestion Maintenance and documentation Management issues Recommended vendors Cables and connectors Test equipment Routersswitches Recommended reading Chapter IP Routing Packet forwarding a closer look Routing daemons and routing protocols Distancevector protocols Linkstate protocols Cost metrics Interior and exterior protocols Protocols on parade RIP and RIPng Routing Information Protocol OSPF Open Shortest Path First EIGRP Enhanced Interior Gateway Routing Protocol BGP Border Gateway Protocol Routing protocol multicast coordination Routing strategy selection criteria Routing daemons routed obsolete RIP implementation Quagga mainstream routing daemon XORP router in a box Cisco routers Recommended reading Chapter DNS The Domain Name System DNS architecture Queries and responses DNS service providers DNS for lookups resolvconf client resolver configuration nsswitchconf who do I ask for a name The DNS namespace Registering a domain name Creating your own subdomains How DNS works Name servers Authoritative and cachingonly servers Recursive and nonrecursive servers Resource records Delegation Caching and efficiency Multiple answers and round robin DNS load balancing Debugging with query tools The DNS database Parser commands in zone files Resource records The SOA record NS records A records AAAA records PTR records MX records CNAME records SRV records TXT records SPF DKIM and DMARC records DNSSEC records The BIND software Components of BIND Configuration files The include statement The options statement The acl statement The TSIG key statement The server statement The masters statement The logging statement The statisticschannels statement The zone statement Configuring the master server for a zone Configuring a slave server for a zone Setting up the root server hints Setting up a forwarding zone The controls statement for rndc Split DNS and the view statement BIND configuration examples The localhost zone A small security company Zone file updating Zone transfers Dynamic updates DNS security issues Access control lists in BIND revisited Open resolvers Running in a chrooted jail Secure servertoserver communication with TSIG and TKEY Setting up TSIG for BIND DNSSEC DNSSEC policy DNSSEC resource records Turning on DNSSEC Key pair generation Zone signing The DNSSEC chain of trust DNSSEC key rollover DNSSEC tools ldns tools nlnetlabsnlprojectsldns dnssectoolsorg RIPE tools ripenet OpenDNSSEC opendnssecorg Debugging DNSSEC BIND debugging Logging in BIND Channels Categories Log messages Sample BIND logging configuration Debug levels in BIND Name server control with rndc Commandline querying for lame delegations Recommended reading Books and other documentation Online resources The RFCs Chapter Single SignOn Core SSO elements LDAP lightweight directory services Uses for LDAP The structure of LDAP data OpenLDAP the traditional open source LDAP server Directory Server alternative open source LDAP server LDAP Querying Conversion of passwd and group files to LDAP Using directory services for login Kerberos Linux Kerberos configuration for AD integration FreeBSD Kerberos configuration for AD integration sssd the System Security Services Daemon nsswitchconf the name service switch PAM cooking spray or authentication wonder PAM configuration PAM example Alternative approaches NIS the Network Information Service rsync transfer files securely Recommended reading Chapter Electronic Mail Mail system architecture User agents Submission agents Transport agents Local delivery agents Message stores Access agents Anatomy of a mail message The SMTP protocol You had me at EHLO SMTP error codes SMTP authentication Spam and malware Forgeries SPF and Sender ID DKIM Message privacy and encryption Mail aliases Getting aliases from files Mailing to files Mailing to programs Building the hashed alias database Email configuration sendmail The switch file Starting sendmail Mail queues sendmail configuration The m preprocessor The sendmail configuration pieces A configuration file built from a sample mc file Configuration primitives Tables and databases Generic macros and features OSTYPE macro DOMAIN macro MAILER macro FEATURE macro usecwfile feature redirect feature alwaysadddomain feature accessdb feature virtusertable feature ldaprouting feature Masquerading features MAILHUB and SMARTHOST macros Client configuration m configuration options Spamrelated features in sendmail Relay control User or site blacklisting Throttles rates and connection limits Security and sendmail Ownerships Permissions Safer mail to files and programs Privacy options Running a chrooted sendmail for the truly paranoid Denial of service attacks TLS Transport Layer Security sendmail testing and debugging Queue monitoring Logging Exim Exim installation Exim startup Exim utilities Exim configuration language Exim configuration file Global options Options Lists Macros Access control lists ACLs Content scanning at ACL time Authenticators Routers The accept router The dnslookup router The manualroute router The redirect router Peruser filtering through forward files Transports The appendfile transport The smtp transport Retry configuration Rewriting configuration Local scan function Logging Debugging Postfix Postfix architecture Receiving mail Managing mailwaiting queues Sending mail Security Postfix commands and documentation Postfix configuration What to put in maincf Basic settings Null client Use of postconf Lookup tables Local delivery Virtual domains Virtual alias domains Virtual mailbox domains Access control Access tables Authentication of clients and encryption Debugging Looking at the queue Softbouncing Recommended reading sendmail references Exim references Postfix references RFCs Chapter Web Hosting HTTP the Hypertext Transfer Protocol Uniform Resource Locators URLs Structure of an HTTP transaction HTTP requests HTTP responses Headers and the message body curl HTTP from the command line TCP connection reuse HTTP over TLS Virtual hosts Web software basics Web servers and HTTP proxy software Load balancers Caches Browser caches Proxy cache Reverse proxy cache Cache problems Cache software Content delivery networks Languages of the web Ruby Python Java Nodejs PHP Go Application programming interfaces APIs Web hosting in the cloud Build versus buy PlatformasaService Static content hosting Serverless web applications Apache httpd httpd in use httpd configuration logistics Virtual host configuration HTTP basic authentication Configuring TLS Running web applications within Apache Logging NGINX Installing and running NGINX Configuring NGINX Configuring TLS for NGINX Load balancing with NGINX HAProxy Health checks Server statistics Sticky sessions TLS termination Recommended reading SECTION THREE STORAGEChapter Storage I just want to add a disk Linux recipe FreeBSD recipe Storage hardware Hard disks Hard disk reliability Failure modes and metrics Drive types Warranties and retirement Solid state disks Rewritability limits Flash memory and controller types Page clusters and preerasing SSD reliability Hybrid drives Advanced Format and KiB blocks Storage hardware interfaces The SATA interface The PCI Express interface The SAS interface USB Attachment and lowlevel management of drives Installation verification at the hardware level Disk device files Ephemeral device names Formatting and bad block management ATA secure erase hdparm and camcontrol set disk and interface parameters Hard disk monitoring with SMART The software side of storage peeling the onion Elements of a storage system The Linux device mapper Disk partitioning Traditional partitioning MBR partitioning GPT GUID partition tables Linux partitioning FreeBSD partitioning Logical volume management Linux logical volume management Volume snapshots Filesystem resizing FreeBSD logical volume management RAID redundant arrays of inexpensive disks Software vs hardware RAID RAID levels Disk failure recovery Drawbacks of RAID mdadm Linux software RAID Creating an array mdadmconf document array configuration Simulating a failure Filesystems Traditional filesystems UFS ext and XFS Filesystem terminology Filesystem polymorphism Filesystem formatting fsck check and repair filesystems Filesystem mounting Setup for automatic mounting USB drive mounting Swapping recommendations Nextgeneration filesystems ZFS and Btrfs Copyonwrite Error detection Performance ZFS all your storage problems solved ZFS on Linux ZFS architecture Example disk addition Filesystems and properties Property inheritance One filesystem per user Snapshots and clones Raw volumes Storage pool management Btrfs ZFS lite for Linux Btrfs vs ZFS Setup and storage conversion Volumes and subvolumes Volume snapshots Shallow copies Data backup strategy Recommended reading Chapter The Network File System Meet network file services The competition Issues of state Performance concerns Security The NFS approach Protocol versions and history Remote procedure calls Transport protocols State Filesystem exports File locking Security concerns Identity mapping in version Root access and the nobody account Performance considerations in version Serverside NFS Linux exports FreeBSD exports nfsd serve files Clientside NFS Mounting remote filesystems at boot time Restricting exports to privileged ports Identity mapping for NFS version nfsstat dump NFS statistics Dedicated NFS file servers Automatic mounting Indirect maps Direct maps Master maps Executable maps Automount visibility Replicated filesystems and automount Automatic automounts V all but Linux Specifics for Linux Recommended reading Chapter SMB Samba SMB server for UNIX Installing and configuring Samba File sharing with local authentication File sharing with accounts authenticated by Active Directory Configuring shares Sharing home directories Sharing project directories Mounting SMB file shares Browsing SMB file shares Ensuring Samba security Debugging Samba Querying Sambas state with smbstatus Configuring Samba logging Managing character sets Recommended reading SECTION FOUR OPERATIONSChapter Configuration Management Configuration management in a nutshell Dangers of configuration management Elements of configuration management Operations and parameters Variables Facts Change handlers Bindings Bundles and bundle repositories Environments Client inventory and registration Popular CM systems compared Terminology Business models Architectural options Language options Dependency management options General comments on Chef General comments on Puppet General comments on Ansible and Salt YAML a rant Introduction to Ansible Ansible example Client setup Client groups Variable assignments Dynamic and computed client groups Task lists state parameters Iteration Interaction with Jinja Template rendering Bindings plays and playbooks Roles Recommendations for structuring the configuration base Ansible access options Introduction to Salt Minion setup Variable value binding for minions Minion matching Salt states Salt and Jinja State IDs and dependencies State and execution functions Parameters and names State binding to minions Highstates Salt formulas Environments Documentation roadmap Ansible and Salt compared Deployment flexibility and scalability Builtin modules and extensibility Security Miscellaneous Best practices Recommended reading Chapter Virtualization Virtual vernacular Hypervisors Full virtualization Paravirtualization Hardwareassisted virtualization Paravirtualized drivers Modern virtualization Type vs type hypervisors Live migration Virtual machine images Containerization Virtualization with Linux Xen Xen guest installation KVM KVM guest installation FreeBSD bhyve VMware VirtualBox Packer Vagrant Recommended reading Chapter Containers Background and core concepts Kernel support Images Networking Docker the open source container engine Basic architecture Installation Client setup The container experience Volumes Data volume containers Docker networks Namespaces and the bridge network Network overlays Storage drivers dockerd option editing Image building Choosing a base image Building from a Dockerfile Composing a derived Dockerfile Registries Containers in practice Logging Security advice Restrict access to the daemon Use TLS Run processes as unprivileged users Use a readonly root filesystem Limit capabilities Secure images Debugging and troubleshooting Container clustering and management A synopsis of container management software Kubernetes Mesos and Marathon Docker Swarm AWS EC Container Service Recommended reading Chapter Continuous Integration and Delivery CICD essentials Principles and practices Use revision control Build once deploy often Automate endtoend Build every integration commit Share responsibility Build fast fix fast Audit and verify Environments Feature flags Pipelines The build process Testing Deployment Zerodowntime deployment techniques Jenkins the open source automation server Basic Jenkins concepts Distributed builds Pipeline as code CICD in practice UlsahGo a trivial web application Unit testing UlsahGo Taking first steps with the Jenkins Pipeline Building a DigitalOcean image Provisioning a single system for testing Testing the droplet Deploying UlsahGo to a pair of droplets and a load balancer Concluding the demonstration pipeline Containers and CICD Containers as a build environment Container images as build artifacts Recommended reading Chapter Security Elements of security How security is compromised Social engineering Software vulnerabilities Distributed denialofservice attacks DDoS Insider abuse Network system or application configuration errors Basic security measures Software updates Unnecessary services Remote event logging Backups Viruses and worms Root kits Packet filtering Passwords and multifactor authentication Vigilance Application penetration testing Passwords and user accounts Password changes Password vaults and password escrow Password aging Group logins and shared logins User shells Rootly entries Security power tools Nmap network port scanner Nessus nextgeneration network scanner Metasploit penetration testing software Lynis onbox security auditing John the Ripper finder of insecure passwords Bro the programmable network intrusion detection system Snort the popular network intrusion detection system OSSEC hostbased intrusion detection OSSEC basic concepts OSSEC installation OSSEC configuration FailBan bruteforce attack response system Cryptography primer Symmetric key cryptography Public key cryptography Public key infrastructure Transport Layer Security Cryptographic hash functions Random number generation Cryptographic software selection The openssl command Preparing keys and certificates Debugging TLS servers PGP Pretty Good Privacy Kerberos a unified approach to network security SSH the Secure SHell OpenSSH essentials The ssh client Public key authentication The sshagent Host aliases in sshconfig Connection multiplexing Port forwarding sshd the OpenSSH server Host key verification with SSHFP File transfers Alternatives for secure logins Firewalls Packetfiltering firewalls Filtering of services Stateful inspection firewalls Firewalls safe Virtual private networks VPNs IPsec tunnels All I need is a VPN right Certifications and standards Certifications Security standards ISO PCI DSS NIST series The Common Criteria OWASP the Open Web Application Security Project CIS the Center for Internet Security Sources of security information SecurityFocuscom the BugTraq mailing list and the OSS mailing list Schneier on Security The Verizon Data Breach Investigations Report The SANS Institute Distributionspecific security resources Other mailing lists and web sites When your site has been attacked Recommended reading Chapter Monitoring An overview of monitoring Instrumentation Data types Intake and processing Notifications Dashboards and UIs The monitoring culture The monitoring platforms Open source realtime platforms Nagios and Icinga Sensu Open source timeseries platforms Graphite Prometheus InfluxDB Munin Open source charting platforms Commercial monitoring platforms Hosted monitoring platforms Data collection StatsD generic data submission protocol Data harvesting from command output Network monitoring Systems monitoring Commands for systems monitoring collectd generalized system data harvester sysdig and dtrace execution tracers Application monitoring Log monitoring Supervisor Munin a simple option for limited domains Commercial application monitoring tools Security monitoring System integrity verification Intrusion detection monitoring SNMP the Simple Network Management Protocol SNMP organization SNMP protocol operations NetSNMP tools for servers Tips and tricks for monitoring Recommended reading Chapter Performance Analysis Performance tuning philosophy Ways to improve performance Factors that affect performance Stolen CPU cycles Analysis of performance problems System performance checkup Taking stock of your equipment Gathering performance data Analyzing CPU usage Understanding how the system manages memory Analyzing memory usage Analyzing disk IO fio testing storage subsystem performance sar collecting and reporting statistics over time Choosing a Linux IO scheduler perf profiling Linux systems in detail Help My server just got really slow Recommended reading Chapter Data Center Basics Racks Power Rack power requirements kVA vs kW Energy efficiency Metering Cost Remote control Cooling and environment Cooling load estimation Roof walls and windows Electronic gear Light fixtures Operators Total heat load Hot aisles and cold aisles Humidity Environmental monitoring Data center reliability tiers Data center security Location Perimeter Facility access Rack access Tools Recommended reading Chapter Methodology Policy and Politics The grand unified theory DevOps DevOps is CLAMS Culture Lean Automation Measurement Sharing System administration in a DevOps world Ticketing and task management systems Common functions of ticketing systems Ticket ownership User acceptance of ticketing systems Sample ticketing systems Ticket dispatching Local documentation maintenance Infrastructure as code Documentation standards Environment separation Disaster management Risk assessment Recovery planning Staffing for a disaster Security incidents IT policies and procedures The difference between policies and procedures Policy best practices Procedures Service level agreements Scope and descriptions of services Queue prioritization policies Conformance measurements Compliance regulations and standards Legal issues Privacy Policy enforcement Control liability Software licenses Organizations conferences and other resources Recommended reading Index A Brief History of System Administration Colophon About the Contributors About the Authors Modern technologists are masters at the art of searching Google for answers Ifanother system administrator has already encountered and possibly solved aproblem chances are you can find their writeup on the Internet We applaud andencourage this open sharing of ideas and solutionsIf great information is already available on the Internet why write another editionof this book Heres how this book helps system administrators grow We offer philosophy guidance and context for applying technology appropriately As with the blind men and the elephant its important tounderstand any given problem space from a variety of angles Valuableperspectives include background on adjacent disciplines such as securitycompliance DevOps cloud computing and software development life cycles We take a handson approach Our purpose is to summarize our collective perspective on system administration and to recommend approachesthat stand the test of time This book contains numerous war stories anda wealth of pragmatic advice This is not a book about how to run UNIX or Linux at home in your garage or on your smartphone Instead we describe the management ofproduction environments such as businesses government offices anduniversities These environments have requirements that are differentfrom and far outstrip those of a typical hobbyist We teach you how to be a professional Effective system administrationrequires both technical and soft skills It also requires a sense of humorPrefaceThe organization of this bookThis book is divided into four large chunks Basic Administration NetworkingStorage and OperationsBasic Administration presents a broad overview of UNIX and Linux from a systemadministrators perspective The chapters in this section cover most of the facts andtechniques needed to run a standalone systemThe Networking section describes the protocols used on UNIX systems and thetechniques used to set up extend and maintain networks and Internetfacing servers Highlevel network software is also covered here Among the featured topicsare the Domain Name System electronic mail single signon and web hostingThe Storage section tackles the challenges of storing and managing data This sectionalso covers subsystems that allow file sharing on a network such as the NetworkFile System and the Windowsfriendly SMB protocolThe Operations section addresses the key topics that a system administrator faceson a daily basis when managing production environments These topics includemonitoring security performance interactions with developers and the politicsof running a system administration groupOur contributorsWere delighted to welcome James Garnett Fabrizio Branca and Adrian Mouat ascontributing authors for this edition These contributors deep knowledge of a variety of areas has greatly enriched the content of this bookContact informationPlease send suggestions comments and bug reports to ulsahbookadmincomWe do answer mail but please be patient it is sometimes a few days before one ofus is able to respond Because of the volume of email that this alias receives weregret that we are unable to answer technical questionsTo view a copy of our current bug list and other latebreaking information visitour web site admincomWe hope you enjoy this book and we wish you the best of luck with your adventures in system administrationGarth SnyderTrent R HeinBen WhaleyDan MackinJuly In Winston Churchill described an early battle of WWII this is not the endit is not even the beginning of the endbut it is perhaps the end of the beginningI was reminded of these words when I was approached to write this Foreword forthe fifth edition of UNIX and Linux System Administration Handbook The loss at seaof Evi Nemeth has been a great sadness for the UNIX community but Im pleasedto see her legacy endure in the form of this book and in her many contributions tothe field of system administrationThe way the world got its Internet was originally through UNIX A remarkabledeparture from the complex and proprietary operating systems of its day UNIXwas minimalistic toolsdriven portable and widely used by people who wantedto share their work with others What we today call open source software was already pervasivebut namelessin the early days of UNIX and the Internet Opensource was just how the technical and academic communities did things becausethe benefits so obviously outweighed the costsDetailed histories of UNIX Linux and the Internet have been lovingly presentedelsewhere I bring up these highlevel touchpoints only to remind us all that themodern world owes much to open source software and to the Internet and thatthe original foundation for this bounty was UNIXAs early UNIX and Internet companies fought to hire the most brilliant peopleand to deliver the most innovative features software portability was often sacrificed Eventually system administrators had to know a little bit about a lot of thingsbecause no two UNIXstyle operating systems then or now were entirely alike Asa working UNIX system administrator in the mids and later I had to know notjust shell scripting and Sendmail configuration but also kernel device drivers It wasalso important to know how to fix a filesystem with an octal debugger Fun timesForewordOut of that era came the first edition of this book and all the editions that followedit In the parlance of the times we called the authors Evi and crew or perhaps Eviand her kids Because of my work on Cron and BIND Evi spent a week or two withme and my family and my workplace every time an edition of this book was inprogress to make sure she was saying enough saying nothing wrong and hopefullysaying something unique and useful about each of those programs Frankly beingaround Evi was exhausting especially when she was curious about something oron a deadline or in my case both That having been said I miss Evi terribly and Itreasure every memory and every photograph of herIn the decades of this books multiple editions much has changed It has been fascinating to watch this book evolve along with UNIX itself Every new edition omittedsome technologies that were no longer interesting or relevant to make room fornew topics that were just becoming important to UNIX administrators or that theauthors thought soon would beIts hard to believe that we ever spent dozens of kilowatts of power on trucksizedcomputers whose capabilities are now dwarfed by an Android smartphone Its equally hard to believe that we used to run hundreds or thousands of individual serverand desktop computers with nowantiquated technologies like rdist In those yearsvarious editions of this book helped people like me and like Evi herself cope withheterogeneous and sometimes proprietary computers that were each real rather thanvirtualized and which each had to be maintained rather than being reinstalled orin Docker rebuilt every time something needed patching or upgradingWe adapt or we exit The Evi kids who carry on Evis legacy have adapted andthey are back in this fifth edition to tell you what you need to know about howmodern UNIX and Linux computers work and how you can make them workthe way you want them to Evis loss marks the end of an era but its also soberingto consider how many aspects of system administration have passed into historyalongside her I know dozens of smart and successful technologists who will neverdress cables in the back of an equipment rack hear the tone of a modem or seean RS cable This edition is for those whose systems live in the cloud or invirtualized data centers those whose administrative work largely takes the formof automation and configuration source code those who collaborate closely withdevelopers network engineers compliance officers and all the other worker beeswho inhabit the modern hiveYou hold in your hand the latest best edition of a book whose birth and evolutionhave precisely tracked the birth and evolution of the UNIX and Internet communityEvi would be extremely proud of her kids both because of this book and becauseof who they have each turned out to be I am proud to know themPaul VixieLa Honda CaliforniaJune SECTION ONEBASIC ADMINISTRATIONWeve designed this book to occupy a specific niche in the vast ecosystem of manpages blogs magazines books and other reference materials that address the needsof UNIX and Linux system administratorsFirst its an orientation guide It reviews the major administrative systems identifiesthe different pieces of each and explains how they work together In the many caseswhere you must choose among various implementations of a concept we describethe advantages and drawbacks of the most popular optionsSecond its a quickreference handbook that summarizes what you need to knowto perform common tasks on a variety of common UNIX and Linux systems Forexample the ps command which shows the status of running processes supportsmore than commandline options on Linux systems But a few combinationsof options satisfy the majority of a system administrators needs we summarizethem on page Finally this book focuses on the administration of enterprise servers and networksThat is serious professional system administration Its easy to set up a single systemharder to keep a distributed cloudbased platform running smoothly in the face ofviral popularity network partitions and targeted attacks We describe techniques Where to Startand rules of thumb that help you recover systems from adversity and we help youchoose solutions that scale as your empire grows in size complexity and heterogeneityWe dont claim to do all of this with perfect objectivity but we think weve made ourbiases fairly clear throughout the text One of the interesting things about systemadministration is that reasonable people can have dramatically different notions ofwhat constitutes the most appropriate solution We offer our subjective opinionsto you as raw data Decide for yourself how much to accept and how much of ourcomments apply to your environment Essential duties of a system administratorThe sections below summarize some of the main tasks that administrators areexpected to perform These duties need not necessarily be carried out by a singleperson and at many sites the work is distributed among the members of a teamHowever at least one person should understand all the components and ensurethat every task is performed correctlyControlling accessThe system administrator creates accounts for new users removes the accounts ofinactive users and handles all the accountrelated issues that come up in betweeneg forgotten passwords and lost key pairs The process of actually adding andremoving accounts is typically automated by a configuration management systemor centralized directory serviceAdding hardwareAdministrators who work with physical hardware as opposed to cloud or hostedsystems must install it and configure it to be recognized by the operating systemHardware support chores might range from the simple task of adding a networkinterface card to configuring a specialized external storage arrayAutomating tasksUsing tools to automate repetitive and timeconsuming tasks increases your efficiency reduces the likelihood of errors caused by humans and improves your abilityto respond rapidly to changing requirements Administrators strive to reduce theamount of manual labor needed to keep systems functioning smoothly Familiaritywith scripting languages and automation tools is a large part of the jobOverseeing backupsBacking up data and restoring it successfully when required are important administrative tasks Although backups are time consuming and boring the frequency ofrealworld disasters is simply too high to allow the job to be disregardedSee Chapters and for information about useraccount provisioningSee Chapter Scripting and theShell for information about scriptingand automationSee page forsome tips on performing backupsOperating systems and some individual software packages provide wellestablishedtools and techniques to facilitate backups Backups must be executed on a regularschedule and restores must be tested periodically to ensure that they are functioning correctlyInstalling and upgrading softwareSoftware must be selected installed and configured often on a variety of operating systems As patches and security updates are released they must be testedreviewed and incorporated into the local environment without endangering thestability of production systemsThe term software delivery refers to the process of releasing updated versions ofsoftwareespecially software developed inhouseto downstream users Continuous delivery takes this process to the next level by automatically releasing softwareto users at a regular cadence as it is developed Administrators help implement robust delivery processes that meet the requirements of the enterpriseMonitoringWorking around a problem is usually faster than taking the time to document andreport it and users internal to an organization often follow the path of least resistance External users are more likely to voice their complaints publicly than to opena support inquiry Administrators can help to prevent both of these outcomes bydetecting problems and fixing them before public failures occurSome monitoring tasks include ensuring that web services respond quickly andcorrectly collecting and analyzing log files and keeping tabs on the availability ofserver resources such as disk space All of these are excellent opportunities for automation and a slew of open source and commercial monitoring systems can helpsysadmins with these tasksTroubleshootingNetworked systems fail in unexpected and sometimes spectacular fashion Its theadministrators job to play mechanic by diagnosing problems and calling in subjectmatter experts as needed Finding the source of a problem is often more challenging than resolving itMaintaining local documentationAdministrators choose vendors write scripts deploy software and make many other decisions that may not be immediately obvious or intuitive to others Thoroughand accurate documentation is a blessing for team members who would otherwiseneed to reverseengineer a system to resolve problems in the middle of the nightA lovingly crafted network diagram is more useful than many paragraphs of textwhen describing a designSee Chapter forinformation aboutsoftware managementSee Chapter for informationabout softwaredeployment andcontinuous deliverySee Chapter for informationabout monitoringSee page for anintroduction to network troubleshootingSee page forsuggestions regardingdocumentationVigilantly monitoring securityAdministrators are the first line of defense for protecting networkattached systems The administrator must implement a security policy and set up proceduresto prevent systems from being breached This responsibility might include only afew basic checks for unauthorized access or it might involve an elaborate networkof traps and auditing programs depending on the context System administratorsare cautious by nature and are often the primary champions of security across atechnical organizationTuning performanceUNIX and Linux are general purpose operating systems that are well suited to almost any conceivable computing task Administrators can tailor systems for optimalperformance in accord with the needs of users the available infrastructure and theservices the systems provide When a server is performing poorly it is the administrators job to investigate its operation and identify areas that need improvementDeveloping site policiesFor legal and compliance reasons most sites need policies that govern the acceptable use of computer systems the management and retention of data the privacyand security of networks and systems and other areas of regulatory interest Systemadministrators often help organizations develop sensible policies that meet the letterand intent of the law and yet still promote progress and productivityWorking with vendorsMost sites rely on third parties to provide a variety of ancillary services and products related to their computing infrastructure These providers might includesoftware developers cloud infrastructure providers hosted softwareasaserviceSaaS shops helpdesk support staff consultants contractors security experts andplatform or infrastructure vendors Administrators may be tasked with selectingvendors assisting with contract negotiations and implementing solutions once thepaperwork has been completedFire fightingAlthough helping other people with their various problems is rarely included in asystem administrators job description these tasks claim a measurable portion ofmost administrators workdays System administrators are bombarded with problems ranging from It worked yesterday and now it doesnt What did you changeto I spilled coffee on my keyboard Should I pour water on it to wash it outIn most cases your response to these issues affects your perceived value as an administrator far more than does any actual technical skill you might possess Youcan either howl at the injustice of it all or you can delight in the fact that a singleSee Chapter formore informationabout securitySee Chapter formore informationabout performanceSee the sections starting on page forinformation aboutlocal policymakingwellhandled trouble ticket scores more brownie points than five hours of midnightdebugging Your choice Suggested backgroundWe assume in this book that you have a certain amount of Linux or UNIX experience In particular you should have a general concept of how the system looks andfeels from a users perspective since we do not review that material Several goodbooks can get you up to speed see Recommended reading on page We love welldesigned graphical interfaces Unfortunately GUI tools for systemadministration on UNIX and Linux remain rudimentary in comparison with therichness of the underlying software In the real world administrators must be comfortable using the command lineFor text editing we strongly recommend learning vi now seen more commonlyin its enhanced form vim which is standard on all systems It is simple powerfuland efficient Mastering vim is perhaps the single best productivity enhancementavailable to administrators Use the vimtutor command for an excellent interactive introductionAlternatively GNUs nano is a simple and lowimpact starter editor that has onscreen prompts Use it discreetly professional administrators may be visibly distressed if they witness a peer running nanoAlthough administrators are not usually considered software developers industrytrends are blurring the lines between these functions Capable administrators areusually polyglot programmers who dont mind picking up a new language whenthe need arisesFor new scripting projects we recommend Bash aka bash aka sh Ruby or PythonBash is the default command shell on most UNIX and Linux systems It is primitiveas a programming language but it serves well as the duct tape in an administrativetool box Python is a clever language with a highly readable syntax a large developer community and libraries that facilitate many common tasks Ruby developersdescribe the language as a joy to work with and beautiful to behold Ruby andPython are similar in many ways and weve found them to be equally functional foradministration The choice between them is mostly a matter of personal preferenceWe also suggest that you learn expect which is not a programming language somuch as a front end for driving interactive programs Its an efficient glue technology that can replace some complex scripting and is easy to learnChapter Scripting and the Shell summarizes the most important things toknow about scripting for Bash Python and Ruby It also reviews regular expressions text matching patterns and some shell idioms that are useful for sysadminsSee Chapter for an introduction to scripting Linux distributionsA Linux distribution comprises the Linux kernel which is the core of the operatingsystem and packages that make up all the commands you can run on the systemAll distributions share the same kernel lineage but the format type and numberof packages differ quite a bit Distributions also vary in their focus support andpopularity There continue to be hundreds of independent Linux distributions butour sense is that distributions derived from the Debian and Red Hat lineages willpredominate in production environments in the years aheadBy and large the differences among Linux distributions are not cosmically significant In fact it is something of a mystery why so many different distributionsexist each claiming easy installation and a massive software library as its distinguishing features Its hard to avoid the conclusion that people just like to makenew Linux distributionsMost major distributions include a relatively painless installation procedure a desktop environment and some form of package management You can try them outeasily by starting up a cloud instance or a local virtual machineMuch of the insecurity of generalpurpose operating systems derives from theircomplexity Virtually all leading distributions are cluttered with scores of unusedsoftware packages security vulnerabilities and administrative anguish often comealong for the ride In response a relatively new breed of minimalist distributionshas been gaining traction CoreOS is leading the charge against the status quo andprefers to run all software in containers Alpine Linux is a lightweight distributionthat is used as the basis of many public Docker images Given this reductionist trendwe expect the footprint of Linux to shrink over the coming yearsBy adopting a distribution you are making an investment in a particular vendorsway of doing things Instead of looking only at the features of the installed softwareits wise to consider how your organization and that vendor are going to work witheach other Some important questions to ask are Is this distribution going to be around in five years Is this distribution going to stay on top of the latest security patches Does this distribution have an active community and sufficient documentation If I have problems will the vendor talk to me and how much will that costTable lists some of the most popular mainstream distributionsThe most viable distributions are not necessarily the most corporate For examplewe expect Debian Linux OK OK Debian GNULinux to remain viable for a longtime despite the fact that Debian is not a company doesnt sell anything and offersno enterpriselevel support Debian benefits from a committed group of contributorsand from the enormous popularity of the Ubuntu distribution which is based on itA comprehensive list of distributions including many nonEnglish distributionscan be found at lwnnetDistributions or distrowatchcomSee Chapter Containers for moreinformation aboutDocker and containers Example systems used in this bookWe have chosen three popular Linux distributions and one UNIX variant as ourprimary examples for this book Debian GNULinux Ubuntu Linux Red Hat Enterprise Linux and its dopplegnger CentOS and FreeBSD These systems arerepresentative of the overall marketplace and account collectively for a substantialportion of installations in use at large sites todayInformation in this book generally applies to all of our example systems unless aspecific attribution is given Details particular to one system are marked with a logoDebian GNULinux StretchUbuntu Zesty ZapusRed Hat Enterprise Linux and CentOS FreeBSD Most of these marks belong to the vendors that release the corresponding softwareand are used with the kind permission of their respective owners However thevendors have not reviewed or endorsed the contents of this bookRHELTable Most popular generalpurpose Linux distributionsDistribution Web site CommentsArch archlinuxorg For those who fear not the command lineCentOS centosorg Free analog of Red Hat EnterpriseCoreOS coreoscom Containers containers everywhereDebian debianorg Free as in freedom most GNUish distroFedora fedoraprojectorg Test bed for Red Hat LinuxKali kaliorg For penetration testersLinux Mint linuxmintcom Ubuntubased desktopfriendlyopenSUSE opensuseorg Free analog of SUSE Linux EnterpriseopenWRT openwrtorg Linux for routers and embedded devicesOracle Linux oraclecom Oraclesupported version of RHELRancherOS ranchercom MiB everything in containersRed Hat Enterprise redhatcom Reliable slowchanging commercialSlackware slackwarecom Grizzled longsurviving distroSUSE Linux Enterprise susecom Strong in Europe multilingualUbuntu ubuntucom Cleanedup version of DebianWe repeatedly attempted and failed to obtain permission from Red Hat to use theirfamous red fedora logo so youre stuck with yet another technical acronym At leastthis one is in the marginsThe paragraphs below provide a bit more detail about each of the example systemsExample Linux distributionsInformation thats specific to Linux but not to any particular distribution is markedwith the Tux penguin logo shown at leftDebian pronounced debian named after the late founder Ian Murdock and hiswife Debra is one of the oldest and most wellregarded distributions It is a noncommercial project with more than a thousand contributors worldwide Debianmaintains an ideological commitment to community development and open access so theres never any question about which parts of the distribution are free orredistributableDebian defines three releases that are maintained simultaneously stable targetingproduction servers unstable with current packages that may have bugs and security vulnerabilities and testing which is somewhere in betweenUbuntu is based on Debian and maintains Debians commitment to free and opensource software The business behind Ubuntu is Canonical Ltd founded by entrepreneur Mark ShuttleworthCanonical offers a variety of editions of Ubuntu targeting the cloud the desktopand bare metal There are even releases intended for phones and tablets Ubuntuversion numbers derive from the year and month of release so version isfrom October Each release also has an alliterative code name such as VividVervet or Wily WerewolfTwo versions of Ubuntu are released annually one in April and one in October TheApril releases in evennumbered years are longterm support LTS editions thatpromise five years of maintenance updates These are the releases recommendedfor production useRed Hat has been a dominant force in the Linux world for more than two decadesand its distributions are widely used in North America and beyond By the numbersRed Hat Inc is the most successful open source software company in the worldRed Hat Enterprise Linux often shortened to RHEL targets production environments at large enterprises that require support and consulting services to keeptheir systems running smoothly Somewhat paradoxically RHEL is open sourcebut requires a license If youre not willing to pay for the license youre not goingto be running Red HatRed Hat also sponsors Fedora a communitybased distribution that serves as anincubator for bleedingedge software not considered stable enough for RHELRHELFedora is used as the initial test bed for software and configurations that later findtheir way to RHELCentOS is virtually identical to Red Hat Enterprise Linux but free of charge TheCentOS Project centosorg is owned by Red Hat and employs its lead developersHowever they operate separately from the Red Hat Enterprise Linux team TheCentOS distribution lacks Red Hats branding and a few proprietary tools but isin other respects equivalentCentOS is an excellent choice for sites that want to deploy a productionorienteddistribution without paying tithes to Red Hat A hybrid approach is also feasiblefrontline servers can run Red Hat Enterprise Linux and avail themselves of RedHats excellent support even as nonproduction systems run CentOS This arrangement covers the important bases in terms of risk and support while also minimizingcost and administrative complexityCentOS aspires to full binary and bugforbug compatibility with Red Hat Enterprise Linux Rather than repeating Red Hat and CentOS ad nauseam we generallymention only one or the other in this book The text applies equally to Red Hat andCentOS unless we note otherwiseOther popular distributions are also Red Hat descendants Oracle sells a rebrandedand customized version of CentOS to customers of its enterprise database softwareAmazon Linux available to Amazon Web Services users was initially derived fromCentOS and still shares many of its conventionsMost administrators will encounter a Red Hatlike system at some point in theircareers and familiarity with its nuances is helpful even if it isnt the system ofchoice at your siteExample UNIX distributionThe popularity of UNIX has been waning for some time and most of the stalwartUNIX distributions eg Solaris HPUX and AIX are no longer in common useThe open source descendants of BSD are exceptions to this trend and continue toenjoy a cult following particularly among operating system experts free softwareevangelists and securityminded administrators In other words some of the worldsforemost operating system authorities rely on the various BSD distributions ApplesmacOS has a BSD heritageFreeBSD first released in late is the most widely used of the BSD derivatives Itcommands a market share among BSD variants according to some usage statistics Users include major Internet companies such as WhatsApp Google and NetflixUnlike Linux FreeBSD is a complete operating system not just a kernel Both thekernel and userland software are licensed under the permissive BSD License afact that encourages development by and additions from the business community Notation and typographical conventionsIn this book filenames commands and literal arguments to commands are shownin boldface Placeholders eg command arguments that should not be taken literally are in italics For example in the commandcp file directoryyoure supposed to replace file and directory with the names of an actual file andan actual directoryExcerpts from configuration files and terminal sessions are shown in a code fontSometimes we annotate sessions with the bash comment character and italictext For example grep Bob pubphonelist Look up Bobs phone numberBob Knowles Bob Smith We use to denote the shell prompt for a normal unprivileged user and for theroot user When a command is specific to a distribution or family of distributionswe prefix the prompt with the distribution name For example sudo su root Become root passwd Change roots passworddebian dpkg l List installed packages on Debian and UbuntuThis convention is aligned with the one used by standard UNIX and Linux shellsOutside of these specific cases we have tried to keep special fonts and formattingconventions to a minimum as long as we could do so without compromising intelligibility For example we often talk about entities such as the daemon group withno special formatting at allWe use the same conventions as the manual pages for command syntax Anything between square brackets and is optional Anything followed by an ellipsis can be repeated Curly braces and mean that you should select one of the itemsseparated by vertical bars For example the specificationbork x on off filename would match any of the following commandsbork on etcpasswdbork x off etcpasswd etcsmartdconfbork off usrlibtmacWe use shellstyle globbing characters for pattern matching A star matches zero or more characters A question mark matches one character A tilde or twiddle means the home directory of the current useruser means the home directory of userFor example we might refer to the startup script directories etcrcd etcrcdand so on with the shorthand pattern etcrcdText within quotation marks often has a precise technical meaning In these caseswe ignore the normal rules of US English and put punctuation outside the quotesso that there can be no confusion about whats included and whats not UnitsMetric prefixes such as kilo mega and giga are defined as powers of onemegabuck is However computer types have long poached these prefixesand used them to refer to powers of For example one megabyte of memory isreally or bytes The stolen units have even made their way into formalstandards such as the JEDEC Solid State Technology Associations Standard Bwhich recognizes the prefixes as denoting powers of albeit with some misgivingsIn an attempt to restore clarity the International Electrotechnical Commission hasdefined a set of numeric prefixes kibi mebi gibi and so on abbreviated Ki Miand Gi based explicitly on powers of Those units are always unambiguous butthey are just starting to be widely used The original kiloseries prefixes are stillused in both sensesContext helps with decoding RAM is always denominated in powers of but network bandwidth is always a power of Storage space is usually quoted in powerof units but block and page sizes are in fact powers of In this book we use IEC units for powers of metric units for powers of andmetric units for rough values and cases in which the exact basis is unclear undocumented or impossible to determine In command output and in excerpts fromconfiguration files or where the delineation is not important we leave the originalvalues and unit designators We abbreviate bit as b and byte as B Table on thenext page shows some examplesThe abbreviation K as in KB of RAM is not part of any standard Its a computerese adaptation of the metric abbreviation k for kilo and originally meant as opposed to But since the abbreviations for the larger metric prefixes arealready upper case the analogy doesnt scale Later people became confused aboutthe distinction and started using K for factors of tooMost of the world doesnt consider this to be an important matter and like the useof imperial units in the United States metric prefixes are likely to be misused forTable Unit decoding examplesExample MeaningkB file A file that contains bytesKiB SSD pages SSD pages that contain bytesKB of memory Not used in this book see note on page MB file size limit Nominally bytes in context ambiguousMB disk partition Nominally bytes in context probably bytes aGiB of RAM bytes of memory Gbs Ethernet A network that transmits bits per secondTB hard disk A hard disk that stores about bytesa That is rounded down to the nearest whole multiple of the disks byte block sizethe foreseeable future Ubuntu maintains a helpful units policy though we suspectit has not been widely adopted even at Canonical see wikiubuntucomUnitsPolicyfor some additional details Man pages and other online documentationThe manual pages usually called man pages because they are read with the mancommand constitute the traditional online documentation Of course these daysall documentation is online in some form or another Programspecific man pagescome along for the ride when you install new software packages Even in the ageof Google we continue to consult man pages as an authoritative resource becausethey are accessible from the command line typically include complete details on aprograms options and show helpful examples and related commandsMan pages are concise descriptions of individual commands drivers file formatsor library routines They do not address more general topics such as How do I install a new device or Why is this system so damn slowOrganization of the man pagesFreeBSD and Linux divide the man pages into sections Table shows the basicschema Other UNIX variants sometimes define the sections slightly differentlyThe exact structure of the sections isnt important for most topics because manfinds the appropriate page wherever it is stored Just be aware of the section definitions when a topic with the same name appears in multiple sections For example passwd is both a command and a configuration file so it has entries in bothsection and section man read man pagesman title formats a specific manual page and sends it to your terminal throughmore less or whatever program is specified in your PAGER environment variabletitle is usually a command device filename or name of a library routine The sections of the manual are searched in roughly numeric order although sections thatdescribe commands sections and are usually searched firstThe form man section title gets you a man page from a particular section Thus onmost systems man sync gets you the man page for the sync command and man sync gets you the man page for the sync system callman k keyword or apropos keyword prints a list of man pages that have keywordin their oneline synopses For example man k translateobjcopy copy and translate object filesdcgettext translate messagetr translate or delete characterssnmptranslate translate SNMP OID values into useful informationtr p translate charactersThe keywords database can become outdated If you add additional man pagesto your system you may need to rebuild this file with makewhatis Red Hat andFreeBSD or mandb UbuntuStorage of man pagesnroff input for man pages ie the man page source code is stored in directoriesunder usrshareman and compressed with gzip to save space The man commandknows how to decompress them on the flySee page tolearn about environment variablesTable Sections of the man pagesSection Contents Userlevel commands and applications System calls and kernel error codes Library calls Device drivers and network protocols Standard file formats Games and demonstrations Miscellaneous files and documents System administration commands Obscure kernel specs and interfacesman maintains a cache of formatted pages in varcacheman or usrsharemanif the appropriate directories are writable however this is a security risk Most systems preformat the man pages once at installation time see catman or not at allThe man command can search several man page repositories to find the manualpages you request On Linux systems you can find out the current default searchpath with the manpath command This path from Ubuntu is typicalubuntu manpathusrlocalmanusrlocalsharemanusrsharemanIf necessary you can set your MANPATH environment variable to override thedefault path export MANPATHhomesharelocalmanusrsharemanSome systems let you set a custom systemwide default search path for man pageswhich can be useful if you need to maintain a parallel tree of man pages such asthose generated by OpenPKG To distribute local documentation in the form of manpages however it is simpler to use your systems standard packaging mechanismand to put man pages in the standard man directories See Chapter SoftwareInstallation and Management for more details Other authoritative documentationMan pages are just a small part of the official documentation Most of the rest unfortunately is scattered about on the webSystemspecific guidesMajor vendors have their own dedicated documentation projects Many continueto produce useful booklength manuals including administration and installationguides These are generally available online and as downloadable PDF files Table shows where to lookAlthough this documentation is helpful its not the sort of thing you keep next toyour bed for light evening reading though some vendors versions would makeuseful sleep aids We generally Google for answers before turning to vendor docsPackagespecific documentationMost of the important software packages in the UNIX and Linux world are maintained by individuals or by third parties such as the Internet Systems Consortiumand the Apache Software Foundation These groups write their own documentationThe quality runs the gamut from embarrassing to spectacular but jewels such asPro Git from gitscmcombook make the hunt worthwhileSupplemental documents include white papers technical reports design rationalesand book or pamphletlength treatments of particular topics These supplementalmaterials are not limited to describing just one command so they can adopt a tutorial or procedural approach Many pieces of software have both a man page anda longform article For example the man page for vim tells you about the commandline arguments that vim understands but you have to turn to an indepthtreatment to learn how to actually edit a fileMost software projects have user and developer mailing lists and IRC channels Thisis the first place to visit if you have questions about a specific configuration issueor if you encounter a bugBooksThe OReilly books are favorites in the technology industry The business beganwith UNIX in a Nutshell and now includes a separate volume on just about everyimportant UNIX and Linux subsystem and command OReilly also publishes bookson network protocols programming languages Microsoft Windows and othernonUNIX tech topics All the books are reasonably priced timely and focusedMany readers turn to OReillys Safari Books Online a subscription service thatoffers unlimited electronic access to books videos and other learning resourcesContent from many publishers is includednot just OReillyand you can choosefrom an immense library of materialRFC publicationsRequest for Comments documents describe the protocols and procedures used onthe Internet Most of these are relatively detailed and technical but some are writtenas overviews The phrase reference implementation applied to software usuallytranslates to implemented by a trusted source according to the RFC specificationRFCs are absolutely authoritative and many are quite useful for system administrators See page for a more complete description of these documents We referto various RFCs throughout this bookTable Where to find OS vendors proprietary documentationOS URL CommentsDebian debianorgdoc Admin handbook lags behind the current versionUbuntu helpubuntucom User oriented see server guide for LTS releasesRHEL redhatcomdocs Comprehensive docs for administratorsCentOS wikicentosorg Includes tips HowTos and FAQsFreeBSD freebsdorgdocshtml See the FreeBSD Handbook for sysadmin info Other sources of informationThe sources discussed in the previous section are peer reviewed and written by authoritative sources but theyre hardly the last word in UNIX and Linux administration Countless blogs discussion forums and news feeds are available on the InternetIt should go without saying but Google is a system administrators best friend Unless youre looking up the details of a specific command or file format Google or anequivalent search engine should be the first resource you consult for any sysadminquestion Make it a habit if nothing else youll avoid the delay and humiliation ofhaving your questions in an online forum answered with a link to Google Whenstuck search the webKeeping currentOperating systems and the tools and techniques that support them change rapidlyRead the sites in Table with your morning coffee to keep abreast of industry trendsTable Resources for keeping up to dateWeb site Descriptiondarkreadingcom Security news trends and discussiondevopsreactionstumblrcom Sysadmin humor in animated GIF formlinuxcom A Linux Foundation site forum good for new userslinuxfoundationorg Nonprofit fostering OSS employer of Linus Torvaldslwnnet Highquality timely articles on Linux and OSSlxercom Linux news aggregatorsecurityfocuscom Vulnerability reports and securityrelated mailing listsSwiftOnSecurity Infosec opinion from Taylor Swift parody accountnixcraft Tweets about UNIX and Linux administrationeverythingsysadmincom Blog of Thomas Limoncelli respected sysadminasysadventblogspotcom Advent for sysadmins with articles each Decemberoreillycomtopics Learning resources from OReilly on many topicsschneiercom Blog of Bruce Schneier privacy and security experta See also Toms collection of April Fools Day RFCs at rfchumorcomSocial media are also useful Twitter and reddit in particular have strong engagedcommunities with a lot to offer though the signaltonoise ratio can sometimes bequite bad On reddit join the sysadmin linux linuxadmin and netsec subreddits Or worse yet a link to Google through lmgtfycomHowTos and reference sitesThe sites listed in Table contain guides tutorials and articles about how to accomplish specific tasks on UNIX and LinuxTable Taskspecific forums and reference sitesWeb site Descriptionwikiarchlinuxorg Articles and guides for Arch Linux many are more generalaskubuntucom QA for Ubuntu users and developersdigitaloceancom Tutorials on many OSS development and sysadmin topics akernelorg Official Linux kernel siteserverfaultcom Collaboratively edited database of sysadmin questions bserversforhackerscom Highquality videos forums and articles on administrationa See digitaloceancomcommunitytutorialsb Also see the sister site stackoverflowcom which is dedicated to programming but useful for sysadminsStack Overflow and Server Fault both listed in Table and both members ofthe Stack Exchange group of sites warrant a closer look If youre having a problem chances are that somebody else has already seen it and asked for help on oneof these sites The reputationbased QA format used by the Stack Exchange siteshas proved well suited to the kinds of problems that sysadmins and programmersencounter Its worth creating an account and joining this large communityConferencesIndustry conferences are a great way to network with other professionals keep tabson technology trends take training classes gain certifications and learn about thelatest services and products The number of conferences pertinent to administration has exploded in recent years Table on the next page highlights some ofthe most prominent onesMeetups meetupcom are another way to network and engage with likemindedpeople Most urban areas in the United States and around the world have a Linuxuser group or DevOps meetup that sponsors speakers discussions and hack days Ways to find and install softwareChapter Software Installation and Management addresses software provisioning in detail But for the impatient heres a quick primer on how to find out whatsinstalled on your system and how to obtain and install new softwareModern operating systems divide their contents into packages that can be installedindependently of one another The default installation includes a range of starterpackages that you can expand and contract according to your needs When addingTable Conferences relevant to system administratorsConference Location When DescriptionLISA Varies Q Large Installation System AdministrationMonitorama Portland June Monitoring tools and techniquesOSCON Varies USEU Q or Q Longrunning OReilly OSS conferenceSCALE Pasadena Jan Southern California Linux ExpoDefCon Las Vegas July Oldest and largest hacker conventionVelocity Global Varies OReilly conference on web operationsBSDCan Ottawa MayJune Everything BSD from novices to gurusreInvent Las Vegas Q AWS cloud computing conferenceVMWorld Varies USEU Q or Q Virtualization and cloud computingLinuxCon Global Varies The future of LinuxRSA San Francisco Q or Q Enterprise cryptography and infosecDevOpsDays Global Varies A range of topics on bridging the gapbetween development and ops teamsQCon Global Varies A conference for software developerssoftware don your security hat and remember that additional software creates additional attack surface Only install whats necessaryAddon software is often provided in the form of precompiled packages as wellalthough the degree to which this is a mainstream approach varies widely amongsystems Most software is developed by independent groups that release the software in the form of source code Package repositories then pick up the source codecompile it appropriately for the conventions in use on the systems they serve andpackage the resulting binaries Its usually easier to install a systemspecific binarypackage than to fetch and compile the original source code However packagersare sometimes a release or two behind the current versionThe fact that two systems use the same package format doesnt necessarily meanthat packages for the two systems are interchangeable Red Hat and SUSE both useRPM for example but their filesystem layouts are somewhat different Its best touse packages designed for your particular system if they are availableOur example systems provide excellent package management systems that includetools for accessing and searching hosted software repositories Distributors aggressively maintain these repositories on behalf of the community to facilitate patchingand software updates Life is goodWhen the packaged format is insufficient administrators must install software theoldfashioned way by downloading a tar archive of the source code and manuallyconfiguring compiling and installing it Depending on the software and the operating system this process can range from trivial to nightmarishIn this book we generally assume that optional software is already installed ratherthan torturing you with boilerplate instructions for installing every package If theresa potential for confusion we sometimes mention the exact names of the packagesneeded to complete a particular project For the most part however we dont repeatinstallation instructions since they tend to be similar from one package to the nextDetermining if software is already installedFor a variety of reasons it can be a bit tricky to determine which package containsthe software you actually need Rather than starting at the package level its easierto use the shells which command to find out if a relevant binary is already in yoursearch path For example the following command reveals that the GNU C compilerhas already been installed on this machineubuntu which gccusrbingccIf which cant find the command youre looking for try whereis it searches a broaderrange of system directories and is independent of your shells search pathAnother alternative is the incredibly useful locate command which consults a precompiled index of the filesystem to locate filenames that match a particular patternFreeBSD includes locate as part of the base system In Linux the current implementation of locate is in the mlocate package On Red Hat and CentOS install themlocate package with yum see page locate can find any type of file it is not specific to commands or packages Forexample if you werent sure where to find the signalh include file you could tryfreebsd locate signalhusrincludemachinesignalhusrincludesignalhusrincludesyssignalhlocates database is updated periodically by the updatedb command in FreeBSDlocateupdatedb which runs periodically out of cron Therefore the results of alocate dont always reflect recent changes to the filesystemIf you know the name of the package youre looking for you can also use your systems packaging utilities to check directly for the packages presence For exampleon a Red Hat system the following command checks for the presence and installedversion of the Python interpreterredhat rpm q pythonpythonelxSee Chapter for more information about packagemanagementYou can also find out which package a particular file belongs toredhat rpm qf etchttpdhttpdelcentosxfreebsd pkg which usrlocalsbinhttpdusrlocalsbinhttpd was installed by package apacheubuntu dpkgquery S etcapacheapache etcapacheAdding new softwareIf you do need to install additional software you first need to determine the canonical name of the relevant software package For example youd need to translate Iwant to install locate to I need to install the mlocate package or translate I neednamed to I have to install BIND A variety of systemspecific indexes on the webcan help with this but Google is usually just as effective For example a search forlocate command takes you directly to several relevant discussionsThe following examples show the installation of the tcpdump command on each ofour example systems tcpdump is a packet capture tool that lets you view the rawpackets being sent to and from the system on the networkDebian and Ubuntu use APT the Debian Advanced Package Toolubuntu sudo aptget install tcpdumpReading package lists DoneBuilding dependency treeReading state information DoneThe following NEW packages will be installed tcpdump upgraded newly installed to remove and not upgradedNeed to get B kB of archivesAfter this operation kB of additional disk space will be usedSelecting previously unselected package tcpdumpReading database files and directories currently installedPreparing to unpack tcpdumpubuntuamddeb Unpacking tcpdump ubuntu Processing triggers for mandb Setting up tcpdump ubuntu The Red Hat and CentOS version isredhat sudo yum install tcpdumpLoaded plugins fastestmirrorDetermining fastest mirrors base mirrorsxmissioncom epel linuxmirrorsesnet extras centosarvixecom updates reposlaxquadranetcomRHELResolving Dependencies Running transaction check Package tcpdumpx el will be installed Finished Dependency Resolutiontcpdumpelxrpm kB Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing tcpdumpelx Verifying tcpdumpelx Installed tcpdumpx elCompleteThe package manager for FreeBSD is pkgfreebsd sudo pkg install y tcpdumpUpdating FreeBSD repository catalogueFetching metatxz B kBs Fetching packagesitetxz MiB MBs Processing entries FreeBSD repository update completed packages processedAll repositories are uptodateThe following packages will be affected of checkedNew packages to be INSTALLEDtcpdump libsmi The process will require MiB more space MiB to be downloadedFetching tcpdumptxz KiB kBs Fetching libsmitxz MiB MBs Checking integrity done conflicting Installing libsmi Extracting libsmi Installing tcpdump Extracting tcpdump Building software from source codeAs an illustration heres how you build a version of tcpdump from the source codeThe first chore is to identify the code Software maintainers sometimes keep an index of releases on the projects web site that are downloadable as tarballs For opensource projects youre most likely to find the code in a Git repositoryThe tcpdump source is kept on GitHub Clone the repository in the tmp directory create a branch of the tagged version you want to build then unpack configurebuild and install itredhat cd tmpredhat git clone httpsgithubcomthetcpdumpgrouptcpdumpgitstatus messages as repository is clonedredhat cd tcpdumpredhat git checkout tagstcpdump b tcpdumpSwitched to a new branch tcpdumpredhat configurechecking build system type xunknownlinuxgnuchecking host system type xunknownlinuxgnuchecking for gcc gccchecking whether the C compiler works yesredhat makeseveral pages of compilation outputredhat sudo make installfiles are moved in to placeThis configuremakemake install sequence is common to most software writtenin C and works on all UNIX and Linux systems Its always a good idea to checkthe packages INSTALL or README file for specifics You must have the development environment and any packagespecific prerequisites installed In the case oftcpdump libpcap and its libraries are prerequisitesYoull often need to tweak the build configuration so use configure help to seethe options available for each particular package Another useful configure optionis prefixdirectory which lets you compile the software for installation somewhereother than usrlocal which is usually the defaultInstalling from a web scriptCrossplatform software bundles increasingly offer an expedited installation process thats driven by a shell script you download from the web with curl fetch orwget For example to set up a machine as a Salt client you can run the followingcommands curl o tmpsaltboot sL httpsbootstrapsaltstackcom sudo sh tmpsaltbootThe bootstrap script investigates the local environment then downloads installsand configures an appropriate version of the software This type of installation isparticularly common in cases where the process itself is somewhat complex butthe vendor is highly motivated to make things easy for users Another good example is RVM see page These are all simple HTTP clients that download the contents of a URL to a local file or optionallyprint the contents to their standard outputThis installation method is perfectly fine but it raises a couple of issues that areworth mentioning To begin with it leaves no proper record of the installation forfuture reference If your operating system offers a packagized version of the software its usually preferable to install the package instead of running a web installerPackages are easy to track upgrade and remove On the other hand most OSlevelpackages are out of date You probably wont end up with the most current versionof the softwareBe very suspicious if the URL of the boot script is not secure that is it does not startwith https Unsecured HTTP is trivial to hijack and installation URLs are of particular interest to hackers because they know youre likely to run as root whatevercode comes back By contrast HTTPS validates the identity of the server througha cryptographic chain of trust Not foolproof but reliable enoughA few vendors publicize an HTTP installation URL that automatically redirects toan HTTPS version This is dumb and is in fact no more secure than straightupHTTP Theres nothing to prevent the initial HTTP exchange from being intercepted so you might never reach the vendors redirect However the existence of suchredirects does mean its worth trying your own substitution of https for http in insecure URLs More often than not it works just fineThe shell accepts script text on its standard input and this feature enables tidy oneline installation procedures such as the following curl L httpsbadvendorcom sudo shHowever theres a potential issue with this construction in that the root shell stillruns even if curl outputs a partial script and then failssay because of a transientnetwork glitch The end result is unpredictable and potentially not goodWe are not aware of any documented cases of problems attributable to this causeNevertheless it is a plausible failure mode More to the point piping the output ofcurl to a shell has entered the collective sysadmin unconscious as a prototypicalrookie blunder so if you must do it at least keep it on the slyThe fix is easy just save the script to a temporary file then run the script in a separate step after the download successfully completes Where to hostOperating systems and software can be hosted in private data centers at colocationfacilities on a cloud platform or on some combination of these Most burgeoningstartups choose the cloud Established enterprises are likely to have existing datacenters and may run a private cloud internallySee Chapter for more information about packageinstallationSee page fordetails on HTTPSschain of trustThe most practical choice and our recommendation for new projects is a publiccloud provider These facilities offer numerous advantages over data centers No capital expenses and low initial operating costs No need to install secure and manage hardware Ondemand adjustment of storage bandwidth and compute capacity Readymade solutions for common ancillary needs such as databases loadbalancers queues monitoring and more Cheaper and simpler implementation of highly availableredundant systemsEarly cloud systems acquired a reputation for inferior security and performance butthese are no longer major concerns These days most of our administration workis in the cloud See Chapter for a general introduction to this spaceOur preferred cloud platform is the leader in the space Amazon Web Services AWSGartner a leading technology research firm found that AWS is ten times the sizeof all competitors combined AWS innovates rapidly and offers a much broaderarray of services than does any other provider It also has a reputation for excellentcustomer service and supports a large and engaged community AWS offers a freeservice tier to cut your teeth on including a years use of a low powered cloud serverGoogle Cloud Platform GCP is aggressively improving and marketing its products Some claim that its technology is unmatched by other providers GCPs growthhas been slow in part due to Googles reputation for dropping support for popularofferings However its customerfriendly pricing terms and unique features areappealing differentiatorsDigitalOcean is a simpler service with a stated goal of high performance Its targetmarket is developers whom it woos with a clean API low pricing and extremely fastboot times DigitalOcean is a strong proponent of open source software and theirtutorials and guides for popular Internet technologies are some of the best available Specialization and adjacent disciplinesSystem administrators do not exist in a vacuum a team of experts is required tobuild and maintain a complex network This section describes some of the roleswith which system administrators overlap in skills and scope Some administratorschoose to specialize in one or more of these areasYour goal as a system administrator or as a professional working in any of theserelated areas is to achieve the objectives of the organization Avoid letting politicsor hierarchy interfere with progress The best administrators solve problems andshare information freely with othersDevOpsDevOps is not so much a specific function as a culture or operational philosophyIt aims to improve the efficiency of building and delivering software especially atSee page for morecomments on DevOpslarge sites that have many interrelated services and teams Organizations with aDevOps practice promote integration among engineering teams and may drawlittle or no distinction between development and operations Experts who work inthis area seek out inefficient processes and replace them with small shell scripts orlarge and unwieldy Chef repositoriesSite reliability engineersSite reliability engineers value uptime and correctness above all else Monitoringnetworks deploying production software taking pager duty planning future expansion and debugging outages all lie within the realm of these availability crusadersSingle points of failure are site reliability engineers nemesesSecurity operations engineersSecurity operations engineers focus on the practical daytoday side of an information security program These folks install and operate tools that search for vulnerabilities and monitor for attacks on the network They also participate in attacksimulations to gauge the effectiveness of their prevention and detection techniquesNetwork administratorsNetwork administrators design install configure and operate networks Sites thatoperate data centers are most likely to employ network administrators thats because these facilities have a variety of physical switches routers firewalls and otherdevices that need management Cloud platforms also offer a variety of networkingoptions but these usually dont require a dedicated administrator because most ofthe work is handled by the providerDatabase administratorsDatabase administrators sometimes known as DBAs are experts at installingand managing database software They manage database schemas perform installations and upgrades configure clustering tune settings for optimal performanceand help users formulate efficient queries DBAs are usually wizards with one ormore query languages and have experience with both relational and nonrelationalNoSQL databasesNetwork operations center NOC engineersNOC engineers monitor the realtime health of large sites and track incidents andoutages They troubleshoot tickets from users perform routine upgrades and coordinate actions among other teams They can most often be found watching a wallof monitors that show graphs and measurementsData center techniciansData center technicians work with hardware They receive new equipment trackequipment inventory and life cycles install servers in racks run cabling maintainpower and air conditioning and handle the daily operations of a data center As asystem administrator its in your best interest to befriend data center techniciansand bribe them with coffee caffeinated soft drinks and alcoholic beveragesArchitectsSystems architects have deep expertise in more than one area They use their experience to design distributed systems Their job descriptions may include definingsecurity zones and segmentation eliminating single points of failure planning forfuture growth ensuring connectivity among multiple networks and third partiesand other sitewide decision making Good architects are technically proficient andgenerally prefer to implement and test their own designs Recommended readingAbbott Martin L and Michael T Fisher The Art of Scalability Scalable WebArchitecture Processes and Organizations for the Modern Enterprise nd EditionAddisonWesley Professional Gancarz Mike Linux and the Unix Philosophy Boston Digital Press Limoncelli Thomas A and Peter Salus The Complete April Fools Day RFCsPeertoPeer Communications LLC Engineering humor You can read thiscollection online for free at rfchumorcomRaymond Eric S The Cathedral The Bazaar Musings on Linux and Open Sourceby an Accidental Revolutionary Sebastopol CA OReilly Media Salus Peter H The Daemon the GNU the Penguin How Free and Open Software is Changing the World Reed Media Services This fascinating historyof the open source movement by UNIXs bestknown historian is also available atgroklawcom under the Creative Commons license The URL for the book itself isquite long look for a current link at groklawcom or try this compressed equivalent tinyurlcomdujSiever Ellen Stephen Figgins Robert Love and Arnold Robbins Linux ina Nutshell th Edition Sebastopol CA OReilly Media System administration and DevOpsKim Gene Kevin Behr and George Spafford The Phoenix Project A Novelabout IT DevOps and Helping Your Business Win Portland OR IT RevolutionPress A guide to the philosophy and mindset needed to run a modern ITorganization written as a narrative An instant classicKim Gene Jez Humble Patrick Debois and John Willis The DevOps Handbook How to Create WorldClass Agility Reliability and Security in Technology Organizations Portland OR IT Revolution Press Limoncelli Thomas A Christina J Hogan and Strata R Chalup ThePractice of System and Network Administration nd Edition Reading MA AddisonWesley This is a good book with particularly strong coverage of thepolicy and procedural aspects of system administration The authors maintain asystem administration blog at everythingsysadmincomLimoncelli Thomas A Christina J Hogan and Strata R Chalup The Practice of Cloud System Administration Reading MA AddisonWesley Fromthe same authors as the previous title now with a focus on distributed systems andcloud computingEssential toolsBlum Richard and Christine Bresnahan Linux Command Line and ShellScripting Bible rd Edition Wiley Dougherty Dale and Arnold Robins Sed Awk nd Edition SebastopolCA OReilly Media Classic OReilly book on the powerful indispensabletext processors sed and awkKim Peter The Hacker Playbook Practical Guide To Penetration Testing CreateSpace Independent Publishing Platform Neil Drew Practical Vim Edit Text at the Speed of Thought Pragmatic BookshelfShotts William E The Linux Command Line A Complete Introduction San Francisco CA No Starch Press Sweigart Al Automate the Boring Stuff with Python Practical Programming forTotal Beginners San Francisco CA No Starch Press Booting is the standard term for starting up a computer Its a shortened form ofthe word bootstrapping which derives from the notion that the computer has topull itself up by its own bootstrapsThe boot process consists of a few broadly defined tasks Finding loading and running bootstrapping code Finding loading and running the OS kernel Running startup scripts and system daemons Maintaining process hygiene and managing system state transitionsThe activities included in that last bullet point continue as long as the system remainsup so the line between bootstrapping and normal operation is inherently a bit blurry Boot process overviewStartup procedures have changed a lot in recent years The advent of modern UEFIBIOSs has simplified the early stages of booting at least from a conceptual standpoint In later stages most Linux distributions now use a system manager daemoncalled systemd instead of the traditional UNIX init systemd streamlines the boot Booting and SystemManagement Daemonsprocess by adding dependency management support for concurrent startup processes and a comprehensive approach to logging among other featuresBoot management has also changed as systems have migrated into the cloud Thedrift toward virtualization cloud instances and containerization has reduced theneed for administrators to touch physical hardware Instead we now have imagemanagement APIs and control panelsDuring bootstrapping the kernel is loaded into memory and begins to execute Avariety of initialization tasks are performed and the system is then made availableto users The general overview of this process is shown in Exhibit AExhibit A Linux UNIX boot processLoad BIOSUEFIfrom NVRAMSelect boot devicedisk network Identify EFIsystem partitionLoad boot loadereg GRUBDetermine whichkernel to bootLoad kernel Instantiate kerneldata structuresStart initsystemdas PID Executestartup scriptsRunning systemProbe forhardwarePowerOnAdministrators have little direct interactive control over most of the steps requiredto boot a system Instead admins can modify bootstrap configurations by editingconfig files for the system startup scripts or by changing the arguments the bootloader passes to the kernelBefore the system is fully booted filesystems must be checked and mounted andsystem daemons started These procedures are managed by a series of shell scriptssometimes called init scripts or unit files that are run in sequence by init orparsed by systemd The exact layout of the startup scripts and the manner in whichthey are executed varies among systems We cover the details later in this chapter System firmwareWhen a machine is powered on the CPU is hardwired to execute boot code storedin ROM On virtualized systems this ROM may be imaginary but the conceptremains the sameThe system firmware typically knows about all the devices that live on the motherboard such as SATA controllers network interfaces USB controllers and sensorsfor power and temperature In addition to allowing hardwarelevel configurationof these devices the firmware lets you either expose them to the operating systemor disable and hide themOn physical as opposed to virtualized hardware most firmware offers a user interface However its generally crude and a bit tricky to access You need control ofthe computer and console and must press a particular key immediately after powering on the system Unfortunately the identity of the magic key varies by manufacturer see if you can glimpse a cryptic line of instructions at the instant the systemfirst powers on Barring that try Delete Control F F F or F For the bestchance of success tap the key several times then hold it downDuring normal bootstrapping the system firmware probes for hardware and disksruns a simple set of health checks and then looks for the next stage of bootstrapping code The firmware UI lets you designate a boot device usually by prioritizinga list of available options eg try to boot from the DVD drive then a USB drivethen a hard diskIn most cases the systems disk drives populate a secondary priority list To bootfrom a particular drive you must both set it as the highestpriority disk and makesure that hard disk is enabled as a boot mediumBIOS vs UEFITraditional PC firmware was called the BIOS for Basic InputOutput System Overthe last decade however BIOS has been supplanted by a more formalized andmodern standard the Unified Extensible Firmware Interface UEFI Youll oftensee UEFI referred to as UEFI BIOS but for clarity well reserve the term BIOSfor the legacy standard in this chapter Most systems that implement UEFI canfall back to a legacy BIOS implementation if the operating system theyre bootingdoesnt support UEFIUEFI is the current revision of an earlier standard EFI References to the nameEFI persist in some older documentation and even in some standard terms suchas EFI system partition In all but the most technically explicit situations you cantreat these terms as equivalent Virtual systems pretend to have this same set of devices You might find it helpful to disable the monitors power management features temporarilyUEFI support is pretty much universal on new PC hardware these days but plenty of BIOS systems remain in the field Moreover virtualized environments oftenadopt BIOS as their underlying boot mechanism so the BIOS world isnt in dangerof extinction just yetAs much as wed prefer to ignore BIOS and just talk about UEFI its likely that youllencounter both types of systems for years to come UEFI also buildsin several accommodations to the old BIOS regime so a working knowledge of BIOS can bequite helpful for deciphering the UEFI documentationLegacy BIOSTraditional BIOS assumes that the boot device starts with a record called the MBRMaster Boot Record The MBR includes both a firststage boot loader aka bootblock and a primitive disk partitioning table The amount of space available forthe boot loader is so small less than bytes that its not able to do much otherthan load and run a secondstage boot loaderNeither the boot block nor the BIOS is sophisticated enough to read any type ofstandard filesystem so the secondstage boot loader must be kept somewhere easyto find In one typical scenario the boot block reads the partitioning informationfrom the MBR and identifies the disk partition marked as active It then reads andexecutes the secondstage boot loader from the beginning of that partition Thisscheme is known as a volume boot recordAlternatively the secondstage boot loader can live in the dead zone that lies between the MBR and the beginning of the first disk partition For historical reasonsthe first partition doesnt start until the th disk block so this zone normally contains at least KB of storage still not a lot but enough to store a filesystem driverThis storage scheme is commonly used by the GRUB boot loader see page To effect a successful boot all components of the boot chain must be properly installed and compatible with one another The MBR boot block is OSagnostic butbecause it assumes a particular location for the second stage there may be multipleversions that can be installed The secondstage loader is generally knowledgeableabout operating systems and filesystems it may support several of each and usually has configuration options of its ownUEFIThe UEFI specification includes a modern disk partitioning scheme known as GPTGUID Partition Table where GUID stands for globally unique identifier UEFIalso understands FAT File Allocation Table filesystems a simple but functionallayout that originated in MSDOS These features combine to define the conceptof an EFI System Partition ESP At boot time the firmware consults the GPTpartition table to identify the ESP It then reads the configured target applicationdirectly from a file in the ESP and executes itPartitioning is away to subdividephysical disks Seepage for a moredetailed discussionSee page formore informationabout GPT partitionsBecause the ESP is just a generic FAT filesystem it can be mounted read writtenand maintained by any operating system No mystery meat boot blocks are required anywhere on the diskIn fact no boot loader at all is technically required The UEFI boot target can be aUNIX or Linux kernel that has been configured for direct UEFI loading thus effecting a loaderless bootstrap In practice though most systems still use a boot loaderpartly because that makes it easier to maintain compatibility with legacy BIOSesUEFI saves the pathname to load from the ESP as a configuration parameter Withno configuration it looks for a standard path usually efibootbootxefi onmodern Intel systems A more typical path on a configured system this one forUbuntu and the GRUB boot loader would be efiubuntugrubxefi Other distributions follow a similar conventionUEFI defines standard APIs for accessing the systems hardware In this respect itssomething of a miniature operating system in its own right It even provides forUEFIlevel addon device drivers which are written in a processorindependentlanguage and stored in the ESP Operating systems can use the UEFI interface orthey can take over direct control of the hardwareBecause UEFI has a formal API you can examine and modify UEFI variables including boot menu entries on a running system For example efibootmgr v showsthe following summary of the boot configuration efibootmgr vBootCurrent BootOrder Boot EFI DVDCDROM PciRootxPcixfxSataBoot EFI Hard Drive PciRootxPcixfxSataBoot EFI Network PciRootxPcixxMACcfbbafBoot EFI Internal Shell MemoryMappedxeddxfdcfffFvFilecadbadeBoot ubuntu HDGPTcdefdcbefcffcdcxxFileEFIubuntushimxefiefibootmgr lets you change the boot order select the next configured boot optionor even create and destroy boot entries For example to set the boot order to trythe system drive before trying the network and to ignore other boot options wecould use the command sudo efibootmgr o Here were specifying the options Boot and Boot from the output aboveThe ability to modify the UEFI configuration from user space means that the firmwares configuration information is mounted readwritea blessing and a curse On Truth be told UEFI does maintain an MBRcompatible record at the beginning of each disk to facilitate interoperability with BIOS systems BIOS systems cant see the full GPTstyle partition table butthey at least recognize the disk as having been formatted Be careful not to run MBRspecific administrative tools on GPT disks They may think they understand the disk layout but they do notsystems typically those with systemd that allow write access by default rm rf can be enough to permanently destroy the system at the firmware level in additionto removing files rm also removes variables and other UEFI information accessiblethrough sys Yikes Dont try this at home Boot loadersMost bootstrapping procedures include the execution of a boot loader that is distinct from both the BIOSUEFI code and the OS kernel Its also separate from theinitial boot block on a BIOS system if youre counting stepsThe boot loaders main job is to identify and load an appropriate operating systemkernel Most boot loaders can also present a boottime user interface that lets youselect which of several possible kernels or operating systems to invokeAnother task that falls to the boot loader is the marshaling of configuration arguments for the kernel The kernel doesnt have a command line per se but its startupoption handling will seem eerily similar from the shell For example the argumentsingle or s usually tells the kernel to enter singleuser mode instead of completingthe normal boot processSuch options can be hardwired into the boot loaders configuration if you want themused on every boot or they can be provided on the fly through the boot loaders UIIn the next few sections we discuss GRUB the Linux worlds predominant bootloader and the boot loaders used with FreeBSD GRUB the GRand Unified Boot loaderGRUB developed by the GNU Project is the default boot loader on most Linuxdistributions The GRUB lineage has two main branches the original GRUB nowcalled GRUB Legacy and the newer extracrispy GRUB which is the currentstandard Make sure you know which GRUB youre dealing with as the two versions are quite differentGRUB has been the default boot manager for Ubuntu since version and itrecently became the default for RHEL All our example Linux distributions useit as their default In this book we discuss only GRUB and we refer to it simplyas GRUBFreeBSD has its own boot loader covered in more detail starting on page However GRUB is perfectly happy to boot FreeBSD too This might be an advantageous configuration if youre planning to boot multiple operating systems on asingle computer Otherwise the FreeBSD boot loader is more than adequate See googlQMSiSG link to Phoronix article for some additional detailsGRUB configurationGRUB lets you specify parameters such as the kernel to boot specified as a GRUBmenu entry and the operating mode to boot intoSince this configuration information is needed at boot time you might imaginethat it would be stored somewhere strange such as the systems NVRAM or thedisk blocks reserved for the boot loader In fact GRUB understands most of thefilesystems in common use and can usually find its way to the root filesystem on itsown This feat lets GRUB read its configuration from a regular text fileThe config file is called grubcfg and its usually kept in bootgrub bootgrub inRed Hat and CentOS along with a selection of other resources and code modulesthat GRUB might need to access at boot time Changing the boot configuration isa simple matter of updating the grubcfg fileAlthough you can create the grubcfg file yourself its more common to generateit with the grubmkconfig utility which is called grubmkconfig on Red Hatand CentOS and wrapped as updategrub on Debian and Ubuntu In fact mostdistributions assume that grubcfg can be regenerated at will and they do so automatically after updates If you dont take steps to prevent this your handcraftedgrubcfg file will get clobberedAs with all things Linux distributions configure grubmkconfig in a variety of waysMost commonly the configuration is specified in etcdefaultgrub in the form ofsh variable assignments Table shows some of the commonly modified optionsTable Common GRUB configuration options from etcdefaultgrubShell variable name Contents or functionGRUBBACKGROUND Background image aGRUBCMDLINELINUX Kernel parameters to add to menu entries for Linux bGRUBDEFAULT Number or title of the default menu entryGRUBDISABLERECOVERY Prevents the generation of recovery mode entriesGRUBPRELOADMODULES List of GRUB modules to be loaded as early as possibleGRUBTIMEOUT Seconds to display the boot menu before autoboota The background image must be a png tga jpg or jpeg fileb Table on page lists some of the available options If youre familiar with UNIX filesystem conventions see Chapter The Filesystem starting onpage you might wonder why bootgrub isnt named something more standardlooking suchas varlibgrub or usrlocaletcgrub The reason is that the filesystem drivers used at boot time aresomewhat simplified Boot loaders cant handle advanced features such as mount points as they traverse the filesystem Everything in boot should be a simple file or directorySee page for moreabout operating modesAfter editing etcdefaultgrub run updategrub or grubmkconfig to translateyour configuration into a proper grubcfg file As part of the configurationbuildingprocess these commands inventory the systems bootable kernels so they can beuseful to run after you make kernel changes even if you havent explicitly changedthe GRUB configurationYou may need to edit the etcgrubdcustom file to change the order in whichkernels are listed in the boot menu after you create a custom kernel for example set a boot password or change the names of boot menu items As usual runupdategrub or grubmkconfig after making changesAs an example heres a custom file that invokes a custom kernel on an Ubuntusystembinshexec tail n This file provides an easy way to add custom menu entries Just type the menu entries you want to add after this comment Be careful not to change the exec tail line abovemenuentry My Awesome Kernel set roothdmsdoslinux awesomekernel rootUUIDXXXXXXXXX ro quietinitrd initrdimgawesomekernelIn this example GRUB loads the kernel from awesomekernel Kernel paths arerelative to the boot partition which historically was mounted as boot but with theadvent of UEFI now is likely an unmounted EFI System Partition Use gpart showand mount to examine your disk and determine the state of the boot partitionThe GRUB command lineGRUB supports a commandline interface for editing config file entries on the flyat boot time To enter commandline mode type c at the GRUB boot screenFrom the command line you can boot operating systems that arent listed in thegrubcfg file display system information and perform rudimentary filesystemtesting Anything that can be done through grubcfg can also be done throughthe command linePress the Tab key to see a list of possible commands Table on the next pageshows some of the more useful onesSee page for moreinformation aboutmounting filesystemsTable GRUB commandsCmd Functionboot Boots the system from the specified kernel imagehelp Gets interactive help for a commandlinux Loads a Linux kernelreboot Reboots the systemsearch Searches devices by file filesystem label or UUIDusb Tests USB supportFor detailed information about GRUB and its commandline options refer to theofficial manual at gnuorgsoftwaregrubmanualLinux kernel optionsKernel startup options typically modify the values of kernel parameters instructthe kernel to probe for particular devices specify the path to the init or systemdprocess or designate a particular root device Table shows a few examplesTable Examples of kernel boot time optionsOption Meaningdebug Turns on kernel debugginginitbinbash Starts only the bash shell useful for emergency recoveryrootdevfoo Tells the kernel to use devfoo as the root devicesingle Boots to singleuser modeWhen specified at boot time kernel options are not persistent Edit the appropriatekernel line in etcgrubdcustom or etcdefaultsgrub the variable namedGRUBCMDLINELINUX to make the change permanent across rebootsSecurity patches bug fixes and features are all regularly added to the Linux kernelUnlike other software packages however new kernel releases typically do not replace old ones Instead the new kernels are installed side by side with the previousversions so that you can return to an older kernel in the event of problemsThis convention helps administrators back out of an upgrade if a kernel patchbreaks their system although it also means that the boot menu tends to get cluttered with old versions of the kernel Try choosing a different kernel if your systemwont boot after an updateSee Chapter for more about kernel parameters The FreeBSD boot processFreeBSDs boot system is a lot like GRUB in that the finalstage boot loader calledloader uses a filesystembased configuration file supports menus and offers aninteractive commandlinelike mode loader is the final common pathway for boththe BIOS and UEFI boot pathsThe BIOS path bootAs with GRUB the full loader environment is too large to fit in an MBR boot blockso a chain of progressively more sophisticated preliminary boot loaders get loaderup and running on a BIOS systemGRUB bundles all of these components under the umbrella name GRUB but inFreeBSD the early boot loaders are part of a separate system called boot thatsused only on BIOS systems boot has options of its own mostly because it storeslater stages of the boot chain in a volume boot record see Legacy BIOS on page rather than in front of the first disk partitionFor that reason the MBR boot record needs a pointer to the partition it should useto continue the boot process Normally all this is automatically set up for you aspart of the FreeBSD installation process but if you should ever need to adjust theconfiguration you can do so with the bootcfg commandThe UEFI pathOn UEFI systems FreeBSD creates an EFI system partition and installs boot codethere under the path bootbootxefi This is the default path that UEFI systems check at boot time at least on modern PC platforms so no firmwarelevelconfiguration should be needed other than ensuring that device boot prioritiesare properly setBy default FreeBSD doesnt keep the EFI system partition mounted after bootingYou can inspect the partition table with gpart to identify it gpart show ada GPT G efi K freebsdufs G freebsdswap G free BAlthough you can mount the ESP if youre curious to see whats in it use mountst msdos option the whole filesystem is actually just a copy of an image found inbootbootefifat on the root disk No userserviceable parts inside Dont confuse the boot directory in the EFI system partition with the boot directory in the FreeBSDroot filesystem They are separate and serve different purposes although of course both are bootstrapping relatedSee page for moreinformation aboutmounting filesystemsIf the ESP partition gets damaged or removed you can recreate it by setting up thepartition with gpart and then copying in the filesystem image with dd sudo dd ifbootbootefifat ofdevadapOnce the firststage UEFI boot loader finds a partition of type freebsdufs it loadsa UEFI version of the loader software from bootloaderefi From there bootingproceeds as under BIOS with loader determining the kernel to load the kernelparameters to set and so onloader configurationloader is actually a scripting environment and the scripting language is ForthTheres a bunch of Forth code stored under boot that controls loaders operationsbut its designed to be selfcontainedyou neednt learn ForthThe Forth scripts execute bootloaderconf to obtain the values of configurationvariables so thats where your customizations should go Dont confuse this file withbootdefaultsloaderconf which contains the configuration defaults and isnt intended for modification Fortunately the variable assignments in loaderconf aresyntactically similar to standard sh assignmentsThe man pages for loader and loaderconf give the dirt on all the boot loader options and the configuration variables that control them Some of the more interesting options include those for protecting the boot menu with a password changingthe splash screen displayed at boot and passing kernel optionsloader commandsloader understands a variety of interactive commands For example to locate andboot an alternate kernel youd use a sequence of commands like thisType for a list of commands help for more detailed helpOK ls d snap d devd rescuel homeOK unloadOK load bootkernelkerneloldbootkernelkernelold textxff datax bOK boot As of FreeBSD it is now possible to use ZFS as the root partition on a UEFI system This is a remarkable and interesting fact if youre a historian of programming languages and unimportant otherwiseHere we listed the contents of the default root filesystem unloaded the defaultkernel bootkernelkernel loaded an older kernel bootkernelkerneloldand then continued the boot processSee man loader for complete documentation of the available commands System management daemonsOnce the kernel has been loaded and has completed its initialization process it creates a complement of spontaneous processes in user space Theyre called spontaneous processes because the kernel starts them autonomouslyin the normalcourse of events new processes are created only at the behest of existing processesMost of the spontaneous processes are really part of the kernel implementationThey dont necessarily correspond to programs in the filesystem Theyre not configurable and they dont require administrative attention You can recognize themin ps listings see page by their low PIDs and by the brackets around theirnames for example pagedaemon on FreeBSD or kdump on LinuxThe exception to this pattern is the system management daemon It has process ID and usually runs under the name init The system gives init a couple of specialprivileges but for the most part its just a userlevel program like any other daemonResponsibilities of initinit has multiple functions but its overarching goal is to make sure the system runsthe right complement of services and daemons at any given timeTo serve this goal init maintains a notion of the mode in which the system shouldbe operating Some commonly defined modes Singleuser mode in which only a minimal set of filesystems is mountedno services are running and a root shell is started on the console Multiuser mode in which all customary filesystems are mounted and allconfigured network services have been started along with a window system and graphical login manager for the console Server mode similar to multiuser mode but with no GUI running onthe consoleEvery mode is associated with a defined complement of system services and theinitialization daemon starts or stops services as needed to bring the systems actual state into line with the currently active mode Modes can also have associatedmilepost tasks that run whenever the mode begins or ends Dont take these mode names or descriptions too literally theyre just examples of common operatingmodes that most systems define in one way or anotherAs an example init normally takes care of many different startup chores as a sideeffect of its transition from bootstrapping to multiuser mode These may include Setting the name of the computer Setting the time zone Checking disks with fsck Mounting filesystems Removing old files from the tmp directory Configuring network interfaces Configuring packet filters Starting up other daemons and network servicesinit has very little builtin knowledge about these tasks In simply runs a set of commands or scripts that have been designated for execution in that particular contextImplementations of initToday three very different flavors of system management processes are in widespread use An init styled after the init from ATTs System V UNIX which we refer to as traditional init This was the predominant init used on Linuxsystems until the debut of systemd An init variant that derives from BSD UNIX and is used on most BSDbased systems including FreeBSD OpenBSD and NetBSD This one isjust as triedandtrue as the SysV init and has just as much claim to beingcalled traditional but for clarity we refer to it as BSD init This variantis quite simple in comparison with SysVstyle init We discuss it separatelystarting on page A more recent contender called systemd which aims to be onestopshopping for all daemon and staterelated issues As a consequence systemdcarves out a significantly larger territory than any historical version of initThat makes it somewhat controversial as we discuss below Neverthelessall our example Linux distributions have now adopted systemdAlthough these implementations are the predominant ones today theyre far frombeing the only choices Apples macOS for example uses a system called launchdUntil it adopted systemd Ubuntu used another modern init variant called UpstartOn Linux systems you can theoretically replace your systems default init withwhichever variant you prefer But in practice init is so fundamental to the operation of the system that a lot of addon software is likely to break If you cant abidesystemd standardize on a distribution that doesnt use itTraditional initIn the traditional init world system modes eg singleuser or multiuser are knownas run levels Most run levels are denoted by a single letter or digitTraditional init has been around since the early s and grizzled folks in the antisystemd camp often cite the principle If it aint broke dont fix it That said traditional init does have a number of notable shortcomingsTo begin with the traditional init on its own is not really powerful enough to handle the needs of a modern system Most systems that use it actually have a standardand fixed init configuration that never changes That configuration points to a second tier of shell scripts that do the actual work of changing run levels and lettingadministrators make configuration changesThe second layer of scripts maintains yet a third layer of daemon and systemspecific scripts which are crosslinked to runlevelspecific directories that indicatewhat services are supposed to be running at what run level Its all a bit hackishand unsightlyMore concretely this system has no general model of dependencies among servicesso it requires that all startups and takedowns be run in a numeric order thats maintained by the administrator Later actions cant run until everything ahead of themhas finished so its impossible to execute actions in parallel and the system takesa long time to change statessystemd vs the worldFew issues in the Linux space have been more hotly debated than the migrationfrom traditional init to systemd For the most part complaints center on systemdsseemingly everincreasing scopesystemd takes all the init features formerly implemented with sticky tape shellscript hacks and the sweat of administrators and formalizes them into a unifiedfield theory of how services should be configured accessed and managedMuch like a package management system systemd defines a robust dependencymodel not only among services but also among targets systemds term for theoperational modes that traditional init calls run levels systemd not only managesprocesses in parallel but also manages network connections networkd kernellog entries journald and logins logindThe antisystemd camp argues that the UNIX philosophy is to keep system components small simple and modular A component as fundamental as init theysay should not have monolithic control over so many of the OSs other subsystemssystemd not only breeds complexity but also introduces potential security weaknesses and muddies the distinction between the OS platform and the services thatrun on top of itSee Chapter Software Installationand Managementfor more information about packagemanagementsystemd has also received criticism for imposing new standards and responsibilities on the Linux kernel for its code quality for the purported unresponsiveness ofits developers to bug reports for the functional design of its basic features and forlooking at people funny We cant fairly address these issues here but you may find itinformative to peruse the Arguments against systemd section at withoutsystemdorgthe Internets premier systemd hate siteinits judged and assigned their proper punishmentsThe architectural objections to systemd outlined above are all reasonable pointssystemd does indeed display most of the telltale stigmata of an overengineeredsoftware projectIn practice however many administrators quite like systemd and we fall squarelyinto this camp Ignore the controversy for a bit and give systemd a chance to winyour love Once youve become accustomed to it you will likely find yourself appreciating its many meritsAt the very least keep in mind that the traditional init that systemd displaces wasno national treasure If nothing else systemd delivers some value just by eliminating a few of the unnecessary differences among Linux distributionsThe debate really doesnt matter anymore because the systemd coup is over The argument was effectively settled when Red Hat Debian and Ubuntu switched Manyother Linux distributions are now adopting systemd either by choice or by beingdragged along kicking and screaming by their upstream distributionsTraditional init still has a role to play when a distribution either targets a smallinstallation footprint or doesnt need systemds advanced process managementfunctions Theres also a sizable population of revanchists who disdain systemd onprinciple so some Linux distributions are sure to keep traditional init alive indefinitely as a form of protest theaterNevertheless we dont think that traditional init has enough of a future to merit adetailed discussion in this book For Linux we mostly limit ourselves to systemdWe also discuss the mercifully simple system used by FreeBSD starting on page systemd in detailThe configuration and control of system services is an area in which Linux distributions have traditionally differed the most from one another systemd aims tostandardize this aspect of system administration and to do so it reaches furtherinto the normal operations of the system than any previous alternativesystemd is not a single daemon but a collection of programs daemons librariestechnologies and kernel components A post on the systemd blog at pointerdeblognotes that a full build of the project generates different binaries Think of it as ascrumptious buffet at which you are forced to consume everythingSince systemd depends heavily on features of the Linux kernel its a Linuxonlyproposition You wont see it ported to BSD or to any other variant of UNIX within the next five yearsUnits and unit filesAn entity that is managed by systemd is known generically as a unit More specifically a unit can be a service a socket a device a mount point an automountpoint a swap file or partition a startup target a watched filesystem path a timercontrolled and supervised by systemd a resource management slice a group ofexternally created processes or a wormhole into an alternate universe OK wemade up the part about the alternate universe but that still covers a lot of territoryWithin systemd the behavior of each unit is defined and configured by a unit fileIn the case of a service for example the unit file specifies the location of the executable file for the daemon tells systemd how to start and stop the service andidentifies any other units that the service depends onWe explore the syntax of unit files in more detail soon but heres a simple examplefrom an Ubuntu system as an appetizer This unit file is rsyncservice it handlesstartup of the rsync daemon that mirrors filesystemsUnitDescriptionfast remote file copy program daemonConditionPathExistsetcrsyncdconfServiceExecStartusrbinrsync daemon nodetachInstallWantedBymultiusertargetIf you recognize this as the file format used by MSDOS ini files you are well onyour way to understanding both systemd and the anguish of the systemd hatersUnit files can live in several different places usrlibsystemdsystem is the mainplace where packages deposit their unit files during installation on some systemsthe path is libsystemdsystem instead The contents of this directory are considered stock so you shouldnt modify them Your local unit files and customizationscan go in etcsystemdsystem Theres also a unit directory in runsystemdsystemthats a scratch area for transient unitssystemd maintains a telescopic view of all these directories so theyre pretty muchequivalent If theres any conflict the files in etc have the highest priority Mostly quoted from the systemdunit man pageSee page formore informationabout rsyncBy convention unit files are named with a suffix that varies according to the typeof unit being configured For example service units have a service suffix and timers use timer Within the unit file some sections eg Unit apply genericallyto all kinds of units but others eg Service can appear only in the context ofa particular unit typesystemctl manage systemdsystemctl is an allpurpose command for investigating the status of systemd andmaking changes to its configuration As with Git and several other complex softwaresuites systemctls first argument is typically a subcommand that sets the generalagenda and subsequent arguments are specific to that particular subcommand Thesubcommands could be toplevel commands in their own right but for consistencyand clarity theyre bundled into the systemctl omnibusRunning systemctl without any arguments invokes the default listunits subcommand which shows all loaded and active services sockets targets mounts anddevices To show only loaded and active services use the typeservice qualifier systemctl listunits typeserviceUNIT LOAD ACTIVE SUB DESCRIPTIONaccountsdaemonservice loaded active running Accounts Servicewpasupplicantservice loaded active running WPA supplicantIts also sometimes helpful to see all the installed unit files regardless of whetheror not theyre active systemctl listunitfiles typeserviceUNIT FILE STATEcronservice enabledcryptdisksearlyservice maskedcryptdisksservice maskedcupsbrowsedservice enabledcupsservice disabledwpasupplicantservice disabledxcommonservice masked unit files listedFor subcommands that act on a particular unit eg systemctl status systemctlcan usually accept a unit name without a unittype suffix eg cups instead ofcupsservice However the default unit type with which simple names are fleshedout varies by subcommandTable shows the most common and useful systemctl subcommands See thesystemctl man page for a complete listSee page for moreinformation about GitTable Commonly used systemctl subcommandsSubcommand Functionlistunitfiles pattern Shows installed units optionally matching patternenable unit Enables unit to activate at bootdisable unit Prevents unit from activating at bootisolate target Changes operating mode to targetstart unit Activates unit immediatelystop unit Deactivates unit immediatelyrestart unit Restarts or starts if not running unit immediatelystatus unit Shows units status and recent log entrieskill pattern Sends a signal to units matching patternreboot Reboots the computerdaemonreload Reloads unit files and systemd configurationUnit statusesIn the output of systemctl listunitfiles above we can see that cupsservice is disabled We can use systemctl status to find out more details sudo systemctl status l cupscupsservice CUPS Scheduler Loaded loaded libsystemdsystemcupsservice disabled vendorpreset enabled Active inactive dead since Sat MST s ago Docs mancupsd Main PID codeexited statusSUCCESSDec ulsah systemd Started CUPS SchedulerDec ulsah systemd Started CUPS SchedulerDec ulsah systemd Stopping CUPS SchedulerDec ulsah systemd Stopped CUPS SchedulerHere systemctl shows us that the service is currently inactive dead and tells uswhen the process died Just a few seconds ago we disabled it for this example Italso shows in the section marked Loaded that the service defaults to being enabledat startup but that it is presently disabledThe last four lines are recent log entries By default the log entries are condensedso that each entry takes only one line This compression often makes entries unreadable so we included the l option to request full entries It makes no differencein this case but its a useful habit to acquireTable on the next page shows the statuses youll encounter most frequentlywhen checking up on unitsTable Unit file statusesState Meaningbad Some kind of problem within systemd usually a bad unit filedisabled Present but not configured to start autonomouslyenabled Installed and runnable will start autonomouslyindirect Disabled but has peers in Also clauses that may be enabledlinked Unit file available through a symlinkmasked Banished from the systemd world from a logical perspectivestatic Depended upon by another unit has no install requirementsThe enabled and disabled states apply only to unit files that live in one of systemdssystem directories that is they are not linked in by a symbolic link and that havean Install section in their unit files Enabled units should perhaps really bethought of as installed meaning that the directives in the Install section havebeen executed and that the unit is wired up to its normal activation triggers Inmost cases this state causes the unit to be activated automatically once the systemis bootstrappedLikewise the disabled state is something of a misnomer because the only thingthats actually disabled is the normal activation path You can manually activate aunit that is disabled by running systemctl start systemd wont complainMany units have no installation procedure so they cant truly be said to be enabledor disabled theyre just available Such units status is listed as static They onlybecome active if activated by hand systemctl start or named as a dependency ofother active unitsUnit files that are linked were created with systemctl link This command creates asymbolic link from one of systemds system directories to a unit file that lives elsewhere in the filesystem Such unit files can be addressed by commands or namedas dependencies but they are not full citizens of the ecosystem and have some notable quirks For example running systemctl disable on a linked unit file deletesthe link and all references to itUnfortunately the exact behavior of linked unit files is not well documented Although the idea of keeping local unit files in a separate repository and linking theminto systemd has a certain appeal its probably not the best approach at this pointJust make copiesThe masked status means administratively blocked systemd knows about the unitbut has been forbidden from activating it or acting on any of its configuration directives by systemctl mask As a rule of thumb turn off units whose status is enabledor linked with systemctl disable and reserve systemctl mask for static unitsReturning to our investigation of the cups service we could use the following commands to reenable and start it sudo systemctl enable cupsSynchronizing state of cupsservice with SysV init with libsystemdsystemdsysvinstallExecuting libsystemdsystemdsysvinstall enable cupsinsserv warning current start runlevels empty of script cupsoverrides LSB defaults insserv warning current stop runlevels of script cupsoverrides LSB defaults Created symlink from etcsystemdsystemsocketstargetwantscupssocketto libsystemdsystemcupssocketCreated symlink from etcsystemdsystemmultiusertargetwantscupspath to libsystemdsystemcupspath sudo systemctl start cupsTargetsUnit files can declare their relationships to other units in a variety of ways In theexample on page for example the WantedBy clause says that if the systemhas a multiusertarget unit that unit should depend on this one rsyncservicewhen this unit is enabledBecause units directly support dependency management no additional machinery is needed to implement the equivalent of inits run levels For clarity systemddoes define a distinct class of units of type target to act as wellknown markersfor common operating modes However targets have no real superpowers beyondthe dependency management thats available to any other unitTraditional init defines at least seven numeric run levels but many of those arentactually in common use In a perhapsilladvised gesture toward historical continuity systemd defines targets that are intended as direct analogs of the init runlevels runleveltarget etc It also defines mnemonic targets for daytoday usesuch as powerofftarget and graphicaltarget Table on the next page showsthe mapping between init run levels and systemd targetsThe only targets to really be aware of are multiusertarget and graphicaltarget fordaytoday use and rescuetarget for accessing singleuser mode To change thesystems current operating mode use the systemctl isolate command sudo systemctl isolate multiusertargetThe isolate subcommand is sonamed because it activates the stated target and itsdependencies but deactivates all other unitsUnder traditional init you use the telinit command to change run levels once thesystem is booted Some distributions now define telinit as a symlink to the systemctlcommand which recognizes how its being invoked and behaves appropriatelyTable Mapping between init run levels and systemd targetsRun level Target Description powerofftarget System haltemergency emergencytarget Barebones shell for system recovery s single rescuetarget Singleuser mode multiusertarget a Multiuser mode command line multiusertarget a Multiuser mode with networking multiusertarget a Not normally used by init graphicaltarget Multiuser mode with networking and GUI reboottarget System reboota By default multiusertarget maps to runleveltarget multiuser mode with networkingTo see the target the system boots into by default run the getdefault subcommand systemctl getdefaultgraphicaltargetMost Linux distributions boot to graphicaltarget by default which isnt appropriate for servers that dont need a GUI But thats easily changed sudo systemctl setdefault multiusertargetTo see all the systems available targets run systemctl listunits systemctl listunits typetargetDependencies among unitsLinux software packages generally come with their own unit files so administrators dont need a detailed knowledge of the entire configuration language Howeverthey do need a working knowledge of systemds dependency system to diagnoseand fix dependency problemsTo begin with not all dependencies are explicit systemd takes over the functions ofthe old inetd and also extends this idea into the domain of the DBus interprocesscommunication system In other words systemd knows which network ports orIPC connection points a given service will be hosting and it can listen for requestson those channels without actually starting the service If a client does materializesystemd simply starts the actual service and passes off the connection The serviceruns if its actually used and remains dormant otherwiseSecond systemd makes some assumptions about the normal behavior of most kindsof units The exact assumptions vary by unit type For example systemd assumes thatthe average service is an addon that shouldnt be running during the early phases ofsystem initialization Individual units can turn off these assumptions with the lineDefaultDependenciesfalsein the Unit section of their unit file the default is true See the man page forsystemdunittype to see the exact assumptions that apply to each type of unit egman systemdserviceA third class of dependencies are those explicitly declared in the Unit sections ofunit files Table shows the available optionsTable Explicit dependencies in the Unit section of unit filesOption MeaningWants Units that should be coactivated if possible but are not requiredRequires Strict dependencies failure of any prerequisite terminates this serviceRequisite Like Requires but must already be activeBindsTo Similar to Requires but even more tightly coupledPartOf Similar to Requires but affects only starting and stoppingConflicts Negative dependencies cannot be coactive with these unitsWith the exception of Conflicts all the options in Table express the basic ideathat the unit being configured depends on some set of other units The exact distinctions among these options are subtle and primarily of interest to service developers The least restrictive variant Wants is preferred when possibleYou can extend a units Wants or Requires cohorts by creating a unitfilewants orunitfilerequires directory in etcsystemdsystem and adding symlinks there toother unit files Better yet just let systemctl do it for you For example the command sudo systemctl addwants multiusertarget mylocalserviceadds a dependency on mylocalservice to the standard multiuser target ensuringthat the service will be started whenever the system enters multiuser modeIn most cases such ad hoc dependencies are automatically taken care of for youcourtesy of the Install sections of unit files This section includes WantedByand RequiredBy options that are read only when a unit is enabled with systemctlenable or disabled with systemctl disable On enablement they make systemctlperform the equivalent of an addwants for every WantedBy or an addrequiresfor every RequiredByThe Install clauses themselves have no effect in normal operation so if a unitdoesnt seem to be started when it should be make sure that it has been properlyenabled and symlinkedExecution orderYou might reasonably guess that if unit A Requires unit B then unit B will be startedor configured before unit A But in fact that is not the case In systemd the orderin which units are activated or deactivated is an entirely separate question fromthat of which units to activateWhen the system transitions to a new state systemd first traces the various sourcesof dependency information outlined in the previous section to identify the unitsthat will be affected It then uses Before and After clauses from the unit files tosort the work list appropriately To the extent that units have no Before or Afterconstraints they are free to be adjusted in parallelAlthough potentially surprising this is actually a praiseworthy design feature One ofthe major design goals of systemd was to facilitate parallelism so it makes sense thatunits do not acquire serialization dependencies unless they explicitly ask for themIn practice After clauses are typically used more frequently than Wants or RequiresTarget definitions and in particular the reverse dependencies encoded in WantedByand RequiredBy clauses establish the general outlines of the services running ineach operating mode and individual packages worry only about their immediateand direct dependenciesA more complex unit file exampleNow for a closer look at a few of the directives used in unit files Heres a unit filefor the NGINX web server nginxserviceUnitDescriptionThe nginx HTTP and reverse proxy serverAfternetworktarget remotefstarget nsslookuptargetServiceTypeforkingPIDFilerunnginxpidExecStartPreusrbinrm f runnginxpidExecStartPreusrsbinnginx tExecStartusrsbinnginxExecReloadbinkill s HUP MAINPIDKillModeprocessKillSignalSIGQUITTimeoutStopSecPrivateTmptrueInstallWantedBymultiusertargetThis service is of type forking which means that the startup command is expectedto terminate even though the actual daemon continues running in the backgroundSince systemd wont have directly started the daemon the daemon records its PIDprocess ID in the stated PIDFile so that systemd can determine which processis the daemons primary instanceThe Exec lines are commands to be run in various circumstances ExecStartPrecommands are run before the actual service is started the ones shown here validatethe syntax of NGINXs configuration file and ensure that any preexisting PID fileis removed ExecStart is the command that actually starts the service ExecReloadtells systemd how to make the service reread its configuration file systemd automatically sets the environment variable MAINPID to the appropriate valueTermination for this service is handled through KillMode and KillSignal whichtell systemd that the service daemon interprets a QUIT signal as an instruction toclean up and exit The lineExecStopbinkill s HUP MAINPIDwould have essentially the same effect If the daemon doesnt terminate withinTimeoutStopSec seconds systemd will force the issue by pelting it with a TERMsignal and then an uncatchable KILL signalThe PrivateTmp setting is an attempt at increasing security It puts the servicestmp directory somewhere other than the actual tmp which is shared by all thesystems processes and usersLocal services and customizationsAs you can see from the previous examples its relatively trivial to create a unit filefor a homegrown service Browse the examples in usrlibsystemdsystem andadapt one thats close to what you want See the man page for systemdservice fora complete list of configuration options for services For options common to alltypes of units see the page for systemdunitPut your new unit file in etcsystemdsystem You can then run sudo systemctl enable customserviceto activate the dependencies listed in the service files Install sectionAs a general rule you should never edit a unit file you didnt write Instead createa configuration directory in etcsystemdsystemunitfiled and add one or moreconfiguration files there called xxxconf The xxx part doesnt matter just makesure the file has a conf suffix and is in the right location overrideconf is thestandard nameconf files have the same format as unit files and in fact systemd smooshes themall together with the original unit file However override files have priority overthe original unit file should both sources try to set the value of a particular optionOne point to keep in mind is that many systemd options are allowed to appearmore than once in a unit file In these cases the multiple values form a list and areall active simultaneously If you assign a value in your overrideconf file that value joins the list but does not replace the existing entries This may or may not bewhat you want To remove the existing entries from a list just assign the option anempty value before adding your ownSee page formore informationabout signalsLets look at an example Suppose that your site keeps its NGINX configurationfile in a nonstandard place say usrlocalwwwnginxconf You need to run thenginx daemon with a c usrlocalwwwnginxconf option so that it can find theproper configuration fileYou cant just add this option to usrlibsystemdsystemnginxservice becausethat file will be replaced whenever the NGINX package is updated or refreshedInstead you can use the following command sequence sudo mkdir etcsystemdsystemnginxserviced sudo cat overrideconfServiceExecStartExecStartusrsbinnginx c usrlocalwwwnginxconfControlD sudo systemctl daemonreload sudo systemctl restart nginxserviceThe first ExecStart removes the current entry and the second sets an alternative start command systemctl daemonreload makes systemd reparse unit filesHowever it does not restart daemons automatically so youll also need an explicitsystemctl restart to make the change take effect immediatelyThis command sequence is such a common idiom that systemctl now implementsit directly sudo systemctl edit nginxserviceedit the override file in the editor sudo systemctl restart nginxservice As shown you must still do the restart by handOne last thing to know about override files is that they cant modify the Installsection of a unit file Any changes you make are silently ignored Just add dependencies directly with systemctl addwants or systemctl addrequiresService and startup control caveatssystemd has many architectural implications and adopting it is not a simple task forthe teams that build Linux distributions Current releases are mostly Frankensteinsystems that adopt much of systemd but also retain a few links to the past Sometimes the holdovers just havent yet been fully converted In other cases variousforms of glue have been left behind to facilitate compatibility The and are shell metacharacters The redirects output to a file and the expands to the lastcomponent of the previous command line so that you dont have to retype it All shells understandthis notation See Shell basics starting on page for some other handy featuresThough systemctl can and should be used for managing services and daemons dontbe surprised when you run into traditional init scripts or their associated helpercommands If you attempt to use systemctl to disable the network on a CentOS orRed Hat system for example youll receive the following output sudo systemctl disable networknetworkservice is not a native service redirecting to sbinchkconfigExecuting sbinchkconfig network offTraditional init scripts often continue to function on a systemd system For example an init script etcrcdinitdmyoldservice might be automatically mappedto a unit file such as myoldserviceservice during system initialization or whensystemctl daemonreload is run Apache on Ubuntu is a case in point attempting to disable the apacheservice results in the following output sudo systemctl disable apacheSynchronizing state of apacheservice with SysV service script withlibsystemdsystemdsysvinstallExecuting libsystemdsystemdsysvinstall disable apacheThe end result is what you wanted but it goes through a rather circuitous routeRed Hat and by extension CentOS still run the etcrcdrclocal script at boottime if you configure it to be executable In theory you can use this script to hackin sitespecific tweaks or postboot tasks if desired At this point though youshould really skip the hacks and do things systemds way by creating an appropriate set of unit filesSome Red Hat and CentOS boot chores continue to use config files found in theetcsysconfig directory Table summarizes theseTable Files and subdirectories of Red Hats etcsysconfig directoryFile or directory Contentsconsole A directory that historically allowed for custom keymappingcrond Arguments to pass to the crond daemoninit Configuration for handling messages from startup scriptsiptablesconfig Loads additional iptables modules such as NAT helpersnetworkscripts Accessory scripts and network config filesnfs Optional RPC and NFS argumentsntpd Commandline options for ntpdselinux Symlink to etcselinuxconfigaa Sets arguments for SELinux or allows you to disable it altogether see page A quick sudo chmod x etcrcdrclocal will ensure that the file is executableSee page formore informationabout ApacheRHELA couple of the items in Table merit additional comment The networkscripts directory contains additional material related to network configuration The only things you might need to change here arethe files named ifcfginterface For example networkscriptsifcfgethcontains the configuration parameters for the interface eth It sets theinterfaces IP address and networking options See page for moreinformation about configuring network interfaces The iptablesconfig file doesnt actually allow you to modify the iptablesfirewall rules themselves It just provides a way to load additional modules such as those for network address translation NAT if youre goingto be forwarding packets or using the system as a router See page for more information about configuring iptablessystemd loggingCapturing the log messages produced by the kernel has always been something ofa challenge It became even more important with the advent of virtual and cloudbased systems since it isnt possible to simply stand in front of these systems consoles and watch what happens Frequently crucial diagnostic information was lostto the ethersystemd alleviates this problem with a universal logging framework that includes allkernel and service messages from early boot to final shutdown This facility calledthe journal is managed by the journald daemonSystem messages captured by journald are stored in the run directory rsyslogcan process these messages and store them in traditional log files or forward themto a remote syslog server You can also access the logs directly with the journalctlcommandWithout arguments journalctl displays all log entries oldest first journalctl Logs begin at Fri UTC end at Fri UTC Feb ubuntu systemdjournal Runtime journal is usingM max allowed M tFeb ubuntu systemdjournal Runtime journal is usingM max allowed M tFeb ubuntu kernel Initializing cgroup subsys cpusetFeb ubuntu kernel Initializing cgroup subsys cpuFeb ubuntu kernel Linux version generic builddlcy gcc version Feb ubuntu kernel Command line BOOTIMAGEbootvmlinuzgeneric rootUUIFeb ubuntu kernel KERNEL supported cpusFeb ubuntu kernel Intel GenuineIntelYou can configure journald to retain messages from prior boots To do this editetcsystemdjournaldconf and configure the Storage attributeJournalStoragepersistent Once youve configured journald you can obtain a list of prior boots with journalctl listboots afadeeefbeaddc Fri UTCFri UTC cfaecaadbded Fri UTC Fri UTCYou can then access messages from a prior boot by referring to its index or by naming its longform ID journalctl b journalctl b afadeeefbeaddcTo restrict the logs to those associated with a specific unit use the u flag journalctl u ntp Logs begin at Fri UTC end at Fri UTC Feb ubtest systemd Stopped LSB Start NTP daemonFeb ubtest systemd Starting LSB Start NTP daemonFeb ubtest ntp Starting NTP server ntpdSystem logging is covered in more detail in Chapter Logging FreeBSD init and startup scriptsFreeBSD uses a BSDstyle init which does not support the concept of run levelsTo bring the system to its fully booted state FreeBSDs init just runs etcrc Thisprogram is a shell script but it should not be directly modified Instead the rcsystem implements a couple of standardized ways for administrators and softwarepackages to extend the startup system and make configuration changesetcrc is primarily a wrapper that runs other startup scripts most of which live inusrlocaletcrcd and etcrcd Before it runs any of those scripts however rcexecutes three files that hold configuration information for the system etcdefaultsconfig etcrcconf etcrcconflocalThese files are themselves scripts but they typically contain only definitions for thevalues of shell variables The startup scripts then check these variables to determineSee Chapter formore informationabout shell scriptinghow to behave etcrc uses some shell magic to ensure that the variables definedin these files are visible everywhereetcdefaultsrcconf lists all the configuration parameters and their default settingsNever edit this file lest the startup script bogeyman hunt you down and overwriteyour changes the next time the system is updated Instead just override the defaultvalues by setting them again in etcrcconf or etcrcconflocal The rcconf manpage has an extensive list of the variables you can specifyIn theory the rcconf files can also specify other directories in which to look forstartup scripts by your setting the value of the localstartup variable The defaultvalue is usrlocaletcrcd and we recommend leaving it that wayAs you can see from peeking at etcrcd there are many different startup scriptsmore than on a standard installation etcrc runs these scripts in the ordercalculated by the rcorder command which reads the scripts and looks for dependency information thats been encoded in a standard wayFreeBSDs startup scripts for gardenvariety services are fairly straightforward Forexample the top of the sshd startup script is as followsbinsh PROVIDE sshd REQUIRE LOGIN FILESYSTEMS KEYWORD shutdown etcrcsubrnamesshdrcvarsshdenablecommandusrsbinnameThe rcvar variable contains the name of a variable thats expected to be definedin one of the rcconf scripts in this case sshdenable If you want sshd the realdaemon not the startup script both are named sshd to run automatically at boottime put the linesshdenableYESinto etcrcconf If this variable is set to NO or commented out the sshd scriptwill not start the daemon or check to see whether it should be stopped when thesystem is shut downThe service command provides a realtime interface into FreeBSDs rcd systemTo stop the sshd service manually for example you could run the command sudo service sshd stop For local customizations you have the option of either creating standard rcdstyle scripts that go inusrlocaletcrcd or editing the systemwide etcrclocal script The former is preferred The version of service that FreeBSD uses derives from the Linux service command which manipulates traditional init servicesNote that this technique works only if the service is enabled in the etcrcconffiles If it is not use the subcommand onestop onestart or onerestart depending on what you want to do service is generally forgiving and will remind you ifneed be however Reboot and shutdown proceduresHistorically UNIX and Linux machines were touchy about how they were shut downModern systems have become less sensitive especially when a robust filesystemis used but its always a good idea to shut down a machine nicely when possibleConsumer operating systems of yesteryear trained many sysadmins to reboot thesystem as the first step in debugging any problem It was an adaptive habit backthen but these days it more commonly wastes time and interrupts service Focuson identifying the root cause of problems and youll probably find yourself rebooting less oftenThat said its a good idea to reboot after modifying a startup script or making significant configuration changes This check ensures that the system can boot successfully If youve introduced a problem but dont discover it until several weeks lateryoure unlikely to remember the details of your most recent changesShutting down physical systemsThe halt command performs the essential duties required for shutting down the system halt logs the shutdown kills nonessential processes flushes cached filesystemblocks to disk and then halts the kernel On most systems halt p powers downthe system as a final flourishreboot is essentially identical to halt but it causes the machine to reboot insteadof haltingThe shutdown command is a layer over halt and reboot that provides for scheduled shutdowns and ominous warnings to loggedin users It dates back to thedays of timesharing systems and is now largely obsolete shutdown does nothingof technical value beyond halt or reboot so feel free to ignore it if you dont havemultiuser systemsShutting down cloud systemsYou can halt or restart a cloud system either from within the server with halt orreboot as described in the previous section or from the cloud providers web console or its equivalent APIGenerally speaking powering down from the cloud console is akin to turning off thepower Its better if the virtual server manages its own shutdown but feel free to killa virtual server from the console if it becomes unresponsive What else can you doEither way make sure you understand what a shutdown means from the perspective of the cloud provider It would be a shame to destroy your system when all youmeant to do was reboot itIn the AWS universe the Stop and Reboot operations do what youd expect Terminate decommissions the instance and removes it from your inventory If theunderlying storage device is set to delete on termination not only will your instance be destroyed but the data on the root disk will also be lost Thats perfectlyfine as long as its what you expect You can enable termination protection if youconsider this a bad thing Stratagems for a nonbooting systemA variety of problems can prevent a system from booting ranging from faulty devices to kernel upgrades gone wrong There are three basic approaches to this situation listed here in rough order of desirability Dont debug just restore the system to a knowngood state Bring the system up just enough to run a shell and debug interactively Boot a separate system image mount the sick systems filesystems andinvestigate from thereThe first option is the one most commonly used in the cloud but it can be helpfulon physical servers too as long as you have access to a recent image of the entireboot disk If your site does backups by filesystem a wholesystem restore may bemore trouble than its worth We discuss the wholesystem restore option in Recovery of cloud systems which starts on page The remaining two approaches focus on giving you a way to access the systemidentify the underlying issue and make whatever fix is needed Booting the ailingsystem to a shell is by far the preferable option but problems that occur very earlyin the boot sequence may stymie this approachThe boot to a shell mode is known generically as singleuser mode or rescue modeSystems that use systemd have an even more primitive option available in the formof emergency mode its conceptually similar to singleuser mode but does an absolute minimum of preparation before starting a shellBecause singleuser rescue and emergency modes dont configure the networkor start networkrelated services youll generally need physical access to the console to make use of them As a result singleuser mode normally isnt available forcloudhosted systems We review some options for reviving broken cloud imagesstarting on page Singleuser modeIn singleuser mode also known as rescuetarget on systems that use systemd onlya minimal set of processes daemons and services are started The root filesystemis mounted as is usr in most cases but the network remains uninitializedAt boot time you request singleuser mode by passing an argument to the kernelusually single or s You can do this through the boot loaders commandline interface In some cases it may be set up for you automatically as a boot menu optionIf the system is already running you can bring it down to singleuser mode with ashutdown FreeBSD telinit traditional init or systemctl systemd commandSane systems prompt for the root password before starting the singleuser rootshell Unfortunately this means that its virtually impossible to reset a forgottenroot password through singleuser mode If you need to reset the password youllhave to access the disk by way of separate boot mediaFrom the singleuser shell you can execute commands in much the same way aswhen logged in on a fully booted system However sometimes only the root partition is mounted you must mount other filesystems manually to use programs thatdont live in bin sbin or etcYou can often find pointers to the available filesystems by looking in etcfstab Under Linux you can run fdisk l lowercase L option to see a list of the local systemsdisk partitions The analogous procedure on FreeBSD is to run camcontrol devlistto identify disk devices and then run fdisk s device for each diskIn many singleuser environments the filesystem root directory starts off beingmounted readonly If etc is part of the root filesystem the usual case it will beimpossible to edit many important configuration files To fix this problem youllhave to begin your singleuser session by remounting in readwrite mode UnderLinux the command mount o rwremount usually does the trick On FreeBSD systems the remount option is implicit whenyou repeat an existing mount but youll need to explicitly specify the source device For example mount o rw devgptrootfs Singleuser mode in Red Hat and CentOS is a bit more aggressive than normal Bythe time you reach the shell prompt these systems have tried to mount all localfilesystems Although this default is usually helpful it can be problematic if youhave a sick filesystem In that case you can boot to emergency mode by addingsystemdunitemergencytarget to the kernel arguments from within the bootloader usually GRUB In this mode no local filesystems are mounted and only afew essential services are startedSee Chapter formore informationabout the root accountSee Chapter formore informationabout filesystemsand mountingRHELThe fsck command is run during a normal boot to check and repair filesystemsDepending on what filesystem youre using for the root you may need to run fsckmanually when you bring the system up in singleuser or emergency mode Seepage for more details about fsckSingleuser mode is just a waypoint on the normal booting path so you can terminatethe singleuser shell with exit or ControlD to continue with booting You canalso type ControlD at the password prompt to bypass singleuser mode entirelySingleuser mode on FreeBSDFreeBSD includes a singleuser option in its boot menu Boot Multi User Enter Boot Single User Escape to loader prompt RebootOptions Kernel defaultkernel of Configure Boot OptionsOne nice feature of FreeBSDs singleuser mode is that it asks you what programto use as the shell Just press Enter for binshIf you choose option Escape to loader prompt youll drop into a bootlevelcommandline environment implemented by FreeBSDs finalcommonstage bootloader loaderSingleuser mode with GRUBOn systems that use systemd you can boot into rescue mode by appendingsystemdunitrescuetarget to the end of the existing Linux kernel line At theGRUB splash screen highlight your desired kernel and press the e key to edit itsboot options Similarly for emergency mode use systemdunitemergencytargetHeres an example of a typical configurationlinux vmlinuzelx rootdevmapperrhelrhelrootro crashkernelauto rdlvmlvrhelrhelswap rdlvmlvrhelrhelrootrhgb quiet LANGenUSUTF systemdunitrescuetargetType ControlX to start the system after youve made your changesRecovery of cloud systemsIts inherent in the nature of cloud systems that you cant hook up a monitor or USBstick when boot problems occur Cloud providers do what they can to facilitateproblem solving but basic limitations remainSee Chapter for abroader introductionto cloud computingBackups are important for all systems but cloud servers are particularly easy tosnapshot Providers charge extra for backups but theyre inexpensive Be liberalwith your snapshots and youll always have a reasonable system image to fall backon at short noticeFrom a philosophical perspective youre probably doing something wrong if yourcloud servers require boottime debugging Pets and physical servers receive veterinary care when theyre sick but cattle get euthanized Your cloud servers arecattle replace them with knowngood copies when they misbehave Embracingthis approach helps you not only avoid critical failures but also facilitates scalingand system migrationThat said you will inevitably need to attempt to recover cloud systems or drives sowe briefly discuss that process belowWithin AWS singleuser and emergency modes are unavailable However ECfilesystems can be attached to other virtual servers if theyre backed by Elastic BlockStorage EBS devices This is the default for most EC instances so its likely thatyou can use this method if you need to Conceptually its similar to booting from aUSB drive so that you can poke around on a physical systems boot diskHeres what to do Launch a new instance in the same availability zone as the instance yourehaving issues with Ideally this recovery instance should be launchedfrom the same base image and should use the same instance type as thesick system Stop the problem instance But be careful not to terminate it thatoperation deletes the boot disk image With the AWS web console or CLI detach the volume from the problemsystem and attach the volume to the recovery instance Log in to the recovery system Create a mount point and mount the volume then do whatevers necessary to fix the issue Then unmount thevolume Wont unmount Make sure youre not cded there In the AWS console detach the volume from the recovery instance andreattach it to the problem instance Start the problem instance and hopefor the bestDigitalOcean droplets offer a VNCenabled console that you can access throughthe web although the web apps behavior is a bit wonky on some browsers DigitalOcean does not afford a way to detach storage devices and migrate them to arecovery system the way Amazon does Instead most system images let you bootfrom an alternate recovery kernelTo access the recovery kernel first power off the droplet and then mount the recovery kernel and reboot If all went well the virtual terminal will give you accessto a singleuserlike mode More detailed instructions for this process are availableat digitaloceancomBoot issues within a Google Compute Engine instance should first be investigatedby examination of the instances serial port information gcloud compute instances getserialportoutput instanceThe same information is available through GCP web consoleA diskshuffling process similar to that described above for the Amazon cloud isalso available on Google Compute Engine You use the CLI to remove the disk fromthe defunct instance and boot a new instance that mounts the disk as an addonfilesystem You can then run filesystem checks modify boot parameters and selecta new kernel if necessary This process is nicely detailed in Googles documentationat cloudgooglecomcomputedocstroubleshooting The recovery kernel is not available on all modern distributions If youre running a recent releaseand the recovery tab tells you that The kernel for this Droplet is managed internally and cannot bechanged from the control panel youll need to open a support ticket with DigitalOcean to have themassociate your instance with a recovery ISO allowing you to continue your recovery effortsThis chapter is about access control as opposed to security by which we meanthat it focuses on the mechanical details of how the kernel and its delegates makesecurityrelated decisions Chapter Security addresses the more general question of how to set up a system or network to minimize the chance of unwelcomeaccess by intrudersAccess control is an area of active research and it has long been one of the majorchallenges of operating system design Over the last decade UNIX and Linux haveseen a Cambrian explosion of new options in this domain A primary driver of thissurge has been the advent of kernel APIs that allow third party modules to augment or replace the traditional UNIX access control system This modular approachcreates a variety of new frontiers access control is now just as open to change andexperimentation as any other aspect of UNIXNevertheless the traditional system remains the UNIX and Linux standard andits adequate for the majority of installations Even for administrators who wantto venture into the new frontier a thorough grounding in the basics is essential Access Control andRootly Powers Standard UNIX access controlThe standard UNIX access control model has remained largely unchanged for decades With a few enhancements it continues to be the default for generalpurposeOS distributions The scheme follows a few basic rules Access control decisions depend on which user is attempting to perform anoperation or in some cases on that users membership in a UNIX group Objects eg files and processes have owners Owners have broad butnot necessarily unrestricted control over their objects You own the objects you create The special user account called root can act as the owner of any object Only root can perform certain sensitive administrative operationsCertain system calls eg settimeofday are restricted to root the implementationsimply checks the identity of the current user and rejects the operation if the useris not root Other system calls eg kill implement different calculations that involve both ownership matching and special provisions for root Finally filesystemshave their own access control systems which they implement in cooperation withthe kernels VFS layer These are generally more elaborate than the access controlsfound elsewhere in the kernel For example filesystems are much more likely tomake use of UNIX groups for access controlComplicating this picture is that the kernel and the filesystem are intimately intertwined For example you control and communicate with most devices throughfiles that represent them in dev Since device files are filesystem objects they aresubject to filesystem access control semantics The kernel uses that fact as its primary form of access control for devicesFilesystem access controlIn the standard model every file has both an owner and a group sometimes referredto as the group owner The owner can set the permissions of the file In particularthe owner can set them so restrictively that no one else can access it We talk moreabout file permissions in Chapter The Filesystem see page Although the owner of a file is always a single person many people can be groupowners of the file as long as they are all part of a single group Groups are traditionally defined in the etcgroup file but these days group information is oftenstored in a network database system such as LDAP see Chapter Single SignOn for details Keep in mind that we are here describing the original design of the access control system These daysnot all of these statements remain literally true For example a Linux process that bears appropriatecapabilities see page can now perform some operations that were previously restricted to rootSee page formore informationabout device filesSee page formore informationabout groupsThe owner of a file gets to specify what the group owners can do with it This schemeallows files to be shared among members of the same projectYou can determine the ownerships of a file with ls l ls l garthtodorwr garth staff May UsersgarthtodoThis file is owned by user garth and group staff The letters and dashes in the firstcolumn symbolize the permissions on the file see page for details on how todecode this information In this case the codes mean that garth can read or writethe file and that members of the staff group can read itBoth the kernel and the filesystem track owners and groups as numbers rather thanas text names In the most basic case user identification numbers UIDs for shortare mapped to usernames in the etcpasswd file and group identification numbers GIDs are mapped to group names in etcgroup See Chapter SingleSignOn for information about the more sophisticated optionsThe text names that correspond to UIDs and GIDs are defined only for the convenience of the systems human users When commands such as ls should displayownership information in a humanreadable format they must look up each namein the appropriate file or databaseProcess ownershipThe owner of a process can send the process signals see page and can alsoreduce degrade the processs scheduling priority Processes actually have multiple identities associated with them a real effective and saved UID a real effectiveand saved GID and under Linux a filesystem UID that is used only to determinefile access permissions Broadly speaking the real numbers are used for accountingnow largely vestigial and the effective numbers are used for the determination ofaccess permissions The real and effective numbers are normally the sameThe saved UID and GID are parking spots for IDs that are not currently in use butthat remain available for the process to invoke The saved IDs allow a program torepeatedly enter and leave a privileged mode of operation this precaution reducesthe risk of unintended misbehaviorThe filesystem UID is generally explained as an implementation detail of NFS theNetwork File System It is usually the same as the effective UIDThe root accountThe root account is UNIXs omnipotent administrative user Its also known as thesuperuser account although the actual username is rootThe defining characteristic of the root account is its UID of Nothing prevents youfrom changing the username on this account or from creating additional accountsSee Chapter formore informationabout the passwdand group filesSee Chapter formore about NFSwhose UIDs are however these are both bad ideas Such changes have a tendency to create inadvertent breaches of system security They also create confusionwhen other people have to deal with the strange way youve configured your systemTraditional UNIX allows the superuser that is any process for which the effectiveUID is to perform any valid operation on any file or processSome examples of restricted operations are Creating device files Setting the system clock Raising resource usage limits and process priorities Setting the systems hostname Configuring network interfaces Opening privileged network ports those numbered below Shutting down the systemAn example of superuser powers is the ability of a process owned by root to changeits UID and GID The login program and its GUI equivalents are a case in point theprocess that prompts you for your password when you log in to the system initiallyruns as root If the password and username that you enter are legitimate the loginprogram changes its UID and GID to your UID and GID and starts up your shellor GUI environment Once a root process has changed its ownerships to becomea normal user process it cant recover its former privileged stateSetuid and setgid executionTraditional UNIX access control is complemented by an identity substitution systemthats implemented by the kernel and the filesystem in collaboration This schemeallows specially marked executable files to run with elevated permissions usuallythose of root It lets developers and administrators set up structured ways for unprivileged users to perform privileged operationsWhen the kernel runs an executable file that has its setuid or setgid permissionbits set it changes the effective UID or GID of the resulting process to the UID orGID of the file containing the program image rather than the UID and GID of theuser that ran the command The users privileges are thus promoted for the execution of that specific command onlyFor example users must be able to change their passwords But since passwords aretraditionally stored in the protected etcmasterpasswd or etcshadow file users need a setuid passwd command to mediate their access The passwd command Jennine Townsend one of our stalwart technical reviewers commented Such bad ideas that I feareven mentioning them might encourage someone Valid is the operative word here Certain operations such as executing a file on which the executepermission bit is not set are forbidden even to the superuserchecks to see whos running it and customizes its behavior accordingly users canchange only their own passwords but root can change any passwordPrograms that run setuid especially ones that run setuid to root are prone to security problems The setuid commands distributed with the system are theoreticallysecure however security holes have been discovered in the past and will undoubtedly be discovered in the futureThe surest way to minimize the number of setuid problems is to minimize thenumber of setuid programs Think twice before installing software that needs torun setuid and avoid using the setuid facility in your own homegrown softwareNever use setuid execution on programs that were not explicitly written with setuid execution in mindYou can disable setuid and setgid execution on individual filesystems by specifyingthe nosuid option to mount Its a good idea to use this option on filesystems thatcontain users home directories or that are mounted from less trustworthy administrative domains Management of the root accountRoot access is required for system administration and its also a pivot point for system security Proper husbandry of the root account is a crucial skillRoot account loginSince root is just another user most systems let you log in directly to the root accountHowever this turns out to be a bad idea which is why Ubuntu forbids it by defaultTo begin with root logins leave no record of what operations were performed asroot Thats bad enough when you realize that you broke something last night at am and cant remember what you changed its even worse when an accesswas unauthorized and you are trying to figure out what an intruder has done toyour system Another disadvantage is that the loginasroot scenario leaves no record of who was actually doing the work If several people have access to the rootaccount you wont be able to tell who used it and whenFor these reasons most systems allow root logins to be disabled on terminalsthrough window systems and across the networkeverywhere but on the systemconsole We suggest that you use these features See PAM cooking spray or authentication wonder starting on page to see how to implement this policy on yourparticular systemIf root does have a password that is the root account is not disabled see page that password must be of high quality See page for some additionalcomments regarding password selectionSee page formore informationabout filesystemmount optionssu substitute user identityA marginally better way to access the root account is to use the su command Ifinvoked without arguments su prompts for the root password and then starts upa root shell Root privileges remain in effect until you terminate the shell by typingControlD or the exit command su doesnt record the commands executed asroot but it does create a log entry that states who became root and whenThe su command can also substitute identities other than root Sometimes the onlyway to reproduce or debug a users problem is to su to their account so that youreproduce the environment in which the problem occursIf you know someones password you can access that persons account directly byexecuting su username As with an su to root you are prompted for the passwordfor username The dash option makes su spawn the shell in login modeThe exact implications of login mode vary by shell but login mode normally changesthe number or identity of the files that the shell reads when it starts up For examplebash reads bashprofile in login mode and bashrc in nonlogin mode Whendiagnosing other users problems it helps to reproduce their login environmentsas closely as possible by running in login modeOn some systems the root password allows an su or login to any account On others you must first su explicitly to root before suing to another account root cansu to any account without entering a passwordGet in the habit of typing the full pathname to su eg binsu or usrbinsu rather than relying on the shell to find the command for you This precaution gives yousome protection against arbitrary programs called su that might have been sneakedinto your search path with the intention of harvesting passwordsOn most systems you must be a member of the group wheel to use suWe consider su to have been largely superseded by sudo described in the nextsection su is best reserved for emergencies Its also helpful for fixing situations inwhich sudo has been broken or misconfiguredsudo limited suWithout one of the advanced access control systems outlined starting on page its hard to enable someone to do one task backups for example without givingthat person free run of the system And if the root account is used by several administrators you really have only a vague idea of whos using it or what theyve doneThe most widely used solution to these problems is a program called sudo that iscurrently maintained by Todd Miller It runs on all our example systems and is For the same reason do not include the current directory in your shells search path which youcan see by typing echo PATH Although convenient including makes it easy to inadvertentlyrun special versions of system commands that an intruder has left lying around as a trap Naturallythis advice goes double for rootalso available in source code form from sudows We recommend it as the primarymethod of access to the root accountsudo takes as its argument a command line to be executed as root or as anotherrestricted user sudo consults the file etcsudoers usrlocaletcsudoers onFreeBSD which lists the people who are authorized to use sudo and the commandsthey are allowed to run on each host If the proposed command is permitted sudoprompts for the users own password and executes the commandAdditional sudo commands can be executed without the doer having to type apassword until a fiveminute period configurable has elapsed with no furthersudo activity This timeout serves as a modest protection against users with sudoprivileges who leave terminals unattendedsudo keeps a log of the command lines that were executed the hosts on which theywere run the people who ran them the directories from which they were run andthe times at which they were invoked This information can be logged by syslog orplaced in the file of your choice We recommend using syslog to forward the logentries to a secure central hostA log entry for randys executing sudo bincat etcsudoers might look like thisDec tigger sudo randy TTYttyp PWDtiggerusersrandyUSERroot COMMANDbincat etcsudoersExample configurationThe sudoers file is designed so that a single version can be used on many differenthosts at once Heres a typical example Define aliases for machines in CS Physics departmentsHostAlias CS tigger anchor piper moet sigiHostAlias PHYSICS eprince pprince icarus Define collections of commandsCmndAlias DUMP sbindump sbinrestoreCmndAlias WATCHDOG usrlocalbinwatchdogCmndAlias SHELLS binsh bindash binbash Permissionsmark ed PHYSICS ALLherb CS usrsbintcpdump PHYSICS operator DUMPlynda ALL ALL ALL SHELLSwheel ALL PHYSICS NOPASSWD WATCHDOGThe first two sets of lines define groups of hosts and commands that are referredto in the permission specifications later in the file The lists could be included literally in the specifications but aliases make the sudoers file easier to read and understand they also make the file easier to update in the future Its also possible todefine aliases for sets of users and for sets of users as whom commands may be runSee Chapter for more information about syslogEach permission specification line includes information about The users to whom the line applies The hosts on which the line should be heeded The commands that the specified users can run The users as whom the commands can be executedThe first permission line applies to the users mark and ed on the machines in thePHYSICS group eprince pprince and icarus The builtin command alias ALL allows them to run any command Since no list of users is specified in parenthesessudo will run commands as rootThe second permission line allows herb to run tcpdump on CS machines anddumprelated commands on PHYSICS machines However the dump commandscan be run only as operator not as root The actual command line that herb wouldtype would be something likeubuntu sudo u operator usrsbindump u devsdaThe user lynda can run commands as any user on any machine except that shecant run several common shells Does this mean that lynda really cant get a rootshell Of course notubuntu cp p binsh tmpshubuntu sudo tmpshGenerally speaking any attempt to allow all commands except is doomed tofailure at least in a technical sense However it might still be worthwhile to set upthe sudoers file this way as a reminder that root shells are strongly discouragedThe final line allows users in group wheel to run the local watchdog command asroot on all machines except eprince pprince and icarus Furthermore no passwordis required to run the commandNote that commands in the sudoers file are specified with full pathnames to preventpeople from executing their own programs and scripts as root Though no examples are shown above it is possible to specify the arguments that are permissiblefor each command as wellTo manually modify the sudoers file use the visudo command which checks to besure no one else is editing the file invokes an editor on it vi or whichever editoryou specify in your EDITOR environment variable and then verifies the syntaxof the edited file before installing it This last step is particularly important becausean invalid sudoers file might prevent you from sudoing again to fix itsudo pros and consThe use of sudo has the following advantages Accountability is much improved because of command logging Users can do specific chores without having unlimited root privileges The real root password can be known to only one or two people Using sudo is faster than using su or logging in as root Privileges can be revoked without the need to change the root password A canonical list of all users with root privileges is maintained The chance of a root shell being left unattended is lessened A single file can control access for an entire networksudo has a couple of disadvantages as well The worst of these is that any breach inthe security of a sudoers personal account can be equivalent to breaching the rootaccount itself You cant do much to counter this threat other than caution yoursudoers to protect their own accounts as they would the root account You can alsorun a password cracker regularly on sudoers passwords to ensure that they aremaking good password selections All the comments on password selection frompage apply here as wellsudos command logging can easily be subverted by tricks such as shell escapesfrom within an allowed program or by sudo sh and sudo su Such commands doshow up in the logs so youll at least know theyve been runsudo vs advanced access controlIf you think of sudo as a way of subdividing the privileges of the root account itis superior in some ways to many of the dropin access control systems outlinedstarting on page You decide exactly how privileges will be subdivided Your division can becoarser or finer than the privileges defined for you by an offtheshelf system Simple configurationsthe most commonare simple to set up maintain and understand sudo runs on all UNIX and Linux systems You do need not worry aboutmanaging different solutions on different platforms You can share a single configuration file throughout your site You get consistent highquality logging for freeBecause the system is vulnerable to catastrophic compromise if the root accountis penetrated a major drawback of sudobased access control is that the potentialattack surface expands to include the accounts of all administratorssudo works well as a tool for wellintentioned administrators who need generalaccess to root privileges Its also great for allowing nonadministrators to performa few specific operations Despite a configuration syntax that suggests otherwise itis unfortunately not a safe way to define limited domains of autonomy or to placecertain operations out of bounds Or even zero people if you have the right kind of password vault system in placeSee page for moreinformation aboutpassword crackingDont even attempt these configurations If you need this functionality you aremuch better off enabling one of the dropin access control systems described starting on page Typical setupsudos configuration system has accumulated a lot of features over the years It hasalso expanded to accommodate a variety of unusual situations and edge cases Asa result the current documentation conveys an impression of complexity that isntnecessarily warrantedSince its important that sudo be reliable and secure its natural to wonder if youmight be exposing your systems to additional risk if you dont make use of sudosadvanced features and set exactly the right values for all options The answer is no of sudoers files look something like thisUserAlias ADMINS alice bob charlesADMINS ALL ALL ALLThis is a perfectly respectable configuration and in many cases theres no need tocomplicate it further Weve mentioned a few extras you can play with in the sectionsbelow but theyre all problemsolving tools that are helpful for specific situationsNothing more is required for general robustnessEnvironment managementMany commands consult the values of environment variables and modify theirbehavior depending on what they find In the case of commands run as root thismechanism can be both a useful convenience and a potential route of attackFor example several commands run the program specified in your EDITOR environment variable to spawn a text editor If this variable points to a hackers malicious program instead of an editor its likely that youll eventually end up runningthat program as rootTo minimize this risk sudos default behavior is to pass only a minimal sanitizedenvironment to the commands that it runs If your site needs additional environment variables to be passed you can whitelist them by adding them to the sudoersfiles envkeep list For example the linesDefaults envkeep SSHAUTHSOCKDefaults envkeep DISPLAY XAUTHORIZATION XAUTHORITYpreserve several environment variables used by X Windows and by SSH key forwarding Just to be clear the scenario in this case is that your account has been compromised but the attackerdoes not know your actual password and so cannot run sudo directly Unfortunately this is a common situationall it takes is a terminal window left momentarily unattendedIts possible to set different envkeep lists for different users or groups but the configuration rapidly becomes complicated We suggest sticking to a single universal listand being relatively conservative with the exceptions you enshrine in the sudoers fileIf you need to preserve an environment variable that isnt listed in the sudoers fileyou can set it explicitly on the sudo command line For example the command sudo EDITORemacs vipwedits the system password file with emacs This feature has some potential restrictions but theyre waived for users who can run ALL commandssudo without passwordsIts distressingly common to see sudo set up to allow command execution as rootwithout the need to enter a password Just for reference that configuration is achievedwith the NOPASSWD keyword in the sudoers file For exampleansible ALL ALL NOPASSWD ALL Dont do thisSometimes this is done out of laziness but more typically the underlying need is toallow some type of unattended sudo execution The most common cases are whenperforming remote configuration through a system such as Ansible or when running commands out of cronNeedless to say this configuration is dangerous so avoid it if you can At the veryleast restrict passwordless execution to a specific set of commands if you canAnother option that works well in the context of remote execution is to replacemanually entered passwords with authentication through sshagent and forwardedSSH keys You can configure this method of authentication through PAM on theserver where sudo will actually runMost systems dont include the PAM module that implements SSHbased authentication by default but it is readily available Look for a pamsshagentauth packageSSH key forwarding has its own set of security concerns but its certainly an improvement over no authentication at allPrecedenceA given invocation of sudo might potentially be addressed by several entries in thesudoers file For example consider the following configurationUserAlias ADMINS alice bob charlesUserAlias MYSQLADMINS alice bobwheel ALL ALL ALLMYSQLADMINS ALL mysql NOPASSWD ALLADMINS ALL ALL NOPASSWD usrsbinlogrotateSee Chapter formore informationabout AnsibleSee page for moreinformation aboutPAM configurationHere administrators can run the logrotate command as any user without supplying a password MySQL administrators can run any command as mysql without apassword Anyone in the wheel group can run any command under any UID butmust authenticate with a password firstIf user alice is in the wheel group she is potentially covered by each of the last threelines How do you know which one will determine sudos behaviorThe rule is that sudo always obeys the last matching line with matching beingdetermined by the entire tuple of user host target user and command Each ofthose elements must match the configuration line or the line is simply ignoredTherefore NOPASSWD exceptions must follow their more general counterparts asshown above If the order of the last three lines were reversed poor alice would haveto type a password no matter what sudo command she attempted to runsudo without a control terminalIn addition to raising the issue of passwordless authentication unattended executionof sudo eg from cron often occurs without a normal control terminal Theresnothing inherently wrong with that but its an odd situation that sudo can checkfor and reject if the requiretty option is turned on in the sudoers fileThis option is not the default from sudos perspective but some OS distributionsinclude it in their default sudoers files so its worth checking for and removingLook for a line of the formDefaults requirettyand invert its valueDefaults requirettyThe requiretty option does offer a small amount of symbolic protection againstcertain attack scenarios However its easy to work around and so offers little realsecurity benefit In our opinion requiretty should be disabled as a matter of coursebecause it is a common source of problemsSitewide sudo configurationBecause the sudoers file includes the current host as a matching criterion for configuration lines you can use one master sudoers file throughout an administrativedomain that is a region of your site in which hostnames and user accounts areguaranteed to be nameequivalent This approach makes the initial sudoers setupa bit more complicated but its a great idea for multiple reasons You should do itThe main advantage of this approach is that theres no mystery about who has whatpermissions on what hosts Everything is recorded in one authoritative file Whenan administrator leaves your organization for example theres no need to trackdown all the hosts on which that user might have had sudo permissions Whenchanges are needed you simply modify the master sudoers file and redistribute itA natural corollary of this approach is that sudo permissions might be better expressed in terms of user accounts rather than UNIX groups For examplewheel ALL ALLhas some intuitive appeal but it defers the enumeration of privileged users to eachlocal machine You cant look at this line and determine whos covered by it without an excursion to the machine in question Since the idea is to keep all relevantinformation in one place its best to avoid this type of grouping option when sharing a sudoers file on a network Of course if your group memberships are tightlycoordinated sitewide its fine to use groupsDistribution of the sudoers file is best achieved through a broader system of configuration management as described in Chapter But if you havent yet reachedthat level of organization you can easily roll your own Be careful though installing a bogus sudoers file is a quick route to disaster This is also a good file to keepan eye on with a file integrity monitoring solution of some kind see page In the absence of a configuration management system its best to use a pull scriptthat runs out of cron on each host Use scp to copy the current sudoers file froma known central repository then validate it with visudo c f newsudoers beforeinstallation to verify that the format is acceptable to the local sudo scp checks theremote servers host key for you ensuring that the sudoers file is coming from thehost you intended and not from a spoofed serverHostname specifications can be a bit subtle when sharing the sudoers file By default sudo uses the output of the hostname command as the text to be matchedDepending on the conventions in use at your site this name may or may not include a domain portion eg anchor vs anchorcscoloradoedu In either casethe hostnames specified in the sudoers file must match the hostnames as they arereturned on each host You can turn on the fqdn option in the sudoers file to attempt to normalize local hostnames to their fully qualified formsHostname matching gets even stickier in the cloud where instance names often default to algorithmically generated patterns sudo understands simple patternmatching characters globbing in hostnames so consider adopting a namingscheme that incorporates some indication of each hosts security classification fromsudos perspectiveAlternatively you can use your cloud providers virtual networking features to segregate hosts by IP address and then match on IP addresses instead of hostnamesfrom within the sudoers fileDisabling the root accountIf your site standardizes on the use of sudo youll have surprisingly little use foractual root passwords Most of your administrative team will never have occasionto use themThat fact raises the question of whether a root password is necessary at all If youdecide that it isnt you can disable root logins entirely by setting roots encryptedpassword to or to some other fixed arbitrary string On Linux passwd l locksan account by prepending a to the encrypted password with equivalent resultsThe and the are just conventions no software checks for them explicitly Theireffect derives from their not being valid password hashes As a result attempts toverify roots password simply failThe main effect of locking the root account is that root cannot log in even on theconsole Neither can any user successfully run su because that requires a root password check as well However the root account continues to exist and all the softwarethat usually runs as root continues to do so In particular sudo works normallyThe main advantage of disabling the root account is that you neednt record andmanage roots password Youre also eliminating the possibility of the root passwordbeing compromised but thats more a pleasant side effect than a compelling reason to go passwordless Rarely used passwords are already at low risk of violationIts particularly helpful to have a real root password on physical computers as opposed to cloud or virtual instances see Chapters and Real computersare apt to require rescuing when hardware or configuration problems interferewith sudo or the boot process In these cases its nice to have the traditional rootaccount available as an emergency fallbackUbuntu ships with the root account locked and all administrative access is funneledthrough sudo or a GUI equivalent If you prefer its fine to set a root password onUbuntu and then unlock the account with sudo passwd u rootSystem accounts other than rootRoot is generally the only user that has special status in the eyes of the kernel butseveral other pseudousers are defined by most systems You can identify thesesham accounts by their low UIDs usually less than Most often UIDs under are system accounts and UIDs between and are pseudousers associatedwith specific pieces of softwareIts customary to replace the encrypted password field of these special users in theshadow or masterpasswd file with a star so that their accounts cannot be loggedin to Their shells should be set to binfalse or binnologin as well to protectagainst remote login exploits that use password alternatives such as SSH key filesAs with user accounts most systems define a variety of systemrelated groups thathave similarly low GIDsSee page formore informationabout shadow andmasterpasswdFiles and processes that are part of the operating system but that need not be ownedby root are sometimes assigned to the users bin or daemon The theory was thatthis convention would help avoid the security hazards associated with ownershipby root Its not a compelling argument however and current systems often justuse the root account insteadThe main advantage of defining pseudoaccounts and pseudogroups is that theycan be used more safely than the root account to provide access to defined groupsof resources For example databases often implement elaborate access control systems of their own From the perspective of the kernel they run as a pseudousersuch as mysql that owns all databaserelated resourcesThe Network File System NFS uses an account called nobody to represent rootusers on other systems For remote roots to be stripped of their rootly powers theremote UID has to be mapped to something other than the local UID Thenobody account acts as a generic alter ego for these remote roots In NFSv thenobody account can be applied to remote users that correspond to no valid localaccount as wellSince the nobody account is supposed to represent a generic and relatively powerless user it shouldnt own any files If nobody does own files remote roots cantake control of them Extensions to the standard access control modelThe preceding sections outline the major concepts of the traditional access controlmodel Even though this model can be summarized in a couple of pages it has stoodthe test of time because its simple predictable and capable of handling the requirements of the average site All UNIX and Linux variants continue to support thismodel and it remains the default approach and the one thats most widely used todayAs actually implemented and shipped on modern operating systems the modelincludes a number of important refinements Three layers of software contributeto the current status quo The standard model as described to this point Extensions that generalize and finetune this basic model Kernel extensions that implement alternative approachesThese categories are not architectural layers so much as historical artifacts EarlyUNIX derivatives all used the standard model but its deficiencies were widely recognized even then Over time the community began to develop workarounds fora few of the more pressing issues In the interest of maintaining compatibility andencouraging widespread adoption changes were usually structured as refinementsof the traditional system Some of these tweaks eg PAM are now consideredUNIX standardsSee page for moreinformation aboutthe nobody accountOver the last decade great strides have been made toward modularization of access control systems This evolution enables even more radical changes to accesscontrol Weve reviewed some of the more common pluggable options for Linuxand FreeBSD starting on page For now we look at some of the more prosaic extensions that are bundled withmost systems First we consider the problems those extensions attempt to addressDrawbacks of the standard modelDespite its elegance the standard model has some obvious shortcomings To begin with the root account represents a potential single point of failure If its compromised the integrity of the entire system is violated andthere is essentially no limit to the damage an attacker can inflict The only way to subdivide the privileges of the root account is to writesetuid programs Unfortunately as the steady flow of securityrelated software updates demonstrates its difficult to write secure software Everysetuid program is a potential target The standard model has little to say about security on a network No computer to which an unprivileged user has physical access can be trusted toaccurately represent the ownerships of the processes its running Whosto say that someone hasnt reformatted the disk and installed their ownoperating system with UIDs of their choosing In the standard model group definition is a privileged operation Forexample theres no way for a generic user to express the intent that onlyalice and bob should have access to a particular file Because many access control rules are embedded in the code of individualcommands and daemons the classic example being the passwd programyou cannot redefine the systems behavior without modifying the sourcecode and recompiling In the real world thats impractical and error prone The standard model also has little or no support for auditing or loggingYou can see which UNIX groups a user belongs to but you cant necessarily determine what those group memberships permit a user to do Inaddition theres no real way to track the use of elevated privileges or tosee what operations they have performedPAM Pluggable Authentication ModulesUser accounts are traditionally secured by passwords stored in encrypted formin the etcshadow or etcmasterpasswd file or an equivalent network databaseMany programs may need to validate accounts including login sudo su and anyprogram that accepts logins on a GUI workstationSee page formore informationabout the shadow andmasterpasswd filesThese programs really shouldnt have hardcoded expectations about how passwordsare to be encrypted or verified Ideally they shouldnt even assume that passwordsare in use at all What if you want to use biometric identification a network identity system or some kind of twofactor authentication Pluggable AuthenticationModules to the rescuePAM is a wrapper for a variety of methodspecific authentication libraries Administrators specify the authentication methods they want the system to use along withthe appropriate contexts for each one Programs that require user authenticationsimply call the PAM system rather than implement their own forms of authenticationPAM in turn calls the authentication library specified by the system administratorStrictly speaking PAM is an authentication technology not an access control technology That is instead of addressing the question Does user X have permissionto perform operation Y it helps answer the precursor question How do I knowthis is really user XPAM is an important component of the access control chain on most systems andPAM configuration is a common administrative task You can find more details onPAM in the Single SignOn chapter starting on page Kerberos network cryptographic authenticationLike PAM Kerberos deals with authentication rather than access control per se Butwhereas PAM is an authentication framework Kerberos is a specific authenticationmethod At sites that use Kerberos PAM and Kerberos generally work together PAMbeing the wrapper and Kerberos the actual implementationKerberos uses a trusted third party a server to perform authentication for an entirenetwork You dont authenticate yourself to the machine you are using but provideyour credentials to the Kerberos service Kerberos then issues cryptographic credentials that you can present to other services as evidence of your identityKerberos is a mature technology that has been in widespread use for decades Itsthe standard authentication system used by Windows and is part of MicrosoftsActive Directory system Read more about Kerberos starting on page Filesystem access control listsSince filesystem access control is so central to UNIX and Linux it was an early target for elaboration The most common addition has been support for access controllists ACLs a generalization of the traditional usergroupother permission modelthat permits permissions to be set for multiple users and groups at onceACLs are part of the filesystem implementation so they have to be explicitly supported by whatever filesystem you are using However all major UNIX and Linuxfilesystems now support ACLs in one form or anotherACL support generally comes in one of two forms an early POSIX draft standardthat never quite made its way to formal adoption but was widely implementedanyway and the system standardized by NFSv which adapts Microsoft WindowsACLs Both ACL standards are described in more detail in the filesystem chapterstarting on page Linux capabilitiesCapability systems divide the powers of the root account into a handful ofseparate permissionsThe Linux version of capabilities derives from the defunct POSIX e draftwhich totters on despite never having been formally approved as a standard In addition to bearing this zombie stigma Linux capabilities raise the hackles of theoristsbecause of nonconformance to the academic conception of a capability system Nomatter theyre here and Linux calls them capabilities so we will tooCapabilities can be inherited from a parent process They can also be enabled ordisabled by attributes set on an executable file in a process reminiscent of setuidexecution Processes can renounce capabilities that they dont plan to useThe traditional powers of root are simply the union of all possible capabilities sotheres a fairly direct mapping between the traditional model and the capabilitymodel The capability model is just more granularAs an example the Linux capability called CAPNETBINDSERVICE controlsa processs ability to bind to privileged network ports those numbered under Some daemons that traditionally run as root need only this one particularsuperpower In the capability world such a daemon can theoretically run as anunprivileged user and pick up the portbinding capability from its executable fileAs long as the daemon does not explicitly check to be sure that its running as rootit neednt even be capability awareIs all this actually done in the real world Well no As it happens capabilities haveevolved to become more an enabling technology than a userfacing system Theyrewidely employed by higherlevel systems such as AppArmor see page andDocker see Chapter but are rarely used on their ownFor administrators its helpful to review the capabilities man page just to get asense of whats included in each of the capability bucketsLinux namespacesLinux can segregate processes into hierarchical partitions namespaces from whichthey see only a subset of the systems files network ports and processes Amongother effects this scheme acts as a form of preemptive access control Instead ofhaving to base access control decisions on potentially subtle criteria the kernelsimply denies the existence of objects that are not visible from inside a given boxSee Chapter TheNetwork File Systemfor more information about NFSInside a partition normal access control rules apply and in most cases jailed processes are not even aware that they have been confined Because confinement is irreversible processes can run as root within a partition without fear that they mightendanger other parts of the systemThis clever trick is one of the foundations of software containerization and its bestknown implementation Docker The full system is a lot more sophisticated andincludes extensions such as copyonwrite filesystem access We have quite a bitmore to say about containers in Chapter As a form of access control namespacing is a relatively coarse approach The construction of properly configured nests for processes to live in is also somewhat trickyCurrently this technology is applied primarily to addon services as opposed tointrinsic components of the operating system Modern access controlGiven the worlds wide range of computing environments and the mixed successof efforts to advance the standard model kernel maintainers have been reluctantto act as mediators in the larger debate over access control In the Linux world thesituation came to a head in when the US National Security Agency proposed to integrate its SecurityEnhanced Linux SELinux system into the kernelas a standard facilityFor several reasons the kernel maintainers resisted this merge Instead of adoptingSELinux or another alternative system they developed the Linux Security ModulesAPI a kernellevel interface that allows access control systems to integrate themselves as loadable kernel modulesLSMbased systems have no effect unless users load them and turn them on Thisfact lowers the barriers for inclusion in the standard kernel and Linux now shipswith SELinux and four other systems AppArmor Smack TOMOYO and Yamaready to goDevelopments on the BSD side have roughly paralleled those of Linux thankslargely to Robert Watsons work on TrustedBSD This code has been included inFreeBSD since version It also provides the application sandboxing technologyused in Apples macOS and iOSWhen multiple access control modules are active simultaneously an operationmust be approved by all of them to be permitted Unfortunately the LSM systemrequires explicit cooperation among active modules and none of the current modules include this feature For now Linux systems are effectively limited to a choiceof one LSM addon moduleSeparate ecosystemsAccess control is inherently a kernellevel concern With the exception of filesystemaccess control lists see page there is essentially no standardization among systems with regard to alternative access control mechanisms As a result every kernelhas its own array of available implementations and none of them are crossplatformBecause Linux distributions share a common kernel lineage all Linux distributionsare theoretically compatible with all the various Linux security offerings But inpractical terms theyre not these systems all need userlevel support in the formof additional commands modifications to userlevel components and securementprofiles for daemons and services Ergo every distribution has only one or two access control mechanisms that it actively supports if thatMandatory access controlThe standard UNIX model is considered a form of discretionary access controlbecause it allows the owners of accesscontrolled entities to set the permissions onthem For example you might allow other users to view the contents of your homedirectory or you might write a setuid program that lets other people send signalsto your processesDiscretionary access control provides no particular guarantee of security for userlevel data The downside of letting users set permissions is that users can setpermissions theres no telling what they might do with their own files And evenwith the best intentions and training users can make mistakesMandatory access control aka MAC systems let administrators write access controlpolicies that override or supplement the discretionary permissions of the traditionalmodel For example you might establish the rule that users home directories areaccessible only by their owners It then doesnt matter if a user makes a private copyof a sensitive document and is careless with the documents permissions nobodyelse can see into that users home directory anywayMAC capabilities are an enabling technology for implementing security models suchas the Department of Defenses multilevel security system In this model securitypolicies control access according to the perceived sensitivity of the resources beingcontrolled Users are assigned a security classification from a structured hierarchyThey can read and write items at the same classification level or lower but cannot access items at a higher classification For example a user with secret access can readand write secret objects but cannot read objects that are classified as top secretUnless youre handling sensitive data for a government entity it is unlikely that youwill ever encounter or need to deploy such comprehensive foreign security models More commonly MAC is used to protect individual services and it otherwisestays out of users wayA wellimplemented MAC policy relies on the principle of least privilege allowing access only when necessary much as a properly designed firewall allows onlyspecifically recognized services and clients to pass MAC can prevent software withcode execution vulnerabilities eg buffer overflows from compromising the system by limiting the scope of the breach to the few specific resources required bythat softwareMAC has unfortunately become something of a buzzword synonymous with advanced access control Even FreeBSDs generic security API is called the MAC interface despite the fact that some plugins offer no actual MAC featuresAvailable MAC systems range from wholesale replacements for the standard modelto lightweight extensions that address specific domains and use cases The commonthread among MAC implementations is that they generally add centralized administratorwritten or vendorsupplied policies into the access control system alongwith the usual mix of file permissions access controls lists and process attributesRegardless of scope MAC represents a potentially significant departure from thestandard system and its one that programs expecting to deal with the standardUNIX security model may find surprising Before committing to a fullscale MACdeployment make sure you understand the modules logging conventions and knowhow to identify and troubleshoot MACrelated problemsRolebased access controlAnother feature commonly namechecked by access control systems is rolebasedaccess control aka RBAC a theoretical model formalized in by David Ferraioloand Rick Kuhn The basic idea is to add a layer of indirection to access control calculations Permissions instead of being assigned directly to users are assigned tointermediate constructs known as roles and roles in turn are assigned to usersTo make an access control decision the system enumerates the roles of the current user and checks to see if any of those roles have the appropriate permissionsRoles are similar in concept to UNIX groups but theyre more general because theycan be used outside the context of the filesystem Roles can also have a hierarchicalrelationship to one another a fact that greatly simplifies administration For example you might define a senior administrator role that has all the permissions ofan administrator plus the additional permissions X Y and ZMany UNIX variants including Solaris HPUX and AIX include some form ofbuiltin RBAC system Linux and FreeBSD have no distinct native RBAC facilityHowever it is built into several of the more comprehensive MAC optionsSELinux SecurityEnhanced LinuxSELinux is one of the oldest Linux MAC implementations and is a product of theUS National Security Agency Depending on ones perspective that might be asource of either comfort or suspicion If your tastes incline toward suspicion its worth noting that as part of the Linux kernel distributionthe SELinux code base is open to inspectionSELinux takes a maximalist approach and it implements pretty much every flavorof MAC and RBAC one might envision Although it has gained footholds in a fewdistributions it is notoriously difficult to administer and troubleshoot This unattributed quote from a former version of the SELinux Wikipedia page vents thefrustration felt by many sysadminsIntriguingly although the stated raison dtre of SELinux is to facilitatethe creation of individualized access control policies specifically attuned toorganizational data custodianship practices and rules the supportive software tools are so sparse and unfriendly that the vendors survive chiefly onconsulting which typically takes the form of incremental modifications toboilerplate security policiesDespite its administrative complexity SELinux adoption has been slowly growingparticularly in environments such as government finance and health care thatenforce strong and specific security requirements Its also a standard part of theAndroid platformOur general opinion regarding SELinux is that its capable of delivering moreharm than benefit Unfortunately that harm can manifest not only as wasted timeand aggravation for system administrators but also ironically as security lapsesComplex models are hard to reason about and SELinux isnt really a level playingfield hackers that focus on it understand the system far more thoroughly than theaverage sysadminIn particular SELinux policy development is a complicated endeavor To protect anew daemon for example a policy must carefully enumerate all the files directories and other objects to which the process needs access For complicated softwarelike sendmail or httpd this task can be quite complex At least one company offersa threeday class on policy developmentFortunately many general policies are available online and most SELinuxenableddistributions come with reasonable defaults These can easily be installed and configured for your particular environment A fullblown policy editor that aims toease policy application can be found at seeditsourceforgenetSELinux is well supported by both Red Hat and hence CentOS and Fedora RedHat enables it by defaultDebian and SUSE Linux also have some available support for SELinux but youmust install additional packages and the system is less aggressive in its defaultconfigurationUbuntu inherits some SELinux support from Debian but over the last few releasesUbuntus focus has been on AppArmor see page Some vestigial SELinuxrelated packages are still available but they are generally not up to dateRHELetcselinuxconfig is the toplevel control for SELinux The interesting lines areSELINUXenforcingSELINUXTYPEtargetedThe first line has three possible values enforcing permissive or disabled Theenforcing setting ensures that the loaded policy is applied and prohibits violationspermissive allows violations to occur but logs them through syslog which is valuable for debugging and policy development disabled turns off SELinux entirelySELINUXTYPE refers to the name of the policy database to be applied This is essentially the name of a subdirectory within etcselinux Only one policy can be activeat a time and the available policy sets vary by systemRed Hats default policy is targeted which defines additional security for a few daemons that Red Hat has explicitly protected but leaves the rest of the system aloneThere used to be a separate policy called strict that applied MAC to the entire system but that policy has now been merged into targeted Remove the unconfinedand unconfineduser modules with semodule d to achieve fullsystem MACRed Hat also defines an mls policy that implements DoDstyle multilevel securityYou must install it separately with yum install selinuxpolicymlsIf youre interested in developing your own SELinux policies check out the auditallowutility It builds policy definitions from logs of violations The idea is to permissivelyprotect a subsystem so that its violations are logged but not enforced You can thenput the subsystem through its paces and build a policy that allows everything thesubsystem actually did Unfortunately its hard to guarantee complete coverage ofall code paths with this sort of ad hoc approach so the autogenerated profiles areunlikely to be perfectAppArmorAppArmor is a product of Canonical Ltd releasers of the Ubuntu distributionIts supported by Debian and Ubuntu but has also been adopted as a standard bySUSE distributions Ubuntu and SUSE enable it on default installs although thecomplement of protected services is not extensiveAppArmor implements a form of MAC and is intended as a supplement to the traditional UNIX access control system Although any configuration is possible AppArmor is not designed to be a userfacing system Its main goal is service securement that is limiting the damage that individual programs can do if they shouldbe compromised or run amokProtected programs continue to be subject to all the limitations imposed by thestandard model but in addition the kernel filters their activities through a designated and taskspecific AppArmor profile By default AppArmor denies all requests so the profile must explicitly name everything the process is allowed to doRHELPrograms without profiles such as user shells have no special restrictions and runas if AppArmor were not installedThis service securement role is essentially the same configuration thats implementedby SELinux in Red Hats targeted environment However AppArmor is designedmore specifically for service securement so it sidesteps some of the more puzzlingnuances of SELinuxAppArmor profiles are stored in etcapparmord and theyre relatively readableeven without detailed knowledge of the system For example heres the profile forthe cupsbrowsed daemon part of the printing system on Ubuntuinclude tunablesglobalusrsbincupsbrowsed include abstractionsbaseinclude abstractionsnameserviceinclude abstractionscupsclientinclude abstractionsdbusinclude abstractionspkitetccupscupsbrowsedconf retccupslpoptions rvarruncupscerts rvarcachecups rwtmp rw Sitespecific additions and overrides See localREADMEinclude localusrsbincupsbrowsedMost of this code is modular boilerplate For example this daemon needs to perform hostname lookups so the profile interpolates abstractionsnameservicewhich gives access to name resolution libraries etcnsswitchconf etchosts thenetwork ports used with LDAP and so onThe profiling information thats specific to this daemon consists in this case of alist of files the daemon can access along with the permissions allowed on each fileThe pattern matching syntax is a bit idiosyncratic can match multiple pathnamecomponents and var matches whether var appears at that location or notEven this simple profile is quite complex under the hood With all the includeinstructions expanded the profile is nearly lines long And we chose this example for its brevity YikesAppArmor refers to files and programs by pathname which makes profiles readable and independent of any particular filesystem implementation This approachis something of a compromise however For example AppArmor doesnt recognizehard links as pointing to the same underlying entity Recommended readingFerraiolo David F D Richard Kuhn and Ramaswamy Chandramouli RoleBased Access Control nd Edition Boston MA Artech House Haines Richard The SELinux Notebook th Edition This compendiumof SELinuxrelated information is the closest thing to official documentation Itsavailable for download from freecomputerbookscomVermeulen Sven SELinux Cookbook Birmingham UK Packt Publishing This book includes a variety of practical tips for dealing with SELinux It coversboth service securement and userfacing security modelsA process represents a running program Its the abstraction through which memoryprocessor time and IO resources can be managed and monitoredIt is an axiom of the UNIX philosophy that as much work as possible be done within the context of processes rather than being handled specially by the kernel System and user processes follow the same rules so you can use a single set of toolsto control them both Components of a processA process consists of an address space and a set of data structures within the kernel The address space is a set of memory pages that the kernel has marked for theprocesss use These pages contain the code and libraries that the process is executing the processs variables its stacks and various extra information needed by thekernel while the process is running The processs virtual address space is laid outrandomly in physical memory and tracked by the kernels page tables Pages are the units in which memory is managed They are usually KiB or KiB in size Process ControlThe kernels internal data structures record various pieces of information abouteach process Here are some of the more important of these The processs address space map The current status of the process sleeping stopped runnable etc The execution priority of the process Information about the resources the process has used CPU memory etc Information about the files and network ports the process has opened The processs signal mask a record of which signals are blocked The owner of the processA thread is an execution context within a process Every process has at least onethread but some processes have many Each thread has its own stack and CPUcontext but operates within the address space of its enclosing processModern computer hardware includes multiple CPUs and multiple cores per CPUA processs threads can run simultaneously on different cores Multithreaded applications such as BIND and Apache benefit quite a bit from this architecture becauseit lets them farm out requests to individual threadsMany of the parameters associated with a process directly affect its execution theamount of processor time it gets the files it can access and so on In the followingsections we discuss the meaning and significance of the parameters that are mostinteresting from a system administrators point of view These attributes are common to all versions of UNIX and LinuxPID process ID numberThe kernel assigns a unique ID number to every process Most commands andsystem calls that manipulate processes require you to specify a PID to identify thetarget of the operation PIDs are assigned in order as processes are createdLinux now defines the concept of process namespaces which further restrictprocesses ability to see and affect each other Container implementations use thisfeature to keep processes segregated One side effect is that a process might appearto have different PIDs depending on the namespace of the observer Its kind oflike Einsteinian relativity for process IDs Refer to Chapter Containers formore informationPPID parent PIDNeither UNIX nor Linux has a system call that initiates a new process running aparticular program Instead its done in two separate steps First an existing processmust clone itself to create a new process The clone can then exchange the programits running for a different oneWhen a process is cloned the original process is referred to as the parent and thecopy is called the child The PPID attribute of a process is the PID of the parentfrom which it was clonedThe parent PID is a useful piece of information when youre confronted with anunrecognized and possibly misbehaving process Tracing the process back to itsorigin whether that is a shell or some other program may give you a better ideaof its purpose and significanceUID and EUID real and effective user IDA processs UID is the user identification number of the person who created it ormore accurately it is a copy of the UID value of the parent process Usually onlythe creator aka the owner and the superuser can manipulate a processThe EUID is the effective user ID an extra UID that determines what resourcesand files a process has permission to access at any given moment For most processesthe UID and EUID are the same the usual exception being programs that are setuidWhy have both a UID and an EUID Simply because its useful to maintain a distinction between identity and permission and because a setuid program might not wishto operate with expanded permissions all the time On most systems the effectiveUID can be set and reset to enable or restrict the additional permissions it grantsMost systems also keep track of a saved UID which is a copy of the processsEUID at the point at which the process first begins to execute Unless the processtakes steps to obliterate this saved UID it remains available for use as the real oreffective UID A conservatively written setuid program can therefore renounce itsspecial privileges for the majority of its execution and access them only at the pointswhere extra privileges are neededLinux also defines a nonstandard FSUID process parameter that controls the determination of filesystem permissions It is infrequently used outside the kernel andis not portable to other UNIX systemsGID and EGID real and effective group IDThe GID is the group identification number of a process The EGID is related tothe GID in the same way that the EUID is related to the UID in that it can be upgraded by the execution of a setgid program As with the saved UID the kernelmaintains a saved GID for each processThe GID attribute of a process is largely vestigial For purposes of access determination a process can be a member of many groups at once The complete grouplist is stored separately from the distinguished GID and EGID Determinations ofaccess permissions normally take into account the EGID and the supplementalgroup list but not the GID itself At least initially If the original parent dies init or systemd process becomes the new parent Seepage See page formore informationabout UIDsSee page for moreinformation aboutsetuid executionSee page formore informationabout groupsThe only time at which the GID is actually significant is when a process createsnew files Depending on how the filesystem permissions have been set new filesmight default to adopting the GID of the creating process See page for detailsNicenessA processs scheduling priority determines how much CPU time it receives Thekernel computes priorities with a dynamic algorithm that takes into account theamount of CPU time that a process has recently consumed and the length of timeit has been waiting to run The kernel also pays attention to an administratively setvalue thats usually called the nice value or niceness so called because it specifieshow nice you are planning to be to other users of the system We discuss nicenessin detail on page Control terminalMost nondaemon processes have an associated control terminal The control terminal determines the default linkages for the standard input standard output andstandard error channels It also distributes signals to processes in response to keyboard events such as ControlC see the discussion starting on page Of course actual terminals are rare outside of computer museums these days Nevertheless they live on in the form of pseudoterminals which are still widely usedthroughout UNIX and Linux systems When you start a command from the shellfor example your terminal window typically becomes the processs control terminal The life cycle of a processTo create a new process a process copies itself with the fork system call fork creates a copy of the original process and that copy is largely identical to the parentThe new process has a distinct PID and has its own accounting informationfork has the unique property of returning two different values From the childspoint of view it returns zero The parent receives the PID of the newly created childSince the two processes are otherwise identical they must both examine the returnvalue to figure out which role they are supposed to playAfter a fork the child process often uses one of the exec family of routines to beginthe execution of a new program These calls change the program that the processis executing and reset the memory segments to a predefined initial state The various forms of exec differ only in the ways in which they specify the commandlinearguments and environment to be given to the new programWhen the system boots the kernel autonomously creates and installs several processes The most notable of these is init or systemd which is always process num Technically Linux systems use clone a superset of fork that handles threads and includes additionalfeatures fork remains in the kernel for backward compatibility but calls clone behind the scenesSee page for moreinformation aboutthe standard communication channelsber This process executes the systems startup scripts although the exact mannerin which this is done differs slightly between UNIX and Linux All processes otherthan the ones the kernel creates are descendants of this primordial process SeeChapter Booting and System Management Daemons for more informationabout booting and the various flavors of init daemoninit or systemd also plays another important role in process management When aprocess completes it calls a routine named exit to notify the kernel that it is readyto die It supplies an exit code an integer that tells why its exiting By conventionzero indicates a normal or successful terminationBefore a dead process can be allowed to disappear completely the kernel requiresthat its death be acknowledged by the processs parent which the parent does witha call to wait The parent receives a copy of the childs exit code or if the child didnot exit voluntarily an indication of why it was killed and can also obtain a summary of the childs resource use if it wishesThis scheme works fine if parents outlive their children and are conscientious aboutcalling wait so that dead processes can be disposed of If a parent dies before itschildren however the kernel recognizes that no wait is forthcoming The kerneladjusts the orphan processes to make them children of init or systemd which politely performs the wait needed to get rid of them when they dieSignalsSignals are processlevel interrupt requests About thirty different kinds are definedand theyre used in a variety of ways They can be sent among processes as a means of communication They can be sent by the terminal driver to kill interrupt or suspend processes when keys such as ControlC and ControlZ are pressed They can be sent by an administrator with kill to achieve various ends They can be sent by the kernel when a process commits an infraction suchas division by zero They can be sent by the kernel to notify a process of an interesting condition such as the death of a child process or the availability of data onan IO channelWhen a signal is received one of two things can happen If the receiving processhas designated a handler routine for that particular signal the handler is called withinformation about the context in which the signal was delivered Otherwise thekernel takes some default action on behalf of the process The default action variesfrom signal to signal Many signals terminate the process some also generate coredumps if core dumps have not been disabled The functions of ControlZ and ControlC can be reassigned to other keys with the stty command but this is rare in practice In this chapter we refer to them by their conventional bindingsA core dump is acopy of a processsmemory image whichis sometimes usefulfor debuggingSpecifying a handler routine for a signal is referred to as catching the signal Whenthe handler completes execution restarts from the point at which the signal wasreceivedTo prevent signals from arriving programs can request that they be either ignoredor blocked A signal that is ignored is simply discarded and has no effect on theprocess A blocked signal is queued for delivery but the kernel doesnt require theprocess to act on it until the signal has been explicitly unblocked The handler fora newly unblocked signal is called only once even if the signal was received severaltimes while reception was blockedTable lists some signals that administrators should be familiar with The uppercase convention for the names derives from C language tradition You mightalso see signal names written with a SIG prefix eg SIGHUP for similar reasonsTable Signals every administrator should knowa b Name Description DefaultCancatchCanblockDumpcore HUP Hangup Terminate Yes Yes No INT Interrupt Terminate Yes Yes No QUIT Quit Terminate Yes Yes Yes KILL Kill Terminate No No No BUS Bus error Terminate Yes Yes Yes SEGV Segmentation fault Terminate Yes Yes Yes TERM Software termination Terminate Yes Yes No STOP Stop Stop No No No TSTP Keyboard stop Stop Yes Yes No CONT Continue after stop Ignore Yes No No WINCH Window changed Ignore Yes Yes No USR Userdefined Terminate Yes Yes No USR Userdefined Terminate Yes Yes Noa A list of signal names and numbers is also available from the bash builtin command kill lb May vary on some systems See usrincludesignalh or man signal for more informationOther signals not shown in Table mostly report obscure errors such as illegalinstruction The default handling for such signals is to terminate with a core dumpCatching and blocking are generally allowed because some programs are smartenough to try to clean up whatever problem caused the error before continuingThe BUS and SEGV signals are also error signals Weve included them in the tablebecause theyre so common when a program crashes its usually one of these twosignals that finally brings it down By themselves the signals are of no specific diagnostic value Both of them indicate an attempt to use or access memory improperlyThe signals named KILL and STOP cannot be caught blocked or ignored TheKILL signal destroys the receiving process and STOP suspends its execution until a CONT signal is received CONT can be caught or ignored but not blockedTSTP is a soft version of STOP that might be best described as a request to stopIts the signal generated by the terminal driver when ControlZ is typed on thekeyboard Programs that catch this signal usually clean up their state then sendthemselves a STOP signal to complete the stop operation Alternatively programscan ignore TSTP to prevent themselves from being stopped from the keyboardThe signals KILL INT TERM HUP and QUIT all sound as if they mean approximately the same thing but their uses are actually quite different Its unfortunatethat such vague terminology was selected for them Heres a decoding guide KILL is unblockable and terminates a process at the kernel level A process can never actually receive or handle this signal INT is sent by the terminal driver when the user presses ControlC Itsa request to terminate the current operation Simple programs should quitif they catch the signal or simply allow themselves to be killed whichis the default if the signal is not caught Programs that have interactivecommand lines such as shells should stop what theyre doing clean upand wait for user input again TERM is a request to terminate execution completely Its expected thatthe receiving process will clean up its state and exit HUP has two common interpretations First its understood as a resetrequest by many daemons If a daemon is capable of rereading its configuration file and adjusting to changes without restarting a HUP cangenerally trigger this behaviorSecond HUP signals are sometimes generated by the terminal driver inan attempt to clean up ie kill the processes attached to a particularterminal This behavior is largely a holdover from the days of wired terminals and modem connections hence the name hangup Shells in the C shell family tcsh et al usually make background processes immune to HUP signals so that they can continue to run after theuser logs out Users of Bourneish shells ksh bash etc can emulate thisbehavior with the nohup command QUIT is similar to TERM except that it defaults to producing a core dumpif not caught A few programs cannibalize this signal and interpret it tomean something elseThe signals USR and USR have no set meaning Theyre available for programsto use in whatever way theyd like For example the Apache web server interpretsa HUP signal as a request for an immediate restart A USR signal initiates a moregraceful transition in which existing client conversations are allowed to finishkill send signalsAs its name implies the kill command is most often used to terminate a processkill can send any signal but by default it sends a TERM kill can be used by normalusers on their own processes or by root on any process The syntax iskill signal pidwhere signal is the number or symbolic name of the signal to be sent as shown inTable and pid is the process identification number of the target processA kill without a signal number does not guarantee that the process will die becausethe TERM signal can be caught blocked or ignored The command kill pidguarantees that the process will die because signal KILL cannot be caught Usekill only if a polite request fails We put quotes around guarantees becauseprocesses can on occasion become so wedged that even KILL does not affect themusually because of some degenerate IO vapor lock such as waiting for a volumethat has disappeared Rebooting is usually the only way to get rid of these processeskillall kills processes by name For example the following command kills all Apacheweb server processes sudo killall httpdThe pkill command searches for processes by name or other attributes such asEUID and sends the specified signal For example the following command sendsa TERM signal to all processes running as the user ben sudo pkill u benProcess and thread statesAs you saw in the previous section a process can be suspended with a STOP signaland returned to active duty with a CONT signal The state of being suspended orrunnable applies to the process as a whole and is inherited by all the processs threadsEven when nominally runnable threads must often wait for the kernel to completesome background work for them before they can continue execution For examplewhen a thread reads data from a file the kernel must request the appropriate diskblocks and then arrange for their contents to be delivered into the requesting processs address space During this time the requesting thread enters a shorttermsleep state in which it is ineligible to execute Other threads in the same processcan continue to run howeverYoull sometimes see entire processes described as sleeping for example in psoutputsee the next section Since sleeping is a threadlevel attribute this convention is a bit deceptive A process is generally reported as sleeping when all its Individual threads can in fact be managed similarly However those facilities are primarily of interestto developers system administrators neednt concern themselvesthreads are asleep Of course the distinction is moot in the case of singlethreadedprocesses which remain the most common caseInteractive shells and system daemons spend most of their time sleeping waitingfor terminal input or network connections Since a sleeping thread is effectivelyblocked until its request has been satisfied its process generally receives no CPUtime unless it receives a signal or a response to one of its IO requestsSome operations can cause processes or threads to enter an uninterruptible sleepstate This state is usually transient and is not observed in ps output denoted by aD in the STAT column see Table on page However a few degenerate situations can cause it to persist The most common cause involves server problems onan NFS filesystem mounted with the hard option Since processes in the uninterruptible sleep state cannot be roused even to service a signal they cannot be killedTo get rid of them you must fix the underlying problem or rebootIn the wild you might occasionally see zombie processes that have finished execution but that have not yet had their status collected by their parent process orby init or systemd If you see zombies hanging around check their PPIDs withps to find out where theyre coming from ps monitor processesThe ps command is the system administrators main tool for monitoring processes Although versions of ps differ in their arguments and display they all deliveressentially the same information Part of the enormous variation among versionsof ps can be traced back to differences in the development history of UNIX However ps is also a command that vendors tend to customize for other reasons Itsclosely tied to the kernels handling of processes so it tends to reflect all a vendorsunderlying kernel changesps can show the PID UID priority and control terminal of processes It also informsyou how much memory a process is using how much CPU time it has consumedand what its current status is running stopped sleeping etc Zombies show upin a ps listing as exiting or defunctImplementations of ps have become hopelessly complex over the years Severalvendors have abandoned the attempt to define meaningful displays and made theirpses completely configurable With a little customization work almost any desiredoutput can be producedAs a case in point the ps used by Linux is a highly polymorphous version that understands option sets from multiple historical lineages Almost uniquely among UNIXSee page formore informationabout hardmountingNFS filesystemscommands Linuxs ps accepts commandline flags with or without dashes but mightassign different interpretations to those forms For example ps a is not the same as ps aDo not be alarmed by all this complexity its there mainly for developers not forsystem administrators Although you will use ps frequently you only need to knowa few specific incantationsYou can obtain a useful overview of all the processes running on the system withps aux The a option says show all processes and x says show even processes thatdont have a control terminal u selects the user oriented output format Heresan example of ps aux output on a machine running Red Hatredhat ps auxUSER PID CPU MEM VSZ RSS TTY STAT TIME COMMANDroot S init root SN ksoftirqdroot S eventsroot S khelperroot S kacpidroot S kblockdroot S pdflushroot S kjournaldroot Ss udevdroot Ss sbindhclient root Ss sbindhclient root S kjournaldroot Ss sbindhclient root Ss rsyslog m root Ss klogd xroot Ss usrsbinatdroot Ss usrsbinsshdroot Ss sendmail acceptCommand names in brackets are not really commands at all but rather kernelthreads scheduled as processes The meaning of each field is shown in Table on the next pageAnother useful set of arguments is lax which gives more technical information Thea and x options are as above show every process and l selects the long outputformat ps lax might be slightly faster to run than ps aux because it doesnt haveto translate every UID to a usernameefficiency can be important if the systemis already bogged downTable Explanation of ps aux outputField ContentsUSER Username of the processs ownerPID Process IDCPU Percentage of the CPU this process is usingMEM Percentage of real memory this process is usingVSZ Virtual size of the processRSS Resident set size number of pages in memoryTTY Control terminal IDSTAT Current process statusR Runnable D In uninterruptible sleepS Sleeping sec T Traced or stoppedZ ZombieAdditional flagsW Process is swapped out Process has higher than normal priorityN Process has lower than normal priorityL Some pages are locked in cores Process is a session leaderTIME CPU time the process has consumedCOMMAND Command name and arguments aa Programs can modify this information so its not necessarily an accurate representation of the actual command lineShown here in an abbreviated example ps lax includes fields such as the parentprocess ID PPID niceness NI and the type of resource on which the processis waiting WCHAN short for wait channelredhat ps laxF UID PID PPID PRI NI VSZ RSS WCHAN STAT TIME COMMAND select S init ksofti SN ksoftirqd worker S events worker S khelper syslog Ss klogd x Ss portmap select Ss rpcstatd Ss rpcidmapd select Ss acpid select Ss sshd select Ss xinetd sta select Ss sendmail aCommands with long argument lists may have the commandline output cut offAdd w to the list of flags to display more columns in the output Add w twice forunlimited column width handy for those processes that have exceptionally longcommandline arguments such as some java applicationsAdministrators frequently need to identify the PID of a process You can find thePID by grepping the output of ps ps aux grep sshdroot Ss usrsbinsshdbwhaley pts S grep usrsbinsshdNote that the ps output includes the grep command itself since the grep was active in the process list at the time ps was running You can remove this line fromthe output with grep v ps aux grep v grep grep sshdroot Ss usrsbinsshdYou can also determine the PID of a process with the pidof command pidof usrsbinsshdOr with the pgrep utility pgrep sshdpidof and pgrep show all processes that match the passed string We often find asimple grep to offer the most flexibility though it can be a bit more verbose Interactive monitoring with topCommands like ps show you a snapshot of the system as it was at the time Oftenthat limited sample is insufficient to convey the big picture of whats really goingon top is a sort of realtime version of ps that gives a regularly updated interactivesummary of processes and their resource usage For exampleredhat toptop up users load average Tasks total running sleeping stopped zombieCpus us sy ni id wa hi si stKiB Mem total free used buffcacheKiB Swap total free used avail Mem PID USER PR NI VIRT RES SHR S CPU MEM TIME COMMAND root S Xorg ulsah S konsole ulsah S prlcc root S rngd root S prltoolsd root S cupsd ulsah R top root S systemd root S kthreadd root S ksoftirqd root S kworker root rt S migration root S rcubh root S rcuobBy default the display updates every seconds depending on the system Themost CPUconsumptive processes appear at the top top also accepts input fromthe keyboard to send signals and renice processes see the next section You canthen observe how your actions affect the overall condition of the machineThe summary information in the first few lines of top output is one of the firstplaces to look at to analyze the health of the system It shows a condensed view ofthe system load memory usage number of processes and a breakdown of howthe CPU is being usedOn multicore systems CPU usage is an average of all the cores in the system UnderLinux press numeral one while top is open to switch to a display of the individual cores On FreeBSD run top P to achieve the same effectRoot can run top with the q option to goose it up to the highest possible priorityThis option can be useful when you are trying to track down a process that has already brought the system to its kneesWe also like htop an open source crossplatform interactive process viewer thatoffers more features and has a nicer interface than that of top It is not yet availableas a package for our example systems but you can download a binary or sourceversion from the developers web site at hishamhmhtop nice and renice influence scheduling priorityThe niceness of a process is a numeric hint to the kernel about how the processshould be treated in relation to other processes contending for the CPU The strangename is derived from the fact that it determines how nice you are going to be to otherusers of the system A high niceness means a low priority for your process you aregoing to be nice A low or negative value means high priority you are not very niceIts highly unusual to set priorities by hand these days On the puny systems whereUNIX originated performance was significantly affected by which process was on the On FreeBSD systems you can set the TOP environment variable to pass additional arguments totop We recommend H to show all threads for multithreaded processes rather than just a summaryplus P to display all CPU cores Add export TOPHP to your shell initialization file to make thesechanges persistent between shell sessions nice manages only CPU scheduling priority To set IO priority use ioniceLearn how to interpretthe CPU memoryand load detailsin Chapter CPU Today with more than adequate CPU power on every desktop the schedulerdoes a good job of managing most workloads The addition of scheduling classesgives developers additional control when fast response is essentialThe range of allowable niceness values varies among systems In Linux the range is to and in FreeBSD its to Unless the user takes special action a newly created process inherits the nicenessof its parent process The owner of the process can increase its niceness but cannotlower it even to return the process to the default niceness This restriction preventsprocesses running at low priority from bearing highpriority children Howeverthe superuser can set nice values arbitrarilyIO performance has not kept up with increasingly fast CPUs Even with todayshighperformance SSDs disk bandwidth remains the primary bottleneck on mostsystems Unfortunately a processs niceness has no effect on the kernels management of its memory or IO highnice processes can still monopolize a disproportionate share of these resourcesA processs niceness can be set at the time of creation with the nice command andadjusted later with the renice command nice takes a command line as an argumentand renice takes a PID or sometimes a usernameSome examples nice n binlongtask Lowers priority raise nice by sudo renice Sets niceness to sudo renice u boggs Sets niceness of boggss procs to Unfortunately there is little agreement among systems about how the desired priorities should be specified in fact even nice and renice from the same system usuallydont agree To complicate things a version of nice is built into the C shell and someother common shells but not bash If you dont type the full path to nice youll getthe shells version rather than the operating systems To sidestep this ambiguity wesuggest using the fully qualified path to the systems version found at usrbinniceTable summarizes the variations A prio is an absolute niceness while an incris relative to the niceness of the shell from which nice or renice is run Only theshell nice understands plus signs in fact it requires them leave them out in allother circumstancesTable How to express priorities for nice and reniceSystem Range OS nice csh nice reniceLinux to n incr incr or incr prio or n prioFreeBSD to n incr incr or incr incr or n incr The proc filesystemThe Linux versions of ps and top read their process status information from theproc directory a pseudofilesystem in which the kernel exposes a variety of interesting information about the systems stateDespite the name proc and the name of the underlying filesystem type procthe information is not limited to process informationa variety of status information and statistics generated by the kernel are represented here You can evenmodify some parameters by writing to the appropriate proc file See page for some examplesAlthough a lot of the information is easiest to access through frontend commandssuch as vmstat and ps some of the more obscure nuggets must be read directlyfrom proc Its worth poking around in this directory to familiarize yourself witheverything thats there man proc has a comprehensive explanation of its contentsBecause the kernel creates the contents of proc files on the fly as they are readmost appear to be empty byte files when listed with ls l Youll have to cat or lessthe contents to see what they actually contain But be cautiousa few files containor link to binary data that can confuse your terminal emulator if viewed directlyProcessspecific information is divided into subdirectories named by PID For example proc is always the directory that contains information about init Table lists the most useful perprocess filesTable Process information files in Linux proc numbered subdirectoriesFile Contentscgroup The control groups to which the process belongscmd Command or program the process is executingcmdline a Complete command line of the process nullseparatedcwd Symbolic link to the processs current directoryenviron The processs environment variables nullseparatedexe Symbolic link to the file being executedfd Subdirectory containing links for each open file descriptorfdinfo Subdirectory containing further info for each open file descriptormaps Memory mapping information shared segments libraries etc ns Subdirectory with links to each namespace used by the processroot Symbolic link to the processs root directory set with chrootstat General process status information best decoded with psstatm Memory usage informationa Might be unavailable if the process is swapped out of memoryThe individual components contained within the cmdline and environ files areseparated by null characters rather than newlines You can filter their contentsthrough tr to make them more readableThe fd subdirectory represents open files in the form of symbolic links File descriptors that are connected to pipes or network sockets dont have an associated filenameThe kernel supplies a generic description as the link target insteadThe maps file can be useful for determining what libraries a program is linked toor depends onFreeBSD includes a similarbutdifferent implementation of proc However its usehas been deprecated because of neglect in the code base and a history of securityissues Its still available for compatibility but is not mounted by default To mountit use the commandfreebsd sudo mount t procfs proc procThe filesystem layout is similarbut not identicalto the Linux version of procfsThe information for a process includes its status a symbolic link to the file beingexecuted details about the processs virtual memory and other lowlevel information See also man procfs strace and truss trace signals and system callsIts often difficult to figure out what a process is actually doing The first step is generally to make an educated guess based on indirect data collected from the filesystem logs and tools such as psIf those sources of information prove insufficient you can snoop on the process at alower level with the strace Linux usually an optional package or truss FreeBSDcommand These commands display every system call that a process makes andevery signal it receives You can attach strace or truss to a running process snoopfor a while and then detach from the process without disturbing itAlthough system calls occur at a relatively low level of abstraction you can usuallytell quite a bit about a processs activity from the call trace For example the following log was produced by strace run against an active copy of top which wasrunning as PID redhat sudo strace p gettimeofday openproc ORDONLYONONBLOCKOLARGEFILEODIRECTORY fstat stmodeSIFDIR stsize fcntl FSETFD FDCLOEXEC To automatically mount the proc filesystem at boot time append the line proc proc procfs rw to etcfstab Well usually strace can interrupt system calls The monitored process must then be prepared to restart them This is a standard rule of UNIX software hygiene but its not always observedgetdents entries getdents entries statproc stmodeSIFDIR stsize openprocstat ORDONLY read init S close Not only does strace show you the name of every system call made by the processbut it also decodes the arguments and shows the result code that the kernel returnsIn the example above top starts by checking the current time It then opens andstats the proc directory and reads the directorys contents thereby obtaining a listof running processes top goes on to stat the directory representing the init processand then opens procstat to read inits status informationSystem call output can often reveal errors that are not reported by the process itself For example filesystem permission errors or socket conflicts are usually quiteobvious in the output of strace or truss Look for system calls that return error indications and check for nonzero valuesstrace is packed with goodies most of which are documented in the man pageFor example the f flag follows forked processes This feature is useful for tracingdaemons such as httpd that spawn many children The e tracefile option displays only filerelated operations This feature is especially handy for discoveringthe location of evasive configuration filesHeres a similar example from FreeBSD that uses truss In this case we trace howcp copies a filefreebsd truss cp etcpasswd tmppwlstatetcpasswd moderwrr inodesizeblksize xumaskxff xumaskx xfffstatatATFDCWDetcpasswd moderwrr inodesizeblksize x xstattmppwxfffffffe ERR No such file or directoryopenatATFDCWDetcpasswdORDONLY xopenatATFDCWDtmppwOWRONLYOCREAT xmmapxPROTREADMAPSHAREDx xwrite FreeBSD relengetcmast xcclose xclose xAfter allocating memory and opening library dependencies not shown cp usesthe lstat system call to check the current status of the etcpasswd file It then runsstat on the path of the prospective copy tmppw That file does not yet exist sothe stat fails and truss decodes the error for you as No such file or directorycp then invokes the openat system call with the ORDONLY option to read thecontents of etcpasswd followed by an openat of tmppw with OWRONLY tocreate the new destination file It then maps the contents of etcpasswd into memory with mmap and writes out the data with write Finally cp cleans up after itselfby closing both file handlesSystem call tracing is a powerful debugging tool for administrators Turn to thesetools after more traditional routes such as examining log files and configuring aprocess for verbose output have been exhausted Do not be intimidated by the denseoutput its usually sufficient to focus on the humanreadable portions Runaway processesRunaway processes are those that soak up significantly more of the systems CPUdisk or network resources than their usual role or behavior would lead you to expect Sometimes such programs have their own bugs that have led to degeneratebehavior In other cases they fail to deal appropriately with upstream failures andget stuck in maladaptive loops For example a process might reattempt the samefailing operation over and over again flooring the CPU In yet another category ofcases there is no bug per se but the software is simply inefficient in its implementation and greedy with the systems resourcesAll these situations merit investigation by a system administrator not only becausethe runaway process is most likely malfunctioning but also because it typically interferes with the operation of other processes that are running on the systemThe line between pathological behavior and normal behavior under heavy workload is vague Often the first step in diagnosis is to figure out which of these phenomena you are actually observing Generally system processes should alwaysbehave reasonably so obvious misbehavior on the part of one of these processes isautomatically suspicious User processes such as web servers and databases mightsimply be overloadedYou can identify processes that are using excessive CPU time by looking at the output of ps or top Also check the system load averages as reported by the uptimecommand Traditionally these values quantify the average number of processesthat have been runnable over the previous and minute intervals UnderLinux the load average also takes account of busyness caused by disk traffic andother forms of IOFor CPU bound systems the load averages should be less than the total number ofCPU cores available on your system If they are not the system is overloaded Under Linux check total CPU utilization with top or ps to determine whether highload averages relate to CPU load or to IO If CPU utilization is near that isprobably the bottleneckProcesses that use excessive memory relative to the systems physical RAM cancause serious performance problems You can check the memory size of processes by running top The VIRT column shows the total amount of virtual memoryallocated by each process and the RES column shows the portion of that memorycurrently mapped to specific memory pages the resident setBoth of these numbers can include shared resources such as libraries and thus arepotentially misleading A more direct measure of processspecific memory consumption is found in the DATA column which is not shown by default To add thiscolumn to tops display type the f key once top is running and select DATA fromthe list by pressing the space bar The DATA value indicates the amount of memory in each processs data and stack segments so its relatively specific to individualprocesses modulo shared memory segments Look for growth over time as well asabsolute size On FreeBSD SIZE is the equivalent column and is shown by defaultMake a concerted effort to understand whats going on before you terminate aseemingly runaway process The best route to debugging the issue and preventinga recurrence is to have a live example you can investigate Once you kill a misbehaving process most of the available evidence disappearsKeep the possibility of hacking in mind as well Malicious software is typically nottested for correctness in a variety of environments so its more likely than averageto enter some kind of degenerate state If you suspect misfeasance obtain a system call trace with strace or truss to get a sense of what the process is doing egcracking passwords and where its data is storedRunaway processes that produce output can fill up an entire filesystem causingnumerous problems When a filesystem fills up lots of messages will be logged tothe console and attempts to write to the filesystem will produce error messagesThe first thing to do in this situation is to determine which filesystem is full andwhich file is filling it up The df h command shows filesystem disk use in humanreadable units Look for a filesystem thats or more full Use the du hcommand on the identified filesystem to determine which directory is using themost space Rinse and repeat with du until the large files are discovereddf and du report disk usage in subtly different manners df reports the disk spaceused by a mounted filesystem according to disk block totals in the filesystems metadata du sums the sizes of all files in a given directory If a file is unlinked deleted from the filesystem but is still referenced by some running process df reportsthe space but du does not This disparity persists until the open file descriptor isclosed or the file is truncated If you cant determine which process is using a file Most filesystem implementations reserve a portion about of the storage space for breathingroom but processes running as root can encroach on this space resulting in a reported usage that isgreater than try running the fuser and lsof commands covered in detail on page to getmore information Periodic processesIts often useful to have a script or command executed without any human intervention Common use cases include scheduled backups database maintenanceactivities or the execution of nightly batch jobs As is typical of UNIX and Linuxtheres more than one way to achieve this goalcron schedule commandsThe cron daemon is the traditional tool for running commands on a predeterminedschedule It starts when the system boots and runs as long as the system is up Thereare multiple implementations of cron but fortunately for administrators the syntaxand functionality of the various versions is nearly identicalFor reasons that are unclear cron has been renamed crond on Red Hat But it isstill the same cron we all know and lovecron reads configuration files containing lists of command lines and times at whichthey are to be invoked The command lines are executed by sh so almost anythingyou can do by hand from the shell can also be done with cron If you prefer youcan even configure cron to use a different shellA cron configuration file is called a crontab short for cron table Crontabsfor individual users are stored under varspoolcron Linux or varcrontabsFreeBSD There is at most one crontab file per user Crontab files are plain textfiles named with the login names of the users to whom they belong cron uses thesefilenames and the file ownership to figure out which UID to use when runningthe commands contained in each file The crontab command transfers crontab filesto and from this directorycron tries to minimize the time it spends reparsing configuration files and makingtime calculations The crontab command helps maintain crons efficiency by notifying cron when crontab files change Ergo you shouldnt edit crontab files directlybecause this approach might result in cron not noticing your changes If you doget into a situation where cron doesnt seem to acknowledge a modified crontab aHUP signal sent to the cron process forces it to reload on most systemscron normally does its work silently but most versions can keep a log file usuallyvarlogcron that lists the commands that were executed and the times at whichthey ran Glance at the cron log file if youre having problems with a cron job andcant figure out whyRHELSee Chapter for more information about syslogThe format of crontab filesAll the crontab files on a system share a similar format Comments are introducedwith a pound sign in the first column of a line Each noncomment line containssix fields and represents one commandminute hour dom month weekday commandThe first five fields tell cron when to run the command Theyre separated bywhitespace but within the command field whitespace is passed along to the shellThe fields in the time specification are interpreted as shown in Table An entryin a crontab is colloquially known as a cron jobTable Crontab time specificationsField Description Rangeminute Minute of the hour to hour Hour of the day to dom Day of the month to month Month of the year to weekday Day of the week to SundayEach of the timerelated fields can contain A star which matches everything A single integer which matches exactly Two integers separated by a dash matching a range of values A range followed by a slash and a step value eg A commaseparated list of integers or ranges matching any valueFor example the time specification means am Monday through Friday A hint never use stars in every fieldunless you want the command to be run every minute which is useful only in testing scenarios One minute is the finest granularity available to cron jobsTime ranges in crontabs can include a step value For example the series can be written more concisely as You can also usethreeletter text mnemonics for the names of months and days but not in combination with ranges As far as we know this feature works only with English namesThere is a potential ambiguity to watch out for with the weekday and dom fieldsEvery day is both a day of the week and a day of the month If both weekday anddom are specified a day need satisfy only one of the two conditions to be selectedFor example means every halfhour on Friday and every halfhour on the th of the monthnot every halfhour on Friday the thThe command is the sh command line to be executed It can be any valid shell command and should not be quoted The command is considered to continue to theend of the line and can contain blanks or tabsPercent signs indicate newlines within the command field Only the text up tothe first percent sign is included in the actual command The remaining lines aregiven to the command as standard input Use a backslash as an escape characterin commands that have a meaningful percent sign for example date sAlthough sh is involved in executing the command the shell does not act as a loginshell and does not read the contents of profile or bashprofile As a resultthe commands environment variables might be set up somewhat differently fromwhat you expect If a command seems to work fine when executed from the shellbut fails when introduced into a crontab file the environment is the likely culpritIf need be you can always wrap your command with a script that sets up the appropriate environment variablesWe also suggest using the fully qualified path to the command ensuring that thejob will work properly even if the PATH is not set as expected For example thefollowing command logs the date and uptime to a file in the users home directoryevery minute echo bindate usrbinuptime uptimelogAlternatively you can set environment variables explicitly at the top of the crontabPATHbinusrbin echo date uptime uptimelogHere are a few more examples of valid crontab entries echo ruok usrbinnc localhost mail s TCP port status benadmincomThis line emails the results of a connectivity check on port every minuteson Mondays Wednesdays and Fridays Since cron executes command by way ofsh special shell characters like pipes and redirects function as expected Sun usrbinmysqlcheck u maintenance optimizealldatabasesThis entry runs the mysqlcheck maintenance program on Sundays at amSince the output is not saved to a file or otherwise discarded it will be emailed tothe owner of the crontab find tmp mtime type f exec rm f This command runs at each morning It removes all files in the tmp directorythat have not been modified in days The at the end of the line marks the endof the subcommand arguments to findcron does not try to compensate for commands that are missed while the systemis down However it is smart about time adjustments such as shifts into and outof daylight saving timeIf your cron job is a script be sure to make it executable with chmod x or cronwont be able to execute it Alternatively set the cron command to invoke a shellon your script directly eg bash c binmyscriptshCrontab managementcrontab filename installs filename as your crontab replacing any previous versioncrontab e checks out a copy of your crontab invokes your editor on it as specifiedby the EDITOR environment variable and then resubmits it to the crontab directory crontab l lists the contents of your crontab to standard output and crontabr removes it leaving you with no crontab file at allRoot can supply a username argument to edit or view other users crontabs Forexample crontab r jsmith erases the crontab belonging to the user jsmith andcrontab e jsmith edits it Linux allows both a username and a filename argumentin the same command so the username must be prefixed with u to disambiguateeg crontab u jsmith crontabnewWithout commandline arguments most versions of crontab try to read a crontabfrom standard input If you enter this mode by accident dont try to exit withControlD doing so erases your entire crontab Use ControlC instead FreeBSDrequires you to supply a dash as the filename argument to make crontab pay attention to its standard input SmartMany sites have experienced subtle but recurrent network glitches that occur because administrators have configured cron to run the same command on hundredsof machines at exactly the same time causing delays or excessive load Clock synchronization with NTP exacerbates the problem This issue is easy to fix with arandom delay scriptcron logs its activities through syslog using the facility cron with most messages submitted at level info Default syslog configurations generally send cron logdata to its own fileOther crontabsIn addition to looking for userspecific crontabs cron also obeys system crontabentries found in etccrontab and in the etccrond directory These files have aslightly different format from the peruser crontab files they allow commands tobe run as an arbitrary user An extra username field comes before the commandname The username field is not present in gardenvariety crontab files because thecrontabs filename supplies this same informationIn general etccrontab is a file for system administrators to maintain by handwhereas etccrond is a sort of depot into which software packages can install anycrontab entries they might need Files in etccrond are by convention named afterthe packages that install them but cron doesnt care about or enforce this conventionLinux distributions also preinstall crontab entries that run the scripts in a set ofwellknown directories thereby providing another way for software packages toinstall periodic jobs without any editing of a crontab file For example scripts inetccronhourly etccrondaily and etccronweekly are run hourly daily andweekly respectivelycron access controlTwo config files specify which users may submit crontab files For Linux the filesare etccronallowdeny and on FreeBSD they are varcronallowdeny Manysecurity standards require that crontabs be available only to service accounts or tousers with a legitimate business need The allow and deny files facilitate compliance with these requirementsIf the cronallow file exists then it contains a list of all users that may submitcrontabs one per line No unlisted person can invoke the crontab command If thecronallow file doesnt exist then the crondeny file is checked It too is just a list ofusers but the meaning is reversed everyone except the listed users is allowed accessIf neither the cronallow file nor the crondeny file exists systems default apparently at random there being no dominant convention either to allowing all usersto submit crontabs or to limiting crontab access to root In practice a starter configuration is typically included in the default OS installation so the question of howcrontab might behave without configuration files is moot Most default configurations allow all users to access cron by defaultIts important to note that on most systems access control is implemented by crontabnot by cron If a user is able to sneak a crontab file into the appropriate directoryby other means cron will blindly execute the commands it contains Therefore itis vital to maintain root ownership of varspoolcron and varcrontabs OS distributions always set the permissions correctly by defaultsystemd timersIn accordance with its mission to duplicate the functions of all other Linux subsystems systemd includes the concept of timers which activate a given systemdservice on a predefined schedule Timers are more powerful than crontab entriesbut they are also more complicated to set up and manage Some Linux distributionsSee Chapter foran introduction tosystemd and unitseg CoreOS have abandoned cron entirely in favor of systemd timers but ourexample systems all continue to include cron and to run it by defaultWe have no useful advice regarding the choice between systemd timers and crontabentries Use whichever you prefer for any given task Unfortunately you do not reallyhave the option to standardize on one system or the other because software packages add their jobs to a random system of their own choice Youll always have tocheck both systems when you are trying to figure out how a particular job gets runStructure of systemd timersA systemd timer comprises two files A timer unit that describes the schedule and the unit to activate A service unit that specifies the details of what to runIn contrast to crontab entries systemd timers can be described both in absolutecalendar terms Wednesdays at am and in terms that are relative to otherevents seconds after system boot The options combine to allow powerfulexpressions that dont suffer the same constraints as cron jobs Table describesthe time expression optionsTable systemd timer typesType Time basisOnActiveSec Relative to the time at which the timer itself is activatedOnBootSec Relative to system boot timeOnStartupSec Relative to the time at which systemd was startedOnUnitActiveSec Relative to the time the specified unit was last activeOnUnitInactiveSec Relative to the time the specified unit was last inactiveOnCalendar A specific day and timeAs their names suggest values for these timer options are given in seconds Forexample OnActiveSec is seconds after the timer activates The value canactually be any valid systemd time expression as discussed in more detail startingon page systemd timer exampleRed Hat and CentOS include a preconfigured systemd timer that cleans up the systems temporary files once a day Below we take a more detailed look at an example First we enumerate all the defined timers with the systemctl command Werotated the output table below to make it readable Normally each timer producesone long line of outputredhat systemctl listtimersNEXT Sun UTCLEFT h leftLAST Sat UTCPASSED h agoUNIT systemdtmpfilescleantimerACTIVATES systemdtmpfilescleanserviceThe output lists both the name of the timer unit and the name of the service unit itactivates Since this is a default system timer the unit file lives in the standard systemd unit directory usrlibsystemdsystem Heres the timer unit fileredhat cat usrlibsystemdsystemsystemdtmpfilescleantimerUnitDescriptionDaily Cleanup of Temporary DirectoriesTimerOnBootSecminOnUnitActiveSecdThe timer first activates minutes after boot and then fires once a day thereafterNote that some kind of trigger for the initial activation here OnBootSec is alwaysnecessary There is no single specification that achieves an every X minutes effect on its ownAstute observers will notice that the timer does not actually specify which unit torun By default systemd looks for a service unit that has the same name as the timerYou can specify a target unit explicitly with the Unit optionIn this case the associated service unit holds no surprisesredhat cat usrlibsystemdsystemsystemdtmpfilescleanserviceUnitDescriptionCleanup of Temporary DirectoriesDefaultDependenciesnoConflictsshutdowntargetAftersystemdreadaheadcollectservice systemdreadaheadreplayservicelocalfstarget timesynctargetBeforeshutdowntargetServiceTypesimpleExecStartusrbinsystemdtmpfiles cleanIOSchedulingClassidleYou can run the target service directly that is independently of the timer withsystemctl start systemdtmpfilesclean just like any other service This fact greatlyfacilitates the debugging of scheduled tasks which can be a source of much administrative anguish when you are using cronTo create your own timer drop timer and service files in etcsystemdsystem Ifyou want the timer to run at boot addInstallWantedBymultiusertargetto the end of the timers unit file Dont forget to enable the timer at boot time withsystemctl enable You can also start the timer immediately with systemctl startA timers AccuracySec option delays its activation by a random amount of timewithin the specified time window This feature is handy when a timer runs on alarge group of networked machines and you want to avoid having all the timersfire at exactly the same moment Recall that with cron you need to use a randomdelay script to achieve this featAccuracySec defaults to seconds If you want your timer to execute at exactly thescheduled time use AccuracySecns A nanosecond is probably close enoughNote that you wont actually obtain nanosecond accuracysystemd time expressionsTimers allow for flexible specification of dates times and intervals The systemdtimeman page is the authoritative reference for the specification grammarYou can use intervalvalued expressions instead of seconds for relative timingssuch as those used as the values of OnActiveSec and OnBootSec For example thefollowing forms are all validOnBootSech mOnStartupSecweek days hoursOnActiveSechrmsecmsecSpaces are optional in time expressions The minimum granularity is nanosecondsbut if your timer fires too frequently more than once every two seconds systemdtemporarily disables itIn addition to triggering at periodic intervals timers can be scheduled to activateat specific times by including the OnCalendar option This feature offers the closestmatch to the syntax of a traditional cron job but its syntax is more expressive andflexible Table shows some examples of time specifications that could be usedas the value of OnCalendarIn time expressions stars are placeholders that match any plausible value As incrontab files slashes introduce an increment value The exact syntax is a bit different from that used in crontabs however crontabs want the incremented objectto be a range eg every two hours between am and pmbut systemd time expressions take only a start value eg every two hoursstarting at amTable systemd time and date encoding examplesTime specification Meaning July th at midnightFriMon July th each year but only if it falls on FriMonMonWed Mondays Tuesdays and Wednesdays at noonMon Mondays at pmweekly Mondays at midnightmonthly The st day of the month at midnight Every minutes starting at the th minute At and every dayTransient timersYou can use the systemdrun command to schedule the execution of a command according to any of the normal systemd timer types but without creating taskspecifictimer and service unit files For example to pull a Git repository every ten minutes systemdrun oncalendar binsh c cd app git pullRunning timer as unit runtimerWill run service as unit runservicesystemd returns a transient unit identifier that you can list with systemctl Onceagain we futzed with the output format below systemctl listtimers runtimerNEXT Sat UTCLEFT min leftLAST Sat UTCPASSED s ago systemctl listunits runtimerUNIT runtimerLOAD loadedACTIVE activeSUB waitingDESCRIPTION binsh c cd app git pullTo cancel and remove a transient timer just stop it by running systemctl stop sudo systemctl stop runtimersystemdrun functions by creating timer and unit files for you in subdirectoriesof runsystemdsystem However transient timers do not persist after a rebootTo make them permanent you can fish them out of run tweak them as necessaryand install them in etcsystemdsystem Be sure to stop the transient timer beforestarting or enabling the permanent versionCommon uses for scheduled tasksIn this section we look at a couple of common chores that are often automatedthrough cron or systemdSending mailThe following crontab entry implements a simple email reminder You can use anentry like this to automatically email the output of a daily report or the results of acommand execution Lines have been folded to fit the page In reality this is onelong line usrbinmail s Time to do the TPS reportsbenadmincomTPS reports are due at the end of the month GetbusySincerelycronNote the use of the character both to separate the command from the input textand to mark line endings within the input This entry sends email at am onthe th day of each monthCleaning up a filesystemWhen a program crashes the kernel may write out a file usually named corepidcore or programcore that contains an image of the programs address space Corefiles are useful for developers but for administrators they are usually a waste of spaceUsers often dont know about core files so they tend not to disable their creationor delete them on their own You can use a cron job to clean up these core files orother vestiges left behind by misbehaving and crashed processesRotating a log fileSystems vary in the quality of their default log file management and you will probably need to adjust the defaults to conform to your local policies To rotate a logfile means to divide it into segments by size or by date keeping several older versions of the log available at all times Since log rotation is a recurrent and regularlyoccurring event its an ideal task to be scheduled See Management and rotation oflog files on page for more detailsRunning batch jobsSome longrunning calculations are best run as batch jobs For example messagescan accumulate in a queue or database You can use a cron job to process all thequeued messages at once as an ETL extract transform and load to another location such as a data warehouseSome databases benefit from routine maintenance For example the open sourcedistributed database Cassandra has a repair function that keeps the nodes in a cluster in sync These maintenance tasks are good candidates for execution throughcron or systemdBacking up and mirroringYou can use a scheduled task to automatically back up a directory to a remote system We suggest running a full backup once a week with incremental differenceseach night Run backups late at night when the load on the system is likely to be lowMirrors are byteforbyte copies of filesystems or directories that are hosted onanother system They can be used as a form of backup or as a way to make filesavailable at more than one location Web sites and software repositories are oftenmirrored to offer better redundancy and to offer faster access for users that arephysically distant from the primary site Use periodic execution of the rsync command to maintain mirrors and keep them up to dateQuick which of the following would you expect to find in a filesystem Processes Audio devices Kernel data structures and tuning parameters Interprocess communication channelsIf the system is UNIX or Linux the answer is all the above and more And yesyou might find some files in there tooThe basic purpose of a filesystem is to represent and organize the systems storageresources However programmers have been eager to avoid reinventing the wheelwhen it comes to managing other types of objects It has often proved convenientto map these objects into the filesystem namespace This unification has some advantages consistent programming interface easy access from the shell and somedisadvantages filesystem implementations suggestive of Frankensteins monsterbut like it or not this is the UNIX and hence the Linux way Its perhaps more accurate to say that these entities are represented within the filesystem In most cases the filesystem is used as a rendezvous point to connect clients with the drivers they are seeking The FilesystemThe filesystem can be thought of as comprising four main components A namespace a way to name things and organize them in a hierarchy An API a set of system calls for navigating and manipulating objects Security models schemes for protecting hiding and sharing things An implementation software to tie the logical model to the hardwareModern kernels define an abstract interface that accommodates many differentbackend filesystems Some portions of the file tree are handled by traditional diskbased implementations Others are fielded by separate drivers within the kernel Forexample network filesystems are handled by a driver that forwards the requestedoperations to a server on another computerUnfortunately the architectural boundaries are not clearly drawn and quite a fewspecial cases exist For example device files define a way for programs to communicate with drivers inside the kernel Device files are not really data files buttheyre handled through the filesystem and their characteristics are stored on diskAnother complicating factor is that the kernel supports more than one type of diskbased filesystem The predominant standards are the ext XFS and UFS filesystemsalong with Oracles ZFS and Btrfs However many others are available includingVeritass VxFS and JFS from IBMForeign filesystems are also widely supported including the FAT and NTFS filesystemsused by Microsoft Windows and the ISO filesystem used on older CDROMsThe filesystem is a rich topic that we approach from several different angles Thischapter tells where to find things on your system and describes the characteristicsof files the meanings of permission bits and the use of some basic commands thatview and set attributes Chapter Storage is where youll find the more technical filesystem topics such as disk partitioningChapter The Network File System describes NFS a file sharing system that iscommonly used for remote file access between UNIX and Linux systems Chapter SMB describes an analogous system from the Windows worldWith so many different filesystem implementations available it may seem strangethat this chapter reads as if there were only a single filesystem We can be vagueabout the underlying code because most modern filesystems either try to implement the traditional filesystem functionality in a faster and more reliable manneror they add extra features as a layer on top of the standard filesystem semanticsSome filesystems do both For better or worse too much existing software dependson the model described in this chapter for that model to be discarded An API or application programming interface is the generic term for the set of routines that a library operating system or software package permits programmers to callSee the sectionsstarting on page for more information aboutspecific filesystems PathnamesThe filesystem is presented as a single unified hierarchy that starts at the directory and continues downward through an arbitrary number of subdirectories is alsocalled the root directory This singlehierarchy system differs from the one used byWindows which retains the concept of partitionspecific namespacesGraphical user interfaces often refer to directories as folders even on Linux systems Folders and directories are exactly the same thing folder is just linguisticleakage from the worlds of Windows and macOS Nevertheless its worth notingthat the word folder tends to raise the hackles of some techies Dont use it intechnical contexts unless youre prepared to receive funny looksThe list of directories that must be traversed to locate a particular file plus that filesfilename form a pathname Pathnames can be either absolute eg tmpfoo orrelative eg bookfilesystem Relative pathnames are interpreted starting at thecurrent directory You might be accustomed to thinking of the current directory asa feature of the shell but every process has oneThe terms filename pathname and path are more or less interchangeableor atleast we use them interchangeably in this book Filename and path can be usedfor both absolute and relative paths pathname usually suggests an absolute pathThe filesystem can be arbitrarily deep However each component of a pathname thatis each directory must have a name no more than characters long Theres also alimit on the total path length you can pass into the kernel as a system call argument bytes on Linux bytes on BSD To access a file with a pathname longerthan this you must cd to an intermediate directory and use a relative pathname Filesystem mounting and unmountingThe filesystem is composed of smaller chunksalso called filesystemseach ofwhich consists of one directory and its subdirectories and files Its normally apparent from context which type of filesystem is being discussed but for clarity in thefollowing discussion we use the term file tree to refer to the overall layout andreserve the word filesystem for the branches attached to the treeSome filesystems live on disk partitions or on logical volumes backed by physicaldisks but as mentioned earlier filesystems can be anything that obeys the properAPI a network file server a kernel component a memorybased disk emulator etcMost kernels have a nifty loop filesystem that lets you mount individual files asif they were distinct devices Its useful for mounting DVDROM images stored ondisk or for developing filesystem images without having to worry about repartitioning Linux systems can even treat existing portions of the file tree as filesystemsThis trick lets you duplicate move or hide portions of the file treeIn most situations filesystems are attached to the tree with the mount commandmount maps a directory within the existing file tree called the mount point to theroot of the newly attached filesystem The previous contents of the mount pointbecome temporarily inaccessible as long as another filesystem is mounted thereMount points are usually empty directories howeverFor example sudo mount devsda usersinstalls the filesystem stored on the disk partition represented by devsda underthe path users You could then use ls users to see that filesystems contentsOn some systems mount is a just a wrapper that calls filesystemspecific commandssuch as mountntfs or mountsmbfs Youre free to call these helper commandsdirectly if you need to they sometimes offer additional options that the mountwrapper does not understand On the other hand the generic mount commandsuffices for daytoday useYou can run the mount command without any arguments to see all the filesystemsthat are currently mounted On Linux systems there might be or more most ofwhich represent various interfaces to the kernelThe etcfstab file lists filesystems that are normally mounted on the system Theinformation in this file allows filesystems to be automatically checked with fsckand mounted with mount at boot time with options you specify The fstab filealso serves as documentation for the layout of the filesystems on disk and enablesshort commands such as mount usr See page for a discussion of fstabYou detach filesystems with the umount command umount complains if you tryto unmount a filesystem thats in use The filesystem to be detached must not haveopen files or processes whose current directories are located there and if the filesystem contains executable programs none of them can be runningLinux has a lazy unmount option umount l that removes a filesystem from thenaming hierarchy but does not truly unmount it until all existing file referenceshave been closed Its debatable whether this is a useful option To begin with theresno guarantee that existing references will ever close on their own In addition thesemiunmounted state can present inconsistent filesystem semantics to the programs that are using it they can read and write through existing file handles butcannot open new files or perform other filesystem operationsumount f forceunmounts a busy filesystem and is supported on all our examplesystems However its almost always a bad idea to use it on nonNFS mounts andit may not work on certain types of filesystems eg those that keep journals suchas XFS or ext We say in most situations because ZFS adopts a rather different approach to mounting and unmounting not to mention many other aspects of filesystem administration See page for detailsInstead of reaching for umount f when a filesystem youre trying to unmount turnsout to be busy run the fuser command to find out which processes hold referencesto that filesystem fuser c mountpoint prints the PID of every process thats using afile or directory on that filesystem plus a series of letter codes that show the natureof the activity For examplefreebsd fuser c usrhomeusrhome c c x x x x xThe exact letter codes vary from system to system In this example from a FreeBSDsystem c indicates that a process has its current working directory on the filesystemand x indicates a program being executed However the details are usually unimportantthe PIDs are what you wantTo investigate the offending processes just run ps with the list of PIDs returnedby fuser For examplenutrient ps up USER PID CPU MEM STARTED TIME COMMANDfnd Jul ruby slaveaudiochannelbackendfnd ThuPM bash bashHere the quotation marks force the shell to pass the list of PIDs to ps as a singleargumentOn Linux systems you can avoid the need to launder PIDs through ps by runningfuser with the v flag This option produces a more readable display that includesthe command name fuser cv usr USER PID ACCESS COMMANDusr root m atd root m sshd root m lpdThe letter codes in the ACCESS column are the same ones used in fusers nonverbose outputA more elaborate alternative to fuser is the lsof utility lsof is a more complex andsophisticated program than fuser and its output is correspondingly verbose lsofcomes installed by default on all our example Linux systems and is available as apackage on FreeBSDUnder Linux scripts in search of specific information about processes use of filesystems can also read the files in proc directly However lsof F which formats lsofsoutput for easy parsing is an easier and more portable solution Use additionalcommandline flags to request just the information you need Organization of the file treeUNIX systems have never been well organized Various incompatible naming conventions are used simultaneously and different types of files are scattered randomly around the namespace In many cases files are divided by function and not byhow likely they are to change making it difficult to upgrade the operating systemThe etc directory for example contains some files that are never customized andsome that are entirely local How do you know which files to preserve during anupgrade Well you just have to knowor trust the installation software to makethe right decisionsAs a logically minded sysadmin you may be tempted to improve the default organization Unfortunately the file tree has many hidden dependencies so such effortsusually end up creating problems Just let everything stay where the OS installationand the system packages put it When offered a choice of locations always acceptthe default unless you have a specific and compelling reason to do otherwiseThe root filesystem includes at least the root directory and a minimal set of files andsubdirectories The file that contains the OS kernel usually lives under boot butits exact name and location can vary Under BSD and some other UNIX systemsthe kernel is not really a single file so much as a set of componentsAlso part of the root filesystem are etc for critical system and configuration filessbin and bin for important utilities and sometimes tmp for temporary filesThe dev directory was traditionally part of the root filesystem but these days its avirtual filesystem thats mounted separately See page for more informationabout this topicSome systems keep shared library files and a few other oddments such as the Cpreprocessor in the lib or lib directory Others have moved these items intousrlib sometimes leaving lib as a symbolic linkThe directories usr and var are also of great importance usr is where most standardbutnotsystemcritical programs are kept along with various other bootysuch as online manuals and most libraries FreeBSD stores quite a bit of localconfiguration under usrlocal var houses spool directories log files accountinginformation and various other items that grow or change rapidly and that vary oneach host Both usr and var must be available to enable the system to come upall the way to multiuser modeIn the past it was standard practice to partition the system disk and to put someparts of the file tree on their own partitions most commonly usr var and tmpThats not uncommon even now but the secular trend is toward having one big rootfilesystem Large hard disks and increasingly sophisticated filesystem implementations have reduced the value of partitioningSee Chapter formore informationabout configuring the kernelSee page for somereasons why partitioning might be desirableand some rules ofthumb to guide itIn cases where partitioning is used its most frequently an attempt to prevent onepart of the file tree from consuming all available space and bringing the entiresystem to a halt Accordingly var which contains log files that are apt to grow intimes of trouble tmp and user home directories are some of the most commoncandidates for having their own partitions Dedicated filesystems can also storebulky items such as source code libraries and databasesTable lists some of the more important standard directories Alternate rowshave been shaded to improve readabilityOn most systems a hier man page outlines some general guidelines for the layoutof the filesystem Dont expect the actual system to conform to the master plan inevery respect howeverFor Linux systems the Filesystem Hierarchy Standard attempts to codify rationalize and explain the standard directories Its an excellent resource to consult whenyou confront an unusual situation and need to figure out where to put somethingDespite its status as a standard its more a reflection of realworld practice thana prescriptive document It also hasnt undergone much updating recently so itdoesnt describe the exact filesystem layout found on current distributions File typesMost filesystem implementations define seven types of files Even when developersadd something new and wonderful to the file tree such as the process informationunder proc it must still be made to look like one of these seven types Regular files Directories Character device files Block device files Local domain sockets Named pipes FIFOs Symbolic linksYou can determine the type of an existing file with the file command Not only doesfile know about the standard types of files but it also knows a thing or two aboutcommon formats used within regular files file usrincludeusrinclude directory file binshbinsh ELF bit LSB executable x version FreeBSDdynamically linked interpreter libexecldelfso for FreeBSD FreeBSDstyle strippedAll that hoohah about binsh means its an executable command See wikilinuxfoundationorgenFHSTable Standard directories and their contentsPathname Contentsbin Core operating system commandsboot Boot loader kernel and files needed by the kernelcompat On FreeBSD files and libraries for Linux binary compatibilitydev Device entries for disks printers pseudoterminals etcetc Critical startup and configuration fileshome Default home directories for userslib Libraries shared libraries and commands used by bin and sbinmedia Mount points for filesystems on removable mediamnt Temporary mount points mounts for removable mediaopt Optional software packages rarely used for compatibilityproc Information about all running processesroot Home directory of the superuser sometimes just run Rendezvous points for running programs PIDs sockets etcsbin Core operating system commands asrv Files held for distribution through web or other serverssys A plethora of different kernel interfaces Linuxtmp Temporary files that may disappear between rebootsusr Hierarchy of secondary files and commandsusrbin Most commands and executable filesusrinclude Header files for compiling C programsusrlib Libraries also support files for standard programsusrlocal Local software or configuration data mirrors usrusrsbin Less essential commands for administration and repairusrshare Items that might be common to multiple systemsusrshareman Online manual pagesusrsrc Source code for nonlocal software not widely usedusrtmp More temporary space preserved between rebootsvar Systemspecific data and a few configuration filesvaradm Varies logs setup records strange administrative bitsvarlog System log filesvarrun Same function as run now often a symlinkvarspool Spooling that is storage directories for printers mail etcvartmp More temporary space preserved between rebootsa The distinguishing characteristic of sbin was originally that its contents were statically linked and sohad fewer dependencies on other parts of the system These days all binaries are dynamically linkedand there is no real difference between bin and sbinAnother option for investigating files is ls ld The l flag shows detailed information and the d flag forces ls to show the information for a directory rather thanshowing the directorys contentsThe first character of the ls output encodes the type For example the circled d inthe following output demonstrates that usrinclude is a directory ls ld usrincludedrwxrxrx root root Jul usrincludeTable shows the codes ls uses to represent the various types of filesTable Filetype encoding used by lsFile type Symbol Created by Removed byRegular file editors cp etc rmDirectory d mkdir rmdir rm rCharacter device file c mknod rmBlock device file b mknod rmLocal domain socket s socket system call rmNamed pipe p mknod rmSymbolic link l ln s rmAs Table shows rm is the universal tool for deleting files But how would youdelete a file named say f Its a legitimate filename under most filesystems butrm f doesnt work because rm interprets the f as a flag The answer is either torefer to the file by a pathname that doesnt start with a dash such as f or to userms argument to tell it that everything that follows is a filename and not an option ie rm fFilenames that contain control or Unicode characters present a similar problemsince reproducing these names from the keyboard can be difficult or impossible Inthis situation you can use shell globbing pattern matching to identify the files todelete When you use pattern matching its a good idea to get in the habit of usingrms i option to make rm confirm the deletion of each file This feature protectsyou against deleting any good files that your pattern inadvertently matches Todelete the file named fooControlDbar in the following example you could use lsfoobar foose kderoot rm i foorm remove foobar yrm remove foose nNote that ls shows the control character as a question mark which can be a bit deceptive If you dont remember that is a shell patternmatching character and tryto rm foobar you might potentially remove more than one file although not inthis example i is your friendls b shows control characters as octal numbers which can be helpful if you needto identify them specifically ControlA is in octal ControlB is and so on in alphabetical order man ascii and the Wikipedia page for ASCII bothinclude a nice table of control characters and their octal equivalentsTo delete the most horribly named files you might need to resort to rm i Another option for removing files with squirrelly names is to use an alternative interface to the filesystem such as emacss dired mode or a visual tool such as NautilusRegular filesRegular files consist of a series of bytes filesystems impose no structure on theircontents Text files data files executable programs and shared libraries are all storedas regular files Both sequential access and random access are allowedDirectoriesA directory contains named references to other files You can create directories withmkdir and delete them with rmdir if they are empty You can recursively deletenonempty directoriesincluding all their contentswith rm rThe special entries and refer to the directory itself and to its parent directorythey cannot be removed Since the root directory has no real parent directory thepath is equivalent to the path and both are equivalent to Hard linksA files name is stored within its parent directory not with the file itself In factmore than one directory or more than one entry in a single directory can referto a file at one time and the references can have different names Such an arrangement creates the illusion that a file exists in more than one place at the same timeThese additional references links or hard links to distinguish them from symbolic links discussed below are synonymous with the original file as far as thefilesystem is concerned all links to the file are equivalent The filesystem maintainsa count of the number of links that point to each file and does not release the filesdata blocks until its last link has been deleted Hard links cannot cross filesystemboundariesYou create hard links with ln and remove them with rm Its easy to remember thesyntax of ln if you keep in mind that it mirrors the syntax of cp The command cpoldfile newfile creates a copy of oldfile called newfile and ln oldfile newfile makesthe name newfile an additional reference to oldfileIn most filesystem implementations it is technically possible to make hard links todirectories as well as to flat files However directory links often lead to degenerateconditions such as filesystem loops and directories that dont have a single unambiguous parent In most cases a symbolic link see page is a better optionYou can use ls l to see how many links to a given file exist See the ls example output on page for some additional details Also note the comments regardingls i on page as this option is particularly helpful for identifying hard linksHard links are not a distinct type of file Instead of defining a separate thing calleda hard link the filesystem simply allows more than one directory entry to point tothe same file In addition to the files contents the underlying attributes of the filesuch as ownerships and permissions are also sharedCharacter and block device filesDevice files let programs communicate with the systems hardware and peripheralsThe kernel includes or loads driver software for each of the systems devices Thissoftware takes care of the messy details of managing each device so that the kernelitself can remain relatively abstract and hardwareindependentDevice drivers present a standard communication interface that looks like a regularfile When the filesystem is given a request that refers to a character or block devicefile it simply passes the request to the appropriate device driver Its important todistinguish device files from device drivers however The files are just rendezvouspoints that communicate with drivers They are not drivers themselvesThe distinction between character and block devices is subtle and not worth reviewing in detail In the past a few types of hardware were represented by bothblock and character device files but that configuration is rare today As a matter ofpractice FreeBSD has done away with block devices entirely though their spectralpresence can still be glimpsed in man pages and header filesDevice files are characterized by two numbers called the major and minor devicenumbers The major device number tells the kernel which driver the file refers toand the minor device number typically tells the driver which physical unit to address For example major device number on a Linux system denotes the serialdriver The first serial port devtty would have major device number andminor device number Drivers can interpret the minor device numbers that are passed to them in whateverway they please For example tape drivers use the minor device number to determine whether the tape should be rewound when the device file is closedIn the distant past dev was a generic directory and the device files within it werecreated with mknod and removed with rm Unfortunately this crude system wasillequipped to deal with the endless sea of drivers and device types that have appeared over the last few decades It also facilitated all sorts of potential configuration mismatches device files that referred to no actual device devices inaccessiblebecause they had no device files and so onThese days the dev directory is normally mounted as a special filesystem type andits contents are automatically maintained by the kernel in concert with a userlevel daemon There are a couple of different versions of this same basic system SeeSee Chapter for more information about devicesand driversChapter Drivers and the Kernel for more information about each systemsapproach to this taskLocal domain socketsSockets are connections between processes that allow them to communicate hygienically UNIX defines several kinds of sockets most of which involve the networkLocal domain sockets are accessible only from the local host and are referred tothrough a filesystem object rather than a network port They are sometimes knownas UNIX domain sockets Syslog and the X Window System are examples of standard facilities that use local domain sockets but there are many more includingmany databases and app serversLocal domain sockets are created with the socket system call and removed with therm command or the unlink system call once they have no more usersNamed pipesLike local domain sockets named pipes allow communication between two processes running on the same host Theyre also known as FIFO files As in financialaccounting FIFO is short for the phrase first in first out You can create namedpipes with mknod and remove them with rmNamed pipes and local domain sockets serve similar purposes and the fact thatboth exist is essentially a historical artifact Most likely neither of them would existif UNIX and Linux were designed today network sockets would stand in for bothSymbolic linksA symbolic or soft link points to a file by name When the kernel comes upon asymbolic link in the course of looking up a pathname it redirects its attention tothe pathname stored as the contents of the link The difference between hard linksand symbolic links is that a hard link is a direct reference whereas a symbolic linkis a reference by name Symbolic links are distinct from the files they point toYou create symbolic links with ln s and remove them with rm Since symboliclinks can contain arbitrary paths they can refer to files on other filesystems or tononexistent files A series of symbolic links can also form a loopA symbolic link can contain either an absolute or a relative path For example sudo ln s archivedsecure vardatasecurelinks vardatasecure to vardataarchivedsecure with a relative path It createsthe symbolic link vardatasecure with a target of archivedsecure as demonstrated by this output from ls ls l vardatasecurelrwxrwxrwx root root Aug vardatasecure archivedsecureSee Chapter for more information about syslogThe entire vardata directory could then be moved elsewhere without causing thesymbolic link to stop workingThe file permissions that ls shows for a symbolic link lrwxrwxrwx are dummy values Permission to create remove or follow the link is controlled by the containingdirectory whereas read write and execute permission on the link target are grantedby the targets own permissions Therefore symbolic links do not need and do nothave any permission information of their ownA common mistake is to think that the first argument to ln s is interpreted relative tothe current working directory However that argument is not actually resolved as afilename by ln its simply a literal string that becomes the target of the symbolic link File attributesUnder the traditional UNIX and Linux filesystem model every file has a set of ninepermission bits that control who can read write and execute the contents of thefile Together with three other bits that primarily affect the operation of executableprograms these bits constitute the files modeThe twelve mode bits are stored along with four bits of filetype information Thefour filetype bits are set when the file is first created and cannot be changed butthe files owner and the superuser can modify the twelve mode bits with the chmodchange mode command Use ls l or ls ld for a directory to inspect the valuesof these bits See page for an exampleThe permission bitsNine permission bits determine what operations can be performed on a file and bywhom Traditional UNIX does not allow permissions to be set per user althoughall systems now support access control lists of one sort or another see page Instead three sets of permissions define access for the owner of the file the groupowners of the file and everyone else in that order Each set has three bits a readbit a write bit and an execute bit also in that orderIts convenient to discuss file permissions in terms of octal base numbers becauseeach digit of an octal number represents three bits and each group of permissionbits consists of three bits The topmost three bits with octal values of and control access for the owner The second three and control accessfor the group The last three and control access for everyone else theworld In each triplet the high bit is the read bit the middle bit is the write bitand the low bit is the execute bit If you think of the owner as the user and everyone else as other you can remember the order ofthe permission sets by thinking of the name Hugo u g and o are also the letter codes used by themnemonic version of chmodAlthough a user might fit into two of the three permission categories only the mostspecific permissions apply For example the owner of a file always has access determined by the owner permission bits and never by the group permission bits It ispossible for the other and group categories to have more access than the owneralthough this configuration would be highly unusualOn a regular file the read bit allows the file to be opened and read The write bit allows the contents of the file to be modified or truncated however the ability to deleteor rename or delete and then recreate the file is controlled by the permissionson its parent directory where the nametodataspace mapping is actually storedThe execute bit allows the file to be executed Two types of executable files existbinaries which the CPU runs directly and scripts which must be interpreted bya shell or some other program By convention scripts begin with a line similar tousrbinperlthat specifies an appropriate interpreter Nonbinary executable files that do notspecify an interpreter are assumed to be sh scriptsFor a directory the execute bit often called the search or scan bit in this contextallows the directory to be entered or passed through as a pathname is evaluatedbut not to have its contents listed The combination of read and execute bits allowsthe contents of the directory to be listed The combination of write and execute bitsallows files to be created deleted and renamed within the directoryA variety of extensions such as access control lists see page SELinux seepage and bonus permission bits defined by individual filesystems see page complicate or override the traditional bit permission model If youre having trouble explaining the systems observed behavior check to see whether one ofthese factors might be interferingThe setuid and setgid bitsThe bits with octal values and are the setuid and setgid bits When set onexecutable files these bits allow programs to access files and processes that wouldotherwise be offlimits to the user that runs them The setuidsetgid mechanismfor executables is described on page When set on a directory the setgid bit causes newly created files within the directory to take on the group ownership of the directory rather than the default groupof the user that created the file This convention makes it easier to share a directoryof files among several users as long as they belong to a common group This inter The kernel understands the shebang syntax and acts on it directly However if the interpreter isnot specified completely and correctly the kernel will refuse to execute the file The shell then makesa second attempt to execute the script by calling binsh which is usually a link to the Almquist shellor to bash see page Sven Mascheck maintains an excruciatingly detailed page about the historyimplementation and crossplatform behavior of the shebang at googlJizhLpretation of the setgid bit is unrelated to its meaning when set on an executable filebut no ambiguity can exist as to which meaning is appropriateThe sticky bitThe bit with octal value is called the sticky bit It was of historical importanceas a modifier for executable files on early UNIX systems However that meaningof the sticky bit is now obsolete and modern systems silently ignore the sticky bitwhen its set on regular filesIf the sticky bit is set on a directory the filesystem wont allow you to delete or rename a file unless you are the owner of the directory the owner of the file or thesuperuser Having write permission on the directory is not enough This conventionhelps make directories like tmp a little more private and securels list and inspect filesThe filesystem maintains about forty separate pieces of information for each file butmost of them are useful only to the filesystem itself As a system administrator youwill be concerned mostly with the link count owner group mode size last accesstime last modification time and type You can inspect all these with ls l or ls ldfor a directory without the d flag ls lists the directorys contentsAn attribute change time is also maintained for each file The conventional name forthis time the ctime short for change time leads some people to believe that itis the files creation time Unfortunately it is not it just records the time at whichthe attributes of the file owner mode etc were last changed as opposed to thetime at which the files contents were modifiedConsider the following example ls l usrbingziprwxrxrx root wheel Nov usrbingzipThe first field specifies the files type and mode The first character is a dash so thefile is a regular file See Table on page for other codesThe next nine characters in this field are the three sets of permission bits The orderis ownergroupother and the order of bits within each set is readwriteexecuteAlthough these bits have only binary values ls shows them symbolically with theletters r w and x for read write and execute In this case the owner has all permissions on the file and everyone else has read and execute permissionIf the setuid bit had been set the x representing the owners execute permissionwould have been replaced with an s and if the setgid bit had been set the x for thegroup would also have been replaced with an s The last character of the permissionsexecute permission for other is shown as t if the sticky bit of the file is turned onIf either the setuidsetgid bit or the sticky bit is set but the corresponding executebit is not these bits are shown as S or TThe next field in the listing is the files link count In this case it is indicating thatusrbingzip is just one of four names for this file the others on this system aregunzip gzcat and zcat all in usrbin Each time a hard link is made to a filethe files link count is incremented by Symbolic links do not affect the link countAll directories have at least two hard links the link from the parent directory andthe link from the special file called inside the directory itselfThe next two fields in the ls output are the owner and group owner of the file Inthis example the files owner is root and the file belongs to the group named wheelThe filesystem actually stores these as the user and group ID numbers rather thanas names If the text versions names cant be determined ls shows the fields asnumbers This might happen if the user or group that owns the file has been deletedfrom the etcpasswd or etcgroup file It could also suggest a problem with yourLDAP database if you use one see Chapter The next field is the size of the file in bytes This file is bytes long Next comesthe date of last modification November The last field in the listing is thename of the file usrbingzipls output is slightly different for a device file For example ls l devttycrww root tty Aug devttyMost fields are the same but instead of a size in bytes ls shows the major and minor device numbers devtty is the first virtual console on this Red Hat systemand is controlled by device driver the terminal driver The dot at the end of themode indicates the absence of an access control list ACL discussed starting onpage Some systems show this by default and some dontOne ls option thats useful for scoping out hard links is i which tells ls to showeach files inode number Briefly the inode number is an integer associated withthe contents of a file Inodes are the things that are pointed to by directory entriesentries that are hard links to the same file have the same inode number To figureout a complex web of links you need both ls li to show link counts and inodenumbers and find to search for matchesSome other ls options that are important to know are a to show all entries in adirectory even files whose names start with a dot t to sort files by modificationtime or tr to sort in reverse chronological order F to show the names of files ina way that distinguishes directories and executable files R to list recursively andh to show file sizes in humanreadable form eg K or MMost versions of ls now default to colorcoding files if your terminal program supports this most do ls specifies colors according to a limited and abstract palettered blue etc and its up to the terminal program to map these requests tospecific colors You may need to tweak both ls the LSCOLORS or LSCOLORS Try find mountpoint xdev inum inode printenvironment variable and the terminal emulator to achieve colors that are readableand unobtrusive Alternatively you can just remove the default configuration forcolorization usually etcprofiledcolorls to eliminate colors entirelychmod change permissionsThe chmod command changes the permissions on a file Only the owner of the fileand the superuser can change a files permissions To use the command on earlyUNIX systems you had to learn a bit of octal notation but current versions acceptboth octal notation and a mnemonic syntax The octal syntax is generally more convenient for administrators but it can only be used to specify an absolute value for thepermission bits The mnemonic syntax can modify some bits but leave others aloneThe first argument to chmod is a specification of the permissions to be assignedand the second and subsequent arguments are names of files on which permissionsshould be changed In the octal case the first octal digit of the specification is for theowner the second is for the group and the third is for everyone else If you want toturn on the setuid setgid or sticky bits you use four octal digits rather than threewith the three special bits forming the first digitTable illustrates the eight possible combinations for each set of three bits wherer w and x stand for read write and executeTable Permission encoding for chmodOctal Binary Perms Octal Binary Perms r x rx w rw wx rwxFor example chmod myprog gives all permissions to the user owner andexecuteonly permission to everyone elseFor the mnemonic syntax you combine a set of targets u g or o for user groupother or a for all three with an operator to add remove or set and a setof permissions The chmod man page gives the details but the syntax is probablybest learned by example Table exemplifies some mnemonic operationsThe hard part about using the mnemonic syntax is remembering whether o stands forowner or other other is correct Just remember u and g by analogy to UID andGID only one possibility is left Or remember the order of letters in the name Hugo If myprog were a shell script it would need both read and execute permission turned on For thescript to be run by an interpreter it must be opened and read like a text file Binary files are executeddirectly by the kernel and therefore do not need read permission turned onTable Examples of chmods mnemonic syntaxSpec Meaninguw Adds write permission for the owner of the fileugrwor Gives rw permission to owner and group and read permission to othersax Removes execute permission for all categories ownergroupotherugsrxo Makes setuidsetgid and gives rx permission to only owner and groupgu Makes the group permissions be the same as the owner permissionsOn Linux systems you can also specify the modes to be assigned by copying themfrom an existing file For example chmod referencefilea fileb makes filebsmode the same as fileasWith the R option chmod recursively updates the file permissions within a directory However this feat is trickier than it looks because the enclosed files anddirectories may not share the same attributes for example some might be executable files others text files Mnemonic syntax is particularly useful with R becauseit preserves bits whose values you dont set explicitly For example chmod R gw mydiradds group write permission to mydir and all its contents without messing up theexecute bits of directories and programsIf you want to adjust execute bits be wary of chmod R Its blind to the fact thatthe execute bit has a different interpretation on a directory than it does on a flatfile Therefore chmod R ax probably wont do what you intend Use find to selectonly the regular files find mydir type f exec chmod ax chown and chgrp change ownership and groupThe chown command changes a files ownership and the chgrp command changesits group ownership The syntax of chown and chgrp mirrors that of chmod exceptthat the first argument is the new owner or group respectivelyTo change a files group you must either be the superuser or be the owner of thefile and belong to the group youre changing to Older systems in the SysV lineageallowed users to give away their own files with chown but thats unusual these dayschown is now a privileged operationLike chmod chown and chgrp offer the recursive R flag to change the settings ofa directory and all the files underneath it For example the sequence sudo chown R matt mattrestore sudo chgrp R staff mattrestorecould reset the owner and group of files restored from a backup for the user mattDont try to chown dot files with a command such as sudo chown R matt mattsince the pattern matches matt and therefore ends up changing the ownershipsof the parent directory and probably the home directories of other userschown can change both the owner and group of a file at once with the syntaxchown usergroup file For example sudo chown R mattstaff mattrestoreYou can actually omit either user or group which makes the chgrp command superfluous If you include the colon but name no specific group the Linux versionof chown uses the users default groupSome systems accept the notation usergroup as being equivalent to usergroup Thisis just a nod to historical variation among systems it means the same thingumask assign default permissionsYou can use the builtin shell command umask to influence the default permissionsgiven to the files you create Every process has its own umask attribute the shellsbuiltin umask command sets the shells own umask which is then inherited bycommands that you runThe umask is specified as a threedigit octal value that represents the permissionsto take away When a file is created its permissions are set to whatever the creatingprogram requests minus whatever the umask forbids Thus the individual digitsof the umask allow the permissions shown in Table Table Permission encoding for umaskOctal Binary Perms Octal Binary Perms rwx wx rw w rx x r For example umask allows all permissions for the owner but forbids writepermission to the group and allows no permissions for anyone else The defaultumask value is often which denies write permission to the group and worldbut allows read permissionIn the standard access control model you cannot force users to have a particularumask value because they can always reset it to whatever they want However youcan put a suitable default in the sample startup files that you give to new users Ifyou require more control over the permissions on usercreated files youll needto graduate to a mandatory access control system such as SELinux see page Linux bonus flagsLinux defines a set of supplemental flags that can be set on files to request specialhandling For example the a flag makes a file appendonly and the i flag makes itimmutable and undeletableFlags have binary values so they are either present or absent for a given file Theunderlying filesystem implementation must support the corresponding feature sonot all flags can be used on all filesystem types In addition some flags are experimental unimplemented or readonlyLinux uses the commands lsattr and chattr to view and change file attributes Table lists some of the more mainstream flagsTable Linux file attribute flagsFlag FSa MeaningA XBE Never update access time statime for performancea XBE Allow writing only in append mode bC B Disable copyonwrite updatesc B Compress contentsD BE Force directory updates to be written synchronouslyd XBE Do not back up backup utilities should ignore this filei XBE Make file immutable and undeletable bj E Keep a journal for data changes as well as metadataS XBE Force changes to be written synchronously no bufferingX B Avoid data compression if it is the defaulta X XFS B Btrfs E ext and extb Can be set only by rootAs might be expected from such a random grab bag of features the value of theseflags to administrators varies The main thing to remember is that if a particularfile seems to be behaving strangely check it with lsattr to see if it has one or moreflags enabledWaiving maintenance of lastaccess times the A flag can boost performance insome situations However its value depends on the filesystem implementationand access pattern youll have to do your own benchmarking In addition modSee Chapter formore informationabout startup filesern kernels now default to mounting filesystems with the relatime option whichminimizes updates to statime and makes the A flag largely obsoleteThe immutable and appendonly flags i and a were largely conceived as ways tomake the system more resistant to tampering by hackers or hostile code Unfortunately they can confuse software and protect only against hackers that dont knowenough to use chattr ia Realworld experience has shown that these flags are moreoften used by hackers than against themWe have seen several cases in which admins have used the i immutable flag toprevent changes that would otherwise be imposed by a configuration managementsystem such as Ansible or Salt Needless to say this hack creates confusion oncethe details have been forgotten and no one can figure out why configuration management isnt working Never do thisjust think of the shame your mother wouldfeel if she knew what youd been up to Fix the issue within the configuration management system like Mom would wantThe no backup flag d is potentially of interest to administrators but since its anadvisory flag make sure that your backup system honors itFlags that affect journaling and write synchrony D j and S exist primarily tosupport databases They are not of general use for administrators All these optionscan reduce filesystem performance significantly In addition tampering with writesynchrony has been known to confuse fsck on ext filesystems Access control listsThe traditional bit ownergroupother access control system is powerful enoughto accommodate the vast majority of administrative needs Although the systemhas clear limitations its very much in keeping with the UNIX traditions somemight say former traditions of simplicity and predictabilityAccess control lists aka ACLs are a more powerful but also more complicated wayof regulating access to files Each file or directory can have an associated ACL thatlists the permission rules to be applied to it Each of the rules within an ACL iscalled an access control entry or ACEAn access control entry identifies the user or group to which it applies and specifies a set of permissions to be applied to those entities ACLs have no set lengthand can include permission specifications for multiple users or groups Most OSeslimit the length of an individual ACL but the limit is high enough usually at least entries that it rarely comes into playThe more sophisticated ACL systems let administrators specify partial sets of permissions or negative permissions Most also have inheritance features that allowaccess specifications to propagate to newly created filesystem entitiesSee Chapter formore informationabout configurationmanagementA cautionary noteACLs are widely supported and occupy our attention for the rest of this chapterHowever neither of these facts should be interpreted as an encouragement to embrace them ACLs have a niche but it lies outside the mainstream of UNIX andLinux administrationACLs exist primarily to facilitate Windows compatibility and to serve the needs ofthe small segment of enterprises that actually require ACLlevel flexibility Theyare not the shiny next generation of access control and are not intended to supplantthe traditional modelACLs complexity creates several potential problems Not only are ACLs tedious touse but they can also cause unexpected interactions with ACLunaware backupsystems network file service peers and even simple programs such as text editorsACLs also tend to become increasingly unmaintainable as the number of entriesgrows Realworld ACLs frequently include vestigial entries and entries that serveonly to compensate for issues caused by previous entries Its possible to refactorand simplify these complex ACLs but thats risky and time consuming so it rarelygets doneIn the past copies of this chapter that weve sent out to professional administratorsfor review have often come back with notes such as This part looks fine but I cantreally say because Ive never used ACLsACL typesTwo types of ACLs have emerged as the predominant standards for UNIX and Linux POSIX ACLs and NFSv ACLsThe POSIX version dates back to specification work done in the mids Unfortunately no actual standard was ever issued and initial implementations variedwidely These days we are in much better shape Systems have largely convergedon a common framing for POSIX ACLs and a common command set getfacl andsetfacl for manipulating themTo a first approximation the POSIX ACL model simply extends the traditional UNIXrwx permission system to accommodate permissions for multiple groups and usersAs POSIX ACLs were coming into focus it became increasingly common for UNIXand Linux to share filesystems with Windows which has its own set of ACL conventions Here the plot thickens because Windows makes a variety of distinctionsthat are not found in either the traditional UNIX model or its POSIX ACL equivalent Windows ACLs are semantically more complex too for example they allownegative permissions deny entries and have a complicated inheritance schemeThe architects of version of NFSa common filesharing protocolwanted toincorporate ACLs as a firstclass entity Because of the UNIXWindows split and theinconsistencies among UNIX ACL implementations it was clear that the systemsSee Chapter for more information about NFSon the ends of an NFSv connection might often be of different types Each systemmight understand NFSv ACLs POSIX ACLs Windows ACLs or no ACLs at allThe NFSv standard would have to be interoperable with all these various worldswithout causing too many surprises or security problemsGiven this constraint its perhaps not surprising that NFSv ACLs are essentially aunion of all preexisting systems They are a strict superset of POSIX ACLs so anyPOSIX ACL can be represented as an NFSv ACL without loss of information Atthe same time NFSv ACLs accommodate all the permission bits found on Windows systems and they have most of Windows semantic features as wellImplementation of ACLsIn theory responsibility for maintaining and enforcing ACLs could be assigned toseveral different components of the operating system ACLs could be implementedby the kernel on behalf of all the systems filesystems by individual filesystems orperhaps by higherlevel software such as NFS and SMB serversIn practice ACL support is both OSdependent and filesystemdependent A filesystem that supports ACLs on one system might not support them on another or itmight feature a somewhat different implementation managed by different commandsFile service daemons map their hosts native ACL scheme or schemes to and fromthe conventions appropriate to the filing protocol NFSv ACLs for NFS and Windows ACLs for SMB The details of that mapping depend on the implementationof the file server Usually the rules are complicated and somewhat tunable withconfiguration optionsBecause ACL implementations are filesystemspecific and because systems supportmultiple filesystem implementations some systems end up supporting multipletypes of ACLs Even a given filesystem might offer several ACL options as seen inthe various ports of ZFS If multiple ACL systems are available the commands tomanipulate them might be the same or different it depends on the system Welcome to sysadmin hellLinux ACL supportLinux has standardized on POSIXstyle ACLs NFSv ACLs are not supported atthe filesystem level though of course Linux systems can mount and share NFSvfilesystems over the networkAn advantage of this standardization is that nearly all Linux filesystems now includePOSIX ACL support including XFS Btrfs and the ext family Even ZFS whosenative ACL system is NFSvish has been ported to Linux with POSIX ACLs Thestandard getfacl and setfacl commands can be used everywhere without regard tothe underlying filesystem type You may however need to ensure that the correctmount option has been used to mount the filesystem Filesystems generally supportan acl option a noacl option or both depending on their defaultsSee Chapter for more information about SMBLinux does have a command suite nfsgetfacl nfssetfacl and nfseditfaclfor grooming the NFSv ACLs of files mounted from NFS servers However thesecommands cannot be used on locally stored files Moreover they are rarely includedin distributions default software inventory youll have to install them separatelyFreeBSD ACL supportFreeBSD supports both POSIX ACLs and NFSv ACLs Its native getfacl and setfaclcommands have been extended to include NFSvstyle ACL wrangling NSFv ACLsupport is a relatively recent as of developmentAt the filesystem level both UFS and ZFS support NFSvstyle ACLs and UFS supports POSIX ACLs as well The potential point of confusion here is ZFS which isNFSvonly on BSD and on Solaris its system of origin and POSIXonly on LinuxFor UFS use one of the mount options acls or nfsvacls to specify which worldyou want to live in These options are mutually exclusivePOSIX ACLsPOSIX ACLs are a mostly straightforward extension of the standard bit UNIXpermission model Read write and execute permission are the only capabilitiesthat the ACL system deals with Embellishments such as the setuid and sticky bitsare handled exclusively through the traditional mode bitsACLs allow the rwx bits to be set independently for any combination of users andgroups Table shows what the individual entries in an ACL can look likeTable Entries that can appear in POSIX ACLsFormat Example Sets permissions foruser perms userrw The files owneruser username perms usertrentrw A specific usergroup perms grouprx The group that owns the filegroup groupname perms groupstaffrw A specific groupother perms other All othersmask perms maskrwx All but owner and other aa Masks are somewhat tricky and are explained later in this sectionUsers and groups can be identified by name or by UIDGID The exact number ofentries that an ACL can contain varies with the filesystem implementation but isusually at least Thats probably about the practical limit for manageability anywayInteraction between traditional modes and ACLsFiles with ACLs retain their original mode bits but consistency is automaticallyenforced and the two sets of permissions can never conflict The following example demonstrates that the ACL entries automatically update in response to changesmade with the standard chmod command touch example ls l examplerwrwr garth garth Jun example getfacl example file example owner garth group garthuserrwgrouprwotherr chmod example ls l examplerwr garth garth Jun example getfacl omitheader exampleuserrwgrouprotherThis enforced consistency allows older software with no awareness of ACLs to playreasonably well in the ACL world However theres a twist Even though the groupACL entry in the example above appears to be tracking the middle set of traditionalmode bits that will not always be the caseTo understand why suppose that a legacy program clears the write bits within allthree permission sets of the traditional mode eg chmod ugow file The intention is clearly to make the file unwritable by anyone But what if the resulting ACLwere to look like thisuserrgrouprgroupstaffrwotherrFrom the perspective of legacy programs the file appears to be unmodifiable yetit is actually writable by anyone in group staff Not good To reduce the chance ofambiguity and misunderstandings the following rules are enforced The user and other ACL entries are by definition identical to the owner and other permission bits from the traditional mode Changing themode changes the corresponding ACL entries and vice versa This example is from Linux The FreeBSD version of getfacl uses q instead of omitheader to suppress the commentlike lines in the output In all cases the effective access permission afforded to the files owner andto users not mentioned in another way are those specified in the userand other ACL entries respectively If a file has no explicitly defined ACL or has an ACL that consists of onlyone user one group and one other entry these ACL entries areidentical to the three sets of traditional permission bits This is the caseillustrated in the getfacl example above Such an ACL is termed minimal and need not actually be implemented as a logically separate ACL In more complex ACLs the traditional group permission bits correspondto a special ACL entry called mask rather than the group ACL entryThe mask limits the access that the ACL can confer on all named usersall named groups and the default groupIn other words the mask specifies an upper bound on the access that the ACL canassign to individual groups and users It is conceptually similar to the umask exceptthat the ACL mask is always in effect and that it specifies the allowed permissionsrather than the permissions to be denied ACL entries for named users namedgroups and the default group can include permission bits that are not present inthe mask but filesystems simply ignore themAs a result the traditional mode bits can never understate the access allowed by theACL as a whole Furthermore clearing a bit from the group portion of the traditionalmode clears the corresponding bit in the ACL mask and thereby forbids this permission to everyone but the files owner and those who fall in the category of otherWhen the ACL shown in the previous example is expanded to include entries for aspecific user and group setfacl automatically supplies an appropriate mask ls l examplerwr garth garth Jun example setfacl m userrusertrentrwgroupadminrw example ls l examplerrw garth garth Jun example getfacl omitheader exampleuserrusertrentrwgrouprgroupadminrwmaskrwotherThe m option to setfacl means modify it adds entries that are not already presentand adjusts those that are already there Note that setfacl automatically generates amask that allows all the permissions granted in the ACL to take effect If you wantto set the mask by hand include it in the ACL entry list given to setfacl or use then option to prevent setfacl from regenerating itNote that after the setfacl command ls l shows a sign at the end of the filesmode to denote that it now has a real ACL associated with it The first ls l showsno because at that point the ACL is minimalIf you use the traditional chmod command to manipulate an ACLbearing file beaware that your settings for the group permissions affect only the mask To continue the previous example chmod example ls l examplerwxrwx garth staff Jun example getfacl omitheader exampleuserrwxusertrentrwgrouprgroupadminrwmaskrwxotherThe ls output in this case is misleading Despite the apparently generous grouppermissions no one actually has permission to execute the file by reason of groupmembership To grant such permission you must edit the ACL itselfTo remove an ACL entirely and revert to the standard UNIX permission systemuse setfacl bn Strictly speaking the n flag is needed only on FreeBSD Withoutit FreeBSDs setfacl leaves you with a vestigial mask entry that will screw up latergroupmode changes However you can include the n on Linux without creatingproblemsPOSIX access determinationWhen a process attempts to access a file its effective UID is compared to the UIDthat owns the file If they are the same access is determined by the ACLs userpermissions Otherwise if a matching userspecific ACL entry exists permissionsare determined by that entry in combination with the ACL maskIf no userspecific entry is available the filesystem tries to locate a valid grouprelatedentry that authorizes the requested access these entries are processed in conjunctionwith the ACL mask If no matching entry can be found the other entry prevailsPOSIX ACL inheritanceIn addition to the ACL entry types listed in Table on page the ACLs fordirectories can include default entries that are propagated to the ACLs of newlycreated files and subdirectories created within them Subdirectories receive theseentries both in the form of active ACL entries and in the form of copies of the default entries Therefore the original default entries may eventually propagate downthrough several layers of the directory hierarchyOnce default entries have been copied to new subdirectories there is no ongoingconnection between the parent and child ACLs If the parents default entries changethose changes are not reflected in the ACLs of existing subdirectoriesYou can set default ACL entries with setfacl dm Alternatively you can include default entries within a regular access control entry list by prefixing them with defaultIf a directory has any default entries it must include a full set of defaults for usergroup other and mask setfacl will fill in any default entries you dont specify by copying them from the current permissions ACL generating a summarymask as usualNFSv ACLsIn this section we discuss the characteristics of NFSv ACLs and briefly review thecommand syntax used to set and inspect them on FreeBSD They arent supportedon Linux other than by NFS service daemonsFrom a structural perspective NFSv ACLs are similar to Windows ACLs Themain difference between them lies in the specification of the entity to which anaccess control entry refersIn both systems the ACL stores this entity as a string For Windows ACLs thestring typically contains a Windows security identifier SID whereas for NFSvthe string is typically of the form userusername or groupgroupname It can alsobe one of the special tokens owner group or everyone These latter entries arethe most common because they correspond to the mode bits found on every fileSystems such as Samba that share files between UNIX and Windows systems mustprovide some way of mapping between Windows and NFSv identitiesThe NFSv and Windows permission models are more granular than the traditionalUNIX readwriteexecute model In the case of NFSv the main refinements areas follows NFSv distinguishes permission to create files within a directory frompermission to create subdirectories NFSv has a separate append permission bit NFSv has separate read and write permissions for data file attributesextended attributes and ACLs NFSv controls a users ability to change the ownership of a file throughthe standard ACL system In traditional UNIX the ability to change theownership of files is usually reserved for rootTable on the next page shows the various permissions that can be assigned inthe NFSv system It also shows the oneletter codes used to represent them andthe more verbose canonical namesTable NFSv file permissionsCode Verbose name Permissionr readdata Read data file or list directory contents directoryw writedata Write data file or create file directoryx execute Execute as a programp appenddata Append data file or create subdirectory directoryD deletechild Delete child within a directoryd delete Deletea readattributes Read nonextended attributesA writeattributes Write nonextended attributesR readxattr Read named extended attributesW writexattr Write named extended attributesc readacl Read access control listC writeacl Write access control listo writeowner Change ownerships synchronize Allow requests for synchronous IO usually ignoredAlthough the NFSv permission model is fairly detailed the individual permissionsshould mostly be selfexplanatory The synchronize permission allows a client tospecify that its modifications to a file should be synchronousthat is calls to writeshould not return until the data has actually been saved on diskAn extended attribute is a named chunk of data that is stored along with a file mostmodern filesystems support such attributes At this point the predominant use ofextended attributes is to store ACLs themselves However the NFSv permissionmodel treats ACLs separately from other extended attributesIn FreeBSDs implementation a files owner always has readacl writeaclreadattributes and writeattributes permissions even if the files ACL itselfspecifies otherwiseNFSv entities for which permissions can be specifiedIn addition to the gardenvariety userusername and groupgroupname specifiersNFSv defines several special entities that may be assigned permissions in an ACLMost important among these are owner group and everyone which correspondto the traditional categories in the bit permission modelNFSv has several differences from POSIX For one thing it has no default entityused in POSIX to control ACL inheritance Instead any individual access controlentry ACE can be flagged as inheritable see ACL inheritance in NFSv belowNFSv also does not use a mask to reconcile the permissions specified in a filesmode with its ACL The mode is required to be consistent with the settings specified for owner group and everyone and filesystems that implement NFSvACLs must preserve this consistency when either the mode or the ACL is updatedNFSv access determinationThe NFSv system differs from POSIX in that an ACE specifies only a partial setof permissions Each ACE is either an allow ACE or a deny ACE it acts morelike a mask than an authoritative specification of all possible permissions MultipleACEs can apply to any given situationWhen deciding whether to allow a particular operation the filesystem reads theACL in order processing ACEs until either all requested permissions have beengranted or some requested permission has been denied Only ACEs whose entitystrings are compatible with the current users identity are consideredThis iterative evaluation process means that owner group and everyone arenot exact analogs of the corresponding traditional mode bits An ACL can containmultiple copies of these elements and their precedence is determined by their order of appearance in the ACL rather than by convention In particular everyonereally does apply to everyone not just users who arent addressed more specificallyIts possible for the filesystem to reach the end of an ACL without having obtaineda definitive answer to a permission query The NFSv standard considers the resultto be undefined but realworld implementations deny access both because this isthe convention used by Windows and because its the only option that makes senseACL inheritance in NFSvLike POSIX ACLs NFSv ACLs allow newly created objects to inherit access control entries from their enclosing directory However the NFSv system is a bit morepowerful and a lot more confusing Here are the important points You can flag any ACE as inheritable Inheritance for newly created subdirectories dirinherit or d and inheritance for newly created filesfileinherit or f are flagged separately You can apply different access control entries to new files and new directories by creating separate access control entries on the parent directoryand flagging them appropriately You can also apply a single ACE to allnew child entities of whatever type by turning on both the d and f flags From the perspective of access determination access control entries havethe same effect on the parent source directory whether or not they areinheritable If you want an entry to apply to children but not to the parentdirectory itself turn on the ACEs inheritonly i flag New subdirectories normally inherit two copies of each ACE one with theinheritance flags turned off which applies to the subdirectory itself andone with the inheritonly flag turned on which sets up the new subdirectory to propagate its inherited ACEs You can suppress the creation ofthis second ACE by turning on the nopropagate n flag on the parentdirectorys copy of the ACE The end result is that the ACE propagatesonly to immediate children of the original directory Dont confuse the propagation of access control entries with true inheritance Your setting an inheritancerelated flag on an ACE simply meansthat the ACE will be copied to new entities It does not create any ongoingrelationship between the parent and its children If you later change theACE entries on the parent directory the children are not updatedTable summarizes these various inheritance flagsTable NFSv ACE inheritance flagsCode Verbose name Meaningf fileinherit Propagate this ACE to newly created filesd dirinherit Propagate this ACE to newly created subdirectoriesi inheritonly Propagate but dont apply to the current directoryn nopropagate Propagate to new subdirectories but turn off inheritanceNFSv ACL viewingFreeBSD has extended the standard setfacl and getfacl commands used with POSIXACLs to handle NFSv ACLs as well For example heres the ACL for a newly created directoryfreebsd mkdir examplefreebsd ls ld exampledrwxrxrx garth staff Aug example getfacl q example ownerrwxpaARWcCosallow grouprxaRcsallow everyonerxaRcsallowThe v flag requests verbose permission namesfreebsd getfacl qv exampleownerreaddatawritedataexecuteappenddatareadattributes writeattributesreadxattrwritexattrreadaclwriteacl writeownersynchronizeallowgroupreaddataexecutereadattributesreadxattrreadacl synchronizealloweveryonereaddataexecutereadattributesreadxattrreadacl synchronizeallowWe indented these output lines and wrappedthem at slashes toclarify structureThis newly created directory seems to have a complex ACL but in fact this is just thebit mode translated into ACLese It is not necessary for the filesystem to store anactual ACL because the ACL and the mode are equivalent As with POSIX ACLSsuch lists are termed minimal or trivial If the directory had an actual ACL lswould show the mode bits with a on the end ie drwxrxrx to mark its presenceEach clause represents one access control entry The format isentitypermissionsinheritanceflagstypeThe entity can be the keywords owner group or everyone or a form such asuserusername or groupgroupname Both the permissions and the inheritanceflagsare slashseparated lists of options in the verbose output and lsstyle bitmaps inshort output The type of an ACE is either allow or denyThe use of a colon as a subdivider within the entity field makes it tricky for scriptsto parse getfacl output no matter which output format you use If you need to process ACLs programmatically its best to do so through a modular API rather thanby parsing command outputInteractions between ACLs and modesThe mode and the ACL must remain consistent so whenever you adjust one ofthese entities the other automatically updates to conform to it Its easy for the system to determine the appropriate mode for a given ACL However emulating thetraditional behavior of the mode with a series of access control entries is trickierespecially in the context of an existing ACL The system must often generate multiple and seemingly inconsistent sets of entries for owner group and everyonethat depend on evaluation order for their aggregate effectAs a rule its best to avoid tampering with a files or directorys mode once youveapplied an ACLNFSv ACL setupBecause the permission system enforces consistency between a files mode andits ACL all files have at least a trivial ACL Ergo ACL changes are always updatesYou make ACL changes with the setfacl command much as you do under the POSIXACL regime The main difference is that order of access control entries is significantfor an NFSv ACL so you might need to insert new entries at a particular pointwithin the existing ACL You can do that with the a flagsetfacl a position entries file Here position is the index of the existing access control entry numbered starting atzero in front of which the new entries should be inserted For example the command setfacl a userbenfullsetdeny benkeepoutinstalls an access control entry on the file benkeepout that denies all permissions to the user ben The notation fullset is a shorthand notation that includes allpossible permissions Written out those would currently be rwxpDdaARWcCoscompare with Table on page Because the new access control entry is inserted at position zero its the first oneconsulted and takes precedence over later entries Ben will be denied access to thefile even if for example the everyone permissions grant access to other usersYou can also use long names such as writedata to identify permissions Separatemultiple long names with slashes You cannot mix singleletter codes and longnames in a single commandAs with POSIX ACLs you can use the m flag to add new entries to the end of theexisting ACLAs for complex changes to existing ACLs you can best achieve them by dumpingthe ACL to a text file editing the access control entries in a text editor and thenreloading the entire ACL For example getfacl q file tmpfileacl vi tmpfileacl Make any required changes setfacl b M tmpfileacl filesetfacls b option removes the existing ACL before adding the access control entries listed in fileacl Its inclusion lets you delete entries simply by removing themfrom the text fileThe installation configuration and management of software is a large part of mostsysadmins jobs Administrators respond to installation and configuration requestsfrom users apply updates to fix security problems and supervise transitions to newsoftware releases that may offer both new features and incompatibilities Generallyadministrators perform all the following tasks Automating mass installations of operating systems Maintaining custom OS configurations Keeping systems and applications patched and up to date Tracking software licenses Managing addon software packagesThe process of configuring an offtheshelf distribution or software package toconform to your needs and to your local conventions for security file placementand network topology is often referred to as localization This chapter exploressome techniques and software that help reduce the pain of software installation andmake these tasks scale more gracefully We also discuss the installation procedurefor each of our example operating systems including some options for automateddeployment that use common platformspecific tools Software Installation andManagement Operating system installationLinux distributions and FreeBSD have straightforward procedures for basic installation For physical hosts installation typically involves booting from external USBstorage or optical media answering a few basic questions optionally configuringdisk partitions and then telling the installer which software packages to installMost systems including all our example distributions include a live option onthe installation media that lets you run the operating system without actually installing it on a local diskInstalling the base operating system from local media is fairly trivial thanks to theGUI applications that shepherd you through the process Table lists pointers todetailed installation instructions for each of our example systemsTable Installation documentationSystem Documentation sourceRed Hat redhatcomdocsmanualsenterpriseCentOS wikicentosorgManualsReleaseNotesCentOSDebian debianorgreleasesstableinstallmanualUbuntu helpubuntucomltsserverguideinstallationhtmlFreeBSD freebsdorgdochandbookbsdinstallhtmlInstalling from the networkIf you have to install an operating system on more than one computer you willquickly reach the limits of interactive installation Its time consuming error proneand boring to repeat the standard installation process on hundreds of systems Youcan minimize human errors with a localization checklist but even this measuredoes not remove all potential sources of variationTo alleviate some of these problems you can use network installation options tosimplify deployments Network installations are appropriate for sites with morethan ten or so systems The most common methods use DHCP and TFTP to bootthe system sans physical media They then retrieve the OS installation files from anetwork server with HTTP NFS or FTPYou can set up completely handsfree installations through PXE the PrebooteXecution Environment This scheme is a standard from Intel that lets systemsboot from a network interface It works especially well in virtualized environmentsPXE acts like a miniature OS that sits in a ROM on your network card It exposes itsnetwork capabilities through a standardized API for the system BIOS to use Thiscooperation makes it possible for a single boot loader to netboot any PXEenabledPC without having to supply special drivers for each network cardThe external network portion of the PXE protocol is straightforward and is similarto the netboot procedures used on other architectures A host broadcasts a DHCPdiscover request with the PXE flag turned on and a DHCP server or proxy responds with a DHCP packet that includes PXE options the name of a boot serverand boot file The client downloads its boot file over TFTP or optionally multicast TFTP and then executes it The PXE boot procedure is depicted in Exhibit AExhibit A PXE boot and installation processDHCP request includes PXE optionsDHCP response points to TFTP boot serverRequest boot image via TFTPServe boot image and congurationRequest installation image via HTTPNFSotherServe installation lesNetbootclientNetbootserverThe DHCP TFTP and file servers can all be located on different hosts The TFTPprovided boot file includes a menu with pointers to the available OS boot imageswhich can then be retrieved from the file server with HTTP FTP NFS or someother network protocolPXE booting is most commonly used in conjunction with unattended installationtools such as Red Hats kickstart or Debians preseeding system as discussed in theupcoming sections You can also use PXE to boot diskless systems such as thin clientsCobbler discussed on page includes some glue that makes netbooting mucheasier However you will still need a working knowledge of the tools that underlieCobbler beginning with PXESetting up PXEThe most widely used PXE boot system is H Peter Anvins PXELINUX whichis part of his SYSLINUX suite of boot loaders for every occasion Check it out atsyslinuxorg Another option is iPXE ipxeorg which supports additional bootstrapping modes including support for wireless networksPXELINUX supplies a boot file that you install in the TFTP servers tftpboot directory To boot from the network a PC downloads the PXE boot loader and itsconfiguration from the TFTP server The configuration file lists one or more options for operating systems to boot The system can boot through to a specific OSinstallation without any user intervention or it can display a custom boot menuSee page formore informationabout DHCPPXELINUX uses the PXE API for its downloads and is therefore hardware independent all the way through the boot process Despite the name PXELINUX is notlimited to booting Linux You can deploy PXELINUX to install FreeBSD and otheroperating systems including WindowsOn the DHCP side ISCs the Internet Systems Consortiums DHCP server is yourbest bet for serving PXE information Alternatively try Dnsmasq googlFNkaa lightweight server with DNS DHCP and netboot support Or simply use Cobbler discussed belowUsing kickstart the automated installer for Red Hat and CentOSKickstart is a Red Hatdeveloped tool for performing automated installations It isreally just a scripting interface to the standard Red Hat installer software Anaconda and depends both on the base distribution and on RPM packages Kickstart isflexible and quite smart about autodetecting the systems hardware so it works wellfor baremetal and virtual machines alike Kickstart installs can be performed fromoptical media the local hard drive NFS FTP or HTTPSetting up a kickstart configuration fileKickstarts behavior is controlled by a single configuration file generally calledkscfg The format of this file is straightforward If youre visually inclined RedHats handy GUI tool systemconfigkickstart lets you point and click your wayto kscfg nirvanaA kickstart config file consists of three ordered parts The first part is the commandsection which specifies options such as language keyboard and time zone Thissection also specifies the source of the distribution with the url option In the following example its a host called installserverHeres an example of a complete command sectiontextlang enUS lang is used during the installationlangsupport enUS and langsupport at run timekeyboard us Use an American keyboardtimezone utc AmericaEST utc means hardware clock is on GMTmouserootpw iscrypted NaClXjRlREyDqNTCXjHpreboot Reboot after installation Always wisebootloader locationmbr Install default boot loader in the MBRinstall Install a new system dont upgradeurl url httpinstallserverredhatclearpart all initlabel Clear all existing partitionspart fstype ext size part swap size part var fstype ext size grownetwork bootproto dhcpSee page for moreinformation aboutDHCP server softwareRHELauth useshadow enablemdfirewall disabledxconfig defaultdesktopGNOME startxonboot resolution xdepth Kickstart uses graphical mode by default which defeats the goal of unattended installation The text keyword at the top of the example fixes thisThe rootpw option sets the new machines root password The default is to specify the password in cleartext which presents a serious security problem Alwaysuse the iscrypted flag to specify a hashed password To encrypt a password foruse with kickstart use openssl passwd Still this option leaves all your systemswith the same root password Consider running a postboot process to change thepassword at build timeThe clearpart and part directives specify a list of disk partitions and their sizesYou can include the grow option to expand one of the partitions to fill any remaining space on the disk This feature makes it easy to accommodate systems thathave different sizes of hard disk Advanced partitioning options such as the use ofLVM are supported by kickstart but not by the systemconfigkickstart tool Refer to Red Hats online documentation for a complete list of disk layout optionsThe second section is a list of packages to install It begins with a packages directive The list can contain individual packages collections such as GNOME or thenotation Everything to include the whole shebang When selecting individualpackages specify only the package name not the version or the rpm extensionHeres an examplepackages Networked Workstation X Window System GNOMEmylocalpackageIn the third section of the kickstart configuration file you can specify arbitraryshell commands for kickstart to execute There are two possible sets of commandsone introduced with pre that runs before installation and one introduced withpost that runs afterward Both sections have some restrictions on the ability ofthe system to resolve hostnames so its safest to use IP addresses if you want toaccess the network In addition the postinstall commands are run in a chrootedenvironment so they cannot access the installation mediaThe kscfg file is quite easy to generate programmatically One option is to use thepykickstart Python library which can read and write kickstart configurationsFor example suppose you wanted to install different sets of packages on servers andclients and that you also have two separate physical locations that require slightlydifferent customizations You could use pykickstart to write a script that transformsa master set of parameters into a set of four separate configuration files one forservers and one for clients in each officeChanging the complement of packages would then be just a matter of changingthe master configuration file rather than of changing every possible configurationfile There may even be cases in which you need to generate individualized configuration files for specific hosts In this situation you would certainly want the finalkscfg files to be automatically generatedBuilding a kickstart serverKickstart expects its installation files called the installation tree to be laid outas they are on the distribution media with packages stored in a directory calledRedHatRPMS on the server If youre installing over the network via FTP NFSor HTTP you can either copy the contents of the distribution leaving the tree intact or you can simply use the distributions ISO images You can also add yourown packages to this directory There are however a couple of issues to be aware ofFirst if you tell kickstart to install all packages with an Everything in the packages section of your kscfg it installs addon packages in alphabetical order onceall the base packages have been laid down If your package depends on other packages that are not in the base set you might want to call your package somethinglike zzmypackagerpm to make sure that its installed lastIf you dont want to install all packages either list your supplemental packages individually in the packages section of the kscfg file or add your packages to one ormore of the collection lists Collection lists are specified by entries such as GNOMEand stand for a predefined set of packages whose members are enumerated in thefile RedHatbasecomps on the server The collections are the lines that begin with or the number specifies whether the collection is selected by defaultIn general its not a good idea to tamper with the standard collections Leave themas Red Hat defined them and explicitly name all your supplemental packages inthe kscfg filePointing kickstart at your config fileOnce youve created a config file you have a couple of ways to get kickstart to useit The officially sanctioned method is to boot from external media USB or DVDand ask for a kickstart installation by specifying linux instks at the initial bootprompt PXE boot is also an optionIf you dont specify additional arguments the system determines its network addresswith DHCP It then obtains the DHCP boot server and boot file options attemptsto mount the boot server with NFS and uses the value of the boot file option as itskickstart configuration file If no boot file has been specified the system looks fora file called kickstarthostipaddresskickstartSee page formore informationabout PXEAlternatively you can tell kickstart to get its configuration file in some other wayby supplying a path as an argument to the instks option There are several possibilities For example the instructionboot linux instkshttpserverpathtells kickstart to use HTTP to download the file instead of NFSTo eliminate the use of boot media entirely youll need to graduate to PXE Seepage for more information about thatAutomating installation for Debian and UbuntuDebian and Ubuntu can use the debianinstaller for preseeding the recommendedmethod for automated installation As with Red Hats kickstart a preconfigurationfile answers questions asked by the installerAll the interactive parts of the Debian installer use the debconf utility to decidewhich questions to ask and what default answers to use By giving debconf a database of preformulated answers you fully automate the installer You can eithergenerate the database by hand its a text file or you can perform an interactiveinstallation on an example system and then dump out your debconf answers withthe following commands sudo debconfgetselections installer preseedcfg sudo debconfgetselections preseedcfgMake the config file available on the net and then pass it to the kernel at installationtime with the following kernel argumentpreseedurlhttphostpathtopreseedThe syntax of the preseed file usually called preseedcfg is simple and is reminiscent of Red Hats kscfg The sample below has been shortened for simplicitydi debianinstallerlocale string enUSdi consolesetupaskdetect boolean falsedi consolesetuplayoutcode string usdi netcfgchooseinterface select autodi netcfggethostname string unassignedhostnamedi netcfggetdomain string unassigneddomaindi partmanautodisk string devsdadi partmanautomethod string lvmdi partmanautochooserecipe select atomicdi passwduserfullname string Daffy Duckdi passwdusername string dduckdi passwduserpasswordcrypted password mkqGitNxlVSM Prior to RHEL the option was ks Both are understood for now but future versions may drop ksdi usersetupencrypthome boolean falsetasksel taskselfirst multiselect ubuntudesktopdi grubinstalleronlydebian boolean truedi grubinstallerwithotheros boolean truedi finishinstallrebootinprogress notexserverxorg xserverxorgautodetectmonitor boolean trueSeveral options in this list simply disable dialogs that would normally require userinteraction For example the consolesetupaskdetect clause disables manualkeymap selectionThis configuration tries to identify a network interface thats actually connectedto a network chooseinterface select auto and obtains network informationthrough DHCP The system hostname and domain values are presumed to be furnished by DHCP and are not overriddenPreseeded installations cannot use existing partitions they must either use existing free space or repartition the entire disk The partman lines in the code aboveare evidence that the partmanauto package is being used for disk partitioningYou must specify a disk to install to unless the system has only one In this casedevsda is usedSeveral partitioning recipes are available atomic puts all the systems files in one partition home creates a separate partition for home multi creates separate partitions for home usr var and tmpYou can create users with the passwd series of directives As with kickstart configuration we strongly recommend the use of encrypted hashed password valuesPreseed files are often stored on HTTP servers and are apt to be discovered by curious users Of course a hashed password is still subject to brute force attack Usea long complex passwordThe task selection tasksel option chooses the type of Ubuntu system to installAvailable values include standard ubuntudesktop dnsserver lampserverkubuntudesktop edubuntudesktop and xubuntudesktopThe sample preseed file shown above comes from the Ubuntu installation documentation found at helpubuntucom This guide contains full documentation forthe syntax and usage of the preseed fileAlthough Ubuntu does not descend from the Red Hat lineage it has grafted compatibility with kickstart control files onto its own underlying installer Ubuntu alsoincludes the systemconfigkickstart tool for creating these files However the kickstart functionality in Ubuntus installer is missing a number of important featuresthat are supported by Red Hats Anaconda such as LVM and firewall configurationWe recommend sticking with the Debian installer unless you have a good reasonto choose kickstart eg to maintain compatibility with your Red Hat systemsNetbooting with Cobbler the open source Linux provisioning serverBy far the easiest way to bring netbooting services to your network is with Cobbler aproject originally written by Michael DeHaan prolific open source developer Cobblerenhances kickstart to remove some of its most tedious and repetitive administrative elements It bundles all the important netboot features including DHCP DNSand TFTP and helps you manage the OS images used to build physical and virtualmachines Cobbler includes commandline and web interfaces for administrationTemplates are perhaps Cobblers most interesting and useful feature Youll frequently need different kickstart and preseed settings for different host profiles Forexample you might have web servers in two data centers that apart from networksettings require the same configuration You can use Cobbler snippets to sharesections of the configuration between the two types of hostsA snippet is just a collection of shell commands For example this snippet adds apublic key to the authorized SSH keys for the root usermkdir p mode rootsshcat rootsshauthorizedkeys EOFsshrsa AAAABNzaCycEAAAADAQABAAABAQDKErzVdarNkLbzAZotSzU RooyRTCzcBtoqUKRlkuVEOFchmod rootsshauthorizedkeysYou save the snippet to Cobblers snippet directory then refer to it in a kickstarttemplate For example if you saved the snippet above as rootpubkeysnippetyou could refer to it in a template as followspostSNIPPETrootpubkeysnippetkickstartdoneUse Cobbler templates to customize disk partitions conditionally install packagescustomize time zones add custom package repositories and perform any otherkind of localization requirementCobbler can also create new virtual machines under a variety of hypervisors It canintegrate with a configuration management system to provision machines oncethey bootCobbler packages are available in the standard repositories for our sample Linuxdistributions You can also obtain packages and documentation from the CobblerGitHub project at cobblergithubioAutomating FreeBSD installationThe FreeBSD bsdinstall utility is a textbased installer that kicks off when you boota computer from a FreeBSD installation CD or DVD Its automation facilities arerudimentary compared to Red Hats kickstart or Debians preseed and the documentation is limited The best source of information is the bsdinstall man pageCreating a customized unattended installation image is a tedious affair that involves the following steps Download the latest installation ISO CD image from ftpfreebsdorg Unpack the ISO image to a local directory Make any desired edits in the cloned directory Create a new ISO image from your customized layout and burn it tomedia or create a PXE boot image for netbootingFreeBSDs version of tar understands ISO format in addition to many other formatsso you can simply extract the CD image files to a scratch directory Create a subdirectory before extracting because the ISO file unpacks to the current directoryfreebsd sudo mkdir FreeBSDfreebsd sudo tar xpCf FreeBSD FreeBSDisoOnce youve extracted the contents of the image you can customize them to reflect your desired installation settings For example you could add custom DNSresolvers by editing FreeBSDetcresolvconf to include your own name serversbsdinstall normally requires users to select settings such as the type of terminalin use the keyboard mapping and the desired style of disk partitioning You canbypass the interactive questions by putting a file called installerconfig in the etcdirectory of the system imageThis files format is described in the bsdinstall man page It has two sections The preamble which sets certain installation settings A shell script which executes after installation completesWe refer you to the man page rather than regurgitate its contents here Amongother settings it contains options for installing directly to a ZFS root and to othercustom partitioning schemesOnce your customizations are complete you can create a new ISO file with themkisofs command Create a PXE image or burn the ISO to optical media for anunattended installationThe mfsBSD project mfsbsdvxsk is a set of scripts that generate a PXEfriendlyISO image The basic FreeBSD image weighs in at a lean MiB See the sourcescripts at githubcommmatuskamfsbsd Managing packagesUNIX and Linux software assets source code build files documentation and configuration templates were traditionally distributed as compressed archives usually gzipped tarballs targz or tgz files This was OK for developers but inconvenient for end users and administrators These source archives had to be manuallycompiled and built for each system on each release of the software a tedious anderror prone processPackaging systems emerged to simplify and facilitate the job of software management Packages include all the files needed to run a piece of software includingprecompiled binaries dependency information and configuration file templatesthat can be customized by administrators Perhaps most importantly packagingsystems try to make the installation process as atomic as possible If an error occurs during installation a package can be backed out or reapplied New versionsof software can be installed with a simple package updatePackage installers are typically aware of configuration files and will not normallyoverwrite local customizations made by a system administrator They either back upexisting config files that they change or supply example config files under a differentname If you find that a newly installed package breaks something on your systemyou can theoretically back it out to restore your system to its original state Of coursetheory practice so dont try this on a production system without testing it firstPackaging systems define a dependency model that allows package maintainers toensure that the libraries and support infrastructure on which their applications depend are properly installed Unfortunately the dependency graphs are sometimesimperfect Unlucky administrators can find themselves in package dependency hella state where its impossible to update a package because of version incompatibilitiesamong its dependencies Fortunately recent versions of packaging software seemto be less susceptible to this effectPackages can run scripts at various points during the installation so they can domuch more than just disgorge new files Packages frequently add new users andgroups run sanity checks and customize settings according to the environmentConfusingly package versions do not always correspond directly to the versionsof the software that they install For example consider the following RPM packagefor dockerengine rpm qa grep i dockerdockerengineelcentosx docker version grep VersionVersion The package itself claims version but the docker binary reports version In this case the distribution maintainers backported changes and incremented theminor package version Be aware that the package version string is not necessarilyan accurate indication of the software version that is actually installedYou can create packages to facilitate the distribution of your own localizations orsoftware For example you can create a package that when installed reads localization information for a machine or gets it from a central database and uses thatinformation to set up local configuration filesYou can also bundle local applications as packages complete with dependenciesor create packages for third party applications that arent normally distributed inpackage format You can version your packages and use the dependency mechanism to upgrade machines automatically when a new version of your localizationpackage is released We refer you to fpm the Effing Package Manager which is theeasiest way to get started building packages for multiple platforms You can find itat githubcomjordansisselfpmYou can also use the dependency mechanism to create groups of packages Forexample you can create a package that installs nothing of its own but depends onmany other packages Installing the package with dependencies turned on resultsin all the packages being installed in a single step Linux package management systemsTwo package formats are in common use on Linux systems Red Hat CentOSSUSE Amazon Linux and several other distributions use RPM a recursive acronymthat expands to RPM Package Manager Debian and Ubuntu use the separate butequally popular deb format The two formats are functionally similarBoth the RPM and deb packaging systems now function as duallayer souptonuts configuration management tools At the lowest level are the tools that installuninstall and query packages rpm for RPM and dpkg for debOn top of these commands are systems that know how to find and downloadpackages from the Internet analyze interpackage dependencies and upgrade allthe packages on a system yum the Yellowdog Updater Modified works with theRPM system APT the Advanced Package Tool originated in the deb universe butworks well with both deb and RPM packagesOn the next couple of pages we review the lowlevel commands rpm and dpkg Inthe section Highlevel Linux package management systems starting on page we discuss the comprehensive update systems APT and yum which build on theselowlevel facilities Your daytoday administration activities will usually involvethe highlevel tools but youll occasionally need to wade into the deep end of thepool with rpm and dpkgrpm manage RPM packagesThe rpm command installs verifies and queries the status of packages It formerlybuilt them as well but this function has now been relegated to a separate commandcalled rpmbuild rpm options have complex interactions and can be used togetheronly in certain combinations Its most useful to think of rpm as if it were severaldifferent commands that happen to share the same nameThe mode you tell rpm to enter such as i or q specifies which of rpms multiplepersonalities you are hoping to access rpm help lists all the options broken downRHELby mode but its worth your time to read the man page in some detail if you willfrequently be dealing with RPM packagesThe breadandbutter options are i install U upgrade e erase and q query The q option is a bit tricky you must supply an additional commandline flagto pose a specific question For example the command rpm qa lists all the packages installed on the systemLets look at an example We need to install a new version of OpenSSH because ofa recent security fix Once weve downloaded the package well run rpm U to replace the older version with the newerredhat sudo rpm U opensshpelxrpmerror failed dependenciesopenssh p is needed by opensshclientspopenssh p is needed by opensshserverpDoh Perhaps its not so simple after all Here we see that the currently installedversion of OpenSSH p is required by several other packages rpm wontlet us upgrade OpenSSH to p because the change might affect the operation of these other packages This type of conflict happens all the time and its amajor motivation for the development of systems like APT and yum In real lifewe wouldnt attempt to untangle the dependencies by hand but lets continue withrpm alone for the purpose of this exampleWe could force the upgrade with the force option but thats usually a bad idea Thedependency information is there to save time and trouble not just to get in the wayTheres nothing like a broken SSH on a remote system to ruin a sysadmins morningInstead well grab updated versions of the dependent packages as well If we weresmart we could have determined that other packages depended on OpenSSH before we even attempted the upgraderedhat rpm q whatrequires opensshopensshserverpelxopensshclientspelxSuppose that weve obtained updated copies of all the packages We could install themone at a time but rpm is smart enough to handle them all at once If multiple RPMsare listed on the command line rpm sorts them by dependency before installationredhat sudo rpm U opensshredhat rpm q opensshopensshpelCool Looks like it succeeded Note that rpm understands which package we aretalking about even though we didnt specify the packages full name or version Unfortunately rpm does not restart sshd after the installation Youd need to manuallyrestart it to complete the upgradedpkg manage deb packagesJust as RPM packages have the allinone rpm command Debian packages havethe dpkg command Useful options include install remove and l to list thepackages that have been installed on the system A dpkg install of a package thatsalready on the system removes the previous version before installingRunning dpkg l grep package is a convenient way to determine if a particularpackage is installed For example to search for an HTTP server tryubuntu dpkg l grep i httpii lighttpd debu amd fast webserver with minimalmemory footprintThis search found the lighttpd software an excellent open source lightweight webserver The leading ii indicates that the software is installedSuppose that the Ubuntu security team recently released a fix to nvi to patch a potential security problem After grabbing the patch we run dpkg to install it As youcan see its much chattier than rpm and tells us exactly what its doingubuntu sudo dpkg install nviamddebReading database files and directories currently installedPreparing to replace nvi using nviamddeb Unpacking replacement nvi Setting up nvi Checking available versions of ex updating links in etcalternatives You may modify the symlinks there yourself if desired see man lnLeaving ex usrbinex pointing to usrbinnexLeaving exgz usrsharemanmanexgz pointing to usrsharemanmannexgzWe can now use dpkg l to verify that the installation worked The l flag acceptsan optional prefix pattern to match so we can just search for nviubuntu dpkg l nvi Name Version Descriptionii nvi BSD reimplementation of viOur installation seems to have gone smoothly Highlevel Linux package management systemsMetapackage management systems such as APT and yum share several goals To simplify the task of locating and downloading packages To automate the process of updating or upgrading systems To facilitate the management of interpackage dependenciesClearly these systems include more than just clientside commands They all require that distribution maintainers organize their offerings in an agreedon way sothat the software can be accessed and reasoned about by clientsSince no single supplier can encompass the entire world of Linux software thesystems all allow for the existence of multiple software repositories Repositoriescan be local to your network so these systems make a dandy foundation for creating your own internal software distribution systemThe Red Hat Network is closely tied to Red Hat Enterprise Linux Its a commercial service that costs money and offers more in terms of attractive GUIs sitewidesystem management and automation ability than do APT and yum It is a shinyhosted version of Red Hats expensive and proprietary Satellite Server The clientside can reference yum and APT repositories and this ability has allowed distributions such as CentOS to adapt the client GUI for nonproprietary useAPT is better documented than the Red Hat Network is significantly more portable and is free Its also more flexible in terms of what you can do with it APToriginated in the world of Debian and dpkg but it has been extended to encompassRPMs and versions that work with all our example distributions are available Its theclosest thing we have at this point to a universal standard for software distributionyum is an RPMspecific analog of APT Its included by default on Red Hat Enterprise Linux and CentOS although it runs on any RPMbased system provided thatyou can point it toward appropriately formatted repositoriesWe like APT and consider it a solid choice if you run Debian or Ubuntu and wantto set up your own automated package distribution network See the section APTthe Advanced Package Tool on page for more informationPackage repositoriesLinux distributors maintain software repositories that work handinhand with theirchosen package management systems The default configuration for the packagemanagement system usually points to one or more wellknown web or FTP serversthat are under the distributors controlHowever it isnt immediately obvious what such repositories should contain Shouldthey include only the sets of packages blessed as formal major releases Formalreleases plus current security updates Uptodate versions of all the packages thatexisted in the formal releases Useful third party software not officially supported by the distributor Source code Binaries for multiple hardware architecturesWhen you run apt upgrade or yum upgrade to bring the system up to date whatexactly should that meanRHELIn general package management systems must answer all these questions and mustmake it easy for sites to select the crosssections they want to include in their software world The following concepts help structure this process A release is a selfconsistent snapshot of the package universe Beforethe Internet era named OS releases were more or less immutable andwere associated with one specific time security patches were made available separately These days a release is a more nebulous concept Releasesevolve over time as packages are updated Some releases such as Red HatEnterprise Linux are specifically designed to evolve slowly by defaultonly security updates are incorporated Other releases such as beta versions change frequently and dramatically But in all cases the release isthe baseline the target the thing I want to update my system to look like A component is a subset of the software within a release Distributionspartition themselves differently but one common distinction is that between core software blessed by the distributor and extra software madeavailable by the broader community Another distinction thats commonin the Linux world is the one between the free open source portions ofa release and the parts that are tainted by some kind of restrictive licensing agreementOf particular note from an administrative standpoint are minimally activecomponents that include only security fixes Some releases allow you tocombine a security component with an immutable baseline componentto create a relatively stable version of the distribution even though themainline distribution may evolve much faster An architecture represents a class of hardware The expectation is thatmachines within an architecture class are similar enough that they canall run the same binaries Architectures are instances of releases for example Ubuntu Xenial Xerus for x Since components are subdivisions of releases theres a corresponding architecturespecific instancefor each of them as well Individual packages are the elements that make up components and therefore indirectly releases Packages are usually architecturespecific and areversioned independently of the main release and of other packages Thecorrespondence between packages and releases is implicit in the way thenetwork repository is set upThe existence of components that arent maintained by the distributor eg Ubuntus universe and multiverse raises the question of how these components relateto the core OS release Can they really be said to be a component of the specificrelease or are they some other kind of animal entirelyFrom a package management perspective the answer is clear extras are a true component They are associated with a specific release and they evolve in tandem withit The separation of control is interesting from an administrative standpoint butit doesnt affect the package distribution systems except that multiple repositoriesmight need to be manually added by the administratorRHN the Red Hat NetworkWith Red Hat having gracefully departed from the consumer Linux business theRed Hat Network has become the system management platform for Red Hat Enterprise Linux You purchase the right to access the Red Hat Network by subscribing At its simplest you can use the Red Hat Network as a glorified web portal andmailing list Used in this way the Red Hat Network is not much different from thepatch notification mailing lists that have been run by various UNIX vendors foryears But more features are available if youre willing to pay for them For currentpricing and features see rhnredhatcomThe Red Hat Network presents a webbased interface for downloading new packages as well as a commandline alternative Once you register your machines getall the patches and bug fixes that they need without your ever having to interveneThe downside of automatic registration is that Red Hat decides what updates youneed You might consider how much you really trust Red Hat and the softwaremaintainers whose products they package not to screw things upA reasonable compromise might be to sign up one machine in your organizationfor automatic updates You can take snapshots from that machine at periodic intervals to test as possible candidates for internal releasesAPT the Advanced Package ToolAPT is one of the most mature package management systems Its possible to upgrade an entire system full of software with a single apt command or even as withthe Red Hat Network to have your boxes continuously keep themselves up to datewithout human interventionThe first rule of using APT on Ubuntu systems and indeed all management ofDebian packages is to ignore the existence of dselect which acts as a front end forthe Debian package system Its not a bad idea but the user interface is poor andcan be intimidating to the novice user Some documentation will try to steer youtoward dselect but stay strong and stick with aptIf you are using APT to manage a stock Ubuntu installation from a standard repository mirror the easiest way to see the available packages is to visit the masterlist at packagesubuntucom The web site includes a nice search interface If youset up your own APT server see page then of course you will know whatpackages you have made available and you can list them in whatever way you wantDistributions commonly include dummy packages that exist only to claim otherpackages as prerequisites apt downloads and upgrades prerequisite packages asneeded so the dummy packages make it easy to install or upgrade several packagesRHELas a block For example installing the gnomedesktopenvironment package obtains and installs all the packages necessary to run the GNOME UIAPT includes a suite of lowlevel commands like aptget and aptcache that arewrapped for most purposes by an omnibus apt command The wrapper is a later addition to the system so youll still see occasional references to the lowlevelcommands on the web and in documentation To a first approximation commandsthat look similar are in fact the same command Theres no difference between aptinstall and aptget install for exampleOnce you have set up your etcaptsourceslist file described in detail below andknow the name of a package that you want the only remaining task is to run aptupdate to refresh apts cache of package information After that just run apt installpackagename as a privileged user to install the package The same command updates a package that has already been installedSuppose you want to install a new version of the sudo package that fixes a securitybug First its always wise to do an apt updatedebian sudo apt updateGet httphttpusdebianorg stablemain Packages kBGet httpnonusdebianorg stablenonUSmain Release BNow you can actually fetch the package Note the use of sudo to fetch the new sudopackageapt can even upgrade packages that are in usedebian sudo apt install sudoReading Package Lists DoneBuilding Dependency Tree Done packages upgraded newly installed to remove and not upgradedNeed to get BkB of archives After unpacking kB will be usedReading database files and directories currently installedPreparing to replace sudo p using sudopdebuamddeb Unpacking replacement sudo Setting up sudo pdebu Installing new version of config file etcpamdsudo Repository configurationConfiguring APT is straightforward pretty much everything you need to knowcan be found in Ubuntus community documentation on package managementhelpubuntucomcommunityAptGetHowtoThe most important configuration file is etcaptsourceslist which tells APTwhere to get its packages Each line specifies the following A type of package currently deb or debsrc for Debianstyle packages orrpm or rpmsrc for RPMs A URL that points to a file HTTP server or FTP server from which tofetch packages A distribution really a release name that lets you deliver multiple versions of packages A potential list of components categories of packages within a releaseUnless you want to set up your own APT repository or cache the default configuration generally works fine Source packages are downloaded from the entriesbeginning with debsrcOn Ubuntu systems youll almost certainly want to include the universe component which accesses the larger world of Linux open source software The multiverse packages include nonopensource content such as some VMware toolsand componentsAs long as youre editing the sourceslist file you may want to retarget the individual entries to point to your closest mirror A full list of Ubuntu mirrors can befound at launchpadnetubuntuarchivemirrors This is a dynamic and long listof mirrors that changes regularly so be sure to keep an eye on it between releasesMake sure that securityubuntucom is listed as a source so that you have access tothe latest security patchesAn example etcaptsourceslist fileThe following example uses archiveubuntucom as a package source for the maincomponents of Ubuntu those that are fully supported by the Ubuntu team In addition this sourceslist file includes unsupported but open source universe packages and nonfree unsupported packages in the multiverse component There isalso a repository for updates or bugfixed packages in each component Finally thelast six lines are for security updates General format type uri distribution components deb httparchiveubuntucomubuntu xenial main restricteddebsrc httparchiveubuntucomubuntu xenial main restricteddeb httparchiveubuntucomubuntu xenialupdates main restricteddebsrc httparchiveubuntucomubuntu xenialupdates main restricteddeb httparchiveubuntucomubuntu xenial universedebsrc httparchiveubuntucomubuntu xenial universedeb httparchiveubuntucomubuntu xenialupdates universedebsrc httparchiveubuntucomubuntu xenialupdates universedeb httparchiveubuntucomubuntu xenial multiversedebsrc httparchiveubuntucomubuntu xenial multiversedeb httparchiveubuntucomubuntu xenialupdates multiversedebsrc httparchiveubuntucomubuntu xenialupdates multiversedeb httparchiveubuntucomubuntu xenialbackports main restricted Distributors use the distribution field to identify major releases but you can use it however youwant for internal distribution systemsuniverse multiversedebsrc httparchiveubuntucomubuntu xenialbackports main restricteduniverse multiversedeb httpsecurityubuntucomubuntu xenialsecurity main restricteddebsrc httpsecurityubuntucomubuntu xenialsecurity main restricteddeb httpsecurityubuntucomubuntu xenialsecurity universedebsrc httpsecurityubuntucomubuntu xenialsecurity universedeb httpsecurityubuntucomubuntu xenialsecurity multiversedebsrc httpsecurityubuntucomubuntu xenialsecurity multiverseThe distribution and components fields help APT navigate the filesystem hierarchyof the Ubuntu repository which has a standardized layout The root distribution isthe working title given to each release such as trusty xenial or yakkety The available components are typically called main universe multiverse and restrictedAdd the universe and multiverse repositories only if you are comfortable havingunsupported and licenserestricted in the case of multiverse software in yourenvironmentAfter you update the sourceslist file run aptget update to force APT to react toyour changesCreation of a local repository mirrorIf you plan to use apt on a large number of machines you will probably want tocache packages locally Downloading a copy of each package for every machine isnot a sensible use of external bandwidth A mirror of the repository is easy to configure and convenient for local administration Just make sure to keep it updatedwith the latest security patchesThe best tool for the job is the handy aptmirror package which is available fromaptmirrorgithubio You can also install the package from the universe componentwith sudo apt install aptmirrorOnce installed aptmirror drops a file called mirrorlist in etcapt Its a shadowversion of sourceslist but its used only as a source for mirroring operations Bydefault mirrorlist conveniently contains all the repositories for the running version of UbuntuTo actually mirror the repositories in mirrorlist just run aptmirror as rootubuntu sudo aptmirrorDownloading index files using threadsBegin time Sun Feb By default aptmirror puts its repository copies in varspoolaptmirror Feelfree to change this by uncommenting the set basepath directive in mirrorlistbut be aware that you must then create mirror skel and var subdirectories underthe new mirror rootaptmirror takes a long time to run on its first pass because it is mirroring manygigabytes of data currently GB per Ubuntu release Subsequent executions arefaster and should be run automatically out of cron You can run the cleansh scriptfrom the var subdirectory of your mirror to clean out obsolete filesTo start using your mirror share the base directory through HTTP using a webserver of your choice We like to use symbolic links to the web root For instanceln s varspoolaptmirrorusarchiveubuntucomubuntu varwwwubuntuTo make clients use your local mirror edit their sourceslist files just as if you wereselecting a nonlocal mirrorAPT automationUse cron to schedule regular apt runs Even if you dont install packages automatically you may want to run apt update regularly to keep your package summariesup to dateapt upgrade downloads and installs new versions of any packages that are currentlyinstalled on the local machine Note that apt upgrade is defined slightly differently from the lowlevel command aptget upgrade but apt upgrade is usually whatyou want Its equivalent to aptget distupgrade withnewpkgs apt upgrademight want to delete some packages that it views as irreconcilably incompatiblewith the upgraded system so be prepared for potential surprisesIf you really want to play with fire have machines perform the upgrade in an unattended fashion by including the y option to apt upgrade It answers any confirmation questions that apt might ask with an enthusiastic Yes Be aware that someupdates such as kernel packages might not take effect until after a system rebootIts probably not a good idea to perform automated upgrades directly from a distributions mirror However in concert with your own APT servers packages andrelease control system this is a perfect way to keep clients in sync A oneliner likethe following keeps a box up to date with its APT server apt update apt upgrade yUse this command in a cron job if you want it to run on a regular schedule Youcan also refer to it from a system startup script to make the machine update at boottime See page for more information about cron see Chapter Bootingand System Management Daemons for more information about startup scriptsIf you run updates out of cron on many machines its a good idea to use time randomization to make sure that everyone doesnt try to update at onceIf you dont quite trust your source of packages consider automatically downloadingall changed packages without installing them Use apts downloadonly optionto request this behavior then review the packages by hand and install the ones youwant to update Downloaded packages are put in varcacheapt and over time thisdirectory can grow to be quite large Clean out the unused files from this directorywith aptget autocleanyum release management for RPMyum the Yellowdog Updater Modified is a metapackage manager based on RPMIt may be a bit unfair to call yum an APT clone but its thematically and implementationally similar although cleaner and slower in practiceOn the serverside the yumarch command compiles a database of header information from a large set of packages often an entire release The header database isthen shared along with the packages through HTTP Clients use the yum commandto fetch and install packages yum figures out dependency constraints and doeswhatever additional work is needed to complete the installation of the requestedpackages If a requested package depends on other packages yum downloads andinstalls those packages as wellThe similarities between apt and yum extend to the commandline options theyunderstand For example yum install foo downloads and installs the most recentversion of the foo package and its dependencies if necessary There is at least onetreacherous difference though apt update refreshes apts package informationcache but yum update updates every package on the system its analogous to aptupgrade To add to the confusion yum upgrade is the same as yum update butwith obsolescence processing enabledyum does not match on partial package names unless you include globbing characters such as and to explicitly request this behavior For example yum updatelib refreshes all packages whose name starts with lib Remember to quote theglobbing characters so the shell doesnt interfere with themUnlike apt yum defaults to validating its package information cache against thecontents of the network repository every time you run it Use the C option to prevent the validation and yum makecache to update the local cache it takes awhileto run Unfortunately the C option doesnt do much to improve yums sluggishperformanceyums configuration file is etcyumconf It includes general options and pointersto package repositories Multiple repositories can be active at once and each repository can be associated with multiple URLsA replacement for yum called DNF for Dandified Yum is under active development Its already the default package manager for Fedora and will eventually replaceyum completely DNF sports better dependency resolution and an improved APIamong other features Visit dnfbaseurlorg to learn more FreeBSD software managementFreeBSD has had packaging facilities for several releases but its only now transitioning to a completely packagecentric distribution model in which most elementsof the core OS are defined as packages FreeBSDs recent releases have segregatedsoftware into three general categories A base system which includes a bundled set of core software and utilities A set of binary packages managed with the pkg command A separate ports system which downloads source code applies FreeBSDspecific patches then builds and installs itAs of FreeBSD the lines between these territories have become even more muddled The base system has been packagized but the old scheme for managing thebase system as one unit is still in place too Many software packages can be installedeither as binary packages or as ports with essentially similar results but differentimplications for future updates However crosscoverage is not complete somethings can only be installed as a port or as a packagePart of the project definition for FreeBSD is to shift the system more decisivelytoward universal package management The base system and ports may both continue to exist in some form its currently too early to tell exactly how things willwork out but the future direction is clearAccordingly try to manage addon software with pkg to the extent possible Avoidports unless the software you want has no packagized version or you need to customize compiletime optionsAnother peculiar remnant of the bigiron UNIX era is FreeBSDs insistence that addon packages are local even though they are compiled by FreeBSD and releasedas part of an official package repository Packages install binaries under usrlocaland most configuration files end up in usrlocaletc rather than etcThe base systemThe base system is updated as a single unit and is functionally distinct from anyaddon packages at least in theory The base system is maintained in a Subversion repository You can browse the source tree including all the source branchesat svnwebfreebsdorgSeveral development branches are defined The CURRENT branch is meant only for active development purposesIt is the first to receive new features and fixes but is not widely tested bythe user community The STABLE branch is regularly updated with improvements intendedfor the next major release It includes new features but maintains package compatibility and undergoes some testing It may introduce bugs orbreaking changes and is recommended only for the adventurous The RELEASE branch is forked from STABLE when a release target isachieved It remains mostly static The only updates to RELEASE are security fixes and fixes for serious bugs Official ISO images derive from theRELEASE branch and that branch is the only one recommended for useon production systemsView your systems current branch with uname r uname rRELEASERun the freebsdupdate command to keep your system updated with the latestpackages Fetching updates and installing them are separate operations but youcan combine the two into a single command line sudo freebsdupdate fetch installThis command retrieves and installs the latest base binaries Its available only for theRELEASE branch binaries are not built for the STABLE and CURRENT branchesYou can use the same tool to upgrade between releases of the system For example sudo freebsdupdate r RELEASE upgradepkg the FreeBSD package managerpkg is intuitive and fast Its the easiest way to install software that isnt already included in the base system Use pkg help for a quick reference on the available subcommands or pkg help command to display the man page for a particular subcommand Table lists some of the most frequently used subcommandsWhen you install packages with pkg install pkg consults the local package catalog thendownloads the requested package from the repository at pkgFreeBSDorg Once thepackage is installed its registered in a SQLite database kept in vardbpkglocalsqliteTake care not to delete this file lest your system lose track of which packages havebeen installed Create backups of the database with the pkg backup subcommandpkg version a subcommand for comparing package versions has an idiosyncraticsyntax It uses the and characters to show packages that are current olderthan the latest available version or newer than the current version Use the following command to list packages that have updatesfreebsd pkg version vILdri needs updating index has gbm needs updating index has harfbuzz needs updating index has libEGL needs updating index has This command compares all installed packages to the index I looking for thosethat are not L the current version and printing verbose information vpkg search is faster than Google for finding packages For example pkg searchdns finds all packages with dns in their names The search term is a regular expression so you can search for something like pkg search apache See pkg helpsearch for detailsThe ports collectionFreeBSD ports are a collection of all the software that FreeBSD can build from sourceAfter the ports tree is initialized youll find all the available software in categorizedsubdirectories of usrports To initialize the ports tree use the portsnap utilityfreebsd portsnap fetch extractTo update the ports tree in one command use portsnap fetch updateIt takes some time to download the ports metadata The download includes pointers to the source code for all the ports plus any associated patches for FreeBSDcompatibility When installation of the metadata is complete you can search forsoftware then build and install anything you needFor example the zsh shell is not included in the FreeBSD base Use the whereisutility to search for zsh then build and install from the ports treefreebsd whereis zshbash usrportsshellszshfreebsd cd usrportsshellszshfreebsd make install cleanTable Example pkg subcommandsCommand What it doespkg install y package Installs without asking any are you sure questionspkg backup Makes a backup of the local package databasepkg info Lists all installed packagespkg info package Shows extended information for a packagepkg search i package Searches package repository case insensitivepkg audit F Shows packages with known security vulnerabilitiespkg which file Shows which package owns the named filepkg autoremove Removes unused packagespkg delete package Uninstalls a package same as removepkg clean ay Removes cached packages from varcachepkgpkg update Updates local copy of the package catalogpkg upgrade Upgrades packages to the latest versionTo remove software installed through the ports system run make deinstall fromthe appropriate directoryTheres more than one way to update ports but we prefer the portmaster utilityFirst install portmaster from the ports collectionfreebsd cd usrportsportsmgmtportmasterfreebsd make install cleanRun portmaster L to see all the ports having updates available and update themall at once with portmaster aYou can also install ports through the portmaster In fact its somewhat more convenient than the typical makebased process because you dont need to leave yourcurrent directory To install zshfreebsd portmaster shellszshIf you need to free up some disk space clean up the ports working directories withportmaster c Software localization and configurationAdapting systems to your local or cloud environment is one of the prime battlegrounds of system administration Addressing localization issues in a structured andreproducible way helps avoid the creation of snowflake systems that are impossibleto recover after a major incidentWe have more to say in this book about these issues In particular Chapter Configuration Management and Chapter Continuous Integration and Delivery discuss tools that structure these tasks Configuration management systems areyour goto tools for installing and configuring software in a reproducible mannerThey are the master key to sane localizationImplementation issues aside how do you know if your local environment is properly designed Here are a few points to consider Nonadministrators should not have root privileges Any need for rootprivileges in the course of normal operations is suspicious and probablyindicates that something is fishy with your local configuration Systems should facilitate work and not get in users way Users do notwreck the system intentionally Design internal security so that it guardsagainst unintentional errors and the widespread dissemination of administrative privileges Misbehaving users are learning opportunities Interview them before youchastise them for not following proper procedures Users frequently respond to inefficient administrative procedures by working around themso always consider the possibility that noncompliance is an indication ofarchitectural problems Be customercentered Talk to users and ask them which tasks they find difficult in your current configuration Find ways to make these tasks simpler Your personal preferences are yours Let your users have their own Offerchoices wherever possible When administrative decisions affect users experience of the system beaware of the reasons for your decisions Let your reasons be known Keep your local documentation up to date and easily accessible See page for more information on this topicOrganizing your localizationIf your site has a thousand computers and each computer has its own configuration you will spend a major portion of your working time figuring out why onebox has a particular problem and another doesnt Clearly the solution is to makeevery computer the sameright But realworld constraints and the varying needsof users typically make this solution impossibleTheres a big difference in administrability between multiple configurations andcountless configurations The trick is to split your setup into manageable bits Someparts of the localization apply to all managed hosts others apply to only a few andstill others are specific to individual boxes Even with the convenience of configuration management tools try not to allow too much drift among systemsHowever you design your localization system make sure that all original data is keptin a revision control system This precaution lets you keep track of which changeshave been thoroughly tested and are ready for deployment In addition it lets youidentify the originator of any problematic changes The more people involved inthe process the more important this last consideration becomesStructuring updatesIn addition to performing initial installations you will also need to continually rollout updates This remains one of the most important security tasks Keep in mindthough that different hosts have different needs for concurrency stability and uptimeDo not roll out new software releases en masse Instead stage rollouts according toa gradual plan that accommodates other groups needs and allows time for problemsto be discovered while their potential to cause damage is still limited This sometimes referred to as a canary release process named for the fabled canary in thecoal mine In addition never update critical servers until you have some confidencein the changes you are contemplating Avoid rolling out changes on Fridays unlessyoure prepared for a long weekend in front of the terminalIts usually advantageous to separate the base OS release from the localization releaseDepending on the stability needs of your environment you might choose to useminor local releases only for bug fixing However we have found that adding newfeatures in small doses yields a smoother operation than queuing up changes intohorse pill releases that risk a major disruption of service This principle is closelyrelated to the idea of continuous integration and deployment see Chapter Limiting the field of playIts often a good idea to specify a maximum number of releases you are willing tohave in play at any given time Some administrators see no reason to fix softwarethat isnt broken They point out that gratuitously upgrading systems costs timeand money and that cutting edge all too often means bleeding edge Those whoput these principles into practice must be willing to collect an extensive catalog ofactive releasesBy contrast the lean and mean crowd point to the inherent complexity of releasesand the difficulty of comprehending let alone managing a random collection ofreleases dating years into the past Their trump cards are security patches whichmust typically be applied universally and on a strict schedule Patching outdatedversions of the operating system is often infeasible so administrators are facedwith the choice of skipping updates on some computers or crashupgrading thesemachines to a newer internal release Not goodNeither of these perspectives is provably correct but we tend to side with thosewho favor a limited number of releases Better to perform your upgrades on yourown schedule rather than one dictated by an external emergencyTestingIts important to test changes before unleashing them on the world At a minimumthis means that you need to test your own local configuration changes Howeveryou should really test the software that your vendor releases as well A major UNIXvendor once released a patch that performed an rm rf Imagine installing thispatch throughout your organization without testing it firstTesting is an especially pertinent issue if you use a service that offers an automaticpatching capability such as most of the packaging systems discussed in this chapter Never connect missioncritical systems directly to a vendorsponsored updateservice Instead point most of your systems to an internal mirror that you controland test updates on noncritical systems firstIf you foresee that an update might cause uservisible problems or changes notifyusers well in advance and give them a chance to communicate with you if they haveconcerns regarding your intended changes or timing Make sure that users have aneasy way to report bugsIf your organization is geographically distributed make sure that other offices helpwith testing International participation is particularly valuable in multilingual environments If no one in the US office speaks Japanese for example you had betterget the Tokyo office to test anything that might affect Unicode support A surprisingSee page for moreinformation abouttrouble trackingnumber of system parameters vary with location Does the new version of softwareyoure installing break UTF encoding rendering text illegible for some languages Recommended readingIntel Corporation and SystemSoft Preboot Execution Environment PXESpecification v pixnetsoftwarepxebootarchivepxespecpdfLawson Nolan What it feels like to be an opensource maintainer wpmeptCaryPXELinux Questions syslinuxzytorcomwikiindexphpPXELINUXRodin Josip Debian New Maintainers Guide debianorgdocmaintguideThis document contains good information about deb packages See also Chapter of the Debian FAQ and Chapter of the Debian reference manualA scalable approach to system management requires that administrative changes bestructured reproducible and replicable across multiple computers In the real worldthat means those changes should be mediated by software rather than performedby administrators working from checklistsor worse from memoryScripts standardize administrative chores and free up admins time for more important and more interesting tasks Scripts also serve as a kind of lowrent documentation in that they record the steps needed to complete a particular taskSysadmins main alternative to scripting is to use the configuration managementsystems described in Chapter These systems offer a structured approach toadministration that scales well to the cloud and to networks of machines Howeverthey are more complex more formal and less flexible than plainvanilla scriptingIn practice most administrators use a combination of scripting and configurationmanagement Each approach has its strengths and they work well togetherThis chapter takes a quick look at sh Python and Ruby as languages for scriptingWe cover some basic tips for using the shell and also discuss regular expressionsas a general technology Scripting and the Shell Scripting philosophyThis chapter includes a variety of scripting tidbits and language particulars Thatinformation is useful but more important than any of those details is the broaderquestion of how to incorporate scripting or more generally automation into yourmental model of system administrationWrite microscriptsNew sysadmins often wait to learn scripting until theyre confronted with a particularly complex or tedious chore For example maybe its necessary to automatea particular type of backup so that its done regularly and so that the backup datais stored in two different data centers Or perhaps theres a cloud server configuration that would be helpful to create initialize and deploy with a single commandThese are perfectly legitimate scripting projects but they can leave the impressionthat scripting is an elephant gun to be unboxed only when big game is on the horizon After all that first line script probably took several days to write and debug You cant be spending days on every little taskcan youActually you achieve most efficiencies by saving a few keystrokes here and a fewcommands there Marqueelevel scripts that are part of your sites formal procedures are just the visible portion of a much larger iceberg Below the waterline liemany smaller forms of automation that are equally useful for sysadmins As a general rule approach every chore with the question How can I avoid having to dealwith this issue again in the futureMost admins keep a selection of short scripts for personal use aka scriptlets in theirbin directories Use these quickanddirty scripts to address the pain points youencounter in daytoday work They are usually short enough to read at a glanceso they dont need documentation beyond a simple usage message Keep them updated as your needs changeFor shell scripts you also have the option of defining functions that live inside yourshell configuration files eg bashprofile rather than in freestanding script filesShell functions work similarly to standalone scripts but they are independent ofyour search path and automatically travel with you wherever you take your shellenvironmentJust as a quick illustration heres a simple Bash function that backs up files according to a standardized naming conventionfunction backup newnamedate YmdHMbakmv newnameecho Backed up to newnamecp p newname Despite the functionlike syntax you use it just like a script or any other command backup afileBacked up afile to afilebakThe main disadvantage of shell functions is that theyre stored in memory and haveto be reparsed every time you start a new shell But on modern hardware thesecosts are negligibleAt a smaller scale still are aliases which are really just an extrashort variety ofscriptlet These can be defined either with shell functions or with your shells builtin aliasing feature usually called alias Most commonly they set default arguments for individual commands For examplealias lsls Fhmakes the ls command punctuate the names of directories and executables andrequests humanreadable file sizes for long listings eg MLearn a few tools wellSystem administrators encounter a lot of software They cant be experts at everything so they usually become skilled at skimming documentation running experiments and learning just enough about new software packages to configure themfor the local environment Laziness is a virtueThat said some topics are valuable to study in detail because they amplify yourpower and effectiveness In particular you should know a shell a text editor anda scripting language thoroughly Read the manuals from front to back then regularly read books and blogs Theres always more to learnEnabling technologies like these reward upfront study for a couple of reasons Astools they are fairly abstract its hard to envision all the things theyre capable ofdoing without reading about the details You cant use features youre not aware ofAnother reason these tools reward exploration is that theyre made of meat mostfeatures are potentially valuable to most administrators Compare that with theaverage server daemon where your main challenge is often to identify the offeatures that are irrelevant to your situationA shell or editor is a tool you use constantly Every incremental improvement in yourproficiency with these tools translates not only into increased productivity but alsointo greater enjoyment of the work No one likes to waste time on repetitive detailsAutomate all the thingsShell scripts arent system administrators only opportunity to benefit from automation Theres a whole world of programmable systems out therejust keep an eye Not to spoil the rest of this chapter but these should probably be Bash vim and Pythonout for them Exploit these facilities aggressively and use them to impedancematchyour tools to your workflowFor example we created this book in Adobe InDesign which is ostensibly a GUIapplication However its also scriptable in JavaScript so we created a library ofInDesign scripts to implement and enforce many of our conventionsSuch opportunities are everywhere Microsoft Office apps are programmable in Visual Basic or C If your workinvolves analysis or reporting make those TPS reports write themselves Most Adobe applications are scriptable If your responsibilities include database wrangling you can automatemany routine tasks with SQL stored procedures Some databases evensupport additional languages for example PostgreSQL speaks Python PowerShell is the mainstream scripting tool for Microsoft Windows systems Third party addons like AutoHotKey go a long way toward facilitating the automation of Windows apps On macOS systems some applications can be controlled through AppleScript At the system level use the Automator app the Services systemand folder actions to automate various chores and to connect traditionalscripting languages to the GUIWithin the world of system administration specifically a few subsystems have theirown approaches to automation Many others play well with generalpurpose automation systems such as Ansible Salt Chef and Puppet described in Chapter Configuration Management For everything else theres generalpurpose scriptingDont optimize prematurelyTheres no real distinction between scripting and programming Language developers sometimes take offense when their babies are lumped into the scriptingcategory not just because the label suggests a certain lack of completeness but alsobecause some scripting languages of the past have earned reputations for poor designWe still like the term scripting though it evokes the use of software as a kind ofuniversal glue that binds various commands libraries and configuration files intoa more functional wholeAdministrative scripts should emphasize programmer efficiency and code clarityrather than computational efficiency This is not an excuse to be sloppy but simplya recognition that it rarely matters whether a script runs in half a second or twoseconds Optimization can have an amazingly low return on investment even forscripts that run regularly out of cronPick the right scripting languageFor a long time the standard language for administrative scripts was the one definedby the sh shell Shell scripts are typically used for light tasks such as automating asequence of commands or assembling several filters to process dataThe shell is always available so shell scripts are relatively portable and have few dependencies other than the commands they invoke Whether or not you choose theshell the shell might choose you most environments include a hefty complementof existing sh scripts and administrators frequently need to read understand andtweak those scriptsAs a programming language sh is somewhat inelegant The syntax is idiosyncraticand the shell lacks the advanced text processing features of modern languagesfeatures that are often of particular use to system administratorsPerl designed in the late s was a major step forward for scriptwriting administrators Its permissive syntax extensive library of userwritten modules andbuiltin support of regular expressions made it an administrative favorite for manyyears Perl permits and some would say encourages a certain get it done anddamn the torpedoes style of coding Opinions differ on whether thats an advantage or a drawbackThese days Perl is known as Perl to distinguish it from the redesigned and incompatible Perl which has finally reached general release after years of gestationUnfortunately Perl is showing its age in comparison with newer languages anduse of Perl isnt yet widespread enough for us to recommend it as a safe choice Itmight be that the world has moved on from Perl entirely We suggest avoiding Perlfor new work at this pointJavaScript and PHP are best known as languages for web development but they canbe armtwisted into service as generalpurpose scripting tools too Unfortunatelyboth languages have design flaws that limit their appeal and they lack many of thethird party libraries that system administrators rely onIf you come from the web development world you might be tempted to apply yourexisting PHP or JavaScript skills to system administration We recommend againstthis Code is code but living in the same ecosystem as other sysadmins brings avariety of longterm benefits At the very least avoiding PHP means you wonthave to endure the ridicule of your local sysadmin MeetupPython and Ruby are modern generalpurpose programming languages that areboth well suited for administrative work These languages incorporate a coupleof decades worth of language design advancements relative to the shell and theirtext processing facilities are so powerful that sh can only weep and cower in shameThe main drawback to both Python and Ruby is that their environments can be abit fussy to set up especially when you start to use third party libraries that havecompiled components written in C The shell skirts this particular issue by havingno module structure and no third party librariesIn the absence of outside constraints Python is the most broadly useful scriptinglanguage for system administrators Its well designed widely used and widely supported by other packages Table shows some general notes on other languagesTable Scripting language cheat sheetLanguage Designer When to use itBourne shell Stephen Bourne Simple series of commands portable scriptsbash Brian Fox Like Bourne shell nicer but less portableC shell Bill Joy Never for scripting see footnote on page JavaScript Brendan Eich Web development app scriptingPerl Larry Wall Quick hacks oneliners text processingPHP Rasmus Lerdorf Youve been bad and deserve punishmentPython Guido van Rossum Generalpurpose scripting data wranglingRuby Matz Matsumoto Generalpurpose scripting webFollow best practicesAlthough the code fragments in this chapter contain few comments and seldomprint usage messages thats only because weve skeletonized each example to makespecific points Real scripts should behave better There are whole books on bestpractices for coding but here are a few basic guidelines When run with inappropriate arguments scripts should print a usagemessage and exit For extra credit implement help this way too Validate inputs and sanitycheck derived values Before doing an rm rfon a calculated path for example you might have the script doublecheckthat the path conforms to the pattern you expect Return a meaningful exit code zero for success and nonzero for failureYou neednt necessarily give every failure mode a unique exit code however consider what callers will actually want to know Use appropriate naming conventions for variables scripts and routinesConform to the conventions of the language the rest of your sites codebase and most importantly the other variables and functions defined inthe current project Use mixed case or underscores to make long namesreadable Assign variable names that reflect the values they store but keep themshort numberoflinesofinput is way too long try nlines The naming of the scripts themselves is important too In this context dashes are more commonthan underscores for simulating spaces as in systemconfigprinter Consider developing a style guide so you and your colleagues can writecode according to the same conventions A guide makes it easier for youto read other peoples code and for them to read yours Start every script with a comment block that tells what the script does andwhat parameters it takes Include your name and the date If the scriptrequires nonstandard tools libraries or modules to be installed on thesystem list those as well Comment at the level you yourself will find helpful when you return tothe script after a month or two Some useful points to comment on arethe following choices of algorithm web references used reasons for notdoing things in a more obvious way unusual paths through the code anything that was a problem during development Dont clutter code with useless comments assume intelligence and language proficiency on the part of the reader Its OK to run scripts as root but avoid making them setuid its tricky tomake setuid scripts completely secure Use sudo to implement appropriate access control policies instead Dont script what you dont understand Administrators often view scriptsas authoritative documentation of how a particular procedure should behandled Dont set a misleading example by distributing halfbaked scripts Feel free to adapt code from existing scripts for your own needs But dontengage in copy paste and pray programming when you dont understand the code Take the time to figure it out This time is never wasted With bash use x to echo commands before they are executed and n tocheck commands for syntax without executing them Remember that in Python you are in debug mode unless you explicitlyturn it off with a argument on the command line You can test the special debug variable before printing diagnostic outputTom Christiansen suggests the following five golden rules for producing usefulerror messages Error messages should go to STDERR not STDOUT see page Include the name of the program thats issuing the error State what function or operation failed If a system call fails include the perror string Exit with some code other than On the other hand style guide construction can absorb a contentious teams attention for weeksDont fight over the style guide cover the areas of agreement and avoid long negotiations over theplacement of braces and commas The main thing is to make sure everyones on board with a consistent set of naming conventions Shell basicsUNIX has always offered users a choice of shells but some version of the Bourneshell sh has been standard on every UNIX and Linux system The code for theoriginal Bourne shell never made it out of ATT licensing limbo so these dayssh is most commonly manifested in the form of the Almquist shell known as ashdash or simply sh or the Bourneagain shell bashThe Almquist shell is a reimplementation of the original Bourne shell without extrafrills By modern standards its barely usable as a login shell It exists only to runsh scripts efficientlybash focuses on interactive usability Over the years it has absorbed most of theuseful features pioneered by other shells It still runs scripts designed for the original Bourne shell but its not particularly tuned for scripting Some systems egthe Debian lineage include both bash and dash Others rely on bash for bothscripting and interactive useThe Bourne shell has various other offshoots notably ksh the Korn shell and kshssoupedup cousin zsh zsh features broad compatibility with sh ksh and bash aswell as many interesting features of its own including spelling correction and enhanced globbing Its not used as any systems default shell as far as we are awarebut it does have something of a cult followingHistorically BSDderived systems favored the C shell csh as an interactive shellIts now most commonly seen in an enhanced version called tcsh Despite the formerly widespread use of csh as a login shell it is not recommended for use as ascripting languagetcsh is a fine and widely available shell but its not an sh derivative Shells are complex unless youre a shell connoisseur theres not much value in learning one shellfor scripting and a second onewith different features and syntaxfor daily useStick to a modern version of sh and let it do double dutyAmong the sh options bash is pretty much the universal standard these days Tomove effortlessly among different systems standardize your personal environmenton bashFreeBSD retains tcsh as roots default and does not ship bash as part of the basesystem But thats easily fixed run sudo pkg install bash to install bash and usechsh to change your shell or the shell of another user You can set bash as the default for new users by running adduser CBefore taking up the details of shell scripting we should review some of the basicfeatures and syntax of the shell For a detailed explanation of why this is so see Tom Christiansens classic rant Csh ProgrammingConsidered Harmful Its widely reproduced on the web One copy is harmfulcatvorgsoftwarecsh Changing the default might seem presumptuous but standard FreeBSD relegates new users to theAlmquist sh Theres nowhere to go but upThe material in this section applies to the major interactive shells in the sh lineageincluding bash and ksh but not csh or tcsh regardless of the exact platform youare using Try out the forms youre not familiar with and experimentCommand editingWeve watched too many people edit command lines with the arrow keys Youwouldnt do that in your text editor rightIf you like emacs all the basic emacs commands are available to you when youreediting history ControlE goes to the end of the line and ControlA to thebeginning ControlP steps backward through recently executed commands andrecalls them for editing ControlR searches incrementally through your historyto find old commandsIf you like vivim put your shells commandline editing into vi mode like this set o viAs in vi editing is modal however you start in input mode Press Esc to leaveinput mode and i to reenter it In edit mode w takes you forward a word fXfinds the next X in the line and so on You can walk through past command historyentries with Esc k Want emacs editing mode back again set o emacsPipes and redirectionEvery process has at least three communication channels available to it standardinput STDIN standard output STDOUT and standard error STDERR Processes initially inherit these channels from their parents so they dont necessarilyknow where they lead They might connect to a terminal window a file a networkconnection or a channel belonging to another process to name a few possibilitiesUNIX and Linux have a unified IO model in which each channel is named with asmall integer called a file descriptor The exact number assigned to a channel is notusually significant but STDIN STDOUT and STDERR are guaranteed to correspond to file descriptors and so its safe to refer to these channels by numberIn the context of an interactive terminal window STDIN normally reads from thekeyboard and both STDOUT and STDERR write their output to the screenMany traditional UNIX commands accept their input from STDIN and write theiroutput to STDOUT They write error messages to STDERR This convention letsyou string commands together like building blocks to create composite pipelinesThe shell interprets the symbols and as instructions to reroute a commandsinput or output to or from a file A symbol connects the commands STDIN tothe contents of an existing file The and symbols redirect STDOUT replaces the files existing contents and appends to them For example the command grep bash etcpasswd tmpbashuserscopies lines containing the word bash from etcpasswd to tmpbashusers creating the file if necessary The command below sorts the contents of that file andprints them to the terminal sort tmpbashusersrootxrootrootbinbashTo redirect both STDOUT and STDERR to the same place use the symbol Toredirect STDERR only use The find command illustrates why you might want separate handling for STDOUTand STDERR because it tends to produce output on both channels especially whenrun as an unprivileged user For example a command such as find name coreusually results in so many permission denied error messages that genuine hitsget lost in the clutter To discard all the error messages use find name core devnullIn this version only real matches where the user has read permission on the parent directory come to the terminal window To save the list of matching paths toa file use find name core tmpcorefiles devnullThis command line sends matching paths to tmpcorefiles discards errors andsends nothing to the terminal windowTo connect the STDOUT of one command to the STDIN of another use the symbol commonly known as a pipe For example find name core devnull lessThe first command runs the same find operation as the previous example but sendsthe list of discovered files to the less pager rather than to a file Another example ps ef grep httpdThis one runs ps to generate a list of processes and pipes it to the grep commandwhich selects lines that contain the word httpd The output of grep is not redirectedso the matching lines come to the terminal window cut d f etcpasswd sort uHere the cut command picks out the path to each users shell from etcpasswd Thelist of shells is then sent through sort u to produce a sorted list of unique values Truth be told the sort command accepts filenames so the symbol is optional in this context Itsused here for illustrationTo execute a second command only if its precursor completes successfully you canseparate the commands with an symbol For example mkdir foo cd fooattempts to create a directory called foo and if the directory was successfully createdexecutes cd Here the success of the mkdir command is defined as its yielding anexit code of zero so the use of a symbol that suggests logical AND for this purposemight be confusing if youre accustomed to shortcircuit evaluation in other programming languages Dont think about it too much just accept it as a shell idiomConversely the symbol executes the following command only if the precedingcommand fails that is it produces a nonzero exit status For example cd foo echo No such directoryIn a script you can use a backslash to break a command onto multiple lines Thisfeature can help to distinguish errorhandling code from the rest of a commandpipelinecp preserve recursive etc sparebackup echo Did NOT make backupFor the opposite effectmultiple commands combined onto one lineyou can usea semicolon as a statement separator mkdir foo cd foo touch afileVariables and quotingVariable names are unmarked in assignments but prefixed with a dollar sign whentheir values are referenced For example etcdiretc echo etcdiretcOmit spaces around the symbol otherwise the shell mistakes your variablename for a command name and treats the rest of the line as a series of argumentsto that commandWhen referencing a variable you can surround its name with curly braces to clarifyto the parser and to human readers where the variable name stops and other textbegins for example etcdir instead of just etcdir The braces are not normallyrequired but they can be useful when you want to expand variables inside doublequoted strings Often youll want the contents of a variable to be followed byliteral letters or punctuation For example echo Saved revth version of mdadmconfSaved th version of mdadmconfTheres no standard convention for the naming of shell variables but allcaps namestypically suggest environment variables or variables read from global configurationfiles More often than not local variables are alllowercase with components separated by underscores Variable names are case sensitiveThe shell treats strings enclosed in single and double quotes similarly except thatdoublequoted strings are subject to globbing the expansion of filenamematchingmetacharacters such as and and variable expansion For example mylangPennsylvania Dutch echo I speak mylangI speak Pennsylvania Dutch echo I speak mylangI speak mylangBackquotes also known as backticks are treated similarly to double quotes but theyhave the additional effect of executing the contents of the string as a shell commandand replacing the string with the commands output For example echo There are wc l etcpasswd lines in the passwd fileThere are lines in the passwd fileEnvironment variablesWhen a UNIX process starts up it receives a list of commandline arguments andalso a set of environment variables Most shells show you the current environmentin response to the printenv command printenvEDITORviUSERgarthENVhomegarthbashrcLSCOLORSexfxgxgxdxgxgxbxbxcxcxPWDmegaDocumentsProjectsCodesplHOMEhomegarth total of about By convention environment variables have allcaps names but that is not technically requiredPrograms that you run can consult these variables and change their behavior accordingly For example vipw checks the EDITOR environment variable to determine which text editor to run for youEnvironment variables are automatically imported into shs variable namespace sothey can be set and read with the standard syntax Use export varname to promotea shell variable to an environment variable You can also combine this syntax witha value assignment as seen here export EDITORnano vipwstarts the nano editorDespite being called environment variables these values dont exist in some abstract ethereal place outside of space and time The shell passes a snapshot of thecurrent values to any program you run but no ongoing connection exists Moreoverevery shell or programand every terminal windowhas its own distinct copy ofthe environment that can be separately modifiedCommands for environment variables that you want to set up at login time shouldbe included in your profile or bashprofile file Other environment variablessuch as PWD for the current working directory are automatically maintained bythe shellCommon filter commandsAny wellbehaved command that reads STDIN and writes STDOUT can be usedas a filter that is a component of a pipeline to process data In this section webriefly review some of the more widely used filter commands including some usedin passing above but the list is practically endless Filter commands are so teamoriented that its sometimes hard to show their use in isolationMost filter commands accept one or more filenames on the command line Only ifyou do not specify a file do they read their standard inputcut separate lines into fieldsThe cut command prints selected portions of its input lines It most commonly extracts delimited fields as in the example on page but it can return segmentsdefined by column boundaries as well The default delimiter is Tab but you canchange delimiters with the d option The f options specifies which fields to include in the outputFor an example of the use of cut see the section on uniq belowsort sort linessort sorts its input lines Simple right Well maybe notthere are a few potentialsubtleties regarding the exact parts of each line that are sorted the keys and thecollation order to be imposed Table shows a few of the more common optionsbut check the man page for othersThe commands below illustrate the difference between numeric and dictionarysorting which is the default Both commands use the t and k options to sortthe etcgroup file by its third colonseparated field the group ID The first sortsnumerically and the second alphabeticallyTable sort optionsOpt Meaningb Ignore leading whitespacef Sort caseinsensitivelyh Sort human readable numbers eg MBk Specify the columns that form the sort keyn Compare fields as integer numbersr Reverse sort ordert Set field separator the default is whitespaceu Output only unique records sort t k n etcgrouprootxbinxdaemondaemonx sort t k etcgrouprootxbinxdaemonusersxAlso useful is the h option which implements a numeric sort that understandssuffixes such as M for mega and G for giga For example the following commandcorrectly sorts the sizes of directories under usr while maintaining the legibilityof the output du sh usr sort hK usrlocaleK usrlocalK usrgamesM usrsbinM usrincludeM usrsrcM usrbinM usrshareG usrlibuniq print unique linesuniq is similar in spirit to sort u but it has some useful options that sort does notemulate c to count the number of instances of each line d to show only duplicated sort accepts the key specification k rather than k but it probably doesnt do what you expectWithout the terminating field number the sort key continues to the end of the linelines and u to show only nonduplicated lines The input must be presorted usually by being run through sortFor example the command below shows that users have binbash as their loginshell and that have binfalse The latter are either pseudousers or users whoseaccounts have been disabled cut d f etcpasswd sort uniq c binbash binfalsewc count lines words and charactersCounting the number of lines words and characters in a file is another commonoperation and the wc word count command is a convenient way of doing thisRun without options it displays all three counts wc etcpasswd etcpasswdIn the context of scripting it is more common to supply a l w or c option tomake wcs output consist of a single number This form is most commonly seeninside backquotes so that the result can be saved or acted ontee copy input to two placesA command pipeline is typically linear but its often helpful to tap into the datastream and send a copy to a file or to the terminal window You can do this withthe tee command which sends its standard input both to standard out and to afile that you specify on the command line Think of it as a tee fixture in plumbingThe device devtty is a synonym for the current terminal window For example find name core tee devtty wc lprints both the pathnames of files named core and a count of the number of corefiles that were foundA common idiom is to terminate a pipeline that will take a long time to run with atee command That way output goes both to a file and to the terminal window forinspection You can preview the initial results to make sure everything is workingas you expected then leave while the command runs knowing that the results willbe savedhead and tail read the beginning or end of a fileReviewing lines from the beginning or end of a file is a common administrativeoperation These commands display ten lines of content by default but you can usethe n numlines option to specify more or fewerFor interactive use head is more or less obsoleted by the less command whichpaginates files for display But head still finds plenty of use within scriptstail also has a nifty f option thats particularly useful for sysadmins Instead ofexiting immediately after printing the requested number of lines tail f waits fornew lines to be added to the end of the file and prints them as they appeargreatfor monitoring log files Be aware however that the program writing the file mightbe buffering its own output Even if lines are being added at regular intervals froma logical perspective they might only become visible in chunks of KiB or KiBhead and tail accept multiple filenames on the command line Even tail f allowsmultiple files and this feature can be quite handy when new output appears tailprints the name of the file in which it appearedType ControlC to stop monitoringgrep search textgrep searches its input text and prints the lines that match a given pattern Its namederives from the gregularexpressionp command in the ed editor which came withthe earliest versions of UNIX and is still present on current systemsRegular expressions are textmatching patterns written in a standard andwellcharacterized patternmatching language Theyre a universal standard usedby most programs that do pattern matching although there are minor variationsamong implementations The odd name stems from regular expressions originsin theoryofcomputation studies We discuss regular expression syntax in moredetail starting on page Like most filters grep has many options including c to print a count of matchinglines i to ignore case when matching and v to print nonmatching rather thanmatching lines Another useful option is l lower case L which makes grep printonly the names of matching files rather than printing each line that matches Forexample the command sudo grep l mdadm varlogvarlogauthlogvarlogsyslogshows that log entries from mdadm have appeared in two different log filesgrep is traditionally a fairly basic regular expression engine but some versions permit the selection of other dialects For example grep P on Linux selects Perlstyleexpressions though the man page warns darkly that they are highly experimentalIf you need full power just use Ruby Python or Perl See Units on page for an introduction to these unitsIf you filter the output of tail f with grep add the linebuffered option to makesure you see each matching line as soon as it becomes available tail f varlogmessages grep linebuffered ZFSMay nutrient ZFS vdev state changed poolguid vdevguid sh scriptingsh is great for simple scripts that automate things youd otherwise be typing on thecommand line Your commandline skills carry over to sh scripting and vice versa which helps you extract maximum value from the learning time you invest insh derivatives But once an sh script gets above lines or when you need featuresthat sh doesnt have its time to move on to Python or RubyFor scripting theres some value in limiting yourself to the dialect understood by theoriginal Bourne shell which is both an IEEE and a POSIX standard shcompatibleshells often supplement this baseline with additional language features Its fine touse these extensions if you do so deliberately and are willing to require a specificinterpreter But more commonly scripters end up using these extensions inadvertently and are then surprised when their scripts dont run on other systemsIn particular dont assume that the systems version of sh is always bash or eventhat bash is available Ubuntu replaced bash with dash as the default script interpreter in and as part of that conversion process they compiled a handy listof bashisms to watch out for You can find it at wikiubuntucomDashAsBinShExecutionsh comments start with a sharp and continue to the end of the line As on thecommand line you can break a single logical line onto multiple physical lines byescaping the newline with a backslash You can also put more than one statementon a line by separating the statements with semicolonsAn sh script can consist of nothing but a series of command lines For example thefollowing helloworld script simply does an echobinshecho Hello worldThe first line is known as the shebang statement and declares the text file to be ascript for interpretation by binsh which might itself be a link to dash or bashThe kernel looks for this syntax when deciding how to execute the file From the perspective of the shell spawned to execute the script the shebang line is just a commentIn theory you would need to adjust the shebang line if your systems sh was in adifferent location However so many existing scripts assume binsh that systemsare compelled to support it if only through a linkIf you need your script to run under bash or another interpreter that might not havethe same command path on every system you can use usrbinenv to search yourPATH environment variable for a particular command For exampleusrbinenv rubyis a common idiom for starting Ruby scripts Like binsh usrbinenv is such awidelyreliedon path that all systems are obliged to support itTo prepare a script for running just turn on its execute bit see page chmod x helloworld helloworldHello worldYou can also invoke the shell as an interpreter directly sh helloworldHello world source helloworldHello worldThe first command runs helloworld in a new instance of sh and the second makesyour existing login shell read and execute the contents of the file The latter option isuseful when the script sets up environment variables or makes other customizationsthat apply only to the current shell Its commonly used in scripting to incorporatethe contents of a configuration file written as a series of variable assignmentsIf you come from the Windows world you might be accustomed to a files extension indicating what type of file it is and whether it can be executed In UNIX andLinux the file permission bits determine whether a file can be executed and if soby whom If you wish you can give your shell scripts a sh suffix to remind youwhat they are but youll then have to type out the sh when you run the commandsince UNIX doesnt treat extensions speciallyFrom commands to scriptsBefore we jump into shs scripting features a note about methodology Most peoplewrite sh scripts the same way they write Python or Ruby scripts with a text editorHowever its more productive to think of your regular shell command prompt asan interactive script development environment Path searching has security implications particularly when running scripts under sudo See page for more information about sudos handling of environment variables If your shell understands the command helloworld without the prefix that means the current directory is in your search path This is bad because it gives other users the opportunity to lay trapsfor you in the hope that youll try to execute certain commands while cded to a directory on whichthey have write access The dot command is a synonym for source eg helloworldSee page formore informationabout permission bitsFor example suppose you have log files named with the suffixes log and LOGscattered throughout a directory hierarchy and that you want to change them allto the uppercase form First find all the files find name logdonottouchimportantlogadmincomlogfoologgeniusspewlogleatherflogOops it looks like you need to include the dot in the pattern and to leave out directories as well Do a ControlP to recall the command and then modify it find type f name logdonottouchimportantlogfoologgeniusspewlogOK this looks better That donottouch directory looks dangerous though youprobably shouldnt mess around in there find type f name log grep v donottouchfoologgeniusspewlogAll right thats the exact list of files that need renaming Try generating some newnames find type f name log grep v donottouch while read fname do echo mv fname echo fname sed slogLOG donemv foolog fooLOGmv geniusspewlog geniusspewLOGYup those are the commands to run to perform the renaming So how to do it forreal You could recall the command and edit out the echo which would makesh execute the mv commands instead of just printing them However piping thecommands to a separate instance of sh is less error prone and requires less editingof the previous commandWhen you do a ControlP youll find that bash has thoughtfully collapsed yourminiscript into a single line To this condensed command line simply add a pipethat sends the output to sh x find type f name log grep v donottouch while read fnamedo echo mv fname echo fname sed slogLOG done sh x mv foolog fooLOG mv geniusspewlog geniusspewLOGThe x option to sh prints each command before executing itThat completes the actual renaming but save the script for future reuse bashsbuiltin command fc is a lot like ControlP but instead of returning the lastcommand to the command line it transfers the command to your editor of choiceAdd a shebang line and usage comment write the file to a plausible location binor usrlocalbin perhaps make the file executable and you have a scriptTo summarize this approach Develop the script or script component as a pipeline one step at a timeentirely on the command line Use bash for this process even though theeventual interpreter might be dash or another sh variant Send output to standard output and check to be sure it looks right At each step use the shells command history to recall pipelines and theshells editing features to tweak them Until the output looks right you havent actually done anything so theresnothing to undo if the command is incorrect Once the output is correct execute the actual commands and verify thatthey worked as you intended Use fc to capture your work then clean it up and save itIn the example above the command lines were printed and then piped to a subshellfor execution This technique isnt universally applicable but its often helpful Alternatively you can capture output by redirecting it to a file No matter what waituntil you see the right stuff in the preview before doing anything thats potentiallydestructiveInput and outputThe echo command is crude but easy For more control over your output use printfIt is a bit less convenient because you must explicitly put newlines where you wantthem use but it gives you the option to use tabs and enhanced number formatting in your the output Compare the output from the following two commands echo taatbbtcctaatbbtcc printf taatbbtcc aa bb ccSome systems have OSlevel printf and echo commands usually in usrbin andbin respectively Although the commands and the shell builtins are similar theymay diverge subtly in their specifics especially in the case of printf Either adhereto shs syntax or call the external printf with a full pathnameYou can use the read command to prompt for input Heres an examplebinshecho n Enter your name read usernameif n username thenecho Hello usernameexit elseecho Greetings nameless oneexit fiThe n in the echo command suppresses the usual newline but you could also haveused printf here We cover the if statements syntax shortly but its effect shouldbe obvious here The n in the if statement evaluates to true if its string argumentis not null Heres what the script looks like when run sh readexampleEnter your name RonHello RonSpaces in filenamesThe naming of files and directories is essentially unrestricted except that namesare limited in length and must not contain slash characters or nulls In particularspaces are permitted Unfortunately UNIX has a long tradition of separating commandline arguments at whitespace so legacy software tends to break when spacesappear within filenamesSpaces in filenames were once found primarily on filesystems shared with Macsand PCs but they have now metastasized into UNIX culture and are found in somestandard software packages as well There are no two ways about it administrativescripts must be prepared to deal with spaces in filenames not to mention apostrophes asterisks and various other menacing punctuation marksIn the shell and in scripts spaceful filenames can be quoted to keep their piecestogether For example the command less My spacey filepreserves My spacey file as a single argument to less You can also escape individual spaces with a backslash less My spacey fileThe filename completion feature of most shells usually bound to the Tab keynormally adds the backslashes for youWhen you are writing scripts a useful weapon to know about is finds print option In combination with xargs this option makes the findxargs combinationwork correctly regardless of the whitespace contained within filenames For example the command find home type f size M print xargs ls lprints a long ls listing of every file beneath home thats over one megabyte in sizeCommandline arguments and functionsCommandline arguments to a script become variables whose names are numbers is the first commandline argument is the second and so on is thename by which the script was invoked That could be a strange construction suchas binexamplesh so it doesnt necessarily have the same value each time thescript is runThe variable contains the number of commandline arguments that were suppliedand the variable contains all the arguments at once Neither of these variablesincludes or counts Heres an example of the use of argumentsbinshshowusage echo Usage sourcedir destdir exit Main program starts hereif ne thenshowusageelse There are two argumentsif d then sourcedirelseecho Invalid source directory showusagefiif d then destdirelseecho Invalid destination directory showusagefifiprintf Source directory is sourcedirprintf Destination directory is destdirIf you call a script without arguments or with inappropriate arguments the scriptshould print a short usage message to remind you how to use it The example scriptabove accepts two arguments validates that the arguments are both directories andprints their names If the arguments are invalid the script prints a usage messageand exits with a nonzero return code If the caller of the script checks the returncode it will know that this script failed to execute correctlyWe created a separate showusage function to print the usage message If the scriptwere later updated to accept additional arguments the usage message would haveto be changed in only one place The notation on lines that print error messages makes the output go to STDERR mkdir aaa bbb sh showusage aaa bbbSource directory is aaaDestination directory is bbb sh showusage foo barInvalid source directoryUsage showusage sourcedir destdirArguments to sh functions are treated like commandline arguments The first argument becomes and so on As you can see above remains the name of the scriptTo make the example more robust we could make the showusage routine accept anerror code as an argument That would allow a more definitive code to be returnedfor each different type of failure The next code excerpt shows how that might lookshowusage echo Usage sourcedir destdir if eq then exit Exit with arbitrary nonzero return codeelse exit fiIn this version of the routine the argument is optional Within a function tells you how many arguments were passed in The script exits with code if nomorespecific code is designated But a specific value for exampleshowusage makes the script exit with that code after printing the usage message The shellvariable contains the exit status of the last command executed whether usedinside a script or at the command lineThe analogy between functions and commands is strong in sh You can define useful functions in your bashprofile file profile for vanilla sh and then usethem on the command line as if they were commands For example if your sitehas standardized on network port for the SSH protocol a form of securitythrough obscurity you might definessh usrbinssh p in your bashprofile to make sure ssh is always run with the option p Like many shells bash has an aliasing mechanism that can reproduce this limitedexample even more concisely but functions are more general and more powerfulControl flowWeve seen several ifthen and ifthenelse forms in this chapter already they dopretty much what youd expect The terminator for an if statement is fi To chainyour if clauses you can use the elif keyword to mean else if For exampleif base eq dm eq theninstallDMBaseelif base ne dm eq theninstallBaseelif base eq dm ne theninstallDMelseecho Installing nothingfiBoth the peculiar syntax for comparisons and the commandlineoptionlikenames of the integer comparison operators eg eq are inherited from the original Bourne shells channeling of bintest The brackets are actually a shorthandway of invoking test and are not a syntactic requirement of the if statementTable shows the sh comparison operators for numbers and strings sh uses textual operators for numbers and symbolic operators for stringsTable Elementary sh comparison operatorsString Numeric True ifx y x eq y x is equal to yx y x ne y x is not equal to yx a y x lt y x is less than yna x le y x is less than or equal to yx a y x gt y x is greater than yna x ge y x is greater than or equal to yn x na x is not nullz x na x is nulla Must be backslashescaped or double bracketed to prevent interpretation as an input or output redirection character In reality these operations are now built into the shell and do not actually run bintestsh shines in its options for evaluating the properties of files once again courtesyof its bintest legacy Table shows a few of shs many file testing and file comparison operatorsTable sh file evaluation operatorsOperator True ifd file file exists and is a directorye file file existsf file file exists and is a regular filer file User has read permission on files file file exists and is not emptyw file User has write permission on filefile nt file file is newer than filefile ot file file is older than fileAlthough the elif form is useful a case selection is often a better choice for clarityIts syntax is shown below in a sample routine that centralizes logging for a scriptOf particular note are the closing parenthesis after each condition and the twosemicolons that follow the statement block to be executed when a condition is metexcept for the last condition The case statement ends with esac The log level is set in the global variable LOGLEVEL The choices are from most to least severe Error Warning Info and DebuglogMsg messagelevelmessageitselfif messagelevel le LOGLEVEL then case messagelevel in messageleveltextError messageleveltextWarning messageleveltextInfo messageleveltextDebug messageleveltextOther esac echo messageleveltext messageitselffiThis routine illustrates the common log level paradigm used by many administrative applications The code of the script generates messages at many different levelsof detail but only the ones that pass a globally set threshold LOGLEVEL are actually logged or acted on To clarify the importance of each message the messagetext is preceded by a label that denotes its associated log levelLoopsshs forin construct makes it easy to take some action for a group of values orfiles especially when combined with filename globbing the expansion of simplepatternmatching characters such as and to form filenames or lists of filenamesThe sh pattern in the for loop below returns a list of matching filenames in thecurrent directory The for statement then iterates through that list assigning eachfilename in turn to the variable scriptbinshsuffixBACKUPdate YmdHMfor script in sh donewnamescriptsuffixecho Copying script to newnamecp p script newnamedoneThe output looks like this sh forexampleCopying rhelsh to rhelshBACKUPCopying slessh to slesshBACKUPThe filename expansion is not magic in this context it works exactly as it does onthe command line Which is to say the expansion happens first and the line is thenprocessed by the interpreter in its expanded form You could just as well have entered the filenames statically as in the linefor script in rhelsh slessh doIn fact any whitespaceseparated list of things including the contents of a variableworks as a target of forin You can also omit the list entirely along with the inkeyword in which case the loop implicitly iterates over the scripts commandlinearguments if at the top level or the arguments passed to a functionbinshfor file donewnamefilebackupecho Copying file to newnamecp p file newnamedonebash but not vanilla sh also has the more familiar for loop from traditional programming languages in which you specify starting increment and termination clauses More accurately the filename expansion is a little bit magical in that it does maintain a notion of theatomicity of each filename Filenames that contain spaces go through the for loop in a single passFor example bashspecificfor i i CPUCOUNT i doCPULISTCPULIST idoneThe next example illustrates shs while loop which is useful for processing commandline arguments and for reading the lines of a filebinshexec counterwhile read line doecho counter linecountercounter doneHeres what the output looks like sh whileexample etcpasswd rootxSuperuserrootbinbash binxbinbinbinbash daemonxDaemonsbinbinbashThis scriptlet has a couple of interesting features The exec statement redefines thescripts standard input to come from whatever file is named by the first commandlineargument The file must exist or the script generates an errorThe read statement within the while clause is a shell builtin but it acts like anexternal command You can put external commands in a while clause as well inthat form the while loop terminates when the external command returns a nonzero exit statusThe counter expression is an odd duck indeed The notation forces numeric evaluation It also makes optional the use of to mark variable namesThe expression is replaced with the result of the arithmetic calculationThe shenanigans work in the context of double quotes too In bash whichsupports Cs postincrement operator the body of the loop can be collapseddown to one linewhile read line doecho counter linedone Depending on the invocation exec can also have the more familiar meaning stop this script andtransfer control to another script or expression Its yet another shell oddity that both functions areaccessed through the same statementArithmeticAll sh variables are string valued so sh does not distinguish between the number and the character string in assignments The difference lies in how the variablesare used The following code illustrates the distinctionbinshabcabda becho a b c tplus sign as string literalecho a b d tplus sign as arithmetic additionThis script produces the output plus sign as string literal plus sign as arithmetic additionNote that the plus sign in the assignment to c does not act as a concatenation operator for strings Its just a literal character That line is equivalent tocabTo force numeric evaluation you enclose an expression in as shown withthe assignment to d above But even this precaution does not result in d receivinga numeric value the result of the calculation is the string sh has the usual assortment of arithmetic logical and relational operators see theman page for details Regular expressionsAs we mentioned on page regular expressions are standardized patterns thatparse and manipulate text For example the regular expressionI sent you a chequeck for the graeycoloured aluminiummatches sentences that use either American or British spelling conventionsRegular expressions are supported by most modern languages though some takethem more to heart than others Theyre also used by UNIX commands such as grepand vi They are so common that the name is usually shortened to regex Entirebooks have been written about how to harness their powerThe filename matching and expansion performed by the shell when it interpretscommand lines such as wc l pl is not a form of regular expression matching Itsa different system called shell globbing and it uses a different and simpler syntax You can find citations for two of them at the end of this chapterRegular expressions are not themselves a scripting language but theyre so usefulthat they merit featured coverage in any discussion of scripting hence this sectionThe matching processCode that evaluates a regular expression attempts to match a single given text stringto a single given pattern The text string to match can be very long and can containembedded newlines Its sometimes convenient to use a regex to match the contentsof an entire file or documentFor the matcher to declare success the entire search pattern must match a contiguous section of the search text However the pattern can match at any position Aftera successful match the evaluator returns the text of the match along with a list ofmatches for any specially delimited subsections of the patternLiteral charactersIn general characters in a regular expression match themselves So the patternI am the walrusmatches the string I am the walrus and that string only Since it can match anywhere in the search text the pattern can be successfully matched to the stringI am the egg man I am the walrus Koo koo kachooHowever the actual match is limited to the I am the walrus portion Matchingis case sensitiveSpecial charactersTable shows the meanings of some common special symbols that can appear inregular expressions These are just the basicsthere are many moreMany special constructs such as and affect the matching of the thing totheir left or right In general a thing is a single character a subpattern enclosedin parentheses or a character class enclosed in square brackets For the character however thingness extends indefinitely to both left and right If you want tolimit the scope of the vertical bar enclose the bar and both things in their own setof parentheses For exampleI am the walrusegg manmatches either I am the walrus or I am the egg man This example also demonstrates escaping of special characters here the dot The patternI am the walrusegg man matches any of the following I am the walrus I am the egg manTable Special characters in regular expressions common onesSymbol What it matches or does Matches any characterchars Matches any character from a given setchars Matches any character not in a given set Matches the beginning of a line Matches the end of a linew Matches any word character same as AZazs Matches any whitespace character same as ftr ad Matches any digit same as Matches either the element to its left or the one to its rightexpr Limits scope groups elements allows matches to be captured Allows zero or one match of the preceding element Allows zero one or many matches of the preceding element Allows one or more matches of the preceding element n Matches exactly n instances of the preceding element min Matches at least min instances note the comma minmax Matches any number of instances from min to maxa That is a space a form feed a tab a newline or a return I am the walrus I am the egg man I am the egg man I am the walrus I am the egg man I am the egg man I am the walrus I am the walrusIt also matches I am the walrus I am the egg man I am the walrus even thoughthe number of repetitions is explicitly capped at two Thats because the patternneed not match the entire search text Here the regex matches two sentences andterminates declaring success It doesnt care that another repetition is availableIt is a common error to confuse the regular expression metacharacter the zeroormore quantifier with the shells globbing character The regex version of thestar needs something to modify otherwise it wont do what you expect Use ifany sequence of characters including no characters at all is an acceptable matchExample regular expressionsIn the United States postal zip codes have either five digits or five digits followed by a dash and four more digits To match a regular zip code you must matcha fivedigit number The following regular expression fits the billdThe and match the beginning and end of the search text but do not actually correspond to characters in the text they are zerowidth assertions These charactersensure that only texts consisting of exactly five digits match the regular expressionthe regex will not match five digits within a larger string The d escape matches adigit and the quantifier says that there must be exactly five onedigit matchesTo accommodate either a fivedigit zip code or an extended zip add an optionaldash and four additional digitsddThe parentheses group the dash and extra digits together so that they are considered one optional unit For example the regex wont match a fivedigit zip codefollowed by a dash If the dash is present the fourdigit extension must be presentas well or there is no matchA classic demonstration of regex matching is the following expressionMouamaer AEael GKQhaeudtzdhzafiywhich matches most of the variant spellings of the name of former Libyan head ofstate Moammar Gadhafi including Muammar alKaddafi BBC Moammar Gadhafi Associated Press Muammar alQadhafi AlJazeera Muammar AlQadhafi US Department of StateDo you see how each of these would match the pattern This regular expression also illustrates how quickly the limits of legibility can bereached Most regex systems support an x option that ignores literal whitespace inthe pattern and enables comments allowing the pattern to be spaced out and splitover multiple lines You can then use whitespace to separate logical groups and clarify relationships just as you would in a procedural language For example heres amore readable version of that same Moammar Gadhafi regexM ou a m ae r First name Muammar Moamar etcs Whitespace cant use a literal space here Group for optional last name prefixAEae l Al El al or els Followed by either a dash or whitespaceGKQ h aeu Initial syllable of last name Kha Qua etc Group for consonants at start of nd syllabledtz dhz dd dh etc Group might occur twice as in Quadhdhafiaf iy Final afi or afy Note that this regular expression is designed to be liberal in what it matches Many patterns that arent legitimate spellings also match for example Moammer el Qhuuuzzthaf This helps a bit but its still pretty easy to torture later readers of your code So bekind if you can use hierarchical matching and multiple small matches instead oftrying to cover every possible situation in one large regular expressionCapturesWhen a match succeeds every set of parentheses becomes a capture group thatrecords the actual text that it matched The exact manner in which these pieces aremade available to you depends on the implementation and context In most casesyou can access the results as a list array or sequence of numbered variablesSince parentheses can nest how do you know which match is which Easy thematches arrive in the same order as the opening parentheses There are as manycaptures as there are opening parentheses regardless of the role or lack of rolethat each parenthesized group played in the actual matching When a parenthesized group is not used eg Muammar when matched against Muammarits corresponding capture is emptyIf a group is matched more than once the contents of only the last match are returned For example with the patternI am the walrusegg man matching the textI am the egg man I am the walrusthere are two results one for each set of parenthesesI am the walruswalrusBoth capture groups actually matched twice However only the last text to matcheach set of parentheses is actually capturedGreediness laziness and catastrophic backtrackingRegular expressions match from left to right Each component of the pattern matchesthe longest possible string before yielding to the next component a characteristicknown as greedinessIf the regex evaluator reaches a state from which a match cannot be completed itunwinds a bit of the candidate match and makes one of the greedy atoms give upsome of its text For example consider the regex aaa being matched against theinput text aaaaaaAt first the regex evaluator assigns the entire input to the a portion of the regexbecause the a is greedy When there are no more as to match the evaluator goeson to try to match the next part of the regex But oops its an a and there is nomore input text that can match an a time to backtrack The a has to give up oneof the as it has matchedNow the evaluator can match aa but it still cannot match the last a in the patternSo it backtracks again and takes away a second a from the a Now the second andthird as in the pattern both have as to pair with and the match is completeThis simple example illustrates some important general points First greedy matching plus backtracking makes it expensive to match apparently simple patterns suchas imgtr when processing entire files The portion starts by matchingeverything from the first img to the end of the input and only through repeatedbacktracking does it contract to fit the local tagsFurthermore the tr that this pattern binds to is the last possible valid match inthe input which is probably not what you want More likely you meant to match animg tag followed immediately by a tr tag A better way to write this pattern isimgstr which allows the initial wild card match to expand only tothe end of the current tag because it cannot cross a rightanglebracket boundaryYou can also use lazy as opposed to greedy wild card operators instead of and instead of These versions match as few characters of the input as theycan If that fails they match more In many situations these operators are moreefficient and closer to what you want than the greedy versionsNote however that they can produce matches different from those of the greedyoperators the difference is more than just one of implementation In our HTMLexample the lazy pattern would be imgtr But even here the couldeventually grow to include unwanted s because the next tag after an img mightnot be a tr Again probably not what you wantPatterns with multiple wild card sections can cause exponential behavior in theregex evaluator especially if portions of the text can match several of the wild cardexpressions and especially if the search text does not match the pattern This situation is not as unusual as it might sound especially when pattern matching withHTML Often youll want to match certain tags followed by other tags possiblyseparated by even more tags a recipe that might require the regex evaluator to trymany possible combinationsRegex guru Jan Goyvaerts calls this phenomenon catastrophic backtracking andwrites about it in his blog see regularexpressionsinfocatastrophichtml for detailsand some good solutionsA couple of takehome points from all this If you can do pattern matching linebyline rather than fileatatimethere is much less risk of poor performance Although this section shows HTML excerpts as examples of text to be matched regular expressionsare not really the right tool for this job Our external reviewers were uniformly aghast Ruby andPython both have excellent addons that parse HTML documents the proper way You can then access the portions youre interested in with XPath or CSS selectors See the Wikipedia page for XPathand the respective languages module repositories for details Even though regex notation makes greedy operators the default theyprobably shouldnt be Use lazy operators All uses of are inherently suspicious and should be scrutinized Python programmingPython and Ruby are interpreted languages with a pronounced objectorientedinflection Both are widely used as generalpurpose scripting languages and haveextensive libraries of third party modules We discuss Ruby in more detail startingon page Python offers a straightforward syntax thats usually pretty easy to follow even whenreading other peoples codeWe recommend that all sysadmins become fluent in Python Its the modern erasgoto language for both system administration and generalpurpose scripting Itsalso widely supported as a glue language for use within other systems eg thePostgreSQL database and Apples Xcode development environment It interfacescleanly with REST APIs and has welldeveloped libraries for machine learning dataanalysis and numeric computationThe passion of Python Python was already well on its way to becoming the worlds default scripting language when Python was released in For this release the developers choseto forgo backward compatibility with Python so that a group of modest but fundamental changes and corrections could be made to the language particularly inthe area of internationalized text processingUnfortunately the rollout of Python proved to be something of a debacle Thelanguage updates are entirely sensible but theyre not musthaves for the average Python programmer with an existing code base to maintain For a long timescripters avoided Python because their favorite libraries didnt support it and library authors didnt support Python because their clients were still using Python Even in the best of circumstances its difficult to push a large and interdependentuser community past this sort of discontinuity In the case of Python early entrenchments persisted for the better part of a decade However as of thatsituation finally seems to be changingCompatibility libraries that allow the same Python code to run under either version of the language have helped ease the transition to some extent But even nowPython remains less common in the wild than Python The exact list of changes in Python isnt relevant to this brief discussion but you can find a summary at docspythonorgwhatsnewhtmlAs of this writing pyreadinessorg reports that only of the top Pythonlibraries remain incompatible with Python But the long tail of unported software is more sobering only a tad more than of the libraries warehoused atpypipythonorg the Python Package Index aka PyPI run under Python Ofcourse many of these projects are older and no longer maintained but is stilla concerningly low numberPython or Python The worlds solution to the slowly unfolding Python transition has been to treatPythons and as separate languages You neednt consecrate your systems to oneor the other you can run both simultaneously without conflictAll our example systems ship Python by default usually as usrbinpythonwith a symbolic link from usrbinpython Python can typically be installed asa separate package the binary is called pythonAlthough the Fedora project is working to make Python its system default RedHat and CentOS are far behind and do not even define a prebuilt package forPython However you can pick one up from Fedoras EPEL Extra Packages forEnterprise Linux repository See the FAQ at fedoraprojectorgwikiEPEL for instructions on accessing this repository Its easy to set up but the exact commandsare versiondependentFor new scripting work or for those new to Python altogether it makes sense to jumpdirectly to Python Thats the syntax we show in this chapter though in fact its onlythe print lines that vary between Python and Python in our simple examplesFor existing software use whichever version of Python the software prefers If yourchoice is more complicated than simply new vs old code consult the Python wikiat wikipythonorgmoinPythonorPython for an excellent collection of issuessolutions and recommendationsPython quick startFor a more thorough introduction to Python than we can give here Mark PilgrimsDive Into Python is a great place to start Its available for reading or for downloadwithout charge at diveintopythonnet or as a printed book from Apress A complete citation can be found on page To start heres a quick Hello world scriptusrbinpythonprintHello world See caniusepythoncom for uptodate statisticsRHELTo get it running set the execute bit or invoke the python interpreter directly chmod x helloworld helloworldHello worldPythons most notable break with tradition is that indentation is logically significant Python does not use braces brackets or begin and end to delineate blocksStatements at the same level of indentation automatically form blocks The exactindentation style spaces or tabs depth of indentation does not matterPython blocking is best shown by example Consider this simple ifthenelse statementimport sysa sysargvif a printa is oneprintThis is still the then clause of the if statementelseprinta is aprintThis is still the else clause of the if statementprintThis is after the if statementThe first line imports the sys module which contains the argv array The two pathsthrough the if statement both have two lines each indented to the same level Colons at the end of a line are normally a clue that the line introduces and is associated with an indented block that follows it The final print statement lies outsidethe context of the if statement python blockexample a is oneThis is still the then clause of the if statementThis is after the if statement python blockexample a is This is still the else clause of the if statementThis is after the if statementPythons indentation convention is less flexibile for the formatting of code but itdoes reduce clutter in the form of braces and semicolons Its an adjustment for thoseaccustomed to traditional delimiters but most people ultimately find that they like itPythons print function accepts an arbitrary number of arguments It inserts aspace between each pair of arguments and automatically supplies a newline Youcan suppress or modify these characters by adding end or sep options to the endof the argument listFor example the lineprintone two three sep endproduces the outputonetwothreeComments are introduced with a sharp and last until the end of the line justas in sh Perl and RubyYou can split long lines by backslashing the end of line breaks When you do thisthe indentation of only the first line is significant You can indent the continuationlines however you like Lines with unbalanced parentheses square brackets orcurly braces automatically signal continuation even in the absence of backslashesbut you can include the backslashes if doing so clarifies the structure of the codeSome cutandpaste operations convert tabs to spaces and unless you know whatyoure looking for this can drive you nuts The golden rule is never to mix tabs andspaces use one or the other for indentation A lot of software makes the traditionalassumption that tabs fall at space intervals which is too much indentation forreadable code Most in the Python community seem to prefer spaces and character indentationHowever you decide to attack the indentation problem most editors have optionsthat can help save your sanity either by outlawing tabs in favor of spaces or by displaying spaces and tabs differently As a last resort you can translate tabs to spaceswith the expand commandObjects strings numbers lists dictionaries tuples and filesAll data types in Python are objects and this gives them more power and flexibilitythan they have in most languagesIn Python lists are enclosed in square brackets and indexed from zero They areessentially similar to arrays but can hold objects of any typePython also has tuples which are essentially immutable lists Tuples are fasterthan lists and are helpful for representing constant data The syntax for tuples isthe same as for lists except that the delimiters are parentheses instead of squarebrackets Because thing looks like a simple algebraic expression tuples that contain only a single element need a marker comma to disambiguate them thing Heres some basic variable and data type wrangling in Pythonname Gwenrating characters SpongeBob Patrick Squidward elements lithium carbon boron A homogeneous and more efficient array type is implemented in the array module but for most purposes stick with listsprintnametsratingtd name ratingprintcharactersts charactersprintherots charactersprintelementsts elements This example produces the following output python objectsname Gwenrating characters SpongeBob Patrick Squidwardhero SpongeBobelements lithium carbon boronNote that the default string conversion for list and tuple types represents them asthey would be found in source codeVariables in Python are not syntactically marked or declared by type but the objectsto which they refer do have an underlying type In most cases Python does notautomatically convert types for you but individual functions or operators may doso For example you cannot concatenate a string and a number with the operator without explicitly converting the number to its string representation Howeverformatting operators and statements coerce everything to string formEvery object has a string representation as can be seen in the output above Dictionaries lists and tuples compose their string representations recursively by stringifying their constituent elements and combining these strings with the appropriatepunctuationThe string formatting operator is a lot like the sprintf function from C but it canbe used anywhere a string can appear Its a binary operator that takes the stringon its left and the values to be inserted on its right If more than one value is to beinserted the values must be presented as a tupleA Python dictionary also known as a hash or an associative array represents aset of keyvalue pairs You can think of a hash as an array whose subscripts keysare arbitrary values they do not have to be numbers But in practice numbers andstrings are common keysDictionary literals are enclosed in curly braces with each keyvalue pair being separated by a colon In use dictionaries work much like lists except that the subscriptskeys can be objects other than integersordinal first second third printThe ordinal dictionary contains ordinalprintThe ordinal of is ordinal python dictionaryThe ordinal array contains first second thirdThe ordinal of is firstPython handles open files as objects with associated methods True to its name thereadline method reads a single line so the example below reads and prints twolines from the etcpasswd filef openetcpasswd rprintfreadline endprintfreadline endfclose python fileiorootxrootrootbinbashdaemonxdaemonusrsbinusrsbinnologinThe newlines at the end of the print calls are suppressed with end because eachline already includes a newline character from the original file Python does notautomatically strip theseInput validation exampleOur scriptlet below shows a general scheme for input validation in Python It alsodemonstrates the definition of functions and the use of commandline argumentsalong with a couple of other Pythonismsimport sysimport osdef showusagemessage code printmessageprints sourcedir destdir sysargvsysexitcodeif lensysargv showusage args required you supplied d lensysargv elif not ospathisdirsysargvshowusageInvalid source directoryelif not ospathisdirsysargvshowusageInvalid destination directorysource dest sysargvprintSource directory is sourceprintDestination directory is destIn addition to importing the sys module we also import the os module to gainaccess to the ospathisdir routine Note that import doesnt shortcut your accessto any symbols defined by modules you must use fully qualified names that startwith the module nameThe definition of the showusage routine supplies a default value for the exit codein case the caller does not specify this argument explicitly Since all data types areobjects function arguments are effectively passed by referenceThe sysargv list contains the script name in the first position so its length is onegreater than the number of commandline arguments that were actually suppliedThe form sysargv is a list slice Curiously slices do not include the elementat the far end of the specified range so this slice includes only sysargv andsysargv You could simply say sysargv to include the second and subsequent argumentsLike sh Python has a dedicated else if condition the keyword is elif There isno explicit case or switch statementThe parallel assignment of the source and dest variables is a bit different from somelanguages in that the variables themselves are not in a list Python allows parallelassignments in either formPython uses the same comparison operators for numeric and string values Thenot equal comparison operator is but there is no unary operator use not forthis The Boolean operators and and or are also spelled outLoopsThe fragment below uses a forin construct to iterate through the range to for counter in range printcounter end print Add final newlineAs with the array slice in the previous example the right endpoint of the range isnot actually included The output includes only the numbers through This is Pythons only type of for loop but its a powerhouse Pythons for has severalfeatures that distinguish it from for in other languages Nothing is special about numeric ranges Any object can support Pythonsiteration model and most common objects do You can iterate through astring by character a list a file by character line or block a list slice etc Iterators can yield multiple values and you can have multiple loop variables The assignment at the top of each iteration acts just like Pythonsregular multiple assignments This feature is particularly nice for iteratingthrough dictionaries Both for and while loops can have else clauses at the end The else clauseis executed only if the loop terminates normally as opposed to exitingthrough a break statement This feature may initially seem counterintuitive but it handles certain use cases quite elegantlyThe example script below accepts a regular expression on the command line andmatches it against a list of Snow Whites dwarves and the colors of their dwarf suitsThe first match is printed with the portions that match the regex surrounded byunderscoresimport sysimport resuits Bashfulyellow Sneezybrown Docorange GrumpyredDopeygreen Happyblue Sleepytaupepattern recompiles sysargvfor dwarf color in suitsitemsif patternsearchdwarf or patternsearchcolor printss dwarf suit is s patternsubr dwarf patternsubr color breakelseprintNo dwarves or dwarf suits matched the patternHeres some sample output python dwarfsearch aeiouSleepys dwarf suit is taupe python dwarfsearch gaguNo dwarves or dwarf suits matched the patternThe assignment to suits demonstrates Pythons syntax for encoding literal dictionaries The suitsitems method is an iterator for keyvalue pairsnote that wereextracting both a dwarf name and a suit color on each iteration If you wanted toiterate through only the keys you could just say for dwarf in suitsPython implements regular expression handling through its re module No regexfeatures are built into the language itself so regexwrangling with Python is a bitclunkier than with say Perl Here the regex pattern is initially compiled from thefirst commandline argument surrounded by parentheses to form a capture groupStrings are then tested and modified with the search and sub methods of the regexobject You can also call research et al directly as functions supplying the regexto use as the first argumentThe in the substitution string is a backreference to the contents of the firstcapture group The strangelooking r prefix that precedes the substitution stringr suppresses the normal substitution of escape sequences in string constants r stands for raw Without this the replacement pattern would consist oftwo underscores surrounding a character with numeric code One thing to note about dictionaries is that they have no defined iteration order Ifyou run the dwarf search a second time you may well receive a different answer python dwarfsearch aeiouDopeys dwarf suit is green Ruby programmingRuby designed and maintained by Japanese developer Yukihiro Matz Matsumotoshares many features with Python including a pervasive everythings an objectapproach Although initially released in the mids Ruby did not gain prominence until a decade later with the release of the Rails web development platformRuby is still closely associated with the web in many peoples minds but theresnothing webspecific about the language itself It works well for generalpurposescripting However Python is probably a better choice for a primary scripting language if only because of its wider popularityAlthough Ruby is roughly equivalent to Python in many ways it is philosophicallymore permissive Ruby classes remain open for modification by other software forexample and the Rubyist community attaches little or no shame to extensions thatmodify the standard libraryRuby appeals to those with a taste for syntactic sugar features that dont really changethe basic language but that permit code to be expressed more concisely and clearlyIn the Rails environment for example the lineduedate daysfromnowcreates a Time object without referencing the names of any timerelated classes ordoing any explicit dateandtime arithmetic Rails defines days as an extension toFixnum the Ruby class that represents integers This method returns a Durationobject that acts like a number used as a value its equivalent to the numberof seconds in seven days Inspected in the debugger it describes itself as daysRuby makes it easy for developers to create domainspecific languages aka DSLsminilanguages that are in fact Ruby but that read like specialized configurationsystems Ruby DSLs are used to configure both Chef and Puppet for exampleInstallationSome systems have Ruby installed by default and some do not However its alwaysavailable as a package often in several versionsTo date version Ruby has maintained relatively good compatibility with oldcode In the absence of specific warnings its generally best to install the most recent versionUnfortunately most systems packages lag several releases behind the Ruby trunkIf your package library doesnt include the current release check rubylangorg todetermine what that is install the freshest version through RVM dont try to doit yourself This form of polymorphism is common to both Ruby and Python Its often called duck typing if anobject walks like a duck and quacks like a duck you neednt worry about whether its actually a duckSee Chapter formore informationabout Chef and PuppetSee page formore about RVMRuby quick startSince Ruby is so similar to Python here a perhapseerilyfamiliar look at someRuby snippets modeled on those from the Python section earlier in this chapterusrbinenv rubyprint Hello worldname Gwenrating characters SpongeBob Patrick Squidward elements lithium carbon boron print Namet name Ratingt rating print Characterstcharactersprint Elementstelementselementnames elementsvaluessortmapupcasejoin print Element namest elementnames elementseach do key valueprint Atomic number key is valueendThe output is as followsHello worldName GwenRating Characters SpongeBob Patrick SquidwardElements lithium carbon boronElement names BORON CARBON LITHIUMAtomic number is lithiumAtomic number is carbonAtomic number is boronLike Python Ruby uses brackets to delimit arrays and curly braces to delimit dictionary literals Ruby calls them hashes The operator separates each hashkey from its corresponding value and the keyvalue pairs are separated from eachother by commas Ruby does not have tuplesRubys print is a function or more accurately a global method just like that ofPython However if you want newlines you must specify them explicitly Inaddition the parentheses normally seen around the arguments of function callsare optional in Ruby Developers dont normally include them unless they help toclarify or disambiguate the code Note that some of these calls to print do includemultiple arguments separated by commas Theres also a puts function that adds newlines for you but its perhaps a bit too smart If you try toadd an extra newline of your own puts wont insert its own newlineIn several cases weve used brackets to interpolate the values of variables intodoublequoted strings Such brackets can contain arbitrary Ruby code whatevervalue the code produces is automatically converted to string type and inserted intothe outer string You can also concatenate strings with the operator but interpolation is typically more efficientThe line that calculates elementnames illustrates several more Ruby tropeselementnames elementsvaluessortmapupcasejoin This is a series of method calls each of which operates on the result returned by theprevious method much like a series of pipes in the shell For example elementsvalues method produces an array of strings which sort then orders alphabeticallyThis arrays map method calls the upcase method on each element then reassembles all the results back into a new array Finally join concatenates the elements ofthat array interspersed with commas to produce a stringBlocksIn the code on page the text between do and end is a block also commonlyknown in other languages as a lambda function a closure or an anonymous function elementseach do key valueprint Atomic number key is valueendThis particular block takes two arguments which it calls key and value It printsthe values of botheach looks like it might be a language feature but its just a method defined by hasheseach accepts the block as an argument and calls it once for each keyvalue pair thehash contains This type of iteration function used in combination with a block ishighly characteristic of Ruby code each is the standard name for generic iteratorsbut many classes define more specific versions such as eachline or eachcharacterRuby has a second syntax for blocks that uses curly braces instead of doend asdelimiters It means exactly the same thing but it looks more at home as part ofan expression For examplecharactersmap c creverse boBegnopS kcirtaP drawdiuqSThis form is functionally identical to charactersmapreverse but instead ofjust telling map what method to call we included an explicit block that calls thereverse method The bang at the end of sort warns you that theres something to be wary of when using this method It isnt significant to Ruby its just part of the methods name In this case the issue of note is thatsort sorts the array in place Theres also a sort method without the that returns the elements in anew sorted array Ruby actually has three entities of this general type known as blocks procs and lambdas The differences among them are subtle and not important for this overviewThe value of a block is the value of the last expression it evaluates before completing Conveniently pretty much everything in Ruby is an expression meaning apiece of code that can be evaluated to produce a value including control structures such as case analogous to what most languages call switch and ifelseThe values of these expressions mirror the value produced by whichever case orbranch was activatedBlocks have many uses other than iteration They let a single function perform bothsetup and takedown procedures on behalf of another section of code so they oftenrepresent multistep operations such as database transactions or filesystem operationsFor example the following code opens the etcpasswd file and prints out the linethat defines the root accountopen etcpasswd r do filefileeachline do lineprint line if linestartwith rootendendThe open function opens the file and passes its IO object to the outer block Oncethe block has finished running open automatically closes the file Theres no needfor a separate close operation although it does exist if you want to use it and thefile is closed no matter how the outer block terminatesThe postfix if construct used here might be familiar to those who have used Perl Itsa nice way to express simple conditionals without obscuring the primary action Hereits clear at a glance that the inner block is a loop that prints out some of the linesIn case the structure of that print line is not clear here it is again with the optionalparentheses included The if has the lowest precedence and it has a single method call on either sideprintline if linestartwithrootAs with the sort method we saw on page the question mark is just a namingconvention for methods that return Boolean valuesThe syntax for defining a named function is slightly different from that for a blockdef showusagemsg nilSTDERRputs msg if msgSTDERRputs Usage filename exit endThe parentheses are still optional but in practice they are always shown in this context unless the function takes no arguments Here the msg argument defaults to nilThe global variable is magic and contains the name by which the current programwas invoked Traditionally this would be the first argument of the argv array Butthe Ruby convention is that ARGV contains only actual commandline argumentsAs in C you can treat nonBoolean values as if they were Booleans as illustratedhere in the form of if msg The Ruby mapping for this conversion is a bit unusualthough everything except nil and false counts as true In particular is true Inpractice this usually ends up being what you wantSymbols and option hashesRuby makes extensive use of an uncommon data type called a symbol denoted witha colon eg example You can think of symbols as immutable strings Theyrecommonly used as labels or as wellknown hash keys Internally Ruby implementsthem as numbers so theyre fast to hash and compareSymbols are so commonly used as hash keys that Ruby defined an alternativesyntax for hash literals to reduce the amount of punctuation clutter The standardform hashh animal cat vegetable carrot mineral zeolite can be written in Ruby style ash animal cat vegetable carrot mineral zeolite Outside of this hash literal context symbols retain their prefixes wherever theyappear in the code For example heres how to get specific values back out of a hashhealthysnack hvegetable carrotRuby has an idiosyncratic but powerful convention for handling options in functioncalls If a called function requests this behavior Ruby collects trailing functioncallarguments that resemble hash pairs into a new hash It then passes that hash to thefunction as an argument For example in the Rails expressionfilefieldtag upload accept applicationpdf id commentpdfthe filefieldtag receives only two arguments the upload symbol and a hashcontaining the keys accept and id Because hashes have no inherent order itdoesnt matter in what order the options appearThis type of flexible argument processing is a Ruby standard in other ways tooRuby libraries including the standard library generally do their best to accept thebroadest possible range of inputs Scalars arrays and hashes are often equally validarguments and many functions can be called with or without blocksRegular expressions in RubyUnlike Python Ruby has a little bit of languageside sugar to help you deal withregular expressions Ruby supports the traditional notation for regular expression literals and the contents can include escape sequences much likedoublequoted stringsRuby also defines the operator and its negation to test for a match betweena string and a regular expression It evaluates either to the index of the first matchor to nil if there is no matchHermann Hesse Haeiou To access the components of a match explicitly invoke the regular expressionsmatch method It returns either nil if no match or an object that can be accessedas an array of componentsif m HwsmatchHeinrich Hoffmeyer headed this heist puts m HeinrichendHeres a look at a Ruby version of the dwarfsuit example from page suits Bashful yellow Sneezy brown Doc orange Grumpy redDopey green Happy blue Sleepy taupeabort Usage pattern unless ARGVsize pat ARGVmatches suitslazyselect dwarf color pat dwarf pat colorif matchesanydwarf color matchesfirstprint ss dwarf suit is s dwarftossubpat colorsubpat elseprint No dwarves or dwarf suits matched the patternendThe select method on a collection creates a new collection that includes only theelements for which the supplied block evaluates to true In this case matches is anew hash that includes only pairs for which either the key or the value matches thesearch pattern Since we made the starting hash lazy the filtering wont actuallyoccur until we try to extract values from the result In fact this code checks onlyas many pairs as are needed to find a matchDid you notice that the patternmatching operator was used on the symbolsthat represent the dwarves names It works because is smart enough to convert the symbols to strings before matching Unfortunately we have to perform theconversion explicitly with the tos method when applying the substitution pattern sub is only defined on strings so we need a real live string on which to call itNote also the parallel assignment of dwarf and color matchesfirst returns atwoelement array which Ruby automatically unpacksThe operator for strings works similarly to the same operator in Python its theRuby version of sprintf Here there are two components to fill in so we pass in thevalues as a twoelement arrayRuby as a filterYou can use Ruby without a script by putting isolated expressions on the commandline This is an easy way to do quick text transformations though truth be told Perlis still much better at this roleUse the p and e commandline options to loop through STDIN run a simpleexpression on each line represented as the variable and print the result Forexample the following command translates etcpasswd to upper case ruby pe traz AZ etcpasswdNOBODYUNPRIVILEGED USERVAREMPTYUSRBINFALSEROOTSYSTEM ADMINISTRATORVARROOTBINSHruby a turns on autosplit mode which separates input lines into fields that arestored in the array named F Whitespace is the default field separator but you canset another separator pattern with the F optionAutosplit is handy to use in conjunction with p or its nonautoprinting variant nThe command below uses ruby ane to produce a version of the passwd file thatincludes only usernames and shells ruby F ane print F F etcpasswdnobodyusrbinfalserootbinshThe truly intrepid can use i in conjunction with pe to edit files in place Ruby readsin the files presents their lines for editing and saves the results out to the originalfiles You can supply a pattern to i that tells Ruby how to back up the original version of each file For example ibak backs up passwd as passwdbak Bewareifyou dont supply a backup pattern you dont get backups at all Note that theres nospace between the i and the suffix Library and environment management for Python and RubyLanguages have many of the same packaging and version control issues that operating systems do and they often resolve them in analogous ways Python and Rubyare similar in this area so we discuss them together in this sectionFinding and installing packagesThe most basic requirement is some kind of easy and standardized way to discover obtain install update and distribute addon software Both Ruby and Pythonhave centralized warehouses for this purpose Rubys at rubygemsorg and Pythonsat pypipythonorgIn the Ruby world packages are called gems and the command that wranglesthem is called gem as well gem search regex shows the available gems with matching names and gem install gemname downloads and installs a gem You can usethe userinstall option to install a private copy instead of modifying the systemscomplement of gemsThe Python equivalent is called pip or pip or pip depending on which Pythonversions are installed Not all systems include pip by default Those that dont typically make it available as a separate OSlevel package As with gem pip searchand pip install are the mainstay commands A user option installs packages intoyour home directoryBoth gem and pip understand dependencies among packages at least at a basiclevel When you install a package youre implicitly asking for all the packages itdepends on to be installed as well if they are not already presentIn a basic Ruby or Python environment only a single version of a package can beinstalled at once If you reinstall or upgrade a package the old version is removedYou often have the choice to install a gem or pip package through the standardlanguage mechanism gem or pip or through an OSlevel package thats stockedin your vendors standard repository OS packages are more likely to be installedand run without issues but they are less likely to be up to date Neither option isclearly superiorCreating reproducible environmentsPrograms libraries and languages develop complex webs of dependencies as theyevolve together over time A productionlevel server might depend on tens or hundreds of these components each of which has its own expectations about the installation environment How do you identify which combination of library versionswill create a harmonious environment How do you make sure the configurationyou tested in the development lab is the same one that gets deployed to the cloudMore basically how do you make sure that managing all these parts isnt a big hassleBoth Python and Ruby have a standardized way for packages to express their dependencies In both systems package developers create a text file at the root ofthe project that lists its dependencies For Ruby the file is called Gemfile and forPython requirementstxt Both formats support flexible version specifications fordependencies so its possible for packages to declare that theyre compatible withany release of simplejson version or higher or Rails but not Rails Its alsopossible to specify an exact version requirement for any dependencyBoth file formats allow a source to be specified for each package so dependenciesneed not be distributed through the languages standard package warehouse Allcommon sources are supported from web URLs to local files to GitHub repositoriesYou install a batch of Python dependencies with pip install r requirementstxtAlthough pip does a fine job of resolving individual version specifications its unfortunately not able to solve complex dependency relationships among packages onits own Developers sometimes have to tweak the order in which packages are mentioned in the requirementstxt file to achieve a satisfactory result Its also possiblethough uncommon for new package releases to disturb the version equilibriumpip freeze prints out Pythons current package inventory in requirementstxt format specifying an exact version for each package This feature can be helpful forreplicating the current environment on a production serverIn the Ruby world gem install g Gemfile is a fairly direct analog of pip r In mostcircumstances though its better to use the Bundler gem to manage dependenciesRun gem install bundler to install it if its not already on the system then runbundle install from the root directory of the project youre setting upBundler has several nice tricks up its sleeve It does true recursive dependency management so if theres a set of gemsthat are mutually compatible and that satisfy all constraints Bundler canfind it without help It automatically records the results of version calculations in a file calledGemfilelock Maintaining this context information lets Bundler handleupdates to the Gemfile conservatively and efficiently Bundler modifies onlythe packages it needs to when migrating to a new version of the Gemfile Because Gemfilelock is sticky in this way running bundle install on adeployment server automatically reproduces the package environmentfound in the development environment In deployment mode bundle install deployment Bundler installsmissing gems into the local project directory helping isolate the projectfrom any future changes to the systems package complement You canthen use bundle exec to run specific commands within this hybrid gemenvironmentMultiple environmentspip and bundle handle dependency management for individual Python and Rubyprograms but what if two programs on the same server have conflicting requirements Ideally every program in a production environment would have its ownlibrary environment that was independent of the system and of all other programs Ruby gems can include shelllevel commands They dont typically have man pages though runbundle help for details or see bundlerio for complete documentation Or at least thats the default behavior Its easy to specify different requirements for development anddeployment environments in the Gemfile if you need to Some software packages such as Rails are Bundleraware and will use the locally installed packageseven without a bundle exec commandvirtualenv virtual environments for PythonPythons virtualenv package creates virtual environments that live within their owndirectories After installing the package just run the virtualenv command with apathname to set up a new environment virtualenv myprojectNew python executable in homeulsahmyprojectbinpythonInstalling setuptools pip wheeldoneEach virtual environment has a bin directory that includes binaries for Pythonand PIP When you run one of those binaries youre automatically placed in thecorresponding virtual environment Install packages into the environment as usualby running the virtual environments copy of pipTo start a virtualized Python program from cron or from a system startup scriptexplicitly specify the path to the proper copy of python Alternatively put the pathin the scripts shebang lineWhen working interactively in the shell you can source a virtual environmentsbinactivate script to set the virtual environments versions of python and pip asthe defaults The script rearranges your shells PATH variable Use deactivate toleave the virtual environmentVirtual environments are tied to specific versions of Python At the time a virtualenvironment is created you can set the associated Python binary with virtualenvspython option The Python binary must already be installed and functioningRVM the Ruby enVironment ManagerThings are similar in the Ruby world but somewhat more configurable and morecomplicated You saw on page that Bundler can cache local copies of Rubygems on behalf of a specific application This is a reasonable approach when moving projects into production but it isnt so great for interactive use It also assumesthat you want to use the systems installed version of RubyThose who want a more general solution should investigate RVM a complex andrather unsightly environment virtualizer that uses a bit of shell hackery To be fairRVM is an extremely polished example of the unsightly hack genus In practiceit works smoothlyRVM manages both Ruby versions and multiple gem collections and it lets youswitch among all these on the fly For example the command rvm rubyulsah As with other Pythonrelated commands there are numericsuffixed versions of the virtualenv command that go with particular Python versionsactivates Ruby version and the gemset called ulsah References to ruby or gemnow resolve to the specified versions This magic also works for programs installedby gems such as bundle and rails Best of all gem management is unchanged justuse gem or bundle as you normally would and any newly installed gems automatically end up in the right placeRVMs installation procedure involves fetching a Bash script from the web and executing it locally Currently the commands are curl o tmpinstall sSL httpsgetrvmio sudo bash tmpinstall stablebut check rvmio for the current version and a cryptographic signature Be sure toinstall with sudo as shown here if you dont RVM sets up a private environment inyour home directory That works fine but nothing on a production system shouldrefer to your home directory Youll also need to add authorized RVM users to thervm UNIX groupAfter the initial RVM installation dont use sudo when installing gems or changingRVM configurations RVM controls access through membership in the rvm groupUnder the covers RVM does its magic by manipulating the shells environmentvariables and search path Ergo it has to be invited into your environment like avampire by running some shell startup code at login time When you install RVMat the system level RVM drops an rvmsh scriptlet with the proper commandsinto etcprofiled Some shells automatically run this stub Those that dont justneed an explicit source command which you can add to your shells startup filessource etcprofiledrvmshRVM doesnt modify the systems original Ruby installation in any way In particular scripts that start with ausrbinrubyshebang continue to run under the systems default Ruby and to see only systeminstalled gems The following variant is more liberalusrbinenv rubyIt locates the ruby command according to the RVM context of the user that runs itrvm install installs new versions of Ruby This RVM feature makes it quite painlessto install different versions of Ruby and it should generally be used in preferenceto your OSs native Ruby packages which are seldom up to date rvm install downloads binaries if they are available If not it installs the necessary OS packages andthen builds Ruby from source code Also see page for some comments on why our example commands dont exactly match RVMsrecommendationsHeres how we might set up for deployment a Rails application known to be compatible with Ruby rvm install rubySearching for binary rubies this might take some timeNo binary rubies available for ubuntuxrubyContinuing with compilation Please read rvm help mount to get moreinformation on binary rubiesChecking requirements for ubuntuInstalling required packages gawk libreadlinedev zlibgdevlibncursesdev automake libtool bison libffidevRequirements installation successfulInstalling Ruby from source to usrlocalrvmrubiesruby thismay take a while depending on your cpusIf you installed RVM as described above the Ruby system is installed underneathusrlocalrvm and is accessible to all accounts on the systemUse rvm list known to find out which versions of Ruby RVM knows how to download and build Rubies shown by rvm list have already been installed and are available for use cd myprojectrails rvm rubymyproject create default rubyversionruby gemset created usrlocalrvmgemsrubymyprojectruby generating myproject wrappers gem install bundlerFetching bundlergem Successfully installed bundler gem installed bundleFetching gem metadata from httpsrubygemsorgFetching version metadata from httpsrubygemsorgFetching dependency metadata from httpsrubygemsorgResolving dependenciesThe rubymyproject line specifies both a Ruby version and a gemset Thecreate flag creates the gemset if it doesnt already exist default makes this combination your RVM default and rubyversion writes the names of the Ruby interpreter and gemset to rubyversion and rubygemset in the current directoryIf the version files exist RVM automatically reads and honors them when dealing with scripts in that directory This feature allows each project to specify itsown requirements and frees you from the need to remember what goes with whatTo run a package in its requested environment as documented by rubyversionand rubygemset run the commandrvm in pathtodir do startupcmd startuparg This is a handy syntax to use when running jobs out of startup scripts or cron Itdoesnt depend on the current user having set up RVM or on the current usersRVM configurationAlternatively you can specify an explicit environment for the command as inrvm rubymyproject do startupcmd startuparg Yet a third option is to run a ruby binary from within a wrapper maintained byRVM for this purpose For example runningusrlocalrvmwrappersrubymyprojectruby automatically transports you into the Ruby world with the myproject gemset Revision control with GitMistakes are a fact of life Its important to keep track of configuration and codechanges so that when these changes cause problems you can easily revert to aknowngood state Revision control systems are software tools that track archiveand grant access to multiple revisions of filesRevision control systems address several problems First they define an organizedway to trace the history of modifications to a file such that changes can be understood in context and so that earlier versions can be recovered Second they extendthe concept of versioning beyond the level of individual files Related groups offiles can be versioned together taking into account their interdependencies Finally revision control systems coordinate the activities of multiple editors so thatrace conditions cannot cause anyones changes to be permanently lost and so thatincompatible changes from multiple editors do not become active simultaneouslyBy far the most popular system in use today is Git created by the one and only Linus Torvalds Linus created Git to manage the Linux kernel source code because ofhis frustration with the version control systems that existed at the time It is now asubiquitous and influential as Linux Its difficult to tell which of Linuss inventionshas had a greater impact on the worldMost modern software is developed with help from Git and as result administrators encounter it daily You can find download and contribute to open sourceprojects on GitHub GitLab and other social development sites You can also useGit to track changes to scripts configuration management code templates and anyother text files that need to be tracked over time We use Git to track the contentsof this book Its well suited to collaboration and sharing making it an essential toolfor sites that embrace DevOps For example suppose that sysadmins Alice and Bob both edit the same file and that each makes somechanges Alice saves first When Bob saves his copy of the file it overwrites Alices version If Alicehas quit from the editor her changes are completely gone and unrecoverableSee page formore informationabout DevOpsGits shtick is that it has no distinguished central repository To access a repositoryyou clone it including its entire history and carry it around with you like a hermit crab lugging its shell Your commits to the repository are local operations sotheyre fast and you dont have to worry about communicating with a central serverGit uses an intelligent compression system to reduce the cost of storing the entirehistory and in most cases this system is quite effectiveGit is great for developers because they can pile their source code onto a laptopand work without being connected to a network while still reaping all the benefitsof revision control When the time comes to integrate multiple developers worktheir changes can be integrated from one copy of the repository to another in anyfashion that suits the organizations workflow Its always possible to unwind twocopies of a repository back to their common ancestor state no matter how manychanges and iterations have occurred after the splitGits use of a local repository is a big leap forward in revision controlor perhapsmore accurately its a big leap backward but in a good way Early revision controlsystems such as RCS and CVS used local repositories but were unable to handlecollaboration change merging and independent development Now weve comefull circle to a point where putting files under revision control is once again a fastsimple local operation At the same time all Gits advanced collaboration featuresare available for use in situations that require themGit has hundreds of features and can be quite puzzling in advanced use Howevermost Git users get by with only a handful of simple commands Special situationsare best handled by searching Google for a description of what you want to do eggit undo last commit The top result is invariably a Stack Overflow discussionthat addresses your exact situation Above all dont panic Even if it looks like youscrewed up the repository and deleted your last few hours of work Git very likelyhas a copy stashed away You just need the reflog fairy to go and fetch itBefore you start using Git set your name and email address git config global username John Q Ulsah git config global useremail ulsahadmincomThese commands create the iniformatted Git config file gitconfig if it doesntalready exist Later git commands look in this file for configuration settings Gitpower users make extensive customizations here to match their desired workflowA simple Git exampleWeve contrived for you a simple example repository for maintaining some shellscripts In practice you can use Git to track configuration management code infrastructure templates ad hoc scripts text documents static web sites and anythingelse you need to work on over timeThe following commands create a new Git repository and populate its baseline pwdhomebwhaley mkdir scripts cd scripts git initInitialized empty Git repository in homebwhaleyscriptsgit cat superscriptsh EOF binsh echo Hello world EOF chmod x superscriptsh git add git commit m Initial commitmaster rootcommit adc superscriptsh file changed insertions deletions create mode superscriptshIn the sequence above git init creates the repositorys infrastructure by creating agit directory in homebwhaleyscripts Once you set up an initial hello worldscript the command git add copies it to Gits index which is a staging area forthe upcoming commitThe index is a not just a list of files to commit its a bona fide file tree thats every bitas real as the current working directory and the contents of the repository Files inthe index have contents and depending on what commands you run those contentsmay end up being different from both the repository and the working directory gitadd really just means cp from the working directory to the indexgit commit enters the contents of the index into the repository Every commit needsa log message The m flag lets you include the message on the command line Ifyou leave it out git starts up an editor for youNow make a change and check it into the repository vi superscriptsh git commit superscriptsh m Made the script more supermaster f Made the script more super file changed insertions deletionsNaming the modified files on the git commit command line bypasses Gits normaluse of the index and creates a revision that includes only changes to the namedfiles The existing index remains unchanged and Git ignores any other files thatmay have been modifiedIf a change involves multiple files you have a couple of options If you know exactlywhich files were changed you can always list them on the command line as shownabove If youre lazy you can run git commit a to make Git add all modified files tothe index before doing the commit This last option has a couple of pitfalls howeverFirst there may be modified files that you dont want to include in the commit Forexample if superscriptsh had a config file and you had modified that config file fordebugging you might not want to commit the modified file back to the repositoryThe second issue is that git commit a picks up only changes to files that are currently under revision control It does not pick up new files that you may have created in the working directoryFor an overview of Gits state you can run git status This command informs youof new files modified files and staged files all at once For example suppose thatyou added morescriptsanotherscriptsh Git might show the following git statusOn branch masterChanges not staged for commit use git add file to update what will be committed use git checkout file to discard changes in working directorymodified superscriptshUntracked files use git add file to include in what will be committedmorescripts tmpfileno changes added to commit use git add andor git commit aanotherscriptsh is not listed by name because Git doesnt yet see beneath themorescripts directory that contains it You can see that superscriptsh has beenmodified and you can also see a spurious tmpfile that probably shouldnt be included in the repository You can run git diff superscriptsh to see the changesmade to the script git helpfully suggests commands for the next operations youmay want to performSuppose you want to track the changes to superscriptsh separately from yournew anotherscriptsh git commit superscriptsh m The most super change yetCreated commit fc The most super change yet files changed insertions deletionsTo eradicate tmpfile from Gits universe create or edit a gitignore file and putthe filename inside it This makes Git ignore the tmpfile now and forever Patternswork too echo tmpfile gitignoreFinally commit all the outstanding changes git add sudo git commit m Ignore tmpfile Add anotherscriptsh to the repoCreated commit e Ignore tmpfile add anotherscriptsh to the repo files changed insertions deletions create mode gitignore create mode morescriptsanotherscriptshNote that the gitignore file itself becomes part of the managed set of files which isusually what you want Its fine to readd files that are already under managementso git add is an easy way to say I want to make the new repository image look likethe working directory minus anything listed in gitignore You couldnt just do a gitcommit a in this situation because that would pick up neither anotherscriptshnor gitignore these files are new to Git and so must be explicitly addedGit caveatsIn an effort to fool you into thinking that it manages files permissions as well astheir contents Git shows you file modes when adding new files to the repositoryIts lying Git does not track modes owners or modification timesGit does track the executable bit If you commit a script with the executable bit setany future clones will also be executable But dont expect Git to track ownershipor readonly status A corollary is that you cant count on using Git to recover complex file hierarchies in situations where ownerships and permissions are importantAnother corollary is that you should never include plain text passwords or othersecrets in a Git repository Not only are they open to inspection by anyone withaccess to the repository but they may also be inadvertently unpacked in a formthats accessible to the worldSocial coding with GitThe emergence and rapid growth of social development sites such as GitHub andGitLab is one of the most important trends in recent computing history Millionsof open source software projects are built and managed transparently by huge communities of developers who use every conceivable language Software has neverbeen easier to create and distributeGitHub and GitLab are in essence hosted Git repositories with a lot of added features that relate to communication and workflow Anyone can create a repositoryRepositories are accessible both through the git command and on the web The webUI is friendly and offers features to support collaboration and integrationThe social coding experience can be somewhat intimidating for neophytes but infact it isnt complicated once some basic terms and methodology are understoodmaster is the default name assigned to the first branch in a new repositoryMost software projects use this default as their main line of developmentalthough some may not have a master branch at all The master branch isusually managed to contain current but functional code bleedingedgedevelopment happens elsewhere The latest commit is known as the tipor head of the master branch On GitHub a fork is a snapshot of a repository at a specific point in timeForks happen when a user doesnt have permission to modify the mainrepository but wants to make changes either for future integration withthe primary project or to create an entirely separate development path A pull request is a request to merge changes from one branch or fork toanother Theyre read by the maintainers of the target project and can beaccepted to incorporate code from other users and developers Every pullrequest is also a discussion thread so both principals and kibitzers cancomment on prospective code updates A committer or maintainer is an individual who has write access to arepository For large open source projects this highly coveted status isgiven only to trusted developers who have a long history of contributionsYoull often land in a GitHub or GitLab repository when trying to locate or updatea piece of software Make sure youre looking at the trunk repository and not somerandom persons fork Looked for a forked from indication and follow itBe cautious when evaluating new software from these sites Below are a few questions to ponder before rolling out a random piece of new software at your site How many contributors have participated in development Does the commit history indicate recent regular development What is the license and is it compatible with your organizations needs What language is the software written in and do you know how to manage it Is the documentation complete enough for effective use of the softwareMost projects have a particular branching strategy that they rely on to track changesto the software Some maintainers insist on rigorous enforcement of their chosenstrategy and others are more lenient One of the most widely used is the Git Flowmodel developed by Vincent Driessen see googlGDaF for details Before contributing to a project familiarize yourself with its development practices to helpout the maintainersAbove all remember that open source developers are often unpaid They appreciateyour patience and courtesy when engaging through code contributions or openingsupport issues Recommended readingBrooks Frederick P Jr The Mythical ManMonth Essays on Software Engineering Reading MA AddisonWesley Chacon Scott and Straub Ben Pro Git nd edition gitscmcombookenvThe complete Pro Git book released for free under a Creative Commons licenseShells and shell scriptingRobbins Arnold and Nelson H F Beebe Classic Shell Scripting Sebastopol CAOReilly Media This book addresses the traditional and portable Bourneshell dialect It also includes quite a bit of good info on sed and awkPowers Shelley Jerry Peek Tim OReilly and Mike Loukides Unix PowerTools rd Edition Sebastopol CA OReilly Media This classic UNIX bookcovers a lot of ground including sh scripting and various feats of commandlinefuSome sections are not aging gracefully but the shellrelated material remains relevantSobell Mark G A Practical Guide to Linux Commands Editors and Shell Programming Upper Saddle River NJ Prentice Hall This book is notable for itsinclusion of tcsh as well as bashShotts William E Jr The Linux Command Line A Complete Introduction SanFrancisco CA No Starch Press This book is specific to bash but its a nicecombination of interactive and programming material with some extras thrownin Most of the material is relevant to UNIX as well as LinuxBlum Richard and Christine Bresnahan Linux Command Line and ShellScripting Bible rd Edition Indianapolis IN John Wiley Sons Inc Thisbook focuses a bit more specifically on the shell than does the Shotts book thoughits also bashspecificCooper Mendel Advanced BashScripting Guide wwwtldporgLDPabshtmlA free and very good online book Despite the title its safe and appropriate forthose new to bash as well Includes lots of good example scriptsRegular expressionsFriedl Jeffrey Mastering Regular Expressions rd Edition Sebastopol CAOReilly Media Goyvaerts Jan and Steven Levithan Regular Expressions Cookbook Sebastopol CA OReilly Media Goyvaerts Jan regularexpressionsinfo A detailed online source of informationabout regular expressions in all their various dialectsKrumins Peteris Perl OneLiners Programs That Get Things Done San Francisco CA No Starch Press PythonSweigart Al Automate the Boring Stuff with Python Practical Programming forTotal Beginners San Francisco CA No Starch Press This is an approachableintroductory text for Python and programming generally Examples include common administrative tasksPilgrim Mark Dive Into Python Berkeley CA Apress This classic bookon Python is also available for free on the web at diveintopythonnetPilgrim Mark Dive Into Python Berkeley CA Apress Dive Into Pythonupdated for Python Also available to read free on the web at diveintopythonnetRamalho Luciano Fluent Python Sebastopol CA OReilly Media Advanced idiomatic Python Beazley David and Brian K Jones Python Cookbook rd Edition SebastopolCA OReilly Media Covers Python Gift Noah and Jeremy M Jones Python for Unix and Linux System Administrators Sebastopol CA OReilly Media RubyFlanagan David and Yukihiro Matsumoto The Ruby Programming LanguageSebastopol CA OReilly Media This classic concise and wellwritten summary of Ruby comes straight from the horses mouth Its relatively matteroffact anddoes not cover Ruby and beyond however the language differences are minorBlack David A The WellGrounded Rubyist nd Edition Shelter Island NYManning Publications Dont let the title scare you off if you dont have priorRuby experience this is a good allaround introduction to Ruby Thomas Dave Programming Ruby The Pragmatic Programmers Guideth Edition Pragmatic Bookshelf Classic and frequently updatedFulton Hal The Ruby Way Solutions and Techniques in Ruby Programming rdEdition Upper Saddle River NJ AddisonWesley Another classic and uptodate guide to Ruby with a philosophical bentModern computing environments span physical hardware cloud systems and virtualhosts Along with the flexibility of this hybrid infrastructure comes an increasingneed for centralized and structured account management System administratorsmust understand both the traditional account model used by UNIX and Linux andthe ways in which this model has been extended to integrate with directory servicessuch as LDAP and Microsofts Active DirectoryAccount hygiene is a key determinant of system security Infrequently used accountsare prime targets for attackers as are accounts with easily guessed passwords Evenif you use your systems automated tools to add and remove users its important tounderstand the changes the tools are making For this reason we start our discussion of account management with the flat files you would modify to add users to astandalone machine In later sections we examine the higherlevel user management commands that come with our example operating systems and the configuration files that control their behaviorMost systems also have simple GUI tools for adding and removing users but thesetools dont usually support advanced features such as a batch mode or advancedlocalization The GUI tools are simple enough that we dont think its helpful toreview their operation in detail so in this chapter we stick to the command line User ManagementThis chapter focuses fairly narrowly on adding and removing users Many topicsassociated with user management actually live in other chapters and are referencedhere only indirectly For example Pluggable authentication modules PAM for password encryption andthe enforcement of strong passwords are covered in Chapter SingleSignOn See the material starting on page Password vaults for managing passwords are described in Chapter Security see page Directory services such as OpenLDAP and Active Directory are outlinedin Chapter Single SignOn starting on page Policy and regulatory issues are major topics of Chapter Methodology Policy and Politics Account mechanicsA user is really nothing more than a number Specifically an unsigned bit integerknown as the user ID or UID Almost everything related to user account management revolves around this numberThe system defines an API through standard C library routines that maps UIDnumbers back and forth into more complete sets of information about users Forexample getpwuid accepts a UID as an argument and returns a correspondingrecord that includes information such as the associated login name and home directory Likewise getpwnam looks up this same information by login nameTraditionally these library calls obtained their information directly from a text fileetcpasswd As time went on they began to support additional sources of information such as network information databases eg LDAP and readprotected filesin which encrypted passwords could be stored more securelyThese layers of abstraction which are often configured in the nsswitchconf fileenable higherlevel processes to function without direct knowledge of the underlyingaccount management method in use For example when you log in as dotty theloggingin process window server login getty or whatever does a getpwnamon dotty and then validates the password you supply against the encrypted passwdrecord returned by the library regardless of its actual originWe start with the etcpasswd file approach which is still supported everywhereThe other options emulate this model in spirit if not in formSee page for moredetails regarding thensswitchconf file The etcpasswd fileetcpasswd is a list of users recognized by the system It can be extended or replaced by one or more directory services so its complete and authoritative onlyon standalone systemsHistorically each users encrypted password was also stored in the etcpasswd filewhich is worldreadable However the onset of more powerful processors made itincreasingly feasible to crack these exposed passwords In response UNIX and Linux moved the passwords to a separate file etcmasterpasswd on FreeBSD andetcshadow on Linux that is not worldreadable These days the passwd file itselfcontains only a proforma entry to mark the former location of the password fieldx on Linux and on FreeBSDThe system consults etcpasswd at login time to determine a users UID and homedirectory among other things Each line in the file represents one user and containsseven fields separated by colons Login name Encrypted password placeholder see page UID user ID number Default GID group ID number Optional GECOS information full name office extension home phone Home directory Login shellFor example the following lines are all valid etcpasswd entriesrootxThe SystemxbinshjlJim LaneECOTstaffjlbinshdottyx homedottybintcshIf user accounts are shared through a directory service such as LDAP you mightsee special entries in the passwd file that begin with or These entries tell thesystem how to integrate the directory services data with the contents of the passwdfile This integration can also be set up in the etcnsswitchconf fileThe following sections discuss the etcpasswd fields in more detailLogin nameLogin names also known as usernames must be unique and depending on theoperating system may have character set restrictions All UNIX and Linux flavorscurrently limit logins to charactersLogin names can never contain colons or newlines because these characters areused as field separators and entry separators in the passwd file respectively Depending on the system other character restrictions may also be in place Ubuntuis perhaps the most lax as it allows logins starting withor consisting entirelySee page for moreinformation about thensswitchconf fileofnumbers and other special characters For reasons too numerous to list werecommend sticking with alphanumeric characters for logins using lower case andstarting login names with a letterLogin names are case sensitive We are not aware of any problems caused by mixedcase login names but lowercase names are traditional and also easier to type Confusion could ensue if the login names john and John were different peopleLogin names should be easy to remember so random sequences of letters do notmake good login names Since login names are often used as email addresses itsuseful to establish a standard way of forming them It should be possible for usersto make educated guesses about each others login names First names last namesinitials or some combination of these make reasonable naming schemes Keep inmind that some email systems treat addresses as being case insensitive which is yetanother good reason to standardize on lowercase login namesAny fixed scheme for choosing login names eventually results in duplicate namesso you sometimes have to make exceptions Choose a standard way of dealing withconflicts such as adding a number to the endIts common for large sites to implement a fullname email addressing scheme egJohnQPublicmysitecom that hides login names from the outside world This isa good idea but it doesnt obviate any of the naming advice given above If for noother reason than the sanity of administrators its best if login names have a clearand predictable correspondence to users actual namesFinally a user should have the same login name on every machine This rule ismostly for convenience both yours and the usersEncrypted passwordHistorically systems encrypted users passwords with DES As computing powerincreased those passwords became trivial to crack Systems then moved to hiddenpasswords and to MDbased cryptography Now that significant weaknesses havebeen discovered in MD salted SHAbased password hashes have become thecurrent standard See the Guide to Cryptography document at owasporg for uptodate guidanceOur example systems support a variety of encryption algorithms but they all default to SHA You shouldnt need to update the algorithm choice unless you areupgrading systems from much older releasesOn FreeBSD the default algorithm can be modified through the etcloginconf file For some unfortunate reason the permissible character set even includes Unicode emoticons Thatmakes us RFC requires that the local portion of an address that is the part before the sign be treatedas case sensitive The remainder of the address is handled according to the standards of DNS whichis case insensitive Unfortunately this distinction is subtle and it is not universally implemented Remember also that many legacy email systems predate the authority of the IETFOn Debian and Ubuntu the default was formerly managed through etclogindefsbut this practice has since been obsoleted by Pluggable Authentication ModulesPAM Default password policies including the hashing algorithm to use can befound in etcpamdcommonpasswdOn Red Hat and CentOS the password algorithm can still be set in etclogindefsor through the authconfig command as shown here sudo authconfig passalgosha updateChanging the password algorithm does not update existing passwords so usersmust manually update their passwords before the new algorithm can take effectTo invalidate a users password and force an update use chage d usernamePassword quality is another important issue In theory longer passwords are moresecure as are passwords that include a range of different character types eg uppercase letters punctuation marks and numbersMost systems let you impose password construction standards on your users butkeep in mind that users can be adept at skirting these requirements if they findthem excessive or burdensome Table shows the default standards used by ourexample systemsTable Password quality standardsSystem Default requirements Where setRed HatCentOS characters complexity enforced etclogindefsetcsecuritypwqualityconfetcpamdsystemauthDebianUbuntu characters complexity enforced etclogindefsetcpamdcommonpasswordFreeBSD No constraints etcloginconfPassword quality requirements are a matter of debate but we recommend that youprioritize length over complexity Twelve characters is the minimal length for afutureproof password note that this is significantly longer than any systems default Your site may also have organizationwide standards for password quality Ifit does defer to those settingsIf you choose to bypass your systems tools for adding users and instead modifyetcpasswd by hand by running the vipw commandsee page to create anew account put a FreeBSD or an x Linux in the encrypted password field See xkcdcomcomicspasswordstrengthpng for more commentary on this conceptRHELSee page formore comments onpassword selectionThis measure prevents unauthorized use of the account until you or the user hasset a real passwordEncrypted passwords are of constant length characters for SHA characters for MD and characters for DES regardless of the length of the unencrypted password Passwords are encrypted in combination with a random saltso that a given password can correspond to many different encrypted forms If twousers happen to select the same password this fact usually cannot be discoveredby inspection of the encrypted passwordsMDencrypted password fields in the shadow password file always start with or md Blowfish passwords start with SHA passwords with andSHA passwords with UID user ID numberBy definition root has UID Most systems also define pseudousers such as bin anddaemon to be the owners of commands or configuration files Its customary to putsuch fake logins at the beginning of the etcpasswd file and to give them low UIDsand a fake shell eg binfalse to prevent anyone from logging in as those usersTo allow plenty of room for nonhuman users you might want to add in the futurewe recommend that you assign UIDs to real users starting at or higher Thedesired range for new UIDs can be specified in the configuration files for useraddBy default our Linux reference systems start UIDs at and go up from thereFreeBSD starts the first user at UID and then adds one for each additional userDo not recycle UIDs even when users leave your organization and you delete theiraccounts This precaution prevents confusion if files are later restored from backupswhere users may be identified by UID rather than by login nameUIDs should be kept unique across your entire organization That is a particularUID should refer to the same login name and the same person on every machinethat person is authorized to use Failure to maintain distinct UIDs can result in security problems with systems such as NFS and can also result in confusion when auser moves from one workgroup to anotherIt can be hard to maintain unique UIDs when groups of machines are administeredby different people or organizations The problems are both technical and politicalThe best solution is to have a central database or directory server that contains arecord for each user and enforces uniquenessA simpler scheme is to assign each group within an organization its own range ofUIDs and to let each group manage its own range This solution keeps the UIDspaces separate but does not address the parallel issue of unique login names Regardless of your scheme consistency of approach is the primary goal If consistencyisnt feasible UID uniqueness is the secondbest targetSee page fora description ofthe root accountThe Lightweight Directory Access Protocol LDAP is a popular system for managing and distributing account information and works well for large sites It is brieflyoutlined in this chapter starting on page and is covered more thoroughly inChapter Single SignOn starting on page Default GID group ID numberLike a UID a group ID number is a bit integer GID is reserved for the groupcalled root system or wheel As with UIDs the system uses several predefinedgroups for its own housekeeping Alas there is no consistency among vendors Forexample the group bin has GID on Red Hat and CentOS GID on Ubuntuand Debian and GID on FreeBSDIn ancient times when computing power was expensive groups were used for accounting purposes so that the right department could be charged for your secondsof CPU time minutes of login time and kilobytes of disk used Today groups areused primarily to share access to filesThe etcgroup file defines the groups with the GID field in etcpasswd providinga default or effective GID at login time The default GID is not treated speciallywhen access is determined it is relevant only to the creation of new files and directories New files are normally owned by your effective group to share files withothers in a project group you must manually change the files group ownerTo facilitate collaboration you can set the setgid bit on a directory or mountfilesystems with the grpid option Both of these measures make newly created filesdefault to the group of their parent directoryGECOS fieldThe GECOS field is sometimes used to record personal information about each userThe field is a relic from a much earlier time when some early UNIX systems usedGeneral Electric Comprehensive Operating Systems for various services It has nowelldefined syntax Although you can use any formatting conventions you likeconventionally commaseparated GECOS entries are placed in the following order Full name often the only field used Office number and building Office telephone extension Home phone numberThe chfn command lets users change their own GECOS information chfn is usefulfor keeping things like phone numbers up to date but it can be misused For example a user can change the information to be obscene or incorrect Some systemscan be configured to restrict which fields chfn can modify most college campuses disable it entirely On most systems chfn understands only the passwd file soif you use LDAP or some other directory service for login information chfn maynot work at allSee page for moreinformation aboutsetgid directoriesSee page formore informationabout LDAPHome directoryA users home directory is his or her default directory at login time Home directories are where login shells look for accountspecific customizations such as shellaliases and environment variables as well as SSH keys server fingerprints andother program stateBe aware that if home directories are mounted over a network filesystem they maybe unavailable in the event of server or network problems If the home directoryis missing at login time the system might print a message such as no home directory and put the user in Alternatively it might disallow the login entirelydepending on the system configuration Home directories are covered in moredetail on page Login shellThe login shell is normally a command interpreter but it can be any program ABourneshell compatible sh is the default for FreeBSD and bash the GNU Bourneagain shell is the default for LinuxSome systems permit users to change their shell with the chsh command but aswith chfn this command might not work if you are using LDAP or some otherdirectory service to manage login information If you use the etcpasswd file asysadmin can always change a users shell by editing the passwd file with vipw The Linux etcshadow fileOn Linux the shadow password file is readable only by the superuser and serves tokeep encrypted passwords safe from prying eyes and password cracking programsIt also includes some additional account information that wasnt provided for inthe original etcpasswd format These days shadow passwords are the default onall systemsThe shadow file is not a superset of the passwd file and the passwd file is not generated from it You must maintain both files or use tools such as useradd that maintain both files on your behalf Like etcpasswd etcshadow contains one line foreach user Each line contains nine fields separated by colons Login name Encrypted password Date of last password change Minimum number of days between password changes Maximum number of days between password changes Number of days in advance to warn users about password expiration This message appears when you log in on the console or on a terminal but not when you log inthrough a display manager such as xdm gdm or kdm Not only will you not see the message butyou will generally be logged out immediately because of the display managers inability to write to theproper directory eg gnomeSee page formore informationabout shells Days after password expiration that account is disabled Account expiration date A field reserved for future use which is currently always emptyOnly the values for the username and password are required Absolute date fieldsin etcshadow are specified in terms of days not seconds since Jan whichis not a standard way of reckoning time on UNIX or Linux systemsA typical shadow entry looks like thismillertiTEFbMTMCXmxPwErbEefRUBvfzvEgXQdaZgeOduXyvtsFziGlIqavLilTQgniAHmCzwLoaGzoFzaMmYwOlHere is a more complete description of each field The login name is the same as in etcpasswd This field connects a userspasswd and shadow entries The encrypted password is identical in concept and execution to the onepreviously stored in etcpasswd The last change field records the time at which the users password waslast changed This field is filled in by the passwd command The fourth field sets the number of days that must elapse between password changes The idea is to force authentic changes by preventing usersfrom immediately reverting to a familiar password after a required changeHowever this feature can be somewhat dangerous in the aftermath of asecurity intrusion We suggest setting this field to The fifth field sets the maximum number of days allowed between password changes This feature allows the administrator to enforce passwordaging see page for more information Under Linux the actual enforced maximum number of days is the sum of this field and the seventhgrace period field The sixth field sets the number of days before password expiration whenlogin should begin to warn the user of the impending expiration The eighth field specifies the day in days since Jan on which theusers account will expire The user cannot log in after this date until thefield has been reset by an administrator If the field is left blank the account never expiresYou can use usermod to set the expiration field It accepts dates in theformat yyyymmdd The ninth field is reserved for future use To convert between the date in seconds and in days run expr dates Or at this rate may never be usedLets look again at our example shadow linemillertiTEFbMTMCXmxPwErbEefRUBvfzvEgXQdaZgeOduXyvtsFziGlIqavLilTQgniAHmCzwLoaGzoFzaMmYwOlIn this example the user millert last changed his password on June Thepassword must be changed again within days and millert will receive warningsthat the password needs to be changed for the last two weeks of this period Theaccount does not have an expiration dateUse the pwconv utility to reconcile the contents of the shadow file and those ofthe passwd file picking up any new additions and deleting users that are no longerlisted in passwd FreeBSDs etcmasterpasswd and etcloginconf filesThe adoption of PAM and the availability of similar user management commandson FreeBSD and Linux have made account administration relatively consistentacross platforms at least at the topmost layer However a few differences do existin the underlying implementationThe etcmasterpasswd fileOn FreeBSD the real password file is etcmasterpasswd which is readable onlyby root The etcpasswd file exists for backward compatibility and does not containany passwords instead it has characters as placeholdersTo edit the password file run the vipw command This command invokes youreditor on a copy of etcmasterpasswd then installs the new version and regenerates the etcpasswd file to reflect any changes vipw is standard on all UNIX andLinux systems but its particularly important to use on FreeBSD because the dualpassword files need to stay synchronized See page In addition to containing all the fields of the passwd file the masterpasswd filecontains three bonus fields Unfortunately theyre squeezed in between the defaultGID field and the GECOS field so the file formats are not directly compatible Theextra three fields are Login class Password change time Expiration timeThe login class if one is specified refers to an entry in the etcloginconf file Theclass determines resource consumption limits and controls a variety of other settings See the next section for specificsThe password change time field implements password aging It contains the time inseconds since the UNIX epoch after which the user will be forced to change his or herpassword You can leave this field blank indicating that the password never expiresThe account expiration time gives the time and date in seconds as for passwordexpiration at which the users account will expire The user cannot log in after thisdate unless the field is reset by an administrator If this field is left blank the account will not expireThe etcloginconf fileFreeBSDs etcloginconf file sets accountrelated parameters for users and groupsof users Its format consists of colondelimited keyvalue pairs and Boolean flagsWhen a user logs in the login class field of etcmasterpasswd determines whichentry in etcloginconf to apply If the users masterpasswd entry does not specifya login class the default class is usedA loginconf entry can set any of the following Resource limits maximum process size maximum file size number ofopen files etc Session accounting limits when logins are allowed and for how long Default environment variables Default paths PATH MANPATH etc Location of the message of the day file Host and TTYbased access control Default umask Account controls mostly superseded by the PAM module pampasswdqcThe following example overrides several of the default values Its intended for assignment to system administratorssysadminignorenologinrequirehomemaxprocunlimitedopenfilesunlimitedtcdefaultUsers in the sysadmin login class are allowed to log in even when varrunnologinexists and they need not have a working home directory this option permits loginswhen NFS is not working Sysadmin users can start any number of processes andopen any number of files The last line pulls in the contents of the default entryAlthough FreeBSD has reasonable defaults you might be interested in updating theetcloginconf file to set idle timeout and password expiration warnings For example to set the idle timeout to minutes and enable warnings seven days beforepasswords expire you would add the following clauses to the definition of defaultwarnpassworddidletimem There is still a technical limit on the total number of processes and open files that the kernel can support but no artificial limit is imposedWhen you modify the etcloginconf file you must also run the following command to compile your changes into the hashed version of the file that the systemactually refers to in daily operation capmkdb etcloginconf The etcgroup fileThe etcgroup file contains the names of UNIX groups and a list of each groupsmembers Heres a portion of the group file from a FreeBSD systemwheelrootsysrootbinoperatorrootbinrootftpdanstaffdanbentrentnobodylpdEach line represents one group and contains four fields Group name Encrypted password or a placeholder GID number List of members separated by commas be careful not to add spacesAs in etcpasswd fields are separated by colons Group names should be limited toeight characters for compatibility although many systems do not actually require thisIts possible to set a group password that allows arbitrary users to enter the groupwith the newgrp command However this feature is rarely used The group password can be set with gpasswd which under Linux stores the encrypted passwordin the etcgshadow fileAs with usernames and UIDs group names and GIDs should be kept consistentamong machines that share files through a network filesystem Consistency can behard to maintain in a heterogeneous environment because different operating systems use different GIDs for standard system groupsIf a user defaults to a particular group in etcpasswd but does not appear to bein that group according to etcgroup etcpasswd wins the argument The groupmemberships granted at login time are the union of those found in the passwdand group filesSome older systems limit the number of groups a user can belong to There is noreal limit on current Linux and FreeBSD kernelsMuch as with UIDs we recommend minimizing the potential for GID collisionsby starting local groups at GID or higherThe UNIX tradition was originally to add new users to a group that representedtheir general category such as students or finance However this convention increases the likelihood that users will be able to read one anothers files because ofslipshod permission settings even if that is not really the intention of the files ownerTo avoid this problem system utilities such as useradd and adduser now default toputting each user in his or her own personal group that is a group named after theuser and which includes only that user This convention is much easier to maintainif personal groups GIDs match their corresponding users UIDsTo let users share files by way of the group mechanism create separate groups forthat purpose The idea behind personal groups is not to discourage the use of groupsper seits simply to establish a more restrictive default group for each user so thatfiles are not inadvertently shared You can also limit access to newly created filesand directories by setting your users default umask in a default startup file such asetcprofile or etcbashrc see page Group membership can also serve as a marker for other contexts or privileges Forexample rather than entering the username of each system administrator into thesudoers file you can configure sudo so that everyone in the admin group automatically has sudo privilegesLinux supplies the groupadd groupmod and groupdel commands to createmodify and delete groupsFreeBSD uses the pw command to perform all these functions To add the user danto the group staff and then verify that the change was properly implemented youwould run the following commands sudo pw groupmod staff m dan pw groupshow staffstaffdanevigarthtrentben Manual steps for adding usersBefore you create an account for a new user at a corporate government or educational site its important that the user sign and date a copy of your local useragreement and policy statement What You dont have a user agreement andpolicy statement See page for more information about why you need oneand what to put in itUsers have no particular reason to want to sign a policy agreement so its to youradvantage to secure their signatures while you still have some leverage We find thatit takes extra effort to secure a signed agreement after an account has been releasedIf your process allows for it have the paperwork precede the creation of the accountMechanically the process of adding a new user consists of several steps requiredby the system and a few more that establish a useful environment for the new userand incorporate the user into your local administrative systemSee page formore informationabout sudoRequired Edit the passwd and shadow files or the masterpasswd file on FreeBSDto define the users account Add the user to the etcgroup file not really necessary but nice Set an initial password Create chown and chmod the users home directory Configure roles and permissions if you use RBAC see page For the user Copy default startup files to the users home directoryFor you Have the new user sign your policy agreement Verify that the account is set up correctly Document the users contact information and account statusThis list cries out for a script or tool and fortunately each of our example systemsincludes at least a partial offtheshelf solution in the form of an adduser or useraddcommand We take a look at these tools starting on page Editing the passwd and group filesManual maintenance of the passwd and group files is error prone and inefficient sowe recommend the slightly higherlevel tools such as useradd adduser usermodpw and chsh as daily driversIf you do have to make manual changes use the vipw command to edit the passwdand shadow files or on FreeBSD the masterpasswd file Although it soundsvicentric it actually invokes your favorite editor as defined in the EDITOR environment variable More importantly it locks the files so that editing sessions oryour editing and a users password change cannot collideAfter you run vipw our Linux reference systems remind you to edit the shadowfile after you have edited the passwd file Use vipw s to do soUnder FreeBSD vipw edits the masterpasswd file instead of etcpasswd Afterinstalling your changes vipw runs pwdmkdb to generate the derived passwdfile and two hashed versions of masterpasswd one that contains the encryptedpasswords and is readable only by root and another that lacks the passwords andis worldreadableFor example running vipw and adding the following line would define an accountcalled whitneywhitneyWhitney Sather AMATH xhomestaffwhitneybinsh When you first run vipw or vigr Ubuntu and Debian ask you to select one of vimbasic vimtinynano and ed If you change your mind after the fact run selecteditorNote the star in the encrypted password field This prevents use of the account untila real password is set with the passwd command see the next sectionNext edit etcgroup by running vigr Add a line for the new personal group ifyour site uses them and add the users login name to each of the groups in whichthe user should have membershipAs with vipw using vigr ensures that the changes made to the etcgroup file aresane and atomic After an edit session vigr should prompt you to run vigr s toedit the group shadow gshadow file as well Unless you want to set a passwordfor the groupwhich is unusualyou can skip this stepOn FreeBSD use pw groupmod to make changes to the etcgroup fileSetting a passwordSet a password for a new user with sudo passwd newusernameYoull be prompted for the actual passwordSome automated systems for adding new users do not require you to set an initialpassword Instead they force the user to set a password on first login Althoughthis feature is convenient its a giant security hole anyone who can guess new loginnames or look them up in etcpasswd can swoop down and hijack accounts before the intended users have had a chance to log inAmong many other functions FreeBSDs pw command can also generate and setrandom user passwords sudo pw usermod raphael w randomPassword for raphael is ntcYusWere generally not fans of random passwords for ongoing use However they area good option for transitional passwords that are only intended to last until theaccount is actually usedCreating the home directory and installing startup filesuseradd and adduser create new users home directories for you but youll likelywant to doublecheck the permissions and startup files for new accountsTheres nothing magical about home directories If you neglected to include a homedirectory when setting up a new user you can create it with a simple mkdir Youneed to set ownerships and permissions on the new directory as well but this ismost efficiently done after youve installed any local startup filesStartup files traditionally begin with a dot and end with the letters rc short forrun command a relic of the CTSS operating system The initial dot causes ls tohide these uninteresting files from directory listings unless the a option is usedSee page fortips on selectinggood passwordsWe recommend that you include default startup files for each shell that is popularon your systems so that users continue to have a reasonable default environmenteven if they change shells Table lists a variety of common startup filesTable Common startup files and their usesTarget Filename Typical usesall shells loginconf Sets userspecific login defaults FreeBSDsh profile Sets search path terminal type and environmentbasha bashrc Sets the terminal type if neededSets biff and mesg switchesbashprofile Sets up environment variablesSets command aliasesSets the search pathSets the umask value to control permissionsSets CDPATH for filename searchesSets the PS prompt and HISTCONTROL variablescshtcsh login Read by login instances of cshcshrc Read by all instances of cshvivim vimrcviminfo Sets vivim editor optionsemacs emacs Sets emacs editor options and key bindingsgit gitconfig Sets user editor color and alias options for GitGNOME gconf GNOME user configuration via gconfgconfpath Path for additional user configuration via gconfKDE kde Directory of configuration filesa bash also reads profile or etcprofile in emulation of sh The bashprofile file is read by loginshells and the bashrc file is read by interactive nonlogin shellsSample startup files are traditionally kept in etcskel If you customize your systems startup file examples usrlocaletcskel is a reasonable place to put themodified copiesThe entries in Table for the GNOME and KDE window environments are really just the beginning In particular take a look at gconf which is the tool thatstores application preferences for GNOME programs in a manner analogous tothe Windows registryMake sure that the default shell files you give to new users set a reasonable defaultvalue for umask we suggest or depending on the friendliness andsize of your site If you do not assign new users to individual groups we recomSee page fordetails on umaskmend umask which gives the owner full access but the group and the rest ofthe world no accessDepending on the users shell etc may contain systemwide startup files thatare processed before the users own startup files For example bash and sh readetcprofile before processing profile and bashprofile These files are a goodplace in which to put sitewide defaults but bear in mind that users can overrideyour settings in their own startup files For details on other shells see the man pagefor the shell in questionBy convention Linux also keeps fragments of startup files in the etcprofiled directory Although the directory name derives from sh conventions etcprofiledcan actually include fragments for several different shells The specific shells beingtargeted are distinguished by filename suffixes sh csh etc Theres no magicprofiled support built into the shells themselves the fragments are simply executed by the default startup script in etc eg etcprofile in the case of sh or bashSeparating the default startup files into fragments facilitates modularity and allows software packages to include their own shelllevel defaults For example thecolorls fragments coach shells on how to properly color the output of ls so as tomake it unreadable on dark backgroundsSetting home directory permissions and ownershipsOnce youve created a users home directory and copied in a reasonable default environment turn the directory over to the user and make sure that the permissionson it are appropriate The command sudo chown R newusernewgroup newusersets ownerships properly Note that you cannot use sudo chown newusernewgroup newuserto chown the dot files because newuser would then own not only his or her own filesbut also the parent directory as well This is a common and dangerous mistakeConfiguring roles and administrative privilegesRolebased access control RBAC allows system privileges to be tailored for individual users and is available on many of our example systems RBAC is not a traditional part of the UNIX or Linux access control model but if your site uses itrole configuration must be a part of the process of adding users RBAC is coveredin detail starting on page in the Access Control and Rootly Powers chapterLegislation such as the SarbanesOxley Act the Health Insurance Portability andAccountability Act HIPAA and the GrammLeachBliley Act in the United Stateshas complicated many aspects of system administration in the corporate arenaincluding user management Roles might be your only viable option for fulfillingsome of the SOX HIPAA and GLBA requirementsSee Chapter formore informationabout SOX and GLBAFinishing upTo verify that a new account has been properly configured first log out then login as the new user and execute the following commands pwd To verify the home directory ls la To check ownergroup of startup filesYou need to notify new users of their login names and initial passwords Many sitessend this information by email but thats generally not a secure choice Better optionsare to do it in person over the phone or through a text message If you are adding new freshmen to the campuss CS machines punt the notification problem tothe instructor If you must distribute account passwords by email make sure thepasswords expire in a couple of days if they are not used and changedIf your site requires users to sign a written policy agreement or appropriate usepolicy be sure this step has been completed before you release a new accountThis check prevents oversights and strengthens the legal basis of any sanctions youmight later need to impose This is also the time to point users toward additionaldocumentation on local customsRemind new users to change their passwords immediately You can enforce thisby setting the password to expire within a short time Another option is to have ascript check up on new users and be sure their encrypted passwords have changedIn environments where you know users personally its relatively easy to keep trackof whos using a system and why But if you manage a large and dynamic user baseyou need a more formal way to keep track of accounts Maintaining a database ofcontact information and account statuses helps you figure out once the act of creatingthe account has faded from memory who people are and why they have an account Scripts for adding users useradd adduser and newusersOur example systems all come with a useradd or adduser script that implementsthe basic procedure outlined above However these scripts are configurable andyou will probably want to customize them to fit your environment Unfortunatelyeach system has its own idea of what you should customize where you should implement the customizations and what the default behavior should be Accordinglywe cover these details in vendorspecific sectionsTable is a handy summary of commands and configuration files related to managing users Because the same password can have many encrypted representations this method verifies only thatthe user has reset the password not that it has actually been changed to a different passwordSee page for moreinformation aboutwritten user contractsTable Commands and configuration files for user managementSystem Commands Configuration filesAll Linux useradd usermod userdel etclogindefsetcdefaultuseraddDebianUbuntua adduser deluser etcadduserconfetcdeluserconfFreeBSD adduser rmuser etcloginconfa This suite wraps the standard Linux version and includes a few more featuresuseradd on LinuxMost Linux distributions include a basic useradd suite that draws its configurationparameters from both etclogindefs and etcdefaultuseraddThe logindefs file addresses issues such as password aging choice of encryptionalgorithms location of mail spool files and the preferred ranges of UIDs and GIDsYou maintain the logindefs file by hand The comments do a good job of explaining the various parametersParameters stored in the etcdefaultuseradd file include the location of homedirectories and the default shell for new users You set these defaults through theuseradd command itself useradd D prints the current values and D in combination with various other flags sets the values of specific options For example sudo useradd D s binbashsets bash as the default shellTypical defaults are to put new users in individual groups to use SHA encryption for passwords and to populate new users home directories with startup filesfrom etcskelThe basic form of the useradd command accepts the name of the new account onthe command line sudo useradd hilbertThis command creates an entry similar to this one in etcpasswd along with acorresponding entry in the shadow filehilbertxhomehilbertbinshuseradd disables the new account by default You must assign a real password tomake the account usableA more realistic example is shown below We specify that hilberts primary groupshould be hilbert and that he should also be added to the faculty group Weoverride the default home directory location and shell and ask useradd to createthe home directory if it does not already exist sudo useradd c David Hilbert d homemathhilbert g hilbertG faculty m s bintcsh hilbertThis command creates the following passwd entryhilbertxDavid HilberthomemathhilbertbintcshThe assigned UID is one higher than the highest UID on the system and the corresponding shadow entry ishilbertThe password placeholder characters in the passwd and shadow file vary depending on the operating system useradd also adds hilbert to the appropriate groupsin etcgroup creates the directory homemathhilbert with proper ownershipsand populates it from the etcskel directoryadduser on Debian and UbuntuIn addition to the useradd family of commands the Debian lineage also suppliessomewhat higherlevel wrappers for these commands in the form of adduser anddeluser These addon commands are configured in etcadduserconf where youcan specify options such as these Rules for locating home directories by group by username etc Permission settings for new home directories UID and GID ranges for system users and general users An option to create individual groups for each user Disk quotas Boolean only unfortunately Regexbased matching of user names and group namesOther typical useradd parameters such as rules for passwords are set as parametersto the PAM module that does regular password authentication See page for adiscussion of PAM aka Pluggable Authentication Modules adduser and deluserhave twin cousins addgroup and delgroupadduser on FreeBSDFreeBSD comes with adduser and rmuser shell scripts that you can either use assupplied or modify to fit your needs The scripts are built on top of the facilitiesprovided by the pw commandadduser can be used interactively if you prefer By default it creates user and groupentries and a home directory You can point the script at a file containing a list ofaccounts to create with the f flag or enter in each user interactivelyFor example the process for creating a new user raphael looks like this sudo adduserUsername raphaelFull name Raphael DobbinsUid Leave empty for default returnLogin group raphael returnLogin group is raphael Invite raphael into other groups returnLogin class default returnShell sh csh tcsh bash rbash nologin sh bashHome directory homeraphael returnHome directory permissions Leave empty for default returnUse passwordbased authentication yes returnUse an empty password yesno no returnUse a random password yesno no yesLock out the account after creation no returnUsername raphaelPassword randomFull Name Raphael DobbinsUid Class Groups raphaelHome homeraphaelHome Mode Shell usrlocalbinbashLocked noOK yesno yesadduser INFO Successfully added raphael to the user databaseadduser INFO Password for raphael is RSCAdsfyvxOtAdd another user yesno noGoodbyenewusers on Linux adding in bulkLinuxs newusers command creates multiple accounts at one time from the contents of a text file Its pretty gimpy but it can be handy when you need to add a lotof users at once such as when creating classspecific accounts newusers expectsan input file of lines just like the etcpasswd file except that the password fieldcontains the initial password in clear text Oops better protect that filenewusers honors the password aging parameters set in the etclogindefs file butit does not copy in the default startup files as does useradd The only startup fileit copies in is xauthAt a university whats really needed is a batch adduser script that can use a list ofstudents from enrollment or registration data to generate the input for newuserswith usernames formed according to local rules and guaranteed to be locally uniquewith strong passwords randomly generated and with UIDs and GIDs increasingfor each user Youre probably better off writing your own wrapper for useradd inPython than trying to get newusers to do what you need Safe removal of a users account and filesWhen a user leaves your organization that users login account and files mustbe removed from the system If possible dont do that chore by hand instead letuserdel or rmuser handle it These tools ensure the removal of all references tothe login name that were added by you or your useradd program Once youveremoved the remnants use the following checklist to verify that all residual userdata has been removed Remove the user from any local user databases or phone lists Remove the user from the mail aliases database or add a forwarding address Remove the users crontab file and any pending at jobs or print jobs Kill any of the users processes that are still running Remove the user from the passwd shadow group and gshadow files Remove the users home directory Remove the users mail spool if mail is stored locally Clean up entries on shared calendars room reservation systems etc Delete or transfer ownership of any mailing lists run by the deleted userBefore you remove someones home directory be sure to relocate any files that areneeded by other users You usually cant be sure which files those might be so itsalways a good idea to make an extra backup of the users home directory beforedeleting itOnce you have removed all traces of a user you may want to verify that the usersold UID no longer owns files on the system To find the paths of orphaned filesyou can use the find command with the nouser argument Because find has a wayof escaping onto network servers if youre not careful its usually best to checkfilesystems individually with xdev sudo find filesystem xdev nouserIf your organization assigns individual workstations to users its generally simplestand most efficient to reimage the entire system from a master template beforeturning the system over to a new user Before you do the reinstallation howeverits a good idea to back up any local files on the systems hard disk in case they areneeded in the futureAlthough all our example systems come with commands that automate the process of removing user presence they probably do not do as thorough a job as youmight like unless you have religiously extended them as you expanded the numberof places in which userrelated information is stored Think license keysDebian and Ubuntus deluser is a Perl script that calls the usual userdel it undoesall the things adduser does It runs the script usrlocalsbindeluserlocal if itexists to facilitate easy localization The configuration file etcdeluserconf letsyou set options such as these Whether to remove the users home directory and mail spool Whether to back up the users files and where to put the backup Whether to remove all files on the system owned by the user Whether to delete a group if it now has no membersRed Hat supports a userdellocal script but no pre and postexecution scripts toautomate sequencesensitive operations such as backing up an abouttoberemoved users filesFreeBSDs rmuser script does a good job of removing instances of the users filesand processes a task that other vendors userdel programs do not even attempt User login lockoutOn occasion a users login must be temporarily disabled A straightforward wayto do this is to put a star or some other character in front of the users encryptedpassword in the etcshadow or etcmasterpasswd file This measure preventsmost types of passwordregulated access because the password no longer decryptsto anything sensibleFreeBSD lets you lock accounts with the pw command A simple sudo pw lock someuserputs the string LOCKED at the start of the password hash making the accountunusable Unlock the account by running sudo pw unlock someuserOn all our Linux distributions the usermod L user and usermod U user commands define an easy way to lock and unlock passwords They are just shortcuts forthe password twiddling described above the L puts an in front of the encryptedpassword in the etcshadow file and the U removes itUnfortunately modifying a users password simply makes logins fail It does notnotify the user of the account suspension or explain why the account no longerworks In addition commands such as ssh that do not necessarily check the systempassword may continue to functionAn alternative way to disable logins is to replace the users shell with a program thatprints an explanatory message and supplies instructions for rectifying the situationThe program then exits terminating the login sessionThis approach has both advantages and disadvantages Any forms of access thatcheck the password but do not pay attention to the shell will not be disabled ToRHELfacilitate the disabled shell trick many daemons that afford nonlogin access to thesystem eg ftpd check to see if a users login shell is listed in etcshells and denyaccess if it is not This is the behavior you want Unfortunately its not universalso you may have to do some fairly comprehensive testing if you decide to use shellmodification as a way of disabling accountsAnother issue is that your carefully written explanation of the suspended accountmight never be seen if the user tries to log in through a window system or througha terminal emulator that does not leave output visible after a logout Risk reduction with PAMPluggable Authentication Modules PAM is covered in the Single SignOn chapterstarting on page PAM centralizes the management of the systems authentication facilities through standard library routines That way programs like loginsudo passwd and su need not supply their own tricky authentication code Anorganization can easily expand its authentication methods beyond passwords tooptions such as Kerberos onetime passwords ID dongles or fingerprint readersPAM reduces the risk inherent in writing secured software allows administratorsto set sitewide security policies and defines an easy way to add new authentication methods to the systemAdding and removing users doesnt involve tweaking the PAM configuration but thetools involved operate under PAMs rules and constraints In addition many of thePAM configuration parameters are similar to those used by useradd or usermodIf you change a parameter as described in this chapter and useradd doesnt seemto be paying attention to it check to be sure the systems PAM configuration isntoverriding your new value Centralized account managementSome form of centralized account management is essential for mediumtolargeenterprises of all types be they corporate academic or governmental Users needthe convenience and security of a single login name UID and password across thesite Administrators need a centralized system that allows changes such as accountrevocations to be instantly propagated everywhereSuch centralization can be achieved in a variety of ways most of which includingMicrosofts Active Directory system involve LDAP the Lightweight Directory Access Protocol in some capacity Options range from barebones LDAP installationsbased on open source software to elaborate commercial identity management systems that come with a hefty price tagLDAP and Active DirectoryLDAP is a generalized databaselike repository that can store user managementdata as well as other types of data It uses a hierarchical clientserver model thatsupports multiple servers as well as multiple simultaneous clients One of LDAPsbig advantages as a sitewide repository for login data is that it can enforce uniqueUIDs and GIDs across systems It also plays well with Windows although the reverse is only marginally trueMicrosofts Active Directory uses LDAP and Kerberos and can manage many kindsof data including user information Its a bit egotistical and wants to be the boss ifit is interacting with UNIX or Linux LDAP repositories If you need a single authentication system for a site that includes Windows desktops as well as UNIX andLinux systems it is probably easiest to let Active Directory be in control and to useyour UNIX LDAP databases as secondary serversSee Chapter Single SignOn for more information on integrating UNIX orLinux with LDAP Kerberos and Active DirectoryApplicationlevel single signon systemsApplicationlevel single signon systems balance user convenience with securityThe idea is that a user can sign on once to a login prompt web page or Windowsbox and be authenticated at that time The user then obtains authentication credentials usually implicitly so that no active management is required which canbe used to access other applications The user only has to remember one login andpassword sequence instead of manyThis scheme allows credentials to be more complex since the user does not need toremember or even deal with them That theoretically increases security Howeverthe impact of a compromised account is greater because one login gives an attacker access to multiple applications These systems make your walking away from adesktop machine while still logged in a significant vulnerability In addition theauthentication server becomes a critical bottleneck If its down all useful workgrinds to a halt across the enterpriseAlthough applicationlevel SSO is a simple idea it implies a lot of backend complexity because the various applications and machines that a user might want toaccess must understand the authentication process and SSO credentialsSeveral open source SSO systems exist JOSSO an open source SSO server written in Java CAS the Central Authentication Service from Yale also Java Shibboleth an open source SSO distributed under the Apache licenseA host of commercial systems are also available most of them integrated with identity management suites which are covered in the next sectionSee the section startingon page for moreinformation aboutLDAP and LDAPimplementationsIdentity management systemsIdentity management sometimes referred to as IAM for identity and access management is a common buzzword in user management In plain language it meansidentifying the users of your systems authenticating their identities and grantingprivileges according to those authenticated identities The standardization effortsin this realm are led by the World Wide Web Consortium and by The Open GroupCommercial identity management systems combine several key UNIX conceptsinto a warm and fuzzy GUI replete with marketing jargon Fundamental to all suchsystems is a database of user authentication and authorization data often stored inLDAP format Control is achieved with concepts such as UNIX groups and limited administrative privileges are enforced through tools such as sudo Most suchsystems have been designed with an eye toward regulations that mandate accountability tracking and audit trailsThere are many commercial systems in this space Oracles Identity ManagementCourion Avatier Identity Management Suite AIMS VMware Identity Managerand SailPoints IdentityIQ to name a few In evaluating identity management systems look for capabilities in the following areasOversight Implement a secure web interface for management thats accessible bothinside and outside the enterprise Support an interface through which hiring managers can request that accounts be provisioned according to role Coordinate with a personnel database to automatically remove access foremployees who are terminated or laid offAccount management Generate globally unique user IDs Create change and delete user accounts across the enterprise on all typesof hardware and operating systems Support a workflow engine for example tiered approvals before a user isgiven certain privileges Make it easy to display all users who have a certain set of privileges Dittofor the privileges granted to a particular user Support rolebased access control including user account provisioning byrole Allow exceptions to rolebased provisioning including a workflowfor the approval of exceptions Configure logging of all changes and administrative actions Similarlyconfigure reports generated from logging data by user by day etcEase of use Let users change and reset their own passwords with enforcement ofrules for picking strong passwords Enable users to change their passwords globally in one operationConsider also how the system is implemented at the point where authorizationsand authentications actually take place Does the system require a custom agent tobe installed everywhere or does it conform itself to the underlying systemsCloud computing is the practice of leasing computer resources from a pool ofshared capacity Users of cloud services provision resources on demand and pay ametered rate for whatever they consume Businesses that embrace the cloud enjoyfaster time to market greater flexibility and lower capital and operating expensesthan businesses that run traditional data centersThe cloud is the realization of utility computing first conceived by the late computer scientist John McCarthy who described the idea in a talk at MIT in Many technological advances since McCarthys prescient remarks have helped tobring the idea to fruition To name just a few Virtualization software reliably allocates CPU memory storage and network resources on demand Robust layers of security isolate users and virtual machines from eachother even as they share underlying hardware Standardized hardware components enable the construction of data centers with vast power storage and cooling capacities A reliable global network connects everything Cloud ComputingCloud providers capitalize on these innovations and many others They offer myriad services ranging from hosted private servers to fully managed applicationsThe leading cloud vendors are competitive highly profitable and growing rapidlyThis chapter introduces the motivations for moving to the cloud fills in some background on a few major cloud providers introduces some of the most importantcloud services and offers tips for controlling costs As an even briefer introductionthe section Clouds VPS quick start by platform starting on page shows howto create cloud servers from the command lineSeveral other chapters in this book include sections that relate to the managementof cloud servers Table lists some pointersTable Cloud topics covered elsewhere in this bookPage Heading Recovery of cloud systems bootstrappingrelated issues for the cloud Cloud networking TCPIP networking for cloud platforms Web hosting in the cloud Packer using Packer to build OS images for the cloud Container clustering and management especially the section on AWS ECS CICD in practice a CICD pipeline example that uses cloud services Commercial application monitoring tools monitoring tools for the cloudIn addition Chapter Configuration Management is broadly applicable to themanagement of cloud systems The cloud in contextThe transition from servers in private data centers to the nowubiquitous cloud hasbeen rapid and dramatic Lets take a look at the reasons for this stampedeCloud providers create technically advanced infrastructure that most businessescannot hope to match They locate their data centers in areas with inexpensiveelectric power and copious networking crossconnects They design custom serverchassis that maximize energy efficiency and minimize maintenance They use purposebuilt network infrastructure with custom hardware and software finetunedto their internal networks They automate aggressively to allow rapid expansionand reduce the likelihood of human errorBecause of all this engineering effort not to mention the normal economies of scalethe cost of running distributed computing services is much lower for a cloud provider than for a typical business with a small data center Cost savings are reflectedboth in the price of cloud services and in the providers profitsLayered on top of this hardware foundation are management features that simplifyand facilitate the configuration of infrastructure Cloud providers offer both APIsand userfacing tools that control the provisioning and releasing of resources Asa result the entire life cycle of a systemor group of systems distributed on a virtual networkcan be automated This concept goes by the name infrastructureas code and it contrasts starkly with the manual server procurement and provisioning processes of times pastElasticity is another major driver of cloud adoption Because cloud systems can beprogrammatically requested and released any business that has cyclic demand canoptimize operating costs by adding more resources during periods of peak usageand removing extra capacity when it is no longer needed The builtin autoscalingfeatures available on some cloud platforms streamline this processCloud providers have a global presence With some planning and engineering effort businesses can reach new markets by releasing services in multiple geographicareas In addition disaster recovery is easier to implement in the cloud becauseredundant systems can be run in separate physical locationsAll these characteristics pair well with the DevOps approach to system administration which emphasizes agility and repeatability In the cloud youre no longerrestricted by slow procurement or provisioning processes and nearly everythingcan be automatedStill a certain mental leap is required when you dont control your own hardwareOne industry metaphor captures the sentiment neatly servers should be treatedas cattle not as pets A pet is named loved and cared for When the pet is sick itstaken to a veterinarian and nursed back to health Conversely cattle are commodities that are herded traded and managed in large quantities Sick cattle are shotA cloud server is just one member of a herd and to treat it otherwise is to ignorea basic fact of cloud computing cloud systems are ephemeral and they can fail atany time Plan for that failure and youll be more successful at running a resilientinfrastructureDespite all its advantages the cloud is not a panacea for quickly reducing costsor improving performance Directly migrating an existing enterprise applicationfrom a data center to a cloud provider a socalled lift and shift is unlikely to besuccessful without careful planning Operational processes for the cloud are different and they entail training and testing Furthermore most enterprise softwareis designed for static environments but individual systems in the cloud should betreated as shortlived and unreliable A system is said to be cloud native if it is reliable even in the face of unanticipated eventsSee page formore informationabout DevOps Cloud platform choicesMultiple factors influence a sites choice of cloud provider Cost past experiencecompatibility with existing technology security or compliance requirements andinternal politics are all likely to play a role The selection process can also be swayedby reputation provider size features and of course marketingFortunately there are a lot of cloud providers out there Weve chosen to focus onjust three of the major public cloud providers Amazon Web Services AWS Google Cloud Platform GCP and DigitalOcean DO In this section we mention afew additional options for you to consider Table enumerates the major playersin this spaceTable The most widely used cloud platformsProvider Notable qualitiesAmazon Web Services lb gorilla Rapid innovation Can be expensive ComplexDigitalOcean Simple and reliable Lovable API Good for developmentGoogle Cloud Platform Technically sophisticated and improving quickly Emphasizesperformance Comprehensive bigdata servicesIBM Softlayer More like hosting than cloud Has a global private networkMicrosoft Azure A distant second in size Has a history of outages Possiblyworth consideration for Microsoft shopsOpenStack Modular DIY open source platform for building privateclouds AWScompatible APIsRackspace Public and private clouds running OpenStack Offersmanaged services for AWS and Azure Fanatical supportVMware vCloud Air Buzzwordladen service for public private and hybridclouds Uses VMware technology Probably doomedPublic private and hybrid cloudsIn a public cloud the vendor controls all the physical hardware and affords accessto systems over the Internet This setup relieves users of the burden of installingand maintaining hardware but at the expense of less control over the features andcharacteristics of the platform AWS GCP and DO are all public cloud providersPrivate cloud platforms are similar but are hosted within an organizations owndata center or managed by a vendor on behalf of a single customer Servers in a private cloud are singletenant not shared with other customers as in a public cloudPrivate clouds offer flexibility and programmatic control just as public clouds doThey appeal to organizations that already have significant capital invested in hardware and engineers especially those that value full control of their environmentOpenStack is the leading open source system used to create private clouds It receives financial and engineering support from enterprises such as ATT IBM andIntel Rackspace itself is one of the largest contributors to OpenStackA combination of public and private clouds is called a hybrid cloud Hybrids canbe useful when an enterprise is first migrating from local servers to a public cloudfor adding temporary capacity to handle peak loads and for a variety of other organizationspecific scenarios Administrators beware operating two distinct cloudpresences in tandem increases complexity more than proportionallyVMwares vSphere Air cloud based on vSphere virtualization technology is a seamless hybrid cloud for customers that already use VMware virtualization in theironpremises data center Those users can move applications to and from vCloudAir infrastructure quite transparentlyThe term public cloud is a bit unfortunate connoting as it does the security andhygiene standards of a public toilet In fact customers of public clouds are isolatedfrom each other by multiple layers of hardware and software virtualization A private cloud offers little or no practical security benefit over a public cloudIn addition operating a private cloud is an intricate and expensive prospect thatshould not be undertaken lightly Only the largest and most committed organizations have the engineering resources and wallet needed to implement a robustsecure private cloud And once implemented a private clouds features usually fallshort of those offered by commercial public cloudsFor most organizations we recommend the public cloud over the private or hybridoptions Public clouds offer the highest value and easiest administration For theremainder of this book our cloud coverage is limited to public options The nextfew sections present a quick overview of each of our example platformsAmazon Web ServicesAWS offers scores of services ranging from virtual servers EC to managed databases and data warehouses RDS and Redshift to serverless functions that execute in response to events Lambda AWS releases hundreds of updates and newfeatures each year It has the largest and most active community of users AWS isby far the largest cloud computing businessFrom the standpoint of most users AWS has essentially unlimited capacity However new accounts come with limits that control how much compute power youcan requisition These restrictions protect both Amazon and you since costs canquickly spiral out of control if services arent properly managed To increase yourlimits you complete a form on the AWS support site The service limit documentation itemizes the constraints associated with each serviceThe online AWS documentation located at awsamazoncomdocumentation isauthoritative comprehensive and well organized It should be the first place youlook when researching a particular service The white papers that discuss securitymigration paths and architecture are invaluable for those interested in constructing robust cloud environmentsGoogle Cloud PlatformIf AWS is the reigning champion of the cloud Google is the wouldbe usurper Itcompetes for customers through nefarious tricks such as lowering prices and directly addressing customers AWS pain pointsThe demand for engineers is so fierce that Google has been known to poach employees from AWS In they past theyve hosted parties in conjunction with the AWSreInvent conference in Las Vegas in an attempt to lure both talent and users Asthe cloud wars unfold customers ultimately benefit from this competition in theform of lower costs and improved featuresGoogle runs the most advanced global network in the world a strength that benefitsits cloud platform Google data centers are technological marvels that feature manyinnovations to improve energy efficiency and reduce operational costs Google isrelatively transparent about its operations and their open source contributionshelp advance the cloud industryDespite its technical savvy in some ways Google is a follower in the public cloudnot a leader When it launched in or GCP was already late to the gameIts services have many of the same features and often the same names as theirAWS equivalents If youre familiar with AWS youll find the GCP web interfaceto be somewhat different on the surface However the functionality underneathis strikingly similarWe anticipate that GCP will gain market share in the years to come as it improvesits products and builds customer trust It has hired some of the brightest minds inthe industry and theyre bound to develop some innovative technologies As consumers we all stand to benefitDigitalOceanDigitalOcean is a different breed of public cloud Whereas AWS and GCP competeto serve the large enterprises and growthfocused startups DigitalOcean courts See googlecomaboutdatacenters for photos and facts about how Googles data centers operate Google had released other cloud products as early as including App Engine the first platformasaservice product But Googles strategy and the GCP brand were not apparent until small customers with simpler needs Minimalism is the name of the game We likeDigitalOcean for experiments and proofofconcept projectsDigitalOcean offers data centers in North America Europe and Asia There areseveral centers in each of these regions but they are not directly connected and socannot be considered availability zones see page As a result its considerablymore difficult to build global highly available production services on DigitalOceanthan on AWS or GoogleDigitalOcean servers are called droplets They are simple to provision from the command line or web console and they boot quickly DigitalOcean supplies images forall our example operating systems except Red Hat It also has a handful of images forpopular open source applications such as Cassandra Drupal Django and GitLabDigitalOcean also has load balancer and block storage services In Chapter Continuous Integration and Delivery we include an example of provisioning a DigitalOcean load balancer with two droplets using HashiCorps Terraform infrastructure provisioning tool Cloud service fundamentalsCloud services are loosely grouped into three categories InfrastructureasaService IaaS in which users request raw computememory network and storage resources These are typically delivered inthe form of virtual private servers aka VPSs Under IaaS users are responsible for managing everything above the hardware operating systemsnetworking storage systems and their own software PlatformasaService PaaS in which developers submit their customapplications packaged in a format specified by the vendor The vendorthen runs the code on the users behalf In this model users are responsible for their own code while the vendor manages the OS and network SoftwareasaService SaaS the broadest category in which the vendorhosts and manages software and users pay some form of subscription feefor access Users maintain neither the operating system nor the application Almost any hosted web application think WordPress falls into theSaaS categoryTable shows how each of these abstract models breaks down in terms of thelayers involved in a complete deploymentOf these options IaaS is the most pertinent to system administration In additionto defining virtual computers IaaS providers virtualize the hardware elements thatare typically connected to them such as disks now described more generally asblock storage devices and networks Virtual servers can inhabit virtual networksTable Which layers are you responsible for managingLayer Local a IaaS PaaS SaaSApplication Databases Application runtime Operating system Virtual network storage and servers Virtualization platform Physical servers Storage systems Physical network Power space and cooling a Local local servers and networkIaas InfrastructureasaService virtual serversPaaS PlatformasaService eg Google App EngineSaas SoftwareasaService eg most webbased servicesfor which you specify the topology routes addressing and other characteristicsIn most cases these networks are private to your organizationIaaS can also include other core services such as such as databases queues keyvaluestores and compute clusters These features combine to create a complete replacement for and in many cases an improvement over the traditional data centerPaaS is an area of great promise that is not yet fully realized Current offerings suchas AWS Elastic Beanstalk Google App Engine and Heroku come with environmental constraints or nuances that render them impractical or incomplete for use inbusy production environments Time and again weve seen business outgrow theseservices However new services in this area are receiving a lot of attention We anticipate dramatic improvements in the coming yearsCloud providers differ widely in terms of their exact features and implementationdetails but conceptually many services are quite similar The following sectionsdescribe cloud services generally but because AWS is the frontrunner in this spacewe sometimes adopt its nomenclature and conventions as defaultsAccess to the cloudMost cloud providers primary interface is some kind of webbased GUI New system administrators should use this web console to create an account and to configure their first few resourcesCloud providers also define APIs that access the same underlying functionality asthat of the web console In most cases they also have a standard commandlinewrapper portable to most systems for those APIsEven veteran administrators make frequent use of web GUIs However its also important to get friendly with the commandline tools because they lend themselvesmore readily to automation and repeatability Use scripts to avoid the tedious andsluggish process of requesting everything through a browserCloud vendors also maintain software development kits SDKs for many popular languages to help developers use their APIs Third party tools use the SDKs tosimplify or automate specific sets of tasks Youll no doubt encounter these SDKsif you write your own toolsYou normally use SSH with public key authentication to access UNIX and Linuxsystems running in the cloud See SSH the Secure SHell starting on page formore information about the effective use of SSHSome cloud providers let you access a console session through a web browser whichcan be especially helpful if you mistakenly lock yourself out with a firewall rule orbroken SSH configuration Its not a representation of the systems actual consolethough so you cant use this feature to debug bootstrapping or BIOS issuesRegions and availability zonesCloud providers maintain data centers around the world A few standard termsdescribe geographyrelated featuresA region is a location in which a cloud provider maintains data centers In mostcases regions are named after the territory of intended service even though thedata centers themselves are more concentrated For example Amazons useastregion is served by data centers in north VirginiaSome providers also have availability zones or simply zones which are collections of data centers within a region Zones within a region are peered throughhighbandwidth lowlatency redundant circuits so interzone communicationis fast though not necessarily cheap Anecdotally weve experienced interzonelatency of less than msZones are typically designed to be independent of one another in terms of powerand cooling and theyre geographically dispersed so that a natural disaster that affects one zone has a low probability of affecting others in the same regionRegions and zones are fundamental to building highly available network servicesDepending on availability requirements you can deploy in multiple zones andregions to minimize the impact of a failure within a data center or geographicarea Availability zone outages can occur but are rare regional outages are rarerstill Most services from cloud vendors are aware of zones and use them to achievebuiltin redundancy It takes about ms for a fiber optic signal to travel km so regions the size of the US east coastare fine from a performance standpoint The network connectivity available to a data center is moreimportant than its exact locationExhibit A Servers distributed among multiple regions and zonesInterzone communication isprivate but incurs a cost per GBIntrazone tracis freeRegions connect through theInternet or through privatecircuits fees apply either wayUS west regionUS east regionMultiregion deployments are more complex because of the physical distances between regions and the associated higher latency Some cloud vendors have fasterand more reliable interregion networks than others If your site serves users aroundthe world the quality of your cloud vendors network is paramountChoose regions according to geographic proximity to your user base For scenarios in which the developers and users are in different geographic regions considerrunning your development systems close to the developers and production systemscloser to the usersFor sites that deliver services to a global user base running in multiple regions cansubstantially improve performance for end users Requests can be routed to eachclients regional servers by exploitation of geographic DNS resolution which determines clients locations by their source IP addressesMost cloud platforms have regions for North America South America Europe andthe Asia Pacific countries Only AWS and Azure have a direct presence in ChinaSome platforms notably AWS and vCloud have regions compatible with strict USfederal ITAR requirementsVirtual private serversThe flagship service of the cloud is the virtual private server a virtual machinethat runs on the providers hardware Virtual private servers are sometimes calledinstances You can create as many instances as you need running your preferredoperating system and applications then shut the instances down when theyre nolonger needed You pay only for what you use and theres typically no upfront costBecause instances are virtual machines their CPU power memory disk size andnetwork settings can be customized when the instance is created and even adjustedafter the fact Public cloud platforms define preset configurations called instancetypes They range from singleCPU nodes with MiB of memory to large systems with many CPU cores and multiple TiB of memory Some instance types arebalanced for general use and others are specialized for CPU memory disk ornetworkintensive applications Instance configurations are one area in which cloudvendors compete vigorously to match market needsInstances are created from images the saved state of an operating system thatcontains at minimum a root filesystem and a boot loader An image might alsoinclude disk volumes for additional filesystems and other custom settings You caneasily create custom images with your own software and settingsAll our example operating systems are widely used so cloud platforms typicallysupply official images for them Many third party software vendors also maintaincloud images that have their software preinstalled to facilitate adoption by customers Its also easy to create your own custom images Learn more about how to createvirtual machine images in Packer starting on page NetworkingCloud providers let you create virtual networks with custom topologies that isolateyour systems from each other and from the Internet On platforms that offer thisfeature you can set the address ranges of your networks define subnets configureroutes set firewall rules and construct VPNs to connect to external networks Expect some networkrelated operational overhead and maintenance when buildinglarger more complex cloud deploymentsYou can make your servers accessible to the Internet by leasing publicly routable addresses from your providerall providers have a large pool of such addresses fromwhich users can draw Alternatively servers can be given only a private RFCaddress within the address space you selected for your network rendering thempublicly inaccessibleSystems without public addresses are not directly accessible from the Internet evenfor administrative attention You can access such hosts through a jump server orbastion host that is open to the Internet or through a VPN that connects to yourcloud network For security the smaller the externalfacing footprint of your virtual empire the betterAlthough this all sounds promising you have even less control over virtual networks than you do over traditional networks and youre subject to the whims andvagaries of the feature set made available by your chosen provider Its particularlymaddening when new features launch but cant interact with your private networkWere looking at you Amazon Currently you must build your own FreeBSD image if you use Google Compute EngineSee page formore informationabout VPNsSee page formore informationabout RFCprivate addressesSkip to page for the details on TCPIP networking in the cloudStorageData storage is a major part of cloud computing Cloud providers have the largestand most advanced storage systems on the planet so youll be hard pressed to matchtheir capacity and capabilities in a private data center The cloud vendors bill bythe amount of data you store They are highly motivated to give you as many waysas possible to ingest your dataHere are a few of the most important ways to store data in the cloud Object stores contain collections of discrete objects files essentiallyin a flat namespace Object stores can accommodate a virtually unlimited amount of data with exceptionally high reliability but relatively slowperformance They are designed for a readmostly access pattern Files inan object store are accessed over the network through HTTPS Examplesinclude AWS S and Google Cloud Storage Block storage devices are virtualized hard disks They can be requisitionedat your choice of capacities and attached to a virtual server much likeSAN volumes on a traditional network You can move volumes amongnodes and customize their IO profiles Examples include AWS EBS andGoogle persistent disks Ephemeral storage is local disk space on a VPS that is created from diskdrives on the host server These are normally fast and capacious but thedata is lost when you delete the VPS Therefore ephemeral storage is bestused for temporary files Examples include instance store volumes on AWSand local SSDs on GCPIn addition to these raw storage services cloud providers usually offer a varietyof freestanding database services that you can access over the network Relational databases such as MySQL PostgreSQL and Oracle run as services on the AWSRelational Database Service They offer builtin multizone redundancy and encryption for data at restDistributed analytics databases such as AWS Redshift and GCP BigQuery offer incredible ROI both are worth a second look before you build your own expensivedata warehouse Cloud vendors also offer the usual assortment of inmemory andNoSQL databases such as Redis and memcachedIdentity and authorizationAdministrators developers and other technical staff all need to manage cloud services Ideally access controls should conform to the principle of least privilege each Case in point AWS offers onsite visits from the AWS Snowmobile a foot long shipping containertowed by a semi truck than can transfer PiB from your data center to the cloudprincipal can access only the entities that are relevant to it and nothing more Depending on the context such access control specifications can become quite elaborateAWS is exceptionally strong in this area Their service called Identity and AccessManagement IAM defines not only users and groups but also roles for systems Aserver can be assigned policies for example to allow its software to start and stopother servers store and retrieve data in an object store or interact with queuesall with automatic key rotation IAM also has an API for key management to helpyou store secrets safelyOther cloud platforms have fewer authorization features Unsurprisingly Azuresservice is based on Microsofts Active Directory It pairs well with sites that havean existing directory to integrate with Googles access control service also calledIAM is relatively coarsegrained and incomplete in comparison with AmazonsAutomationThe APIs and CLI tools created by cloud vendors are the basic building blocks ofcustom automation but theyre often clumsy and impractical for orchestratinglarger collections of resources For example what if you need to create a new network launch several VPS instances provision a database configure a firewall andfinally connect all these components Written in terms of a raw cloud API thatwould make for a complex scriptAWS CloudFormation was the first service to address this problem It accepts atemplate in JSON or YAML format that describes the desired resources and theirassociated configuration details You submit the template to CloudFormation whichchecks it for errors sorts out dependencies among resources and creates or updatesthe cloud configuration according to your specificationsCloudFormation templates are powerful but error prone in human hands becauseof their strict syntax requirements A complete template is unbearably verbose anda challenge for humans to even read Instead of writing these templates by handwe prefer to automatically render them with a Python library called Tropospherefrom Mark Peek see githubcomcloudtoolstroposphereThird party services also target this problem Terraform from the open sourcecompany HashiCorp is a cloudagnostic tool for constructing and changing infrastructure As with CloudFormation you describe resources in a custom templateand then let Terraform make the proper API calls to implement your configuration You can then check your configuration file into version control and managethe infrastructure over timeServerless functionsOne of the most innovative features in the cloud since its emergence are the cloudfunction services sometimes called functionsasaservice also referred to as serverless features Cloud functions are a model of code execution that do not requireany longlived infrastructure Functions execute in response to an event such asthe arrival of a new HTTP request or an object being uploaded to a storage locationFor example consider a traditional web server HTTP requests are forwarded by thenetworking stack of the operating system to a web server which routes them appropriately When the response completes the web server continues to wait for requestsContrast this with the serverless model An HTTP request arrives and it triggersthe cloud function to handle the response When complete the cloud function terminates The owner pays for the period of time that the function executes Thereis no server to maintain and no operating system to manageAWS introduced Lambda their cloud function service at a conference in Google followed shortly with a Cloud Functions service Several cloud functionimplementations exist for projects like OpenStack Mesos and KubernetesServerless functions hold great promise for the industry A massive ecosystem oftools is emerging to support simpler and more powerful use of the cloud Wevefound many uses for these shortlived serverless functions in our daytoday administrative duties We anticipate rapid advances in this area in the coming years Clouds VPS quick start by platformThe cloud is an excellent sandbox in which to learn UNIX and Linux This shortsection helps you get up and running with virtual servers on AWS GCP or DigitalOcean As system administrators we rely extensively on the command line asopposed to web GUIs for interacting with the cloud so we illustrate the use ofthose tools hereAmazon Web ServicesTo use AWS first set up an account at awsamazoncom Once you create the account immediately follow the guidance in the AWS Trusted Advisor to configureyour account according to the suggested best practices You can then navigate tothe individual service consoles for EC VPC etcEach AWS service has a dedicated user interface When you log in to the web consoleyoull see the list of services at the top Within Amazon each service is managedby an independent team and the UI unfortunately reflects this fact Although thisdecoupling has helped AWS services grow it does lead to a somewhat fragmenteduser experience Some interfaces are more refined and intuitive than othersTo protect your account enable multifactor authentication MFA for the root userthen create a privileged IAM user for daytoday use We also generally configure analias so that users can access the web console without entering an account numberThis option is found on the landing page for IAMIn the next section we introduce the official aws CLI tool written in Python Newusers might also benefit from Amazons Lightsail quick start service which aimsto start an EC instance with minimum fussaws control AWS subsystemsaws is a unified commandline interface to AWS services It manages instancesprovisions storage edits DNS records and performs most of the other tasks shownin the web console The tool relies on the exceptional Boto library a Python SDKfor the AWS API and it runs on any system with a working Python interpreterInstall it with pip pip install awscliTo use aws first authenticate it to the AWS API by using a pair of random stringscalled the access key ID and the secret access key You generate these credentialsin the IAM web console and then copyandpaste them locallyRunning aws configure prompts you to set your API credentials and default region aws configureAWS Access Key ID AKIAIOSFODNNEXAMPLEAWS Secret Access Key wJalrXUtnFEMIKMDENGbPxRfiCYEXAMPLEKEYDefault region name useast returnDefault output format None returnThese settings are saved to awsconfig As long as youre setting up your environment we also recommend that you configure the bash shells autocompletionfeature so that subcommands are easier to discover See the AWS CLI docs formore informationThe first argument to aws names the specific service you want to manipulate forexample ec for actions that control the Elastic Compute Cloud You can add thekeyword help at the end of any command to see instructions For example aws helpaws ec help and aws ec describeinstances help all produce useful man pagesCreating an EC instanceUse aws ec runinstances to create and launch EC instances Although you cancreate multiple instances with a single command by using the count optionthe instances must all share the same configuration Heres a minimal example ofa complete command aws ec runinstances imageid amidaeinstancetype tnano associatepublicipaddresskeyname adminkey output shown on page See page for moreinformation about pipThis example specifies the following configuration details The base system image is an Amazonsupplied version of CentOS namedamidae AWS calls their images AMIs for Amazon Machine Images Like other AWS objects the image names are unfortunately not mnemonic you must look up IDs in the EC web console or on the commandline aws ec describeimages to decode them The instance type is tnano which is currently the smallest instance typeIt has one CPU core and MiB of RAM Details about the available instance types can be found in the EC web console A preconfigured key pair is also assigned to control SSH access You cangenerate a key pair with the sshkeygen command see page thenupload the public key to the AWS EC consoleThe output of that aws ec runinstances command is shown below Its JSON soits easily consumed by other software For example after launching an instance ascript could extract the instances IP address and configure DNS update an inventory system or coordinate the launch of multiple servers aws ec runinstances Same command as aboveOwnerId ReservationId raInstances PrivateIpAddress InstanceId icf ImageId amidae PrivateDnsName ipuswestcomputeinternal KeyName adminkey SecurityGroups GroupName default GroupId sgebfb SubnetId subnetefa InstanceType tnanoBy default EC instances in VPC subnets do not have public IP addresses attachedrendering them accessible only from other systems within the same VPC To reachinstances directly from the Internet use the associatepublicipaddress option asshown in our example command You can discover the assigned IP address after thefact with aws ec describeinstances or by finding the instance in the web consoleFirewalls in EC are known as security groups Because we didnt specify a securitygroup here AWS assumes the default group which allows no access To connectto the instance adjust the security group to permit SSH from your IP address Inrealworld scenarios security group structure should be carefully planned duringnetwork design We discuss security groups in Security groups and NACLs startingon page aws configure sets a default region so you need not specify a region for the instanceunless you want something other than the default The AMI key pair and subnetare all regionspecific and aws complains if they dont exist in the region you specifyIn this particular case the AMI key pair and subnet are from the useast regionTake note of the InstanceId field in the output which is a unique identifier for thenew instance You can use aws ec describeinstances instanceid id to showdetails about an existing instance or just use aws ec describeinstances to dumpall instances in the default regionOnce the instance is running and the default security group has been adjusted to passtraffic on TCP port you can use SSH to log in Most AMIs are configured witha nonroot account that has sudo privileges For Ubuntu the username is ubuntufor CentOS centos FreeBSD and Amazon Linux both use ecuser The documentation for your chosen AMI should specify the username if its not one of theseProperly configured images allow only public keys for SSH authentication notpasswords Once youve logged in with the SSH private key youll have full sudoaccess with no password required We recommend disabling the default user afterthe first boot and creating personal named accountsViewing the console logDebugging lowlevel problems such as startup issues and disk errors can be challenging without access to the instances console EC lets you retrieve the consoleoutput of an instance which can be useful if the instance is in an error state orappears to be hung You can do this through the web interface or with aws ecgetconsoleoutput as shown aws ec getconsoleoutput instanceid icfInstanceId icfTimestamp TZOutput Initializing cgroup subsys cpusetr Initializing cgroup subsys cpur Initializing cgroup subsys cpuacctr Linux version amznx mockbuildgobibuild gcc version Red Hat SMP Mon Sep UTC rSee Chapter for more information about usermanagementThe full log is of course much longer than this snippet In the JSON dump the contents of the log are unhelpfully concatenated as a single line For better readabilityclean it up with sed aws ec getconsoleoutput instanceid icf sedsrgInstanceId icfTimestamp TZOutput Initializing cgroup subsys cpuset Initializing cgroup subsys cpu Initializing cgroup subsys cpuacct Linux version amznx mockbuildgobibuild gcc version Red Hat SMP Mon Sep UTC This log output comes directly from the Linux boot process The example aboveshows a few lines from the moment the instance was first initialized In most casesyoull find the most interesting information near the end of the logStopping and terminating instancesWhen youre finished with an instance you can stop it to shut the instance downbut retain it for later use or terminate it to delete the instance entirely By defaulttermination also releases the instances root disk into the ether Once terminatedan instance can never be resurrected even by AWS aws ec stopinstances instanceid icfStoppingInstances InstanceId icf CurrentState Code Name stopping PreviousState Code Name running Note that virtual machines dont change state instantly it takes a minute for the hamsters to reset Hence the presence of transitional states such as starting and stopping Be sure to account for them in any instancewrangling scripts you might writeGoogle Cloud PlatformTo get started with GCP establish an account at cloudgooglecom If you alreadyhave a Google identity you can sign up using the same accountGCP services operate within a compartment known as a project Each project hasseparate users billing details and API credentials so you can achieve completeseparation between disparate applications or areas of business Once you createyour account create a project and enable individual GCP services according toyour needs Google Compute Engine the VPS service is one of the first servicesyou might want to enableSetting up gcloudgcloud a Python application is the CLI tool for GCP Its a component of the Google Cloud SDK which contains a variety of libraries and tools for interfacing withGCP To install it follow the installation instructions at cloudgooglecomsdkYour first action should be to set up your environment by running gcloud init Thiscommand starts a small local web server and then opens a browser link to displaythe Google UI for authentication After you authenticate yourself through the webbrowser gcloud asks you back in the shell to select a project profile a default zoneand other defaults The settings are saved under configgcloudRun gcloud help for general information or gcloud h for a quick usage summaryPersubcommand help is also available for example gcloud help compute showsa man page for the Compute Engine serviceRunning an instance on GCEUnlike aws commands which return immediately gcloud compute operates synchronously When you run the create command to provision a new instance forexample gcloud makes the necessary API call then waits until the instance isactually up and running before it returns This convention avoids the need to pollfor the state of an instance after you create itTo create an instance first obtain the name or alias of the image you want to boot gcloud compute images list regexp debianNAME PROJECT ALIAS DEPRECATED STATUSdebianwheezyv debiancloud debian READYdebianjessiev debiancloud debian READYThen create and boot the instance specifying its name and the image you want gcloud compute instances create ulsah image debian waits for instance to launchNAME ZONE MACHINETYPE INTERNALIP EXTERNALIP STATUSulsah uscentralf nstandard RUNNING See aws ec wait for information on polling for events or states within AWS ECThe output normally has a column that shows whether the instance is preemptiblebut in this case it was blank and we removed it to make the output fit on the pagePreemptible instances are less expensive than standard instances but they can runfor only hours and can be terminated at any time if Google needs the resourcesfor another purpose Theyre meant for longlived operations that can tolerate interruptions such as batch processing jobsPreemptible instances are similar in concept to ECs spot instances in that youpay a discounted rate for otherwisespare capacity However weve found Googlespreemptible instances to be more sensible and simpler to manage than AWSs spotinstances Longlived standard instances remain the most appropriate choice formost tasks howevergcloud initializes the instance with a public and private IP address You can usethe public IP with SSH but gcloud has a helpful wrapper to simplify SSH logins gcloud compute ssh ulsahLast login Mon Jan ulsahChachingDigitalOceanWith advertised boot times of seconds DigitalOceans virtual servers droplets are the fastest route to a root shell The entry level cost is per month sothey wont break the bank eitherOnce you create an account you can manage your droplets through DigitalOceansweb site However we find it more convenient to use tugboat a commandline toolwritten in Ruby that uses DigitalOceans published API Assuming that you haveRuby and its library manager gem installed on your local system just run geminstall tugboat to install tugboatA couple of onetime setup steps are required First generate a pair of cryptographickeys that you can use to control access to your droplets sshkeygen t rsa b f sshidrsadoGenerating publicprivate rsa key pairEnter passphrase empty for no passphrase returnEnter same passphrase again returnYour identification has been saved in UsersbensshidrsadoYour public key has been saved in UsersbensshidrsadopubCopy the contents of the public key file and paste them into DigitalOceans webconsole currently under Settings Security As part of that process assign a shortname to the public keyNext connect tugboat to DigitalOceans API by entering an access token that youobtain from the web site tugboat saves the token for future use in tugboatSee page formore details on setting up Ruby gemsSee page formore about SSH tugboat authorizeNote You can get your Access Token from httpsclouddigitaloceancomsettingstokensnewEnter your access token edffaaffddfaffbbdcbEnter your SSH key path defaults to sshidrsa sshidrsadoEnter your SSH user optional defaults to rootEnter your SSH port number optional defaults to Enter your default region optional defaults to nyc sfoAuthentication with DigitalOcean was successfulTo create and start a droplet first identify the name of the system image you wantto use as a baseline For example tugboat images grep i ubuntu x slug id distro Ubuntu x slug id distro Ubuntu x slug ubuntux id distro Ubuntu x slug ubuntux id distro UbuntuYou also need DigitalOceans numeric ID for the SSH key you pasted into the webconsole tugboat keysSSH KeysName idrsado id fingerprintbcfddbaceffeeaedaThis output shows that the numeric ID for the key named idrsado is Create and start a droplet like this tugboat create i ubuntux k ulsahubuntuqueueing creation of droplet ulsahubuntuDroplet createdHere the argument to k is the SSH key ID and the last argument is a short namefor the droplet that you can assign as you wishOnce the droplet has had time to boot you can log in with tugboat ssh tugboat ssh ulsahubuntuDroplet fuzzy name provided Finding droplet IDdone ubuntuxExecuting SSH on Droplet ubuntuxThis droplet has a private IP checking if you asked to use the Private IPYou didnt Using public IP for sshAttempting SSH rootWelcome to Ubuntu GNULinux generic xrootulsahubuntuYou can create as many droplets as you need but keep in mind that youll be billedfor each one even if its powered down To inactivate a droplet power it down usetugboat snapshot dropletname snapshotname to memorialize the state of the system and run tugboat destroy dropletname to decommission the droplet You canlater recreate the droplet by using the snapshot as a source image Cost controlCloud newcomers often navely anticipate that largescale systems will be dramatically cheaper to run in the cloud than in a data center This expectation might stemfrom the inverse sticker shock engendered by cloud platforms low low price perinstancehour Or perhaps the idea is implanted by the siren songs of cloud marketers whose case studies always show massive savingsRegardless of their source its our duty to stamp out hope and optimism whereverthey are found In our experience new cloud customers are often surprised whencosts climb quicklyCloud tariffs generally consist of several components The compute resources of virtual private servers load balancers and everything else that consumes CPU cycles to run your services Pricing isper hour of use Internet data transfer both ingress and egress as well as traffic amongzones and regions Pricing is per GiB or TiB transferred Storage of all types block storage volumes object storage disk snapshotsand in some cases IO to and from the various persistence stores Pricingis per GiB or TiB stored per monthFor compute resources the payasyougo model also known as ondemand pricing is the most expensive On AWS and DigitalOcean the minimum billing increment is one hour and on GCP its a minute Prices range from fractions of a centper hour DigitalOceans smallest droplet type with MiB and one CPU core orAWS tnano instances to several dollars per hour an ixlarge instance on AWSwith cores GiB RAM and GB local SSDsYou can realize substantial savings on virtual servers by paying up front for longerterms On AWS this is called reserved instance pricing Unfortunately its unbearably cumbersome and timeconsuming to determine precisely what to purchaseReserved EC instances are tied to a specific instance family If you decide later thatyou need something different your investment is lost On the upside if you reservean instance you are guaranteed that it will be available for your use With ondemand instances your desired type might not even be available when you go to provision it depending on current capacity and demand AWS continues to tweak itspricing structure so with luck the current system might be simplified in the futureFor number crunching workloads that can tolerate interruptions AWS offers spotpricing The spot market is an auction If your bid exceeds the current spot priceyoull be granted use of the instance type you requested until the price exceedsyour maximum bid at which point your instance is terminated The prices can bedeeply discounted compared to the EC ondemand and reserved prices but theuse cases are limitedGoogle Compute Engine pricing is refreshingly simple by comparison Discountsare automatically applied for sustained use and you never pay up front You pay thefull base price for the first week of the month and the incremental price drops eachweek by of the base rate to a maximum discount of The net discount ona full month of use is Thats roughly comparable to the discount on a oneyearreserved EC instance but you can change instances at any timeNetwork traffic can be even more difficult to predict reliably The culprits commonlyfound to be responsible for high datatransfer costs include Web sites that ingest and serve large media files videos images PDFsand other large documents directly from the cloud rather than offloading them to a CDN Interzone or interregion traffic for database clusters that replicate for faulttolerance for example software such as Cassandra MongoDB and Riak MapReduce or data warehouse clusters that span multiple zones Disk images and volume snapshots transferred between zones or regionsfor backup or by some other automated processIn situations where replication among multiple zones is important for availabilityyoull save on transfer expenses by limiting clusters to two zones rather than usingthree or more Some software offers tweaks such as compression that can reducethe amount of replicated dataOne substantial source of expense on AWS is provisioned IOPS for EBS volumesPricing for EBS is per GiBmonth and IOPSmonth The price of a GiB EBSvolume with IOPS is a few hundred dollars per month A cluster of these justmight break the bankThe best defense against high bills is to measure monitor and avoid overprovisioning Use autoscaling features to remove capacity when it isnt needed lowering costsat times of low demand Use more smaller instances for more finegrained controlWatch usage patterns carefully before spending a bundle on reserved instances orhighbandwidth volumes The cloud is flexible and you can make changes to yourinfrastructure as neededAs environments grow identifying where money is being spent can be a challengeLarger cloud accounts might benefit from third party services that analyze use and For the persnickety and the thrifty because the discount scheme is linked to your billing cycle thetiming of transitions makes a difference You can switch instance types at the start or end of a cyclewith no penalty The worst case is to switch halfway through a billing cycle which incurs a penalty ofabout of an instances monthly base rateSee page formore informationabout CDNsoffer tracking and reporting features The two that weve used are Cloudability andCloudHealth Both tap in to the billing features of AWS to break down reports byuserdefined tag service or geographic location Recommended ReadingWittig Andreas and Michael Wittig Amazon Web Services In Action Manning Publications Google cloudplatformgoogleblogcom The official blog for the Google CloudPlatformBarr Jeff and others at Amazon Web Services awsamazoncomblogsawsThe official blog of Amazon Web ServicesDigitalOcean digitaloceancomcompanyblog Technical and product blogfrom DigitalOceanVogels Werner All Things Distributed allthingsdistributedcom The blog ofWerner Vogels CTO at AmazonWardley Simon Bits or pieces bloggardevianceorg The blog of researcher andcloud trendsetter Simon Wardley Analysis of cloud industry trends along withoccasional rantsBias Randy cloudscalingcomblog Randy Bias is a director at OpenStack andhas insightful info on the private cloud industry and its futureCantrill Bryan The Observation Deck dtraceorgblogsbmc Interesting viewsand technical thoughts on general computing from the CTO of Joyent a niche butinteresting cloud platformAmazon youtubecomAmazonWebServices Conference talks and other videocontent from AWSSystem daemons the kernel and custom applications all emit operational data thatis logged and eventually ends up on your finitesized disks This data has a limiteduseful life and may need to be summarized filtered searched analyzed compressedand archived before it is eventually discarded Access and audit logs may need tobe managed closely according to regulatory retention rules or site security policiesA log message is usually a line of text with a few properties attached including a timestamp the type and severity of the event and a process name and ID PID Themessage itself can range from an innocuous note about a new process starting up toa critical error condition or stack trace Its the responsibility of system administrators to glean useful actionable information from this ongoing torrent of messagesThis task is known generically as log management and it can be divided into a fewmajor subtasks Collecting logs from a variety of sources Providing a structured interface for querying analyzing filtering andmonitoring messages Managing the retention and expiration of messages so that information iskept as long as it is potentially useful or legally required but not indefinitely LoggingUNIX has historically managed logs through an integrated but somewhat rudimentary system known as syslog that presents applications with a standardizedinterface for submitting log messages Syslog sorts messages and saves them to filesor forwards them to another host over the network Unfortunately syslog tacklesonly the first of the logging chores listed above message collection and its stockconfiguration differs widely among operating systemsPerhaps because of syslogs shortcomings many applications network daemonsstartup scripts and other logging vigilantes bypass syslog entirely and write to theirown ad hoc log files This lawlessness has resulted in a complement of logs thatvaries significantly among flavors of UNIX and even among Linux distributionsLinuxs systemd journal represents a second attempt to bring sanity to the loggingmadness The journal collects messages stores them in an indexed and compressedbinary format and furnishes a commandline interface for viewing and filteringlogs The journal can stand alone or it can coexist with the syslog daemon withvarying degrees of integration depending on the configurationA variety of third party tools both proprietary and open source address the morecomplex problem of curating messages that originate from a large network of systems These tools feature such aids as graphical interfaces query languages datavisualization alerting and automated anomaly detection They can scale to handle message volumes on the order of terabytes per day You can subscribe to theseproducts as a cloud service or host them yourself on a private networkExhibit A on the next page depicts the architecture of a site that uses all the logmanagement services mentioned above Administrators and other interested parties can run a GUI against the centralized log cluster to review log messages fromsystems across the network Administrators can also log in to individual nodesand access messages through the systemd journal or the plain text files written bysyslog If this diagram raises more questions than answers for you youre readingthe right chapterWhen debugging problems and errors experienced administrators turn to the logssooner rather than later Log files often contain important hints that point towardthe source of vexing configuration errors software bugs and security issues Logsare the first place you should look when a daemon crashes or refuses to start orwhen a chronic error plagues a system that is trying to bootThe importance of having a welldefined sitewide logging strategy has grownalong with the adoption of formal IT standards such as PCI DSS COBIT and ISO as well as with the maturing of regulations for individual industries Todaythese external standards may require you to maintain a centralized hardened enterprisewide repository for log activity with time stamps validated by NTP andwith a strictly defined retention schedule However even sites without regulatoryor compliance requirements can benefit from centralized logging Of course accurate system time is essential even without the presence of regulations We strongly recommend enabling NTP on all your systemsExhibit A Logging architecture for a site with centralized loggingLinux systemApache httpdSSHNTPcronothersLog sources systemdjournal syslogBinary journal Plain text lesFreeBSD systemApache httpdSSHNTPcronothersLog sources syslogPlain text les Centralized log clusterThis chapter covers the native log management software used on Linux and FreeBSDincluding syslog the systemd journal and logrotate We also introduce some additional tools for centralizing and analyzing logs across the network The chapter closeswith some general advice for setting up a sensible sitewide log management policy Log locationsUNIX is often criticized for being inconsistent and indeed it is Just take a look ata directory of log files and youre sure to find some with names like maillog somelike cronlog and some that use various distribution and daemonspecific namingconventions By default most of these files are found in varlog but some renegadeapplications write their log files elsewhere on the filesystemTable compiles information about some of the more common log files on ourexample systems The table lists the following The log files to archive summarize or truncate The program that creates each An indication of how each filename is specified The frequency of cleanup that we consider reasonable The systems among our examples that use the log file A description of the files contentsFilenames in Table are relative to varlog unless otherwise noted Syslogmaintains many of the listed files but others are written directly by applicationsTable Log files on paradeFile ProgramWhereaFreqaSystems aContentsapache httpd F D D Apache HTTP server logs vapt APT F M D Aptitude package installationsauthlog sudo etcb S M DF Authorizationsbootlog rc scripts F M R Output from system startup scriptscloudinitlog cloudinit F Output from cloud init scriptscron cronlog cron S W RF cron executions and errorsdaemonlog various S W D All daemon facility messagesdebug various S D FD Debugging outputdmesg kernel H all Dump of kernel message bufferdpkglog dpkg F M D Package management logfaillogc login H W D Failed login attemptshttpd httpd F D R Apache HTTP server logskernlog kernel S W D All kern facility messageslastlog login H R Last login time per user binarymail mailrelated S W RF All mail facility messagesmessages various S W R The main system log filesamba smbd etc F W Samba WindowsSMB file sharingsecure sshd etcb S M R Private authorization messagessyslog various S W D The main system log filewtmp login H M RD Login records binaryxen Xen F m RD Xen virtual machine informationXorgnlog Xorg F W R X Windows server errorsyumlog yum F M R Package management loga Where F Configuration file H Hardwired S SyslogFrequency D Daily M Monthly NNm Sizebased in MB eg m W WeeklySystems D Debian and Ubuntu D Debian only R Red Hat and CentOS F FreeBSDb passwd sshd login and shutdown also write to the authorization logc Binary file that must be read with the faillog utilityLog files are generally owned by root although conventions for the ownership andmode of log files vary In some cases a less privileged process such as httpd mayrequire write access to the log in which case the ownership and mode should be setappropriately You might need to use sudo to view log files that have tight permissionsLog files can grow quickly especially the ones for busy services such as web database and DNS servers An outofcontrol log file can fill up the disk and bring thesystem to its knees For this reason its often helpful to define varlog as a separatedisk partition or filesystem Note that this advice is just as relevant to cloudbasedinstances and private virtual machines as it is to physical serversFiles not to manageMost logs are text files to which lines are written as interesting events occur But afew of the logs listed in Table have a rather different contextwtmp sometimes wtmpx contains a record of users logins and logouts as wellas entries that record when the system was rebooted or shut down Its a fairly generic log file in that new entries are simply added to the end of the file Howeverthe wtmp file is maintained in a binary format Use the last command to decodethe informationlastlog contains information similar to that in wtmp but it records only the timeof last login for each user It is a sparse binary file thats indexed by UID It will staysmaller if your UIDs are assigned in some kind of numeric sequence although thisis certainly nothing to lose sleep over in the real world lastlog doesnt need to berotated because its size stays constant unless new users log inFinally some applications notably databases create binary transaction logs Dontattempt to manage these files Dont attempt to view them either or youll be treated to a broken terminal windowHow to view logs in the systemd journalFor Linux distributions running systemd the quickest and easiest way to view logsis to use the journalctl command which prints messages from the systemd journalYou can view all messages in the journal or pass the u flag to view the logs for aspecific service unit You can also filter on other constraints such as time windowprocess ID or even the path to a specific executableFor example the following output shows journal logs from the SSH daemon journalctl u ssh Logs begin at Sat UTC end at Sat UTC Aug uxenial sshd Server listening on port Aug uxenial sshd Server listening on port Aug uxenial systemd Starting Secure Shell serverAug uxenial systemd Started OpenBSD Secure Shell serverAug uxenial sshd Accepted publickey for bwhaley from port ssh RSA SHAaaRfGdluntnUCpxLgkSwcszkAYewukrdBATcSee page foran introduction todisk partitioningSee the sections starting on page formore informationabout systemd andsystemd unitsAug uxenial sshd pamunixsshdsession sessionopened for user bwhaley by uidAug uxenial sshd Did not receive identification stringfrom Use journalctl f to print new messages as they arrive This is the systemd equivalentof the muchbeloved tail f for following plain text files as they are being appended toThe next section covers the systemdjournald daemon and its configuration The systemd journalIn accordance with its mission to replace all other Linux subsystems systemd includes a logging daemon called systemdjournald It duplicates most of syslogsfunctions but can also run peacefully in tandem with syslog depending on how youor the system have configured it If youre leery of switching to systemd becausesyslog has always just worked for you spend some time to get to know systemdAfter a little practice you may be pleasantly surprisedUnlike syslog which typically saves log messages to plain text files the systemdjournal stores messages in a binary format All message attributes are indexed automatically which makes the log easier and faster to search As discussed above youcan use the journalctl command to review messages stored in the journalThe journal collects and indexes messages from several sources The devlog socket to harvest messages from software that submits messages according to syslog conventions The device file devkmsg to collect messages from the Linux kernelThe systemd journal daemon replaces the traditional klogd process thatpreviously listened on this channel and formerly forwarded the kernelmessages to syslog The UNIX socket runsystemdjournalstdout to service software thatwrites log messages to standard output The UNIX socket runsystemdjournalsocket to service software thatsubmits messages through the systemd journal API Audit messages from the kernels auditd daemonIntrepid administrators can use the systemdjournalremote utility and its relatives systemdjournalgateway and systemdjournalupload to stream serializedjournal messages over the network to a remote journal Unfortunately this featuredoes not come preinstalled on vanilla distributions As of this writing packages areavailable for Debian and Ubuntu but not for Red Hat or CentOS We expect thislapse to be rectified soon in the meantime we recommend sticking with syslog ifyou need to forward log messages among systemsConfiguring the systemd journalThe default journal configuration file is etcsystemdjournaldconf however thisfile is not intended to be edited directly Instead add your customized configurationsto the etcsystemdjournaldconfd directory Any files placed there with a confextension are automatically incorporated into the configuration To set your ownoptions create a new conf file in this directory and include the options you wantThe default journaldconf includes a commentedout version of every possible option along with each options default value so you can see at a glance which optionsare available They include the maximum size of journal the retention period formessages and various ratelimiting settingsThe Storage option controls whether to save the journal to disk The possible values are somewhat confusing volatile stores the journal in memory only persistent saves the journal in varlogjournal creating the directoryif it doesnt already exist auto saves the journal in varlogjournal but does not create the directory This is the default value none discards all log dataMost Linux distributions including all our examples default to the value auto anddo not come with a varlogjournal directory Hence the journal is not saved between reboots by default which is unfortunateYou can modify this behavior either by creating the varlogjournal directory orby updating the journal to use persistent storage and restarting systemdjournald mkdir etcsystemdjournaldconfd cat END etcsystemdjournaldconfdstorageconfJournalStoragepersistentEND systemctl restart systemdjournaldThis series of commands creates the custom configuration directory journaldconfdcreates a configuration file to set the Storage option to persistent and restarts thejournal so that the new settings take effect systemdjournald will now create thedirectory and retain the journal We recommend this change for all systems its areal handicap to lose all log data every time the system rebootsOne of the niftiest journal options is Seal which enables Forward Secure SealingFSS to increase the integrity of log messages With FSS enabled messages submitted to the journal cannot be altered without access to a cryptographic key pair Yougenerate the key pair itself by running journalctl setupkeys Refer to the manpages for journaldconf and journalctl for the full scoop on this optionAdding more filtering options for journalctlWe showed a quick example of a basic journalctl log search on page In thissection we show some additional ways to use journalctl to filter messages andgather information about the journalTo allow normal users to read from the journal without needing sudo permissionsadd them to the systemdjournal UNIX groupThe diskusage option shows the size of the journal on disk journalctl diskusageJournals take up M on diskThe listboots option shows a sequential list of system boots with numerical identifiers The most recent boot is always The dates at the end of the line show thetime stamps of the first and last messages generated during that boot journalctl listboots ce Sun UTCMon Mon UTCMon You can use the b option to restrict the log display to a particular boot session Forexample to view logs generated by SSH during the current session journalctl b u sshTo show all the messages from yesterday at midnight until now journalctl sinceyesterday untilnowTo show the most recent journal entries from a specific binary journalctl n usrsbinsshdYou can use journalctl help as a quick reference for these argumentsCoexisting with syslogBoth syslog and the systemd journal are active by default on each of our exampleLinux systems Both packages collect and store log messages Why would you wantboth of them running and how does that even workUnfortunately the journal is missing many of the features that are available in syslogAs the discussion starting on page demonstrates rsyslog can receive messagesfrom a variety of input plugins and forward them to a diverse set of outputs accordingto filters and rules none of which is possible when the systemd journal is used Thesystemd universe does include a remote streaming tool systemdjournalremotebut its relatively new and untested in comparison with syslog Administrators mayalso find it convenient to keep certain log files in plain text as syslog does insteadof in the journals binary formatWe anticipate that over time new features in the journal will usurp syslogs responsibilities But for now Linux distributions still need to run both systems to achievefull functionalityThe mechanics of the interaction between the systemd journal and syslog aresomewhat convoluted To begin with systemdjournald takes over responsibilityfor collecting log messages from devlog the logging socket that was historicallycontrolled by syslog For syslog to get in on the logging action it must now accessthe message stream through systemd Syslog can retrieve log messages from thejournal in two ways The systemd journal can forward messages to another socket typicallyrunsystemdjournalsyslog from which the syslog daemon can readthem In this mode of operation systemdjournald simulates the originalmessage submitters and conforms to the standard syslog API Thereforeonly the basic message parameters are forwarded some systemdspecificmetadata is lost Alternatively syslog can consume messages directly from the journal APIin the same manner as the journalctl command This method requiresexplicit support for cooperation on the part of syslogd but its a morecomplete form of integration that preserves the metadata for each messageDebian and Ubuntu default to the former method but Red Hat and CentOS use thelatter To determine which type of integration has been configured on your systeminspect the ForwardToSyslog option in etcsystemdjournaldconf If its value isyes socketforwarding is in use SyslogSyslog originally written by Eric Allman is a comprehensive logging system andIETFstandard logging protocol It has two important functions to liberate programmers from the tedious mechanics of writing log files and to give administrators control of logging Before syslog every program was free to make up its ownlogging policy System administrators had no consistent control over what information was kept or where it was storedSyslog is flexible It lets administrators sort messages by source facility and importance severity level and route them to a variety of destinations log files users terminals or even other machines It can accept messages from a wide varietyof sources examine the attributes of the messages and even modify their contentsIts ability to centralize the logging for a network is one of its most valuable features More specifically the journal links devlog to runsystemdjournaldevlog See man systemdjournalfields for a rundown of the available metadata RFC is the latest version of the syslog specification but the previous version RFC may better reflect the realworld installed baseOn Linux systems the original syslog daemon syslogd has been replaced with anewer implementation called rsyslog rsyslogd Rsyslog is an open source projectthat extends the capabilities of the original syslog but maintains backward API compatibility It is the most reasonable choice for administrators working on modernUNIX and Linux systems and is the only version of syslog we cover in this chapterRsyslog is available for FreeBSD and we recommend that you adopt it in preferenceto the standard FreeBSD syslog unless you have simple needs For instructions onconverting a FreeBSD system to use rsyslog see wikirsyslogcomindexphpFreeBSDIf you decide to stick with FreeBSDs traditional syslog jump to page for configuration informationReading syslog messagesYou can read plaintext messages from syslog with normal UNIX and Linux textprocessing tools such as grep less cat and awk The snippet below shows typicalevents in varlogsyslog from a Debian hostjessie cat varlogsyslogJul jessie networking bound to renewal in secondsJul jessie rpcbind Starting rpcbind daemonJul jessie nfscommon Starting NFS common utilitiesstatd idmapdJul jessie cron CRON INFO pidfile fd Jul jessie cron CRON INFO Running reboot jobsJul jessie acpid starting up with netlink and the input layerJul jessie docker timeTZ levelinfo msgDaemon has completedinitializationJul jessie docker timeTZ levelinfo msgDocker daemoncommitcb execdrivernative graphdriveraufsversionJul jessie docker timeTZ levelinfo msgAPI listen on varrundockersockThe example contains entries from several different daemons and subsystems networking NFS cron Docker and the power management daemon acpid Eachmessage contains the following spaceseparated fields Time stamp Systems hostname in this case jessie Name of the process and its PID in square brackets Message payloadSome daemons encode the payload to add metadata about the message In the outputabove the docker process includes its own time stamp a log level and informationabout the configuration of the daemon itself This additional information is entirelyup to the sending process to generate and formatRsyslog architectureThink about log messages as a stream of events and rsyslog as an eventstream processing engine Log message events are submitted as inputs processed by filtersand forwarded to output destinations In rsyslog each of these stages is configurableand modular By default rsyslog is configured in etcrsyslogconfThe rsyslogd process typically starts at boot and runs continuously Programs thatare syslog aware write log entries to the special file devlog a UNIX domain socket In a stock configuration for systems without systemd rsyslogd reads messagesfrom this socket directly consults its configuration file for guidance on how to routethem and dispatches each message to an appropriate destination Its also possibleand common to configure rsyslogd to listen for messages on a network socketIf you modify etcrsyslogconf or any of its included files you must restart thersyslogd daemon to make your changes take effect A TERM signal makes the daemon exit A HUP signal causes rsyslogd to close all open log files which is usefulfor rotating renaming and restarting logsBy longstanding convention rsyslogd writes its process ID to varrunsyslogdpidso its easy to send signals to rsyslogd from a script For example the followingcommand sends a hangup signal sudo kill HUP bincat varrunsyslogdpidTrying to compress or rotate a log file that rsyslogd has open for writing is not healthyand has unpredictable results so be sure to send a HUP signal before you do thisRefer to page for information on sane log rotation with the logrotate utilityRsyslog versionsRed Hat and CentOS use rsyslog version but Debian and Ubuntu have updatedto version FreeBSD users installing from ports can choose either version orversion As you might expect the rsyslog project recommends using the mostrecent version and we defer to their advice That said it wont make or break yourlogging experience if your operating system of choice is a version behind the latestand greatestRsyslog is a major rewrite of the core engine and although a lot has changedunder the hood for module developers the userfacing aspects remain mostly unchanged With a few exceptions the configurations in the following sections arevalid for both versions On modern Linux systems varrun is a symbolic link to runSee page formore informationabout signalsRsyslog configurationrsyslogds behavior is controlled by the settings in etcrsyslogconf All our exampleLinux distributions include a simple configuration with sensible defaults that suitmost sites Blank lines and lines beginning with a are ignored Lines in an rsyslogconfiguration are processed in order from beginning to end and order is significantAt the top of the configuration file are global properties that configure the daemon itself These lines specify which input modules to load the default format ofmessages ownerships and permissions of files the working directory in which tomaintain rsyslogs state and other settings The following example configurationis adapted from the default rsyslogconf on Debian Jessie Support local system loggingModLoad imuxsock Support kernel loggingModLoad imklog Write messages in the traditional time stamp formatActionFileDefaultTemplate RSYSLOGTraditionalFileFormat New log files are owned by rootadmFileOwner rootFileGroup adm Default permissions for new files and directoriesFileCreateMode DirCreateMode Umask Location in which to store rsyslog working filesWorkDirectory varspoolrsyslogMost distributions use the IncludeConfig legacy directive to include additionalfiles from a configuration directory typically etcrsyslogdconf Because orderis important distributions organize files by preceding file names with numbers Forexample the default Ubuntu configuration includes the following filesufwconfcloudinitconfdefaultconfrsyslogd interpolates these files into etcrsyslogconf in lexicographic order toform its final configurationFilters sometimes called selectors constitute the bulk of an rsyslog configurationThey define how rsyslog sorts and processes messages Filters are formed from expressions that select specific message criteria and actions that route selected messages to a desired destinationRsyslog understands three configuration syntaxes Lines that use the format of the original syslog configuration file Thisformat is now known as sysklogd format after the kernel logging daemon sysklogd Its simple and effective but has some limitations Use itto construct simple filters Legacy rsyslog directives which always begin with a sign The syntaxcomes from ancient versions of rsyslog and really ought to be obsoleteHowever not all options have been converted to the newer syntax andso this syntax remains authoritative for certain features RainerScript named for Rainer Gerhards the lead author of rsyslog Thisis a scripting syntax that supports expressions and functions You can useit to configure mostbut not allaspects of rsyslogMany realworld configurations include a mix of all three formats sometimes toconfusing effect Although it has been around since RainerScript remainsslightly less common than the others Fortunately none of the dialects are particularly complex In addition many sites will have no need to do major surgery onthe vanilla configurations included with their stock distributionsTo migrate from a traditional syslog configuration simply start with your existingsyslogconf file and add options for the rsyslog features you want to activateModulesRsyslog modules extend the capabilities of the core processing engine All inputssources and outputs destinations are configured through modules and modules can even parse and mutate messages Although most modules were written byRainer Gerhards some were contributed by third parties If youre a C programmeryou can write your ownModule names follow a predictable prefix pattern Those beginning with im areinput modules om are output modules mm are message modifiers and so onMost modules have additional configuration options that customize their behaviorThe rsyslog module documentation is the complete referenceThe following list briefly describes some of the more common or interesting inputand output modules along with a few nuggets of exotica imjournal integrates with the systemd journal as described in Coexistingwith syslog starting on page imuxsock reads messages from a UNIX domain socket This is the defaultwhen systemd is not present imklog understands how to read kernel messages on Linux and BSD imfile converts a plain text file to syslog message format Its useful forimporting log files generated by software that doesnt have native syslogsupport Two modes exist polling mode which checks the file for updatesat a configurable interval and notification mode inotify which uses theLinux filesystem event interface This module is smart enough to resumewhere it left off whenever rsyslogd is restarted imtcp and imudp accept network messages over TCP and UDP respectively They allow you to centralize logging on a network In combinationwith rsyslogs network stream drivers the TCP module can also acceptmutually authenticated syslog messages through TLS For Linux sites withextremely high volume see also the imptcp module If the immark module is present rsyslog produces time stamp messagesat regular intervals These time stamps can help you figure out that yourmachine crashed between and am not just sometime lastnight This information is also a big help when you are debugging problems that seem to occur regularly Use the MarkMessagePeriod option toconfigure the mark interval omfile writes messages to a file This is the most commonly used outputmodule and the only one configured in a default installation omfwd forwards messages to a remote syslog server over TCP or UDPThis is the module youre looking for if your site needs centralized logging omkafka is a producer implementation for the Apache Kafka data streamingengine Users at highvolume sites may benefit from being able to processmessages that have many potential consumers Similarly to omkafka omelasticsearch writes directly to an Elasticsearchcluster See page for more information about the ELK log management stack which includes Elasticsearch as one of its components ommysql sends messages to a MySQL database The rsyslog source distribution includes an example schema Combine this module with theMainMsgQueueSize legacy directive for better reliabilityModules can be loaded and configured through either the legacy or RainerScriptconfiguration formats We show some examples in the formatspecific sections belowsysklogd syntaxThe sysklogd syntax is the traditional syslog configuration format If you encountera standard syslogd such as the version installed on stock FreeBSD this is likely allyoull need to understand But note that the configuration file for the traditionalsyslogd is etcsyslogconf not etcrsyslogconfSee page formore information about TLSThis format is primarily intended for routing messages of a particular type to a desired destination file or network address The basic format isselector actionThe selector is separated from the action by one or more spaces or tabs For example the lineauth varlogauthlogcauses messages related to authentication to be saved in varlogauthlogSelectors identify the source program facility that is sending a log message andthe messages priority level severity with the syntaxfacilityseverityBoth facility names and severity levels must be chosen from a short list of definedvalues programs cant make up their own Facilities are defined for the kernel forcommon groups of utilities and for locally written programs Everything else isclassified under the generic facility userSelectors can contain the special keywords and none meaning all or nothing respectively A selector can include multiple facilities separated by commas Multipleselectors can be combined with semicolonsIn general selectors are ORed together a message matching any selector is subjectto the lines action However a selector with a level of none excludes the listed facilities regardless of what other selectors on the same line might sayHere are some examples of ways to format and combine selectors Apply action to everything from facilitylevelfacilitylevel action Everything from facilitylevel and facilitylevelfacility facilitylevel action Only facilitylevel and facilitylevelfacilitylevel facilitylevel action All facilities with severity levellevel action All facilities except badfacilitylevelbadfacilitynone actionTable lists the valid facility names They are defined in syslogh in the standard libraryTable Syslog facility namesFacility Programs that use it All facilities except markauth Security and authorizationrelated commandsauthpriv Sensitiveprivate authorization messagescron The cron daemondaemon System daemonsftp The FTP daemon ftpd obsoletekern The kernellocal Eight flavors of local messagelpr The line printer spooling systemmail sendmail postfix and other mailrelated softwaremark Time stamps generated at regular intervalsnews The Usenet news system obsoletesyslog syslogd internal messagesuser User processes the default if not specifiedDont take the distinction between auth and authpriv too seriously All authorizationrelated messages are sensitive and none should be worldreadable sudo logsuse authprivTable lists the valid severity levels in order of descending importanceTable Syslog severity levels descending severityLevel Approximate meaningemerg Panic situations system is unusablealert Urgent situations immediate action requiredcrit Critical conditionserr Other error conditionswarning Warning messagesnotice Things that might merit investigationinfo Informational messagesdebug For debugging onlyThe severity level of a message specifies its importance The distinctions betweenthe various levels are sometimes fuzzy Theres a clear difference between notice andwarning and between warning and err but the exact shade of meaning expressedby alert as opposed to crit is a matter of conjectureLevels indicate the minimum importance that a message must have to be logged Forexample a message from SSH at level warning would match the selector authwarningas well as the selectors authinfo authnotice authdebug warning noticeinfo and debug If the configuration directs authinfo messages to a particularfile authwarning messages will go there alsoThe format also allows the characters and to be prefixed to priority levels to indicate this priority only and except this priority and higher respectively Table shows examplesTable Examples of priority level qualifiersSelector Meaningauthinfo Authrelated messages of info priority and higherauthinfo Only messages at info priorityauthinfoautherr Only priorities info notice and warningauthdebugauthwarning All priorities except warningThe action field tells what to do with each message Table lists the optionsTable Common actionsAction Meaningfilename Appends the message to a file on the local machinehostname Forwards the message to the rsyslogd on hostnameipaddress Forwards the message to ipaddress on UDP port ipaddress Forwards the message to ipaddress on TCP port fifoname Writes the message to the named pipe fifoname auseruser Writes the message to the screens of users if they are logged in Writes the message to all users who are currently logged in Discards the messageprogramtemplate Formats the message according to the template specificationand sends it to program as the first argument ba See man mkfifo for more informationb See man rsyslogconf for further details on templatesIf a filename or fifoname action is specified the name should be an absolute pathIf you specify a nonexistent filename rsyslogd will create the file when a messageis first directed to it The ownership and permissions of the file are specified in theglobal configuration directives as shown on page Here are a few configuration examples that use the traditional syntax Kernel messages to kernlogkern varlogkernlog Cron messages to cronlogcron varlogcronlog Auth messages to authlogauthauthpriv varlogauthlog All other messages to syslogauthauthprivcronkernnone varlogsyslogYou can preface a filename action with a dash to indicate that the filesystem shouldnot be synced after each log entry is written syncing helps preserve as much logging information as possible in the event of a crash but for busy log files it can bedevastating in terms of IO performance We recommend including the dashes andthereby inhibiting syncing as a matter of course Remove the dashes only temporarily when investigating a problem that is causing kernel panicsLegacy directivesAlthough rsyslog calls these legacy options they remain in widespread use andyou will find them in the majority of rsyslog configurations Legacy directives canconfigure all aspects of rsyslog including global daemon options modules filtering and rulesIn practice however these directives are most commonly used to configure modules and the rsyslogd daemon itself Even the rsyslog documentation warns againstusing the legacy format for messageprocessing rules claiming that it is extremelyhard to get right Stick with the sysklogd or RainerScript formats for actually filtering and processing messagesDaemon options and modules are straightforward For example the options belowenable logging over UDP and TCP on the standard syslog port They alsopermit keepalive packets to be sent to clients to keep TCP connections open thisoption reduces the cost of reconstructing connections that have timed outModLoad imudpUDPServerRun ModLoad imtcpInputTCPServerRun InputTCPServerKeepAlive onTo put these options into effect you could add the lines to a new file to be includedin the main configuration such as etcrsyslogdnetworkinputsconf Thenrestart rsyslogd Any options that modify a modules behavior must appear afterthe module has been loadedTable on the next page describes a few of the more common legacy directivesTable Rsyslog legacy configuration optionsOption PurposeMainMsgQueueSize Size of memory buffer between received and sent messagesaMaxMessageSize Defaults to kB must precede loading of any input modulesLocalHostName Overrides the local hostnameWorkDirectory Specifies where to save rsyslog working filesModLoad Loads a moduleMaxOpenFiles Modifies the defaults system nofile limit for rsyslogdIncludeConfig Includes additional configuration filesUMASK Sets the umask for new files created by rsyslogda This option is useful for slow outputs such as database insertsRainerScriptThe RainerScript syntax is an eventstreamprocessing language with filtering andcontrolflow capabilities In theory you can also set basic rsyslogd options throughRainerScript But since some legacy options still dont have RainerScript equivalentswhy confuse things by using multiple option syntaxesRainerScript is more expressive and humanreadable than rsyslogds legacy directives but it has an unusual syntax thats unlike any other configuration system weveseen In practice it feels somewhat cumbersome Nonetheless we recommend it forfiltering and rule development if you need those features In this section we discussonly a subset of its functionalityOf our example distributions only Ubuntu uses RainerScript in its default configuration files However you can use RainerScript format on any system runningrsyslog version or newerYou can set global daemon parameters by using the global configuration objectFor exampleglobal workDirectoryvarspoolrsyslog maxMessageSizeMost legacy directives have identically named RainerScript counterparts such asworkDirectory and maxMessageSize in the lines above The equivalent legacy syntax for this configuration would beWorkDirectory varspoolrsyslogMaxMessageSize You can also load modules and set their operating parameters through RainerScriptFor example to load the UDP and TCP modules and apply the same configurationdemonstrated on page youd use the following RainerScriptmoduleloadimudpinputtypeimudp portmoduleloadimtcp KeepAliveoninputtypeimtcp portIn RainerScript modules have both module parameters and input parametersA module is loaded only once and a module parameter eg the KeepAlive optionin the imtcp module above applies to the module globally By contrast input parameters can be applied to the same module multiple times For example we couldinstruct rsyslog to listen on both TCP ports and moduleloadimtcp KeepAliveoninputtypeimtcp portinputtypeimtcp portMost of the benefits of RainerScript relate to its filtering capabilities You can useexpressions to select messages that match a certain set of characteristics then apply a particular action to the matching messages For example the following linesroute authenticationrelated messages to varlogauthlogif syslogfacilitytext auth then actiontypeomfile filevarlogauthlogIn this example syslogfacilitytext is a message propertythat is a part of themessages metadata Properties are prefixed by a dollar sign to indicate to rsyslogthat they are variables In this case the action is to use the omfile output moduleto write matching messages to authlogTable lists some of the most frequently used propertiesTable Commonly used rsyslog message propertiesProperty Meaningmsg The text of the message without metadatarawmsg The full message as received including metadatahostname The hostname from the messagesyslogfacility Syslog facility in numerical form see RFCsyslogfacilitytext Syslog facility in text formsyslogseverity Syslog severity in numeric form see RFCsyslogseveritytext Syslog severity in text formtimegenerated Time at which the message was received by rsyslogdtimereported Time stamp from the message itselfA given filter can include multiple filters and multiple actions The following fragment targets kernel messages of critical severity It logs the messages to a file andsends email to alert an administrator of the problemmoduleloadommailif syslogseveritytext crit and syslogfacilitytext kern then actiontypeomfile filevarlogkerncritlogactiontypeommail serversmtpadmincom port mailfromrsyslogadmincom mailtobenadmincom subjecttextCritical kernel error actionexeconlyonceeveryintervalHere weve specified that we dont want more than one email message generatedper hour secondsFilter expressions support regular expressions functions and other sophisticatedtechniques Refer to the RainerScript documentation for complete detailsConfig file examplesIn this section we show three sample configurations for rsyslog The first is a basicbut complete configuration that writes log messages to files The second exampleis a logging client that forwards syslog messages and httpd access and error logs toa central log server The final example is the corresponding log server that acceptslog messages from a variety of logging clientsThese examples rely heavily on RainerScript because its the suggested syntax forthe latest versions of rsyslog A few of the options are valid only in rsyslog version and include Linuxspecific settings such as inotifyBasic rsyslog configurationThe following file can serve as a generic RainerScript rsyslogconf for any Linuxsystemmoduleloadimuxsock Local system loggingmoduleloadimklog Kernel loggingmoduleloadimmark interval Hourly mark messages Set global rsyslogd parametersglobalworkDirectory varspoolrsyslogmaxMessageSize The output file module does not need to be explicitly loaded but we can load it ourselves to override default parameter valuesmoduleloadbuiltinomfile Use traditional timestamp formattemplateRSYSLOGTraditionalFileFormat Set the default permissions for all log filesfileOwnerrootfileGroupadmdirOwnerrootdirGroupadmfileCreateModedirCreateMode Include files from etcrsyslogd theres no RainerScript equivalentIncludeConfig etcrsyslogdconfThis example begins with a few default log collection options for rsyslogd The default file permissions of for new log files is more restrictive than the omfiledefault of Network logging clientThis logging client forwards system logs and the Apache access and error logs to aremote server over TCP Send all syslog messages to the server this is sysklogd syntax logsadmincom imfile reads messages from a file inotify is more efficient than polling Its the default but noted here for illustrationmoduleloadimfile modeinotify Import Apache logs through the imfile moduleinputtypeimfileTagapacheaccessFilevarlogapacheaccesslogSeverityinfoinputtypeimfileTagapacheerrorFilevarlogapacheerrorlogSeverityinfo Send Apache logs to the central log hostif programname contains apache then actiontypeomfwd Targetlogsadmincom Port ProtocoltcpApache httpd does not write messages to syslog by default so the access and errorlogs are read from text files with imfile The messages are tagged for later use ina filter expressionAt the end of the file the if statement is a filter expression that searches for Apachemessages and forwards those to logsadmincom the central log server Logs aresent over TCP which although more reliable than UDP still can potentially dropmessages You can use RELP the Reliable Event Logging Protocol a nonstandardoutput module to guarantee log deliveryIn a realworld scenario you might render the Apacherelated portion of this configuration to etcrsyslogdapacheconf as part of the configuration management setup for the serverCentral logging hostThe configuration of the corresponding central log server is straightforward listenfor incoming logs on TCP port filter by log type and write to files in the sitewide logging directory Load the TCP input module and listen on port Do not accept more than simultaneous clientsmoduleloadimtcp MaxSessionsinputtypeimtcp port Save to different files based on the type of messageif programname apacheaccess then actiontypeomfile filevarlogsiteapacheaccesslog else if programname apacheerror then actiontypeomfile filevarlogsiteapacheerrorlog else Everything else goes to a sitewide syslog fileactiontypeomfile filevarlogsitesyslog httpd can log directly to syslog with modsyslog but we use imfile here for illustrationSee Chapter formore about configuration managementThe central logging host generates a time stamp for each message as it writes outthe message Apache messages include a separate time stamp that was generatedwhen httpd logged the message Youll find both of these time stamps in the sitewide log filesSyslog message securityRsyslog can send and receive log messages over TLS a layer of encryption and authentication that runs on top of TCP See page for general information about TLSThe example below assumes that the certificate authority public certificates andkeys have already been generated See page for details on public key infrastructure and certificate generationThis configuration introduces a new option the network stream driver a modulethat operates at a layer between the network and rsyslog It typically implementsfeatures that enhance basic network capabilities TLS is enabled by the gtls netstream driverThe following example enables the gtls driver for a log server The gtls driver requires a CA certificate a public certificate and the servers private key The imtcpmodule then enables the gtls stream driverglobaldefaultNetstreamDrivergtlsdefaultNetstreamDriverCAFileetcsslcaadmincompemdefaultNetstreamDriverCertFileetcsslcertsservercompemdefaultNetstreamDriverKeyFileetcsslprivateservercomkeymoduleloadimtcpstreamDrivernamegtlsstreamDrivermodestreamDriverauthModexnameinputtypeimtcp portThe log server listens on the TLS version of the standard syslog port TheauthMode option tells syslog what type of validation to perform xname thedefault checks that the certificate is signed by a trusted authority and also validatesthe subject name that binds a certificate to a specific client through DNSConfiguration for the client side of the TLS connection is similar Use the clientcertificate and private key and use the gtls netstream driver for the log forwarding output moduleglobaldefaultNetstreamDrivergtlsdefaultNetstreamDriverCAFileetcsslcaadmincompemdefaultNetstreamDriverCertFileetcsslcertsclientcompemdefaultNetstreamDriverKeyFileetcsslprivateclientcomkey actiontypeomfwd Protocoltcp Targetlogsadmincom Port StreamDriverMode StreamDrivergtls StreamDriverAuthModexname In this case we forward all log messages with a sort of Frankenstein version of thesysklogd syntax the action component is a RainerScript form instead of one of thestandard sysklogdnative options If you need to be pickier about which messagesto forward or you need to send different classes of message to different destinations you can use RainerScript filter expressions as demonstrated in several ofthe examples earlier in this chapterSyslog configuration debuggingThe logger command is useful for submitting log entries from shell scripts or thecommand line You can also use it to test changes to rsyslogs configuration Forexample if you have just added the linelocalwarning tmpevilogand want to verify that it is working run the command logger p localwarning test messageA line containing test message should be written to tmpevilog If this doesnthappen perhaps youve forgotten to restart rsyslogd Kernel and boottime loggingThe kernel and the system startup scripts present some special challenges in thedomain of logging In the case of the kernel the problem is to create a permanentrecord of the boot process and kernel operation without building in dependencieson any particular filesystem or filesystem organization For startup scripts the challenge is to capture a coherent and accurate narrative of the startup process withoutpermanently tying any system daemons to a startup log file interfering with anyprograms own logging or gooping up the startup scripts with glue that serves onlyto capture boottime messagesFor kernel logging at boot time kernel log entries are stored in an internal bufferof limited size The buffer is large enough to accommodate messages about all thekernels boottime activities When the system is up and running a user processaccesses the kernels log buffer and disposes of its contentsOn Linux systems systemdjournald reads kernel messages from the kernel buffer by reading the device file devkmsg You can view these messages by runningjournalctl k or its alias journalctl dmesg You can also use the traditionaldmesg commandOn FreeBSD and older Linux systems the dmesg command is the best way to viewthe kernel buffer the output even contains messages that were generated beforeinit startedAnother issue related to kernel logging is the appropriate management of the systemconsole As the system is booting its important for all output to come to the console However once the system is up and running console messages may be morean annoyance than a help especially if the console is used for loginsUnder Linux dmesg lets you set the kernels console logging level with a commandline flag For exampleubuntu sudo dmesg n Level is the most verbose and includes debugging information Level includesonly panic messages the lowernumbered levels are the most severe All kernelmessages continue to go to the central buffer and thence to syslog regardless ofwhether they are forwarded to the console Management and rotation of log filesErik Troans logrotate utility implements a variety of log management policies andis standard on all our example Linux distributions It also runs on FreeBSD butyoull have to install it from the ports collection By default FreeBSD uses a different log rotation package called newsyslog see page for detailslogrotate crossplatform log managementA logrotate configuration consists of a series of specifications for groups of log filesto be managed Options that appear outside the context of a log file specificationsuch as errors rotate and weekly in the following example apply to all subsequentspecifications They can be overridden within the specification for a particular fileand can also be respecified later in the file to modify the defaultsHeres a somewhat contrived example that handles several different log files Global optionserrors errorsbookadmincomrotate weekly Logfile rotation definitions and optionsvarlogmessages postrotate binkill HUP cat varrunsyslogdpidendscriptvarlogsambalog notifemptycopytruncatesharedscriptspostrotate binkill HUP cat varlocksambapidendscriptThis configuration rotates varlogmessages every week It keeps five versions ofthe file and notifies rsyslog each time the file is reset Samba log files there mightbe several are also rotated weekly but instead of being moved aside and restartedthey are copied and then truncated The Samba daemons are sent HUP signals onlyafter all log files have been rotatedTable lists the most useful logrotateconf optionsTable logrotate optionsOption Meaningcompress Compresses all noncurrent versions of the log filedaily weekly monthly Rotates log files on the specified scheduledelaycompress Compresses all versions but current and nextmostrecentendscript Marks the end of a prerotate or postrotate scripterrors emailaddr Emails error notifications to the specified emailaddrmissingok Doesnt complain if the log file does not existnotifempty Doesnt rotate the log file if it is emptyolddir dir Specifies that older versions of the log file be placed in dirpostrotate Introduces a script to run after the log has been rotatedprerotate Introduces a script to run before any changes are maderotate n Includes n versions of the log in the rotation schemesharedscripts Runs scripts only once for the entire log groupsize logsize Rotates if log file size logsize eg K Mlogrotate is normally run out of cron once a day Its standard configuration fileis etclogrotateconf but multiple configuration files or directories containingconfiguration files can appear on logrotates command lineThis feature is used by Linux distributions which define the etclogrotated directory as a standard place for logrotate config files logrotateaware software packagesthere are many can drop in log management instructions as part of their installation procedure thus greatly simplifying administrationThe delaycompress option is worthy of further explanation Some applicationscontinue to write to the previous log file for a bit after it has been rotated Usedelaycompress to defer compression for one additional rotation cycle This optionresults in three types of log files lying around the active log file the previously rotated but not yet compressed file and compressed rotated filesIn addition to logrotate Ubuntu has a simpler program called savelog that manages rotation for individual files Its more straightforward than logrotate and doesntuse or need a config file Some packages prefer to use their own savelog configurations rather than logrotatenewsyslog log management on FreeBSDThe misleadingly named newsyslogso named because it was originally intendedto rotate files managed by syslogis the FreeBSD equivalent of logrotate Its syntaxand implementation are entirely different from those of logrotate but aside fromits peculiar date formatting the syntax of a newsyslog configuration is actuallysomewhat simplerThe primary configuration file is etcnewsyslogconf See man newsyslog for theformat and syntax The default etcnewsyslogconf has examples for standard log filesLike logrotate newsyslog runs from cron In a vanilla FreeBSD configurationetccrontab includes a line that runs newsyslog once per hour Management of logs at scaleIts one thing to capture log messages store them on disk and forward them toa central server Its another thing entirely to handle logging data from hundredsor thousands of servers The message volumes are simply too high to be managedeffectively without tools designed to function at this scale Fortunately multiplecommercial and open source tools are available to address this needThe ELK stackThe clear leader in the open source spaceand indeed one of the better softwaresuites weve had the pleasure of working withis the formidable ELK stack consisting of Elasticsearch Logstash and Kibana This combination of tools helps yousort search analyze and visualize large volumes of log data generated by a globalnetwork of logging clients ELK is built by Elastic elasticco which also offerstraining support and enterprise addons for ELKElasticsearch is a scalable database and search engine with a RESTful API for querying data Its written in Java Elasticsearch installations can range from a singlenode that handles a low volume of data to several dozen nodes in a cluster thatindexes many thousands of events each second Searching and analyzing log datais one of the most popular applications for ElasticsearchIf Elasticsearch is the hero of the ELK stack Logstash is its sidekick and trustedpartner Logstash accepts data from many sources including queueing systems suchas RabbitMQ and AWS SQS It can also read data directly from TCP or UDP sockets and from the traditional logging stalwart syslog Logstash can parse messagesto add additional structured fields and can filter out unwanted or nonconformantdata Once messages have been ingested Logstash can write them to a wide varietyof destinations including of course ElasticsearchYou can send log entries to Logstash in a variety of ways You can configure a syslog input for Logstash and use the rsyslog omfwd output module as described inRsyslog configuration on page You can also use a dedicated log shipper Elastics own version is called Filebeat and can ship logs either to Logstash or directlyto ElasticsearchThe final ELK component Kibana is a graphical front end for Elasticsearch It givesyou a search interface through which to find the entries you need among all thedata that has been indexed by Elasticsearch Kibana can create graphs and visualizations that help to generate new insights about your applications Its possiblefor example to plot log events on a map to see geographically whats happeningwith your systems Other plugins add alerting and system monitoring interfacesOf course ELK doesnt come without operational burden Building a large scaleELK stack with a custom configuration is no simple task and managing it takestime and expertise Most administrators we know present company includedhave accidentally lost data because of bugs in the software or operational errors Ifyou choose to deploy ELK be aware that youre signing up for substantial administrative overheadWe are aware of at least one service logzio that offers productiongrade ELKasaservice You can send log messages from your network over encrypted channelsto an endpoint that logzio provides There the messages are ingested indexedand made available through Kibana This is not a lowcost solution but its worthevaluating As with many cloud services you may find that its ultimately more expensive to replicate the service locallyGraylogGraylog is the spunky underdog to ELKs pack leader It resembles the ELK stackin several ways it keeps data in Elasticsearch and it can accept log messages eitherdirectly or through Logstash just as in the ELK stack The real differentiator is theGraylog UI which many users proclaim to be superior and easier to useSome of the enterprise read paid features of ELK are included in the Graylog opensource product including support for rolebased access control and LDAP integration Graylog is certainly worthy of inclusion in a bakeoff when youre choosinga new logging infrastructureLogging as a serviceSeveral commercial log management offerings are available Splunk is the mostmature and trusted both hosted and onpremises versions are available Some ofthe largest corporate networks rely on Splunk not only as a log manager but alsoas a business analytics system But if you choose Splunk be prepared to pay dearlyfor the privilegeAlternative SaaS options include Sumo Logic Loggly and Papertrail all of whichhave native syslog integration and a reasonable search interface If you use AWSAmazons CloudWatch Logs service can collect log data both from AWS servicesand from your own applications Logging policiesOver the years log management has emerged from the realm of system administration minutiae to become a formidable enterprise management challenge inits own right IT standards legislative edicts and provisions for securityincidenthandling may all impose requirements on the handling of log data A majority ofsites will eventually need to adopt a holistic and structured approach to the management of this dataLog data from a single system has a relatively inconsequential effect on storage buta centralized event register that covers hundreds of servers and dozens of applications is a different story entirely Thanks in large part to the missioncritical natureof web services application and daemon logs have become as important as thosegenerated by the operating systemKeep these questions in mind when designing your logging strategy How many systems and applications will be included What type of storage infrastructure is available How long must logs be retained What types of events are importantThe answers to these questions depend on business requirements and on any applicable standards or regulations For example one standard from the Payment CardIndustry Security Standards Council requires that logs be retained on easyaccessmedia eg a locally mounted hard disk for three months and archived to longtermSee pages and for more information aboutRBAC and LDAPstorage for at least one year The same standard also includes guidance about thetypes of data that must be includedOf course as one of our reviewers mentioned you cant be subpoenaed for log datayou do not possess Some sites do not collect or intentionally destroy sensitivelog data for this reason You might or might not be able get away with this kind ofapproach depending on the compliance requirements that apply to youHowever you answer the questions above be sure to gather input from your information security and compliance departments if your organization has themFor most applications consider capturing at least the following information Username or user ID Event success or failure Source address for network events Date and time from an authoritative source such as NTP Sensitive data added altered or removed Event detailsA log server should have a carefully considered storage strategy For example acloudbased system might offer immediate access to days of data with a year ofolder data being rolled over to an object storage service and three additional yearsbeing saved to an archival storage solution Storage requirements evolve over timeso a successful implementation must adapt easily to changing conditionsLimit shell access to centralized log servers to trusted system administrators andpersonnel involved in addressing compliance and security issues These log warehouse systems have no real role in the organizations daily business beyond satisfying auditability requirements so application administrators end users and thehelp desk have no business accessing them Access to log files on the central serversshould itself be loggedCentralization takes work and at smaller sites it may not represent a net benefitWe suggest twenty servers as a reasonable threshold for considering centralizationBelow that size just ensure that logs are rotated properly and are archived frequently enough to avoid filling up a disk Include log files in a monitoring solution thatalerts you if a log file stops growingSee page for information about RAIDThe kernel is the central government of a UNIX or Linux system Its responsiblefor enforcing rules sharing resources and providing the core services that userprocesses rely onWe dont usually think too much about what the kernel is doing Thats fortunatebecause even a simple command such as cat etcpasswd entails a complex seriesof underlying actions If the system were an airliner wed want to think in terms ofcommands such as increase altitude to feet rather than having to worryabout the thousands of tiny internal steps that were needed to manage the airplanescontrol surfacesThe kernel hides the details of the systems hardware underneath an abstracthighlevel interface Its akin to an API for application programmers a welldefined interface that provides useful facilities for interacting with the system Thisinterface provides five basic features Management and abstraction of hardware devices Processes and threads and ways to communicate among them Management of memory virtual memory and memoryspace protection IO facilities filesystems network interfaces serial interfaces etc Housekeeping functions startup shutdown timers multitasking etc Drivers and the KernelOnly device drivers are aware of the specific capabilities and communication protocols of the systems hardware User programs and the rest of the kernel are largelyindependent of that knowledge For example a filesystem on disk is very differentfrom a network filesystem but the kernels VFS layer makes them look the sameto user processes and to other parts of the kernel You dont need to know whetherthe data youre writing is headed to block of disk device or whether itsheaded for Ethernet interface ee wrapped in a TCP packet All you need toknow is that it will go to the file descriptor you specifiedProcesses and threads their lightweight cousins are the mechanisms through whichthe kernel implements CPU time sharing and memory protection The kernel fluidlyswitches among the systems processes giving each runnable thread a small slice oftime in which to get work done The kernel prevents processes from reading andwriting each others memory spaces unless they have explicit permission to do soThe memory management system defines an address space for each process and creates the illusion that the process owns an essentially unlimited region of contiguousmemory In reality different processes memory pages are jumbled together in thesystems physical memory Only the kernels bookkeeping and memory protectionschemes keep them sorted outLayered on top of the hardware device drivers but below most other parts of thekernel are the IO facilities These consist of filesystem services the networkingsubsystem and various other services that are used to get data into and out fromthe system Kernel chores for system administratorsNearly all of the kernels multilayered functionality is written in C with a few dabsof assembly language code thrown in to give access to CPU features that are not accessible through C compiler directives eg the atomic readmodifywrite instructions defined by many CPUs Fortunately you can be a perfectly effective systemadministrator without being a C programmer and without ever touching kernel codeThat said its inevitable that at some point youll need to make some tweaks Thesecan take several formsMany of the kernels behaviors such as networkpacket forwarding are controlledor influenced by tuning parameters that are accessible from user space Settingthese values appropriately for your environment and workload is a common administrative taskAnother common kernelrelated task is the installation of new device drivers Newmodels and types of hardware video cards wireless devices specialized audio cardsetc appear on the market constantly and vendordistributed kernels arent alwaysequipped to take advantage of themIn some cases you may even need to build a new version of the kernel from sourcecode Sysadmins dont have to build kernels as frequently as they used to but it stillmakes sense in some situations Its easier than it soundsKernels are tricky Its surprisingly easy to destabilize the kernel even through minor changes Even if the kernel boots it may not run as well as it should Whatsworse you may not even realize that youve hurt performance unless you have astructured plan for assessing the results of your work Be conservative with kernelchanges especially on production systems and always have a backup plan for reverting to a knowngood configuration Kernel version numberingBefore we dive into the depths of kernel wrangling its worth spending a few wordsto discuss kernel versions and their relationship to distributionsThe Linux and FreeBSD kernels are under continuous active development Overtime defects are fixed new features added and obsolete features removedSome older kernels continue to be supported for an extended period of time Likewise some distributions choose to emphasize stability and so run the older moretested kernels Other distributions try to offer the most recent device support andfeatures but might be a bit less stable as a result Its incumbent upon you as an administrator to select among these options in a manner that accommodates yourusers needs No single solution is appropriate for every contextLinux kernel versionsThe Linux kernel and the distributions based on it are developed separately fromone other so the kernel has its own versioning scheme Some kernel releases doachieve a sort of iconic popularity so its not unusual to find that several independent distributions are all using the same kernel You can check with uname r tosee what kernel a given system is runningLinux kernels are named according to the rules of socalled semantic versioningthat is they include three components a major version a minor version and apatch level At present there is no predictable relationship between a version number and its intended status as a stable or development kernel kernels are blessedas stable when the developers decide that theyre stable In addition the kernelsmajor version number has historically been incremented somewhat capriciouslyMany stable versions of the Linux kernel can be under longterm maintenanceat one time The kernels shipped by major Linux distributions often lag the latestreleases by a substantial margin Some distributions even ship kernels that are formally out of dateYou can install newer kernels by compiling and installing them from the kernelsource tree However we dont recommend that you do this Different distributionsSee semverorg formore informationabout semanticversioninghave different goals and they select kernel versions appropriate to those goals Younever know when a distribution has avoided a newer kernel because of some subtlebut specific concern If you need a more recent kernel install a distribution thatsdesigned around that kernel rather than trying to shoehorn the new kernel intoan existing systemFreeBSD kernel versionsFreeBSD takes a fairly straightforward approach to versions and releases The project maintains two major production versions which as of this writing are versions and The kernel has no separate versioning scheme its released as part of thecomplete operating system and shares its version numberThe older of the two major releases in this case FreeBSD can be thought of asa maintenance version It doesnt receive sweeping new features and its maintainedwith a focus on stability and security updatesThe more recent version FreeBSD right now is where active development occursStable releases intended for general use are issued from this tree as well Howeverthe kernel code is always going to be newer and somewhat less battletested thanthat of the previous major versionIn general dot releases occur about every four months Major releases are explicitlysupported for five years and the dot releases within them are supported for threemonths after the next dot release comes out Thats not an extensive lifetime for olddot releases FreeBSD expects you to stay current with patches Devices and their driversA device driver is an abstraction layer that manages the systems interaction with aparticular type of hardware so that the rest of the kernel doesnt need to know itsspecifics The driver translates between the hardware commands understood by thedevice and a stylized programming interface defined and used by the kernel Thedriver layer helps keep the majority of the kernel deviceindependentGiven the remarkable pace at which new hardware is developed it is practicallyimpossible to keep mainline OS distributions up to date with the latest hardwareHence you will occasionally need to add a device driver to your system to supporta new piece of hardwareDevice drivers are systemspecific and they are often specific to a particular rangeof kernel revisions as well Drivers for other operating systems eg Windows donot work on UNIX and Linux so keep this in mind when you purchase new hardware In addition devices vary in their degree of compatibility and functionalitywhen used with various Linux distributions so its wise to pay some attention tothe experiences that other sites have had with any hardware you are consideringHardware vendors are attracted to the FreeBSD and Linux markets and often publishappropriate drivers for their products In the optimal case your vendor furnishesyou with both a driver and installation instructions Occasionally you might findthe driver you need only on some sketchylooking and uncommented web pageCaveat emptorDevice files and device numbersIn most cases device drivers are part of the kernel they are not user processesHowever a driver can be accessed both from within the kernel and from user spaceusually through device files that live in the dev directory The kernel maps operations on these files into calls to the code of the driverMost nonnetwork devices have one or more corresponding files in dev Complexservers may support hundreds of devices By virtue of being device files the files indev each have a major and minor device number associated with them The kernel uses these numbers to map devicefile references to the corresponding driverThe major device number identifies the driver with which the file is associated inother words the type of device The minor device number usually identifies whichparticular instance of a given device type is to be addressed The minor device number is sometimes called the unit numberYou can see the major and minor number of a device file with ls llinux ls l devsdabrwrw root disk Jul devsdaThis example shows the first SCSISATASAS disk on a Linux system It has a majornumber of and a minor number of The minor device number is sometimes used by the driver to select or enable certaincharacteristics particular to that device For example a tape drive can have one filein dev that rewinds the drive automatically when its closed and another file thatdoes not The driver is free to interpret the minor device number in whatever way itlikes Look up the man page for the driver to determine what convention it is usingThere are actually two types of device files block device files and character devicefiles A block device is read or written one block a group of bytes usually a multiple of at a time a character device can be read or written one byte at a timeThe character b at the start of the ls output above indicates that devsda is a blockdevice ls would show this character as a c if it were a character deviceTraditionally certain devices could act as either block or character devices andseparate device files existed to make them accessible in either mode Disks andtapes led dual lives but most other devices did not However this parallel accesssystem is not used anymore FreeBSD represents all formerly dualmode devicesas character devices and Linux represents them as block devicesIt is sometimes convenient to implement an abstraction as a device driver even whenit controls no actual device Such phantom devices are known as pseudodevicesFor example a user who logs in over the network is assigned a pseudoTTY PTYthat looks feels and smells like a serial port from the perspective of highlevel software This trick allows programs written in the days when everyone used a physicalterminal to continue to function in the world of windows and networks devzerodevnull and devurandom are some other examples of pseudodevicesWhen a program performs an operation on a device file the kernel intercepts thereference looks up the appropriate function name in a table and transfers controlto the appropriate part of the driverTo perform an operation that doesnt have a direct analog in the filesystem modelejecting a DVD for example a program traditionally uses the ioctl system callto pass a message directly from user space into the driver Standard ioctl requesttypes are registered by a central authority in a manner similar to the way that network protocol numbers are maintained by IANAFreeBSD continues to use the traditional ioctl system Traditional Linux devicesalso use ioctl but modern networking code uses the more flexible Netlink socketssystem described in RFC These sockets provide a more flexible messagingsystem than ioctl without the need for a central authorityChallenges of device file managementDevice files have been a tricky problem for many years When systems supportedonly a few types of devices manual maintenance of device files was manageableAs the number of available devices grew however the dev filesystem became cluttered often with files irrelevant to the current system Red Hat Enterprise Linuxversion included more than device files one for every possible device thatcould be attached to the system The creation of static device files quickly becamea crushing problem and an evolutionary dead endUSB FireWire Thunderbolt and other device interfaces introduce additional wrinkles Ideally a drive that is initially recognized as devsda would remain availableas devsda despite intermittent disconnections and regardless of the activity ofother devices and buses The presence of other transient devices such as camerasprinters and scanners not to mention other types of removable media muddiesthe waters and makes the persistent identity problem even worseNetwork interfaces have this same problem they are devices but do not have devicefiles to represent them in dev For these devices the modern approach is to usethe relatively simple Predictable Network Interface Names system which assignsinterface names that are stable across reboots changes in hardware and changes indrivers Modern systems now have analogous methods for dealing with the namesof other devices tooManual creation of device filesModern systems manage their device files automatically However a few rare cornercases may still require you to create devices manually with the mknod commandSo heres how to do itmknod filename type major minorHere filename is the device file to be created type is c for a character device or bfor a block device and major and minor are the major and minor device numbersIf you are creating a device file that refers to a driver thats already present in yourkernel check the documentation for the driver to find the appropriate major andminor device numbersModern device file managementLinux and FreeBSD both automate the management of device files In classic UNIXfashion the systems are more or less the same in concept but entirely separate intheir implementations and in the formats of their configuration files Let a thousand flowers bloomWhen a new device is detected both systems automatically create the devices corresponding device files When a device goes away eg a USB thumb drive is unplugged its device files are removed For architectural reasons both Linux andFreeBSD isolate the creating device files part of this equationIn FreeBSD devices are created by the kernel in a dedicated filesystem type devfsthats mounted on dev In Linux a daemon running in user space called udevis responsible for this activity Both systems listen to an underlying stream ofkernelgenerated events that report the arrival and departure of devicesHowever theres a lot more we might want to do with a newly discovered device thanjust create a device file for it If it represents a piece of removable storage media forexample we might want to automount it as a filesystem If its a hub or a communications device we might want to get it set up with the appropriate kernel subsystemBoth Linux and FreeBSD leave such advanced procedures to a userspace daemonudevd in the case of Linux and devd in the case of FreeBSD The main conceptualdistinction between the two platforms is that Linux concentrates most intelligencein udevd whereas FreeBSDs devfs filesystem is itself slightly configurableTable on the next page summarizes the components of the device file management systems on both platformsLinux device managementLinux administrators should understand how udevds rule system works and shouldknow how to use the udevadm command Before peering into those details however lets first review the underlying technology of sysfs the device informationrepository from which udevd gets its raw dataTable Outline of automatic device managementComponent Linux FreeBSDdev filesystem udev devtmpfs devfsdev FS configuration files etcdevfsconfetcdevfsrulesDevice manager daemon udevd devdDaemon configuration files etcudevudevconfetcudevrulesdlibudevrulesdetcdevdconfFIlesystem automounts udevd autofsSysfs a window into the souls of devicesSysfs was added to the Linux kernel at version It is a virtual inmemory filesystem implemented by the kernel to provide detailed and wellorganized information about the systems available devices their configurations and their state Sysfsdevice information is accessible both from within the kernel and from user spaceYou can explore the sys directory where sysfs is typically mounted to find out everything from what IRQ a device is using to how many blocks have been queuedfor writing to a disk controller One of the guiding principles of sysfs is that eachfile in sys should represent only one attribute of the underlying device This convention imposes a certain amount of structure on an otherwise chaotic data setTable shows the directories within sys each of which is a subsystem that hasbeen registered with sysfs The exact directories vary slightly by distributionTable Subdirectories of sysDirectory What it containsblock Information about block devices such as hard disksbus Buses known to the kernel PCIE SCSI USB and othersclass A tree organized by functional types of devices adev Device information split between character and block devicesdevices An ancestrally correct representation of all discovered devicesfirmware Interfaces to platformspecific subsystems such as ACPIfs A directory for some but not all filesystems known to the kernelkernel Kernel internals such as cache and virtual memory statusmodule Dynamic modules loaded by the kernelpower A few details about the systems power state mostly unuseda For example sound and graphic cards input devices and network interfacesDevice configuration information was formerly found in the proc filesystem if itwas available at all proc was inherited from System V UNIX and grew organicallyand somewhat randomly over time It ended up collecting all manner of unrelatedinformation including many elements unrelated to processes Although extra junkin proc is still supported for backward compatibility sys is a more predictableand organized way of reflecting the kernels internal data structures We anticipatethat all devicespecific information will move to sys over timeudevadm explore devicesThe udevadm command queries device information triggers events controls theudevd daemon and monitors udev and kernel events Its primary use for administrators is to build and test rules which are covered in the next sectionudevadm expects one of six commands as its first argument info trigger settlecontrol monitor or test Of particular interest to system administrators are infowhich prints devicespecific information and control which starts and stopsudevd or forces it to reload its rules files The monitor command displays eventsas they occurThe following command shows all udev attributes for the device sdb The output istruncated here but in reality it goes on to list all parent devicessuch as the USBbusthat are ancestors of sdb in the device treelinux udevadm info a n sdblooking at device devicespciusbhosttargetblocksdbKERNELsdbSUBSYSTEMblockDRIVERATTRrangeATTRextrangeATTRremovableATTRroATTRsizeATTRcapabilityATTRstat All paths in udevadm output such as devicespci are relative to syseven though they may appear to be absolute pathnamesThe output is formatted so that you can feed it back to udev when constructing rulesFor example if the ATTRsize clause were unique to this device youcould copy that snippet into a rule as the identifying criterionRefer to the man page on udevadm for additional options and syntaxRules and persistent namesudevd relies on a set of rules to guide its management of devices The default ruleslive in the libudevrulesd directory but local rules belong in etcudevrulesdYou need never edit or delete the default rules you can ignore or override a file ofdefault rules by creating a new file with the same name in the custom rules directoryThe master configuration file for udevd is etcudevudevconf however the default behaviors are reasonable The udevconf files on our example distributionscontain only comments with the exception of one line that enables error loggingSadly because of political bickering among distributors and developers there islittle rule synergy among distributions Many of the filenames in the default rulesdirectory are the same from distribution to distribution but the contents of thefiles differ significantlyRule files are named according to the pattern nndescriptionrules where nn isusually a twodigit number Files are processed in lexical order so lower numbersare processed first Files from the two rules directories are combined before theudev daemon udevd parses them The rules suffix is mandatory files without itare ignoredRules are of the formmatchclause matchclause assignclause assignclause The match clauses define the situations in which the rule is to be applied and the assignment clauses tell udevd what to do when a device is consistent with all the rulesmatch clauses Each clause consists of a key an operator and a value For examplethe match clause ATTRsize was referred to above as a potentialcomponent of a rule it selects all devices whose size attribute is exactly Most match keys refer to device properties which udevd obtains from the sysfilesystem but some refer to other contextdependent attributes such as the operation being handled eg device addition or removal All match clauses mustmatch in order for a rule to be activatedTable shows the match keys understood by udevdThe assignment clauses specify actions udevd should take to handle any matchingevents Their format is similar to that for match clausesThe most important assignment key is NAME which indicates how udevd shouldname a new device The optional SYMLINK assignment key creates a symbolic linkto the device through its desired path in devHere we put these components together with an example configuration for a USBflash drive Suppose we want to make the drives device name persist across insertions and we want the drive to be mounted and unmounted automaticallyTo start with we insert the flash drive and check to see how the kernel identifies itThis task can be approached in a couple of ways By running the lsusb commandwe can inspect the USB bus directlyubuntu lsusbBus Device ID Transcend Inc USB Flash DriveBus Device ID db Linux Foundation root hubBus Device ID db Linux Foundation root hubAlternatively we can check for kernel log entries by running dmesg or journalctlIn our case the attachment leaves an extensive audit trailAug ubuntu kernel scsi DirectAccess Ut USBFlashStorage PQ ANSI Aug ubuntu kernel sd sdb byte hardware sectors GB MiBAug ubuntu kernel sd sdb byte hardware sectors GB MiBAug ubuntu kernel sdb sdbAug ubuntu kernel sd sdb AttachedSCSI removable diskAug ubuntu kernel sd Attached scsigeneric sg type The log messages above indicate that the drive was recognized as sdb which gives usan easy way to identify the device in sys We can now examine the sys filesystemwith udevadm in search of some rule snippets that are characteristic of the deviceand so might be useful to incorporate in udev rulesTable udevd match keysMatch key FunctionACTION Matches the event type eg add or removeATTRfilename Matches a devices sysfs values aDEVPATH Matches a specific device pathDRIVER Matches the driver used by a deviceENVkey Matches the value of an environment variableKERNEL Matches the kernels name for the devicePROGRAM Runs an external command matches if the return code is RESULT Matches the output of the last call through PROGRAMSUBSYSTEM Matches a specific subsystemTESTomask Tests whether a file exists the omask is optionala The filename is a leaf in the sysfs tree that corresponds to a specific attributeubuntu udevadm info a p blocksdbsdblooking at device devicespciusbhosttargetblocksdbsdbKERNELsdbSUBSYSTEMblockDRIVERATTRpartitionATTRstartATTRsizeATTRstat looking at parent device devicespciusbhosttargetblocksdbKERNELSsdbSUBSYSTEMSblockDRIVERSATTRSscsilevelATTRSvendorUt ATTRSmodelUSBFlashStorageThe output from udevadm shows several opportunities for matching One possibility is the size field which is likely to be unique to this device However if the sizeof the partition were to change the device would not be recognized Instead wecan use a combination of two values the kernels naming convention of sd plus anadditional letter and the contents of the model attribute USBFlashStorage Forcreating rules specific to this particular flash drive another good choice would bethe devices serial number which weve omitted from the output hereWe next put our rules for this device in the file etcudevrulesdlocalrulesBecause we have multiple objectives in mind we need a series of rulesFirst we take care of creating device symlinks in dev The following rule uses ourknowledge of the ATTRS and KERNEL match keys gleaned from udevadm to identify the deviceATTRSmodelUSBFlashStorage KERNELsdazSYMLINKateflashnThe rule has been folded here to fit the page in the original file its all one lineWhen the rule triggers udevd sets up devateflashN as a symbolic link to the device where N is the next integer in sequence starting at We dont really expectmore than one of these devices to appear on the system If more copies do appearthey receive unique names in dev but the exact names will depend on the insertion order of the devicesNext we use the ACTION key to run some commands whenever the device appearson the USB bus The RUN assignment key lets us create an appropriate mount pointdirectory and mount the device thereACTIONadd ATTRSmodelUSBFlashStorage KERNELsdazRUNbinmkdir p mntateflashnACTIONadd ATTRSmodelUSBFlashStorage KERNELsdazPROGRAMlibudevvolid t N RESULTvfatRUNbinmount vfat devk mntateflashnThe PROGRAM and RUN keys look similar but PROGRAM is a match key thats activeduring the rule selection phase whereas RUN is an assignment key thats part ofthe rules actions once triggered The second rule above verifies that the flash drivecontains a Windows filesystem before mounting it with the t vfat option to themount commandSimilar rules clean up when the device is removedACTIONremove ATTRSmodelUSBFlashStorageKERNELsdaz RUNbinumount l mntateflashnACTIONremove ATTRSmodelUSBFlashStorageKERNELsdaz RUNbinrmdir mntateflashnNow that our rules are in place we must notify udevd of our changes udevadmscontrol command is one of the few that require root privilegesubuntu sudo udevadm control reloadrulesTypos are silently ignored after a reload even with the debug flag so be sure todoublecheck the rules syntaxThats it Now when the flash drive is plugged into a USB port udevd creates a symbolic link called devateflash and mounts the drive as mntateflashubuntu ls l devatelrwxrwxrwx root root devateflash sdbubuntu mount grep atedevsdb on mntateflash type vfat rwFreeBSD device managementAs we saw in the brief overview on page FreeBSDs implementation of theselfmanaging dev filesystem is called devfs and its userlevel device managementdaemon is called devdDevfs automatic device file configurationUnlike Linuxs udev filesystem devfs itself is somewhat configurable However theconfiguration system is both peculiar and rather impotent Its split into boottimeetcdevfsconf and dynamic etcdevfsrules portions The two configurationfiles have different syntaxes and somewhat different capabilitiesDevfs for static nonremovable devices is configured in etcdevfsconf Each lineis a rule that starts with an action The possible actions are link own and perm Thelink action sets up symbolic links for specific devices The own and perm actionschange the ownerships and permissions of device files respectivelyEach action accepts two parameters the interpretation of which depends on thespecific action For example suppose we want our DVDROM drive devcd toalso be accessible by the name devdvd The following line would do the tricklink cd dvdWe could set the ownerships and permissions on the device with the following linesown cd rootsysadminperm cd Just as etcdevfsconf specifies actions to take for builtin devices etcdevfsrulescontains rules for removable devices Rules in devfsrules also have the option tomake devices hidden or inaccessible which can be useful for jail environmentsdevd higherlevel device managementThe devd daemon runs in the background watching for kernel events related to devices and acting on the rules defined in etcdevdconf The configuration of devdis detailed in the devdconf man page but the default devdconf file includes manyuseful examples and enlightening commentsThe format of etcdevdconf is conceptually simple consisting of statements containing groups of substatements Statements are essentially rules and substatements provide details about the rule Table lists the available statement typesTable Statement types in etcdevdconfStatement What it specifiesattach What to do when a device is inserteddetach What to do when a device is removednomatch What to do if no other statements match a devicenotify How to respond to kernel events about a deviceoptions Configuration options for devd itselfDespite being conceptually simple the configuration language for substatements isrich and complex For this reason many of the common configuration statementsare already included in the standard distributions configuration file In many casesyou will never need to modify the default etcdevdconfAutomatic mounting of removable media devices such as USB hard disks and thumbdrives is now handled by FreeBSDs implementation of autofs not by devd Seepage for general information about autofs Although autofs is found on mostUNIXlike operating systems FreeBSD is unusual in assigning it this extra task Linux kernel configurationYou can use any of three basic methods to configure a Linux kernel Chances arethat you will have the opportunity to try all of them eventually The methods are Modifying tunable dynamic kernel configuration parameters Building a kernel from scratch by compiling it from the source codepossibly with modifications and additions Loading new drivers and modules into an existing kernel on the flyThese procedures are used in different situations so learning which approaches areneeded for which tasks is half the battle Modifying tunable parameters is the easiest and most common kernel tweak whereas building a kernel from source codeis the hardest and least often required Fortunately all these approaches becomesecond nature with a little practiceTuning Linux kernel parametersMany modules and drivers in the kernel were designed with the knowledge thatone size doesnt fit all To increase flexibility special hooks allow parameters suchas an internal tables size or the kernels behavior in a particular circumstance tobe adjusted on the fly by the system administrator These hooks are accessiblethrough an extensive kerneltouserland interface represented by files in the procfilesystem aka procfs In some cases a large userlevel application especially aninfrastructure application such as a database might require a sysadmin to adjustkernel parameters to accommodate its needsYou can view and set kernel options at run time through special files in procsysThese files mimic standard Linux files but they are really back doors into the kernel If a file in procsys contains a value you want to change you can try writingto it Unfortunately not all files are writable regardless of their apparent permissions and not much documentation is available If you have the kernel source treeinstalled you may be able to read about some of the values and their meanings inthe subdirectory Documentationsysctl or online at kernelorgdocFor example to change the maximum number of files the system can have open atonce try something likelinux cat procsysfsfilemaxlinux echo procsysfsfilemaxOnce you get used to this unorthodox interface youll find it quite useful Howevernote that changes are not remembered across rebootsA more permanent way to modify these same parameters is to use the sysctl command sysctl can set individual variables either from the command line or froma list of variablevalue pairs in a configuration file By default etcsysctlconf isread at boot time and its contents are used to set the initial values of parametersFor example the commandlinux sysctl netipvipforwardturns off IP forwarding Alternatively you can manually edit etcsysctlconf Youform the variable names used by sysctl by replacing the slashes in the procsysdirectory structure with dotsTable lists some commonly tuned parameters for Linux kernel version and higher Default values vary widely among distributionsTable Files in procsys for some tunable kernel parametersFile What it doescdromautoclose Autocloses the CDROM when mountedcdromautoeject Autoejects the CDROM when unmountedfsfilemax Sets max number of open fileskernelctrlaltdel Reboots on ControlAltDelete may increasesecurity on unsecured consoleskernelpanic Sets seconds to wait before rebooting after a kernelpanic loop or hang indefinitelykernelpaniconoops Determines the kernels behavior afterencountering an oops or a bug always panickernelprintkratelimit Sets minimum seconds between kernel messageskernelprintkratelimitburst Sets number of messages in succession before theprintk rate limit is actually enforcedkernelshmmax Sets max amount of shared memorynetipconfdefaultrpfilter Enables IP source route verificationanetipicmpechoignoreall Ignores ICMP pings when set to bnetipipforward Allows IP forwarding when set to cnetipiplocalportrange Sets local port range used during connection setupdnetiptcpsyncookies Protects against SYN flood attacks turn on if yoususpect denialofservice DoS attackstcpfintimeout Sets seconds to wait for a final TCP FIN packet evmovercommitmemory Controls memory overcommit behavior iehow the kernel reacts when physical memory isinsufficient to handle a VM allocation requestvmovercommitratio Defines how much physical memory as apercentage will be used when overcommittinga This antispoofing mechanism makes the kernel drop packets received from impossible pathsb The related variable icmpechoignorebroadcasts ignores broadcast ICMP pings Its almost always agood idea to set this value to c Only set this value to if you explicitly intend to use your Linux box as a network routerd Increase this range to on servers that initiate many outbound connectionse Try setting this value lower on hightraffic servers to increase performanceNote that there are two IP networking subdirectories of procsysnet ipv andipv In the past administrators only had to worry about IPv behaviors becausethat was the only game in town But as of this writing the IPv addressblocks have all been assigned and IPv is deployed and in use almost everywhereeven within smaller organizationsIn general when you change a parameter for IPv you should also change thatparameter for IPv if you are supporting both protocols Its all too easy to modify one version of IP and not the other then run into problems several months oryears later when a user reports strange network behaviorBuilding a custom kernelBecause Linux evolves rapidly youll likely be faced with the need to build a customkernel at some point or another The steady flow of kernel patches device driversand new features that arrive on the scene is something of a mixed blessing On onehand its a privilege to live at the center of an active and vibrant software ecosystem On the other hand just keeping abreast of the constant flow of new materialcan be a job of its ownIf it aint broke dont fix itCarefully weigh your sites needs and risks when planning kernel upgrades andpatches A new release may be the latest and greatest but is it as stable as the current version Could the upgrade or patch be delayed and installed with anothergroup of patches at the end of the month Resist the temptation to let keeping upwith the Joneses in this case the kernelhacking community dominate the bestinterests of your user communityA good rule of thumb is to upgrade or apply patches only when the productivitygains you expect to obtain usually measured in terms of reliability and performance exceed the effort and lost time required for the installation If youre having trouble quantifying the specific gain thats a good sign that the patch can waitfor another day Of course securityrelated patches should be installed promptlySetting up to build the Linux kernelIts less likely that youll need to build a kernel on your own if youre running a distribution that uses a stable kernel to begin with It used to be that the second partof the version number indicated whether the kernel was stable even numbers orin development odd numbers But these days the kernel developers no longerfollow that system Check kernelorg for the official word on any particular kernelversion The kernelorg site is also the best source for Linux kernel source code if youare not relying on a particular distribution or vendor to provide you with a kernelEach distribution has a specific way to configure and build custom kernels However distributions also support the traditional way of doing things which is what wedescribe here Its generally safest to use your distributors recommended procedureConfiguring kernel optionsMost distributions install kernel source files in versioned subdirectories underusrsrckernels In all cases you need to install the kernel source package beforeyou can build a kernel on your system See Chapter Software Installation andManagement for tips on package installationKernel configuration revolves around the config file at the root of the kernel sourcedirectory All the kernel configuration information is specified in this file but itsformat is somewhat cryptic Use the decoding guide inkernelsrcdirDocumentationConfigurehelpto find out what the various options mean Its usually inadvisable to edit the configfile by hand because the effect of changing options is not always obvious Optionsare frequently interdependent so turning on an option might not be a simple matter of changing an n to a yTo save folks from having to edit the config file directly Linux has several maketargets that help you configure the kernel through a user interface If you are running KDE the prettiest configuration interface is provided by make xconfig Likewise if youre running GNOME make gconfig is probably the best option Thesecommands bring up a graphical configuration screen in which you can pick thedevices to add to your kernel or to compile as loadable modulesIf you are not running KDE or GNOME you can use a terminalbased alternativeinvoked with make menuconfig Finally the barebones make config prompts youto respond to every single configuration option thats available which results in a lotof questionsand if you change your mind you have to start over We recommendmake xconfig or make gconfig if your environment supports them otherwise usemake menuconfig Avoid make config the least flexible and most painful optionIf youre migrating an existing kernel configuration to a new kernel version or treeyou can use the make oldconfig target to read in the previous config file and askonly the questions that are new to this edition of the kernelThese tools are straightforward as far as the options you can turn on Unfortunately they are painful to use if you want to maintain multiple versions of the kernelto accompany multiple architectures or hardware configurations found in yourenvironmentAll the various configuration interfaces described above generate a config file thatlooks something like this Automatically generated make config dont edit Code maturity level optionsCONFIGEXPERIMENTALy Processor type and features CONFIGM is not set CONFIGM is not set CONFIGM is not set CONFIGMTSC is not setCONFIGMyCONFIGXWPWORKSOKyCONFIGXINVLPGyCONFIGXBSWAPyCONFIGXPOPADOKyCONFIGXTSCyCONFIGXGOODAPICyAs you can see the contents are cryptic and do not attempt to describe what thevarious CONFIG tags mean Each line refers to a specific kernel configuration option The value y compiles the option into the kernel and the value m enables theoption as a loadable moduleSome options can be configured as modules and some cant You just have to knowwhich is which it will not be clear from the config file Nor are the CONFIG tagseasily mapped to meaningful informationThe option hierarchy is extensive so set aside many hours if you plan to scrutinizeevery possibilityBuilding the kernel binarySetting up an appropriate config file is the most important part of the Linux kernel configuration process but you must jump through several more hoops to turnthat file into a finished kernelHeres an outline of the entire process Change directory cd to the top level of the kernel source directory Run make xconfig make gconfig or make menuconfig Run make clean Run make Run make modulesinstall Run make installYou might also have to update configure and install the GRUB boot loaders configuration file if this was not performed by the make install step The GRUB updater scans the boot directory to see which kernels are available and automaticallyincludes them in the boot menuThe make clean step is not strictly necessary but its generally a good idea to startwith a clean build environment In practice many problems can be traced back tothis step having been skippedSee page formore informationabout GRUBAdding a Linux device driverOn Linux systems device drivers are typically distributed in one of three forms A patch against a specific kernel version A loadable kernel module An installation script or package that installs the driverThe most common form is the installation script or package If youre lucky enoughto have one of these for your new device you should be able to follow the standardprocedure for installing new softwareIn situations where you have a patch against a specific kernel version you can inmost cases install the patch with the following procedurelinux cd kernelsrcdir patch p patchfile FreeBSD kernel configurationFreeBSD supports the same three methods of changing kernel parameters as Linuxdynamically tuning the running kernel building a new kernel from source andloading dynamic modulesTuning FreeBSD kernel parametersMany FreeBSD kernel parameters can be changed dynamically with the sysctlcommand as is done on Linux You can set values automatically at boot time byadding them to etcsysctlconf Many many parameters can be changed this waytype sysctl a to see them all Not everything that shows up in the output of thatcommand can be changed many parameters are readonlyThe following paragraphs outline a few of the more commonly modified or interesting parameters that you might want to adjustnetinetipforwarding and netinetipforwarding control IP packet forwarding for IPv and IPv respectivelykernmaxfiles sets the maximum number of file descriptors that the system canopen You may need to increase this on systems such as database or web serversnetinettcpmssdflt sets the default TCP maximum segment size which is thesize of the TCP packet payload carried over IPv Certain payload sizes are too largefor longhaul network links and hence might be dropped by their routers Changing this parameter can be useful when debugging longhaul connectivity issuesnetinetudpblackhole controls whether an ICMP port unreachable packet isreturned when a packet arrives for a closed UDP port Enabling this option thatis disabling port unreachable packets might slow down port scanners and potential attackersnetinettcpblackhole is similar in concept to the udpblackhole parameter TCPnormally sends an RST connection reset response when packets arrive for closedports Setting this parameter to prevents any SYN connection setup arriving ona closed port from generating an RST Setting it to prevents RST responses to anysegment at all that arrives on a closed portkernipcnmbclusters controls the number of mbuf clusters available to the systemMbufs are the internal storage structure for network packets and mbuf clusters canbe thought of as the mbuf payload For servers that experience heavy networkloads this value may need to be increased from the default currently onFreeBSD kernmaxvnodes sets the maximum number of vnodes which are kernel data structures that track files Increasing the number of available vnodes can improve diskthroughput on a busy server Examine the value of vfsnumvnodes on servers that areexperiencing poor performance if its value is close to the value of kernmaxvnodesincrease the latterBuilding a FreeBSD kernelKernel source comes from the FreeBSD servers in the form of a compressed tarballJust download and unpack to install Once the kernel source tree has been installedthe process for configuring and building the kernel is similar to that of Linux However the kernel source always lives in usrsrcsys Under that directory is a set ofsubdirectories one for each architecture that is supported Inside each architecturedirectory a conf subdirectory includes a configuration file named GENERIC forthe socalled generic kernel which supports every possible device and optionThe configuration file is analogous to the Linux config file The first step in makinga custom kernel is to copy the GENERIC file to a new distinct name in the samedirectory eg MYCUSTOM The second step is to edit the config file and modifyits parameters by commenting out functions and devices that you dont need Thefinal step is to build and install the kernel That final step must be performed in thetoplevel usrsrc directoryFreeBSD kernel configuration files must be edited by hand There are no dedicated user interfaces for this task as there are in the Linux world Information on thegeneral format is available from the config man page and information abouthow the config file is used can be found in the config man pageThe configuration file contains some internal comments that describe what eachoption does However you do still need some background knowledge on a widevariety of technologies to make informed decisions about what to leave in In general youll want to leave all the options from the GENERIC configuration enabledand modify only the devicespecific lines lower in the configuration file Its best toleave options enabled unless youre absolutely certain you dont need themFor the final build step FreeBSD has a single highly automated make buildkerneltarget that combines parsing the configuration file creating the build directoriescopying the relevant source files and compiling those files This target accepts thecustom configuration filename in the form of a build variable KERNCONF Ananalogous install target make installkernel installs the kernel and boot loaderHere is a summary of the process Change directory cd to usrsrcsysarchconf for your architecture Copy the generic configuration cp GENERIC MYCUSTOM Edit your MYCUSTOM configuration file Change directory to usrsrc Run make buildkernel KERNCONFMYCUSTOM Run make installkernel KERNCONFMYCUSTOMNote that these steps are not crosscompilationenabled That is if your build machine has an AMD architecture you cannot cd to usrsrcsyssparcconf followthe normals steps and end up with a SPARCready kernel Loadable kernel modulesLoadable kernel modules LKMs are available in both Linux and FreeBSD LKMsupport allows a device driveror any other kernel componentto be linked intoand removed from the kernel while the kernel is running This capability facilitatesthe installation of drivers because it avoids the need to update the kernel binaryIt also allows the kernel to be smaller because drivers are not loaded unless theyare neededAlthough loadable drivers are convenient they are not safe Any time youload or unload a module you risk causing a kernel panic So dont try out an untested module when you are not willing to crash the machineLike other aspects of device and driver management the implementation of loadable modules is OSdependentLoadable kernel modules in LinuxUnder Linux almost anything can be built as a loadable kernel module The exceptions are the root filesystem type whatever that might be on a given systemand the PS mouse driverLoadable kernel modules are conventionally stored under libmodulesversionwhere version is the version of your Linux kernel as returned by uname rYou can inspect the currently loaded modules with the lsmod commandredhat lsmodModule Size Used byipmidevintf ipmisi ipmimsghandler ipmidevintfipmisiiptablefilter iptables iptablefilterLoaded on this machine are the Intelligent Platform Management Interface modules and the iptables firewall among other modulesAs an example of manually loading a kernel module heres how we would insert amodule that implements sound output to USB devicesredhat sudo modprobe sndusbaudioWe can also pass parameters to modules as they are loaded for exampleredhat sudo modprobe sndusbaudio nrpacks asyncunlinkmodprobe is a semiautomatic wrapper around a more primitive command insmodmodprobe understands dependencies options and installation and removal procedures It also checks the version number of the running kernel and selects anappropriate version of the module from within libmodules It consults the fileetcmodprobeconf to figure out how to handle each individual moduleOnce a loadable kernel module has been manually inserted into the kernel it remainsactive until you explicitly request its removal or reboot the system You could usemodprobe r sndusbaudio to remove the audio module loaded above Removalworks only if the number of current references to the module listed in the Usedby column of lsmods output is You can dynamically generate an etcmodprobeconf file that corresponds to allyour currently installed modules by running modprobe c This command generates a long file that looks like thisThis file was generated by modprobe cpathpcmcialibmodulespreferredpathpcmcialibmodulesdefaultpathpcmcialibmodulespathmisclibmodules Aliasesalias blockmajor rdalias blockmajor floppyalias charmajor serialalias charmajor serialalias charmajor lpalias dos msdosalias plip plipalias ppp pppoptions ne iox irqThe path statements tell where a particular module can be found You can modifyor add entries of this type to keep your modules in a nonstandard locationThe alias statements map between module names and blockmajor device numberscharactermajor device numbers filesystems network devices and network protocolsThe options lines are not dynamically generated but must be manually added by anadministrator They specify options that should be passed to a module when it isloaded For example you could use the following line to pass in additional optionsto the USB sound moduleoptions sndusbaudio nrpacks asyncunlinkmodprobe also understands the statements install and remove These statementsallow commands to be executed when a specific module is inserted into or removedfrom the running kernelLoadable kernel modules in FreeBSDKernel modules in FreeBSD live in bootkernel for standard modules that are partof the distribution or bootmodules for ported proprietary and custom modules Each kernel module uses the ko filename extension but it is not necessary tospecify that extension when loading unloading or viewing the status of the moduleFor example to load a module named fooko run kldload foo in the appropriatedirectory To unload the module run kldunload foo from any location To viewthe modules status run kldstat m foo from any location Running kldstat withoutany parameters displays the status of all currently loaded modulesModules listed in either of the files bootdefaultsloaderconf system defaultsor bootloaderconf are loaded automatically at boot time To add a new entry tobootloaderconf use a line of the formzfsloadYESThe appropriate variable name is just the module basename with load appendedto it The line above ensures that the module bootkernelzfsko will be loaded atboot it implements the ZFS filesystem BootingNow that we have covered kernel basics its time to learn what actually happenswhen a kernel loads and initializes at startup Youve no doubt seen countless bootmessages but do you know what all of those messages actually meanThe following messages and annotations come from some key phases of the bootprocess They almost certainly wont be an exact match for what you see on your ownsystems and kernels However they should give you a notion of some of the majorthemes in booting and a feeling for how the Linux and FreeBSD kernels start upLinux boot messagesThe first boot log we examine is from a CentOS machine running a kernelFeb localhost kernel Initializing cgroup subsys cpusetFeb localhost kernel Initializing cgroup subsys cpuFeb localhost kernel Initializing cgroup subsys cpuacctFeb localhost kernel Linux version elxbuilderkbuilderdevcentosorg gcc version RedHat GCC SMP Thu Nov UTC Feb localhost kernel Command line BOOTIMAGEvmlinuzelx rootdevmappercentosroot rocrashkernelauto rdlvmlvcentosroot rdlvmlvcentosswap rhgbquiet LANGenUSUTFThese initial messages tell us that the toplevel control groups cgroups are startingup on a Linux kernel The messages tell us who built the kernel and where andwhich compiler they used gcc Note that although this log comes from a CentOSsystem CentOS is a clone of Red Hat and the boot messages remind us of that factThe parameters set in the GRUB boot configuration and passed from there into thekernel are listed above as the command lineFeb localhost kernel e BIOSprovided physical RAM mapFeb localhost kernel BIOSe mem xxfbff usableFeb localhost kernel BIOSe mem xfcxffff reservedFeb localhost kernel Hypervisor detected KVMFeb localhost kernel AGP No AGP bridge foundFeb localhost kernel x PAT enabled cpu oldx new xFeb localhost kernel CPU MTRRs all blank virtualizedsystemFeb localhost kernel e lastpfn xdfff maxarchpfn xFeb localhost kernel found SMP MPtable at mem xfffxffff mapped at fffffffFeb localhost kernel initmemorymapping memxxfffffThese messages describe the processor that the kernel has detected and show howthe RAM is mapped Note that the kernel is aware that its booting within a hypervisor and is not actually running on bare hardwareFeb localhost kernel ACPI bus type PCI registeredFeb localhost kernel acpiphp ACPI Hot Plug PCI ControllerDriver version Feb localhost kernel PCI host bridge to bus Feb localhost kernel pcibus root bus resource busffFeb localhost kernel pcibus root bus resource ioxxffffFeb localhost kernel pcibus root bus resourcemem xxfffffffffFeb localhost kernel SCSI subsystem initializedFeb localhost kernel ACPI bus type USB registeredFeb localhost kernel usbcore registered new interface driverusbfsFeb localhost kernel PCI Using ACPI for IRQ routingHere the kernel initializes the systems various data buses including the PCI busand the USB subsystemFeb localhost kernel Nonvolatile memory driver vFeb localhost kernel Linux agpgart interface vFeb localhost kernel crash memory driver version Feb localhost kernel rdac device handler registeredFeb localhost kernel hpsw device handler registeredFeb localhost kernel emc device handler registeredFeb localhost kernel alua device handler registeredFeb localhost kernel libphy Fixed MDIO Bus probedFeb localhost kernel usbserial USB Serial supportregistered for genericFeb localhost kernel i PNP PS ControllerPNPPSKPNPfPSM at xx irq Feb localhost kernel serio i KBD port xx irq Feb localhost kernel serio i AUX port xx irq Feb localhost kernel mousedev PS mouse device common forall miceFeb localhost kernel input AT Translated Set keyboard as devicesplatformiserioinputinputFeb localhost kernel rtccmos rtccmos rtc core registeredrtccmos as rtcFeb localhost kernel rtccmos rtccmos alarms up to oneday bytes nvramFeb localhost kernel cpuidle using governor menuFeb localhost kernel usbhid USB HID core driverThese messages document the kernels discovery of various devices including thepower button a USB hub a mouse and a realtime clock RTC chip Some of thedevices are metadevices rather than actual hardware these constructs managegroups of real related hardware devices For example the usbhid USB HumanInterface Device driver manages keyboards mice tablets game controllers andother types of input devices that follow USB reporting standardsFeb localhost kernel dropmonitor Initializing network dropmonitor serviceFeb localhost kernel TCP cubic registeredFeb localhost kernel Initializing XFRM netlink socketFeb localhost kernel NET Registered protocol family Feb localhost kernel NET Registered protocol family In this phase the kernel initializes a variety of network drivers and facilitiesThe drop monitor is a Red Hat kernel subsystem that implements comprehensivemonitoring of network packet loss TCP cubic is a congestioncontrol algorithmoptimized for highlatency highbandwidth connections socalled long fat pipesAs mentioned on page Netlink sockets are a modern approach to communication between the kernel and userlevel processes The XFRM Netlink socket is thelink between the userlevel IPsec process and the kernels IPsec routinesThe last two lines document the registration of two additional network protocolfamiliesFeb localhost kernel Loading compiledin X certificatesFeb localhost kernel Loaded X cert CentOS Linux kpatchsigning key eacdedebdcafefcefFeb localhost kernel Loaded X cert CentOS Linux Driverupdate signing key feeabbbdbeabFeb localhost kernel Loaded X cert CentOS Linux kernelsigning key adacacfbaabFeb localhost kernel registered taskstats version Feb localhost kernel Key type trusted registeredFeb localhost kernel Key type encrypted registeredLike other OSs CentOS provides a way to incorporate and validate updates Thevalidation portion uses X certificates that are installed into the kernelFeb localhost kernel IMA No TPM chip found activatingTPMbypassFeb localhost kernel rtccmos rtccmos setting system clockto UTC Here the kernel reports that its unable to find a Trusted Platform Module TPMon the system TPM chips are cryptographic hardware devices that provide for secure signing operations When used properly they can make it much more difficultto hack into a systemFor example the TPM can be used to sign kernel code and to make the systemrefuse to execute any portion of the code for which the current signature doesntmatch the TPM signature This measure helps avoid the execution of maliciouslyinjected code An admin who expects to have a working TPM would be unhappyto see this messageThe last message shows the kernel setting the batterybacked realtime clock to thecurrent time of day This is the same RTC that we saw mentioned earlier when itwas identified as a deviceFeb localhost kernel e IntelR PRO NetworkDriver version kNAPIFeb localhost kernel e Copyright c IntelCorporationFeb localhost kernel e ethPCIMHzbit daefFeb localhost kernel e eth IntelRPRO Network ConnectionNow the kernel has found the gigabit Ethernet interface and initialized it The interfaces MAC address daef might be of interest to you if you wanted this machine to obtain its IP address through DHCP Specific IP addresses areoften locked to specific MACs in the DHCP server configuration so that serverscan have IP address continuityFeb localhost kernel scsi host atapiixFeb localhost kernel ata PATA max UDMA cmd xf ctlxf bmdma xd irq Feb localhost kernel ahci d flags bit ncqstag only cccFeb localhost kernel scsi host ahciFeb localhost kernel ata SATA max UDMA abarmxf port xf irq Feb localhost kernel ata ATAPI VBOX CDROM maxUDMAFeb localhost kernel ata configured for UDMAFeb localhost kernel scsi CDROM VBOXCDROM PQ ANSI Feb localhost kernel tsc Refined TSC clocksourcecalibration MHzFeb localhost kernel ata SATA link up Gbps SStatus SControl Feb localhost kernel ata ATA VBOX HARDDISK max UDMAFeb localhost kernel ata sectors multi LBA NCQ depth Feb localhost kernel ata configured for UDMAFeb localhost kernel scsi DirectAccess ATAVBOX HARDDISK PQ ANSI Feb localhost kernel sr sr scsimmc drivexx xaform trayFeb localhost kernel cdrom Uniform CDROM driver RevisionFeb localhost kernel sd sda bytelogical blocks GB GiBFeb localhost kernel sd sda Attached SCSI diskSee page formore informationabout DHCPFeb localhost kernel SGI XFS with ACLs security attributesno debug enabledFeb localhost kernel XFS dm Mounting V FilesystemFeb localhost kernel XFS dm Ending clean mountHere the kernel recognizes and initializes various drives and support devices harddisk drives a SCSIbased virtual CDROM and an ATA hard disk It also mounts afilesystem XFS that is part of the devicemapper subsystem the dm filesystemAs you can see Linux kernel boot messages are verbose almost to a fault Howeveryou can rest assured that youll see everything the kernel is doing as it starts up amost useful feature if you encounter problemsFreeBSD boot messagesThe log below is from a FreeBSD RELEASE system that runs the kernel shippedwith the release Much of the output will look eerily familiar the sequence of eventsis quite similar to that found in Linux One notable difference is that the FreeBSDkernel produces far fewer boot messages than does Linux Compared to LinuxFreeBSD is downright taciturnSep bucephalus kernel FreeBSD RELEASE r FriMar UTC Sep bucephalus kernel rootrelengnyifreebsdorgusrobjusrsrcsysGENERIC amdSep bucephalus kernel FreeBSD clang version tagsRELEASEdotfinal The initial messages above tell you the OS release the time at which the kernel wasbuilt from source the name of the builder the configuration file that was used andfinally the compiler that generated the code Clang version Sep bucephalus kernel real memory MBSep bucephalus kernel avail memory MBAbove are the systems total amount of memory and the amount thats availableto userspace code The remainder of the memory is reserved for the kernel itselfTotal memory of MB probably looks a bit strange However this FreeBSD instance is running under a hypervisor The amount of real memory is an arbitraryvalue that was set when the virtual machine was configured On baremetal systemsthe total memory is likely to be a power of since thats how actual RAM chips aremanufactured eg MBSep bucephalus kernel vgapci VGAcompatible displaymem xexeffffff irq at device on pciSep bucephalus kernel vgapci Boot video deviceTheres the default video display which was found on the PCI bus The output showsthe memory range to which the frame buffer has been mapped Well compiler front end really But lets not quibbleSep bucephalus kernel em IntelR PRO LegacyNetwork Connection port xdxd mem xfxfffff irq at device on pciSep bucephalus kernel em Ethernet addressbfcAnd above the Ethernet interface along with its hardware MAC addressSep bucephalus kernel usbus Mbps Full Speed USB vSep bucephalus kernel ugen Apple at usbusSep bucephalus kernel uhub Apple OHCI root HUB class rev addr on usbusSep bucephalus kernel ada at ata bus scbus tgt lun Sep bucephalus kernel cd at ata bus scbus tgt lun Sep bucephalus kernel cd VBOX CDROM RemovableCDROM SCSI deviceSep bucephalus kernel cd Serial Number VBSep bucephalus kernel cd MBs transfers UDMAATAPI bytes PIO bytesSep bucephalus kernel cd Attempt to query device sizefailed NOT READY Medium not presentSep bucephalus kernel ada VBOX HARDDISK ATAdeviceSep bucephalus kernel ada Serial NumberVBcfbcSep bucephalus kernel ada MBs transfers UDMAPIO bytesSep bucephalus kernel ada MB bytesectorsSep bucephalus kernel ada Previously was known as adAs shown above the kernel initializes the USB bus the USB hub the CDROMdrive actually a DVDROM drive but virtualized to look like a CDROM andthe ada disk driverSep bucephalus kernel random unblocking deviceSep bucephalus kernel Timecounter TSClow frequency Hz quality Sep bucephalus kernel Root mount waiting for usbusSep bucephalus kernel uhub ports with removableself poweredSep bucephalus kernel Trying to mount root from ufsdevadap rwThe final messages in the FreeBSD boot log document a variety of odds and endsThe random pseudodevice harvests entropy from the system and generates random numbers The kernel seeded its number generator and put it in nonblockingmode A few other devices came up and the kernel mounted the root filesystemAt this point the kernel boot messages end Once the root filesystem has beenmounted the kernel transitions to multiuser mode and initiates the userlevelSee page formore comments onthe random driverstartup scripts Those scripts in turn start the system services and make the system available for use Booting alternate kernels in the cloudCloud instances boot differently from traditional hardware Most cloud providerssidestep GRUB and use either a modified open source boot loader or some kindof scheme that avoids the use of a boot loader altogether Therefore booting an alternate kernel on a cloud instance usually requires that you interact with the cloudproviders web console or APIThis section briefly outlines some of the specifics that relate to booting and kernelselection on our example cloud platforms For a more general introduction to cloudsystems see Chapter Cloud ComputingOn AWS youll need to start with a base AMI Amazon machine image that usesa boot loader called PVGRUB PVGRUB runs a patched version of legacy GRUBand lets you specify the kernel in your AMIs menulst fileAfter compiling a new kernel edit bootgrubmenulst to add it to the boot listdefault fallback timeout hiddenmenutitle My Linux Kernelroot hdkernel bootmyvmlinuz rootLABEL consolehvcinitrd bootmyinitrdimgtitle Amazon Linuxroot hdkernel bootvmlinuzamznx rootLABEL consolehvcinitrd bootinitramfsamznximgHere the custom kernel is the default and the fallback option points to the standardAmazon Linux kernel Having a fallback helps ensure that your system can booteven if your custom kernel cant be loaded or doesnt work correctly See AmazonECs User Guide for Linux Instances for more details on this processHistorically DigitalOcean bypassed the boot loader through a QEMU short forQuick Emulator feature that allowed a kernel and RAM disk to be loaded directlyinto a droplet Thankfully DigitalOcean now allows droplets to use their own bootloaders Most modern operating systems are supported including CoreOS FreeBSDFedora Ubuntu Debian and CentOS Changes to boot options including selection of the kernel are handled by the respective OS boot loaders GRUB usuallyGoogle Cloud Platform GCP is the most flexible platform when it comes to bootmanagement Google lets you upload complete system disk images to your ComputeEngine account Note that in order for a GCP image to boot properly it must usethe MBR partitioning scheme and include an appropriate installed boot loaderUEFI and GPT do not apply hereThe cloudgooglecomcomputedocscreatingcustomimage tutorial on buildingimages is incredibly thorough and specifies not only the required kernel optionsbut also the recommended settings for kernel security Kernel errorsKernel crashes aka kernel panics are an unfortunate reality that can happen evenon properly configured systems They have a variety of causes Bad commands entered by privileged users can certainly crash the system but a more common causeis faulty hardware Physical memory failures and hard drive errors bad sectors ona platter or device are both notorious for causing kernel panicsIts also possible for bugs in the implementation of the kernel to result in crashesHowever such crashes are exceedingly rare in kernels anointed as stable Devicedrivers are another matter however They come from many different sources andare often of lessthanexemplary code qualityIf hardware is the underlying cause of a crash keep in mind that the crash may haveoccurred long after the device failure that sparked it For example you can oftenremove a hotswappable hard drive without causing immediate problems The system continues to hum along without much complaint until you try to reboot orperform some other operation that depends on that particular driveDespite the names panic and crash kernel panics are usually relatively structured events Userspace programs rely on the kernel to police them for many kindsof misbehavior but the kernel has to monitor itself Consequently kernels includea liberal helping of sanitychecking code that attempts to validate important datastructures and invariants in passing None of those checks should ever fail if they doits sufficient reason to panic and halt the system and the kernel does so proactivelyOr at least thats the traditional approach Linux has liberalized this rule somewhatthrough the oops system see the next sectionLinux kernel errorsLinux has four varieties of kernel failure soft lockups hard lockups panics and theinfamous Linux oops Each one of these usually provides a complete stack traceexcept for certain soft lockups that are recoverable without a panicA soft lockup occurs when the system is in kernel mode for more than a few seconds thus preventing userlevel tasks from running The interval is configurablebut it is usually around seconds which is a long time for a process to be deniedCPU cycles During a soft lockup the kernel is the only thing running but it is stillservicing interrupts such as those from network interfaces and keyboards Data isstill flowing in and out of the system albeit in a potentially crippled fashionA hard lockup is the same as a soft lockup but with the additional complicationthat most processor interrupts go unserviced Hard lockups are overtly pathological conditions that are detected relatively quickly whereas soft lockups can occureven on correctly configured systems that are experiencing some kind of extremecondition such as a high CPU loadIn both cases a stack trace and a display of the CPU registers a tombstone areusually dumped to the console The trace shows the sequence of function calls thatresulted in the lockup In most cases this trace tells you quite a bit about the causeof the problemA soft or hard lockup is almost always the result of a hardware failure the mostcommon culprit being bad memory The second most common reason for a softlockup is a kernel spinlock that has been held too long however this situation normally occurs only with nonstandard kernel modules If you are running any unusualmodules try unloading them and see if the problem recursWhen a lockup occurs the usual behavior is for the system to stay frozen so thatthe tombstone remains visible on the console But in some environments its preferable to have the system panic and thus reboot For example an automated testrig needs systems to avoid hanging so these systems are often configured to rebootinto a safe kernel after encountering a lockupsysctl can configure both soft and hard lockups to paniclinux sudo sysctl kernelsoftlockuppaniclinux sudo sysctl kernelnmiwatchdogYou can set these parameters at boot by listing them in etcsysctlconf just likeany other kernel parameterThe Linux oops system is a generalization of the traditional UNIX panic afterany anomaly approach to kernel integrity Oops doesnt stand for anything its justthe English word oops as in Oops I zeroed out your SAN again Oopses in theLinux kernel can lead to a panic but they neednt always If the kernel can repairor address an anomaly through a less drastic measure such as killing an individualprocess it might do that insteadWhen an oops occurs the kernel generates a tombstone in the kernel message buffer thats viewable with the dmesg command The cause of the oops is listed at thetop It might be something like unable to handle kernel paging request at virtualaddress xYou probably wont be debugging your own kernel oopses However your chanceof attracting the interest of a kernel or module developer is greatly increased ifyou do a good job of capturing the available context and diagnostic informationincluding the full tombstoneThe most valuable information is at the beginning of the tombstone That fact canpresent a problem after a fullscale kernel panic On a physical system you maybe able to just go to the console and page up through the history to see the fulldump But on a virtual machine the console might be a window that becomes frozen when the Linux instance panics it depends on the hypervisor If the text of thetombstone has scrolled out of view you wont be able to see the cause of the crashOne way to minimize the likelihood of information loss is to increase the resolutionof the console screen Weve found that a resolution of is adequate todisplay the full text of most kernel panicsYou can set the console resolution by modifying etcgrubgrubcfg and addingvga as a kernel startup parameter for the kernel you want to boot You canalso set the resolution by adding this clause to the kernel command line fromGRUBs boot menu screen The latter approach lets you test the waters withoutmaking any permanent changesTo make the change permanent find the menu item with the boot command forthe kernel that you wish to boot and modify it For example if the boot commandlooks like thislinux vmlinuzelx rootdevmappercentosroot ro rdlvmlvcentosroot rdlvmlvcentosswap crashkernelautobiosdevname netifnames LANGenUSUTFthen simply modify it to add the vga parameter at the endlinux vmlinuzelx rootdevmappercentosroot ro rdlvmlvcentosroot rdlvmlvcentosswap crashkernelautobiosdevname netifnames LANGenUSUTF vgaOther resolutions can be achieved by setting the vga boot parameter to other values Table lists the possibilitiesTable VGA mode valuesColor depth bitsGeometry x x x x x x FreeBSD kernel panicsFreeBSD does not divulge much information when the kernel panics If you arerunning a generic kernel from a production release the best thing to do if you areencountering regular panics is to instrument the kernel for debugging Rebuild thegeneric kernel with makeoptions DEBUGg enabled in the kernel configurationand reboot with that new kernel Once the system panics again you can use kgdbto generate a stack trace from the resulting crash dump in varcrashOf course if youre running unusual kernel modules and the kernel doesnt panicwhen you dont load them thats a good indication of where the issue liesAn important note crash dumps are the same size as real physical memory so youmust ensure that varcrash has at least that much space available before you enablethese dumps There are ways to get around this though for more information seethe man pages for dumpon and savecore and the dumpdev variable in etcrcconf Recommended readingYou can visit lwnnet for the latest information on what the kernel community isdoing In addition we recommend the following booksBovet Daniel P and Marco Cesati Understanding the Linux Kernel rd Edition Sebastopol CA OReilly Media Love Robert Linux Kernel Development rd Edition Upper Saddle River NJAddisonWesley Professional McKusick Marshall Kirk et al The Design and Implementation of the FreeBSD Operating System nd Edition Upper Saddle River NJ AddisonWesleyProfessional Rosen Rami Linux Kernel Networking Implementation and Theory Apress Printing is a necessary evil No one wants to deal with it but every user wants toprint For better or worse printing on UNIX and Linux systems typically requires atleast some configuration and occasionally some coddling by a system administratorAges ago there were three common printing systems BSD System V and CUPSthe Common UNIX Printing System Today Linux and FreeBSD both use CUPSan uptodate sophisticated network and securityaware printing system CUPSincludes a modern browserbased GUI as well as shelllevel commands that allowthe printing system to be controlled by scriptsBefore we start a general point system administrators often consider printing alower priority than do users Administrators are accustomed to reading documentsonline but users often need hard copy and they want the printing system to work of the time Satisfying these desires is one of the easier ways for sysadmins toearn Brownie points with usersPrinting relies on a handful of pieces A print spooler that collects and schedules jobs The word spool originated as an acronym for Simultaneous Peripheral Operation OnLineNow its just a generic term Printing Userlevel utilities commandline interfaces or GUIs that talk to thespooler These utilities send jobs to the spooler query the system aboutjobs both pending and complete remove or reschedule jobs and configure the other parts of the system Back ends that talk to the printing devices themselves These are normallyunseen and hidden under the floorboards A network protocol that lets spoolers communicate and transfer jobsModern environments often use networkattached printers that minimize theamount of setup and processing that must be done on the UNIX or Linux side CUPS printingCUPS was created by Michael Sweet and has been adopted as the default printingsystem for Linux FreeBSD and macOS Michael has been at Apple since where he continues to develop CUPS and its ecosystemJust as newer mail transport systems include a command called sendmail that letsolder scripts and older system administrators work as they always did back insendmails glory days CUPS supplies traditional commands such as lp and lpr thatare backward compatible with legacy UNIX printing systemsCUPS servers are also web servers and CUPS clients are web clients The clientscan be commands such as the CUPS versions of lpr and lpq or they can be applications with their own GUIs Under the covers theyre all web apps even if theyretalking only to the CUPS daemon on the local system CUPS servers can also actas clients of other CUPS serversA CUPS server offers a web interface to its full functionality on port For administrators a web browser is usually the most convenient way to manage the systemjust navigate to httpprinthost If you need secure communication with thedaemon and your system offers it use httpsprinthost instead Scripts canuse discrete commands to control the system and users normally access it througha GNOME or KDE interface These routes are all equivalentHTTP is the underlying protocol for all interactions among CUPS servers and theirclients Actually its the Internet Printing Protocol a soupedup version of HTTPClients submit jobs with the HTTPIPP POST operation and request status withHTTPIPP GET The CUPS configuration files also look suspiciously similar toApache web server configuration filesInterfaces to the printing systemCUPS printing is often done from a GUI and administration is often done througha web browser As a sysadmin though you and perhaps some of your hardcoreterminal users might want to use shelllevel commands as well CUPS includesworkalike commands for many of the basic shelllevel printing commands of thelegacy BSD and System V printing systems Unfortunately CUPS doesnt necessarilyemulate all the bells and whistles Sometimes it emulates the old interfaces entirely too well instead of giving you a quick usage summary lpr help and lp helpjust print error messagesHeres how you might print the files foopdf and tmptestprintps to your defaultprinter under CUPS lpr foopdf tmptestprintpsThe lpr command transmits copies of the files to the CUPS server cupsd whichstores them in the print queue CUPS processes each file in turn as the printer becomes availableWhen printing CUPS examines both the document and the printers PostScript Printer Description PPD file to see what needs to be done to get the document to printproperly Despite the name PPD files are used even for nonPostScript printersTo prepare a job for printing on a specific printer CUPS passes it through a seriesof filters For example one filter might reformat the job so that two reducedsizepage images print on each physical page aka up printing and another mighttransform the job from PostScript to PCL Filters can also do printerspecific processing such as printer initialization Some filters perform rasterization turningabstract instructions such as draw a line across the page into a bitmap image Suchrasterizers are useful for printers that do not include their own rasterizers or thatdont speak the language in which a job was originally submittedThe final stage of the print pipeline is a back end that transmits the job from thehost to the printer through an appropriate protocol such as Ethernet The back endalso communicates status information in the other direction back to the CUPSserver After transmitting the print job the CUPS daemon returns to processingits queues and handling requests from clients and the printer goes off to print thejob it was shippedThe print queuecupsds centralized control of the printing system makes it easy to understandwhat the userlevel commands are doing For example the lpq command requestsjob status information from the server and reformats it for display Other CUPSclients ask the server to suspend cancel or reprioritize jobs They can also movejobs from one queue to anotherMost changes require jobs to be identified by their job number which you can getfrom lpq For example to remove a print job just run lprm jobidlpstat t summarizes the print servers overall statusMultiple printers and queuesThe CUPS server maintains a separate queue for each printer Commandline clientsaccept an option typically P printer or p printer by which you specify the queueto address You can also set a default printer for yourself by setting the PRINTERenvironment variable export PRINTERprinternameor by telling CUPS to use a particular default for your account lpoptions dprinternameWhen run as root lpoptions sets systemwide defaults in etccupslpoptions butits more typically used by individual nonroot users lpoptions lets each user definepersonal printer instances and defaults which it stores in cupslpoptions Thecommand lpoptions l lists the current settingsPrinter instancesIf you have only one printer but want to use it in several wayssay both for quickdrafts and for final production workCUPS lets you set up different printer instances for these different usesFor example if you already have a printer named Phaser the command lpoptions p Phaserup o numberup o jobsheetsstandardcreates an instance named Phaserup that performs up printing and addsbanner pages Once the instance has been created the command lpr P Phaserup biglistingpsprints the PostScript file biglistingps as a up job with a banner pageNetwork printer browsingFrom CUPSs perspective a network of machines isnt very different from an isolated machine Every computer runs a cupsd and all the CUPS daemons talk toone anotherIf youre working on the command line you configure a CUPS daemon to acceptprint jobs from remote systems by editing the etccupscupsdconf file see page By default servers that are set up this way broadcast information every seconds about the printers they serve As a result computers on the local networkautomatically learn about the printers that are available to them You can effectthe same configuration by clicking a checkbox in the CUPS GUI in your browserIf someone has plugged in a new printer if youve brought your laptop into work orif youve just installed a new workstation you can tell cupsd to redetermine whatprinting services are available click the Find New Printers button in the Administration tab of the CUPS GUIBecause broadcast packets do not cross subnet boundaries its a bit tricker to makeprinters available to multiple subnets One solution is to designate on each subneta slave server that polls the other subnets servers for information and then relaysthat information to machines on the local subnetFor example suppose the print servers allie and jj liveon different subnets and we want both of them to be accessible to users on a thirdsubnet We designate a slave server say copeland andadd these lines to its cupsdconf fileBrowsePoll allieBrowsePoll jjBrowseRelay The first two lines tell the slaves cupsd to poll the cupsds on allie and jj for information about the printers they serve The third line tells copeland to relay the information it learns to its own subnet SimpleFiltersRather than using a specialized printing tool for every printer CUPS uses a chain offilters to convert each printed file into a form the destination printer can understandThe CUPS filter scheme is elegant Given a document and a target printer CUPSuses its types files to figure out the documents MIME type It consults the printersPPD file to figure out what MIME types the printer can handle It then uses convsfiles to deduce what filter chains could convert one format to the other and whateach prospective chain would cost Finally it picks a chain and passes the document through those filters The final filter in the chain passes the printable formatto a back end which transmits the data to the printer through whatever hardwareor protocol the printer understandsWe can flesh out that process a bit CUPS uses rules in usrsharecupsmimemimetypesto figure out the incoming data type For example the ruleapplicationpdf pdf string PDFmeans If a file has a pdf extension or starts with the string PDF then its MIMEtype is applicationpdfCUPS figures out how to convert one data type to another by looking up rules inthe file mimeconvs usually in etccups or usrsharecupsmime For exampleapplicationpdf applicationpostscript pdftopsmeans To convert an applicationpdf file to an applicationpostscript file run thefilter pdftops The number is the cost of the conversion When CUPS finds thatseveral filter chains can convert a file from one type to another it picks the chainwith the lowest total cost Costs are chosen by whoever creates the mimeconvsfilethe distribution maintainers perhaps If you want to spend time tuning thembecause you think you can do a better job you may have too much free timeThe last component in a CUPS pipeline is a filter that talks directly to the printerIn the PPD of a nonPostScript printer you might see lines such ascupsFilter applicationvndcupspostscript foomaticripor evencupsFilter applicationvndcupspostscript foomaticripThe quoted string has the same format as a line in mimeconvs but theres onlyone MIME type instead of two This line advertises that the foomaticrip filterconverts data of type applicationvndcupspostscript to the printers native dataformat The cost is zero or omitted because theres only one way to do this stepso why pretend theres a cost Some PPDs for nonPostScript printers like thosefrom the Gutenprint project are slightly differentTo find the filters available on your system try running locate pstops pstops is apopular filter that massages PostScript jobs in various ways such as adding a PostScript command to set the number of copies Wherever you find pstops the otherfilters wont be far awayYou can ask CUPS for a list of the available back ends by running lpinfo v If yoursystem lacks a back end for the network protocol you need it may be available fromthe web or from your Linux distributor CUPS server administrationcupsd starts at boot time and runs continuously All our example systems are setup this way by defaultThe CUPS configuration file cupsdconf is usually found in etccups The fileformat is similar to that of the Apache configuration file If youre comfortablewith one of these files youll be comfortable with the other You can view and editcupsdconf with a text editor or once again from the CUPS web GUIThe default config file is well commented The comments and the cupsdconf manpage are good enough that we wont belabor the details of configuration hereCUPS reads its configuration file only at startup If you change the contents ofcupsdconf you must restart cupsd for changes to take effect If you make changesthrough cupsds web GUI cupsd restarts automaticallyNetwork print server setupIf youre having trouble printing over the network review the browserbased CUPSGUI and make sure youve checked all the right boxes Possible problem areas include an unpublished printer a CUPS server that isnt broadcasting its printers tothe network or a CUPS server that wont accept network print jobsSee page fordetails about ApacheconfigurationIf youre editing the cupsdconf file directly youll need to make a couple of changes First changeLocation Order DenyAllowDeny From AllAllow From LocationtoLocation Order DenyAllowDeny From AllAllow From Allow From netaddressLocationReplace netaddress with the IP address of the network from which you want to accept jobs eg Then look for the BrowseAddress keyword and set it to the broadcast address onthat network plus the CUPS port for exampleBrowseAddress These steps tell the server to accept requests from any machine on the designatedsubnet and to broadcast what it knows about the printers its serving to every CUPSdaemon on that network Thats it Once you restart cupsd it comes back as a serverPrinter autoconfigurationYou can use CUPS without a printer eg to convert files to PDF or fax formatbut its typical role is to manage real printers In this section we review the ways inwhich you can deal with the printers per seIn some cases adding a printer is trivial CUPS autodetects USB printers whentheyre plugged into the system and figures out what to do with themEven if you have to do some configuration work yourself adding a printer is oftenno more painful than plugging in the hardware connecting to the CUPS web interface at localhostadmin and answering a few questions KDE and GNOMEcome with their own printer configuration widgets which you may prefer to theCUPS interface We like the CUPS GUIIf someone else adds a printer and one or more CUPS servers running on the network know about it your CUPS server will learn of its existence You dont haveto explicitly add the printer to the local inventory or copy PPDs to your machineIts magicNetwork printer configurationNetwork printersthat is printers whose primary hardware interface is an Ethernetjack or WiFi radioneed some configuration of their own just to be proper citizensof the TCPIP network In particular they need to know their own IP addressesand netmasks That information is usually conveyed to them in one of two waysNetwork printers can get this information from a BOOTP or DHCP server and thismethod works well in environments that have many such printers See DHCP theDynamic Host Configuration Protocol on page for more information about DHCPAlternatively you can assign the printer a static IP address from its console whichusually consists of a set of buttons on the printers front panel and a oneline displayFumble around with the menus until you discover where to set the IP address Ifthere is a menu option to print the menus use it and put the printed version underneath the printer for future referenceOnce configured network printers usually have a web console thats accessible froma browser However printers must have an IP address and must be up and runningon the network before you can access them this way so this interface is unavailablejust when its most neededPrinter configuration examplesBelow we add the parallel printer groucho and the network printer fezmo fromthe command line sudo lpadmin p groucho E v paralleldevlp m pxlcolorppd sudo lpadmin p fezmo E v socket m laserjetppdGroucho is attached to port devlp and fezmo is at IP address Wespecify each device in the form of a universal resource indicator URI and choosean appropriate PPD from the ones in usrsharecupsmodelAs long as cupsd has been configured as a network server it immediately makesthe new printers available to other clients on the network No restart is requiredCUPS accepts a wide variety of URIs for printers Here are a few more examples ippzoeadmincomipp lpdrileyadmincomps serialdevttySbaudparityevenbits socketgillianadmincom usbXEROXPhaserserialYGGSome types take options eg serial and others dont lpinfo v lists the devicesyour system can see and the types of URIs that CUPS understandsService shutoffRemoving a printer is easily done with lpadmin x sudo lpadmin x fezmoOK but what if you just want to disable a printer temporarily for service insteadof removing it You can block the print queue at either end If you disable the tailthe exit or printer side of the queue users can still submit jobs but the jobs wontprint until the outlet is reenabled If you disable the head the entrance of thequeue jobs that are already in the queue can still print but the queue rejects attempts to submit new jobsThe cupsdisable and cupsenable commands control the exit side of the queue andthe reject and accept commands control the submission side For example sudo cupsdisable groucho sudo reject corbetWhich to use Its a bad idea to accept print jobs that have no hope of being printedin the foreseeable future so use reject for extended downtime For brief interruptionsthat should be invisible to users eg changing a toner cartridge use cupsdisableAdministrators occasionally ask for a mnemonic to help them remember whichcommands control which end of the queue Consider if CUPS rejects a job thatmeans you cant inject it Another way to keep the commands straight is to remember that accepting and rejecting are things you can do to print jobs whereasdisabling and enabling are things you can do to printers It doesnt make any senseto accept a printer or queueCUPS itself sometimes temporarily disables a printer that its having trouble witheg if someone has dislodged a cable Once you fix the problem remember torecupsenable the queue If you forget lpstat will tell you For a complete discussion of this issue and an alternative approach see linuxprintingorgbehhtmlOther configuration tasksTodays printers are eminently configurable and CUPS lets you tweak a wide variety of features through its web interface and through the lpadmin and lpoptionscommands As a rule of thumb lpadmin is for systemwide tasks and lpoptionsis for peruser taskslpadmin can restrict access to printers and queues For example you can set upprinting quotas and specify which users can print to which printersTable lists the commands that come with CUPS and classifies them accordingto their originTable CUPS commandline utilities and their originsCommand FunctionCUPScupsconfig Prints API compiler directory and link informationcupsdisable a Stops printing on a printercupsenable a Restarts printing on a printerlpinfo Shows available devices or driverslpoptions Displays or sets printer options and defaultslppasswd Adds changes or deletes digest passwordsSystem Vaccept reject Accepts or rejects queue submissionscancel Cancels print jobslp Queues jobs for printinglpadmin Configures printerslpmove Moves an existing print job to a new destinationlpstat Prints status informationBSDlpc Acts as a general printercontrol programlpq Displays print queueslpr Queues jobs for printinglprm Cancels print jobsa These are actually just the disable and enable commands from System V renamed Troubleshooting tipsPrinters combine all the foibles of a mechanical device with all the communicationeccentricities of a foreign operating system They and the software that drives themseem dedicated to creating problems for you and your users The next sections offersome general tips for dealing with printer adversityPrint daemon restartAlways remember to restart daemons after changing a configuration fileYou can restart cupsd in whatever way your system normally restarts daemons usually systemctl restart orgcupscupsdservice or a similar incantation In theoryyou can also send cupsd a HUP signal Alternatively you can use the CUPS GUILog filesCUPS maintains three logs a page log an access log and an error log The pagelog lists the pages that CUPS has printed The other two logs are just like the access log and error log for Apache which should not be surprising since the CUPSserver is a web serverThe cupsdconf file specifies the logging level and the locations of the log filesTheyre all typically kept underneath varlogHeres an excerpt from a log file that corresponds to a single print jobI June Adding start banner page none to job I June Adding end banner page none to job I June Job queued on Phaser by jshI June Started filter usrlibexeccupsfilterpstopsPID for job I June Started backend usrlibexeccupsbackendusbPID for job Direct printing connectionsUnder CUPS to verify the physical connection to a local printer you can directlyrun the printers back end For example heres what we get when we execute theback end for a USBconnected printer usrlibcupsbackendusbdirect usb Unknown USB Printer usbdirect usbXEROXPhaserserialYGG XEROX Phaser Phaser When the USB cable for the Phaser is disconnected that printer drops out ofthe back ends output usrlibcupsbackendusbdirect usb Unknown USB Printer usbNetwork printing problemsTo begin tracking down a network printing problem first try connecting to theprinter daemon You can connect to cupsd with a web browser hostname orwith the telnet command telnet hostname If you have problems debugging a network printer connection keep in mind thatthere must be a queue for the job on some machine a way to decide where to sendthe job and a method of sending the job to the machine that hosts the print queueOn the print server there must be a place to queue the job sufficient permissionsto allow the job to be printed and a way to output to the deviceAny and all of these prerequisites will at some point go awry Be prepared to huntfor problems in many places including these System log files on the sending machine for name resolution and permission problems System log files on the print server for permission problems Log files on the sending machine for missing filters unknown printersmissing directories etc The print daemons log files on the print servers machine for messagesabout bad device names incorrect formats etc The printer log file on the printing machine for errors in transmitting the job The printer log file on the sending machine for errors about preprocessing or queuing the jobThe locations of CUPS log files are specified in etccupscupsdconf See Chapter Logging for general information about log management Recommended readingCUPS comes with a lot of documentation in HTML format An excellent way toaccess it is to connect to a CUPS server and click the link for online help Of coursethis isnt any help if youre consulting the documentation to figure out why you cantconnect to the CUPS server On your computer the documents should be installedin usrsharedoccups in both HTML and PDF formats If they arent there askyour distributions package manager or look on cupsorgShah Ankur CUPS Administrative Guide A practical tutorial to installing managingand securing this powerful printing system Birmingham UK Packt Publishing SECTION TWONETWORKINGIt would be hard to overstate the importance of networks to modern computingalthough that doesnt seem to stop people from trying At many sitesperhaps eventhe majorityweb and email access are the primary uses of computers As of internetworldstatscom estimates the Internet to have more than billion usersor just slightly less than half of the worlds population In North America Internetpenetration approaches TCPIP Transmission Control ProtocolInternet Protocol is the networking systemthat underlies the Internet TCPIP does not depend on any particular hardware oroperating system so devices that speak TCPIP can all exchange data interoperate despite their many differencesTCPIP works on networks of any size or topology whether or not they are connected to the outside world This chapter introduces the TCPIP protocols in the context of the Internet but standalone networks are quite similar at the TCPIP level TCPIP and its relationship to the InternetTCPIP and the Internet share a history that goes back multiple decades The technical success of the Internet is due largely to the elegant and flexible design of TCPIP TCPIP Networkingand its open and nonproprietary protocol suite In turn the leverage provided bythe Internet has helped TCPIP prevail over several competing protocol suites thatwere favored at one time or another for political or commercial reasonsThe progenitor of the modern Internet was a research network called ARPANETestablished in by the US Department of Defense By the end of the s thenetwork was no longer a research project and we transitioned to the commercialInternet Todays Internet is a collection of private networks owned by Internet service providers ISPs that interconnect at many socalled peering pointsWho runs the InternetOversight of the Internet and the Internet protocols has long been a cooperative andopen effort but its exact structure has changed as the Internet has evolved into apublic utility and a driving force in the world economy Current Internet governanceis split roughly into administrative technical and political wings but the boundaries between these functions are often vague The major players are listed below ICANN the Internet Corporation for Assigned Names and Numbers ifany one group can be said to be in charge of the Internet this is probably itIts the only group with any sort of actual enforcement capability ICANNcontrols the allocation of Internet addresses and domain names along withvarious other snippets such as protocol port numbers It is organized as anonprofit corporation headquartered in California icannorg ISOC the Internet Society ISOC is an openmembership organizationthat represents Internet users Although it has educational and policyfunctions its best known as the umbrella organization for the technicaldevelopment of the Internet In particular it is the parent organization ofthe Internet Engineering Task Force ietforg which oversees most technical work ISOC is an international nonprofit organization with officesin Washington DC and Geneva isocorg IGF the Internet Governance Forum a relative newcomer the IGF wascreated by the United Nations in to establish a home for international and policyoriented discussions related to the Internet Its currently structured as a yearly conference series but its importance is likely togrow over time as governments attempt to exert more control over theoperation of the Internet intgovforumorgOf these groups ICANN has the toughest job establishing itself as the authority incharge of the Internet undoing the mistakes of the past and foreseeing the futureall while keeping users governments and business interests happyNetwork standards and documentationIf your eyes havent glazed over just from reading the title of this section youveprobably already had several cups of coffee Nonetheless accessing the Internetsauthoritative technical documentation is a crucial skill for system administratorsand its more entertaining than it soundsThe technical activities of the Internet community are summarized in documentsknown as Requests for Comments or RFCs Protocol standards proposed changesand informational bulletins all usually end up as RFCs RFCs start their lives as Internet Drafts and after lots of email wrangling and IETF meetings they either die orare promoted to the RFC series Anyone who has comments on a draft or proposedRFC is encouraged to reply In addition to standardizing the Internet protocols theRFC mechanism sometimes just documents or explains aspects of existing practiceRFCs are numbered sequentially currently there are about RFCs also havedescriptive titles eg Algorithms for Synchronizing Network Clocks but to forestall ambiguity they are usually cited by number Once distributed the contents ofan RFC are never changed Updates are distributed as new RFCs with their ownreference numbers Updates may either extend and clarify existing RFCs or supersede them entirelyRFCs are available from numerous sources but rfceditororg is dispatch centraland will always have the most uptodate information Look up the status of anRFC at rfceditororg before investing the time to read it it may no longer be themost current document on that subjectThe Internet standards process itself is detailed in RFC Another useful metaRFCis RFC Years of RFCs which describes some of the cultural and technicalcontext of the RFC systemDont be scared away by the wealth of technical detail found in RFCs Most containintroductions summaries and rationales that are useful for system administratorseven when the technical details are not Some RFCs are specifically written as overviews or general introductions RFCs might not be the gentlest way to learn abouta topic but they are authoritative concise and freeNot all RFCs are full of boring technical details Here are some of our favorites onthe lighter side usually written on April st RFC Standard for Transmission of IP Datagrams on Avian Carriers RFC The Twelve Networking Truths RFC Electricity over IP RFC Requirements for Morality Sections in Routing Area Drafts RFC Adaptation of RFC for IPv RFC Design Considerations for FasterThanLight Communication RFC Scenic Routing for IPvIn addition to being assigned its own serial number an RFC can also be assignedan FYI For Your Information number a BCP Best Current Practice number or A group of Linux enthusiasts from BLUG the Bergen Norway Linux User Group actually implemented the Carrier Pigeon Internet Protocol CPIP as specified in RFC For details see the website bluglinuxnorfca STD Standard number FYIs STDs and BCPs are subseries of the RFCs thatinclude documents of special interest or importanceFYIs are introductory or informational documents intended for a broad audienceThey can be a good place to start research on an unfamiliar topic if you can findone thats relevant Unfortunately this series has languished recently and not manyof the FYIs are up to dateBCPs document recommended procedures for Internet sites They consist of administrative suggestions and for system administrators are often the most valuableof the RFC subseriesSTDs document Internet protocols that have completed the IETFs review and testing process and have been formally adopted as standardsRFCs FYIs BCPs and STDs are numbered sequentially within their own series soa document can bear several different identifying numbers For example RFCTools for DNS Debugging is also known as FYI Networking basicsNow that weve provided a bit of context lets look at the TCPIP protocols themselves TCPIP is a protocol suite a set of network protocols designed to worksmoothly together It includes several components each defined by a standardstrackRFC or series of RFCs IP the Internet Protocol which routes data packets from one machineto another RFC ICMP the Internet Control Message Protocol which defines several kindsof lowlevel support for IP including error messages routing assistanceand debugging help RFC ARP the Address Resolution Protocol which translates IP addresses tohardware addresses RFC UDP the User Datagram Protocol which implements unverified oneway data delivery RFC TCP the Transmission Control Protocol which implements reliable fullduplex flowcontrolled errorcorrected conversations RFCThese protocols are arranged in a hierarchy or stack with the higherlevel protocolsmaking use of the protocols beneath them TCPIP is conventionally described as afivelayer system as shown in Exhibit A but the actual TCPIP protocols inhabitonly three of these layers This is actually a little white lie ARP is not really part of TCPIP and can be used with other protocolsuites However its an integral part of the way TCPIP works on most LAN mediaExhibit A TCPIP layering modelAPPLICATIONTRANSPORTNETWORKLINKPHYSICALLAYERLAYERLAYERLAYERLAYERIP ICMPARP device driversCopper optical ber radio wavesTCP UDParp SSH SMTP HTTP DNS DOTA tracerouteIPv and IPvThe version of TCPIP that has been in widespread use for nearly five decades isprotocol revision aka IPv It uses byte IP addresses A modernized versionIPv expands the IP address space to bytes and incorporates several other lessons learned from the use of IPv It removes several features of IP that experiencehas shown to be of little value making the protocol potentially faster and easier toimplement IPv also integrates security and authentication into the basic protocolOperating systems and network devices have supported IPv for a long time Google reports statistics about its clients use of IPv at googlecomipv As of March the fraction of peers using IPv to contact Google sites has risen to about worldwide In the United States its over Those numbers look healthy but in fact theyre perhaps a bit deceptive because mostmobile devices default to IPv when theyre on the carriers data network and thereare a lot of phones out there Home and enterprise networks remain overwhelmingly centered on IPvThe development and deployment of IPv were to a large extent motivated by theconcern that the world was running out of byte IPv address space And indeedthat concern proved well founded at this point only Africa has any remainingIPv addresses still available for assignment see ipvpotaroonet for details TheAsiaPacific region was the first to run out of addresses on April Given that weve already lived through the IPv apocalypse and have used up all ourIPv addresses how is it that the world continues to rely predominantly on IPvFor the most part weve learned to make more efficient use of the IPv addresses thatwe have Network Address Translation NAT see page lets entire networksof machines hide behind a single IPv address Classless InterDomain RoutingCIDR see page flexibly subdivides networks and promotes efficient backbone routing Contention for IPv addresses still exists but like broadcast spectrumit tends to be reallocated in economic rather than technological ways these daysThe underlying issue that limits IPvs adoption is that IPv support remains mandatory for a device to be a functional citizen of the Internet For example here area few major web sites that as of are still not reachable through IPv AmazonReddit eBay IMDB Hotmail Tumblr MSN Apple The New York Times TwitterPinterest Bing WordPress Dropbox craigslist Stack Overflow We could go onbut you get the driftYour choice is not between IPv and IPv its between supporting IPv alone andsupporting both IPv and IPv When all the services listed aboveand scoresmore in the second tierhave added IPv support then you can reasonably consider adopting IPv instead of IPv Until then it doesnt seem unreasonable to askIPv to justify the effort of its implementation by providing better performancesecurity or features Or perhaps by opening the door to a world of IPvonly services that simply cant be accessed through IPvUnfortunately those services dont exist and IPv doesnt actually offer any of thosebenefits Yes its an elegant and welldesigned protocol that improves on IPv Andyes it is in some ways easier to administer than IPv and requires fewer hacks egless need for NAT But in the end its just a cleanedup version of IPv with a larger address space The fact that you must manage it alongside IPv eliminates anypotential efficiency gain IPvs raison dtre remains the millennial fear of IPvaddress exhaustion and to date the effects of that exhaustion just havent beenpainful enough to motivate widespread migration to IPvWeve been publishing this book for a long time and over the last few editions IPvhas always seemed like it was one more update away from meriting coverage as aprimary technology brings an uncanny sense of deja vu with IPv loomingever brighter on the horizon but still solving no immediate problems and offeringfew specific incentives to convert IPv is the future of networking and evidentlyit always will beThe arguments in favor of actually deploying IPv inside your network remainlargely attitudinal It will have to be done at some point IPv is superior froman engineering standpoint You need to develop IPv expertise so that youre notcaught flatfooted when the IPv rapture finally arrives All the cool kids are doing itWe say sure go ahead support IPv if you feel like it Thats a responsible andforwardthinking path Its civicminded tooyour adoption of IPv hastens the Microsoft Bings presence on this list is particularly interesting given that its one of a handful of major sites showcased in materials for the World IPv Launch marketing campaign of tag lineThis time it is for real We dont know the full story behind this situation but Bing evidently supported IPv at one point then later decided it wasnt worth the trouble See worldipvlaunchorg These are sites whose primary web addresses are not associated with any IPv addresses AAAA records in DNSday when IPv is all we have to deal with But if you dont feel like diving into IPvthats fine too Youll have years of warning before theres any real need to transitionOf course none of these comments apply if your organization offers public serviceson the Internet In that case its your solemn duty to implement IPv Dont screwthings up for the rest of us by continuing to impede IPvs adoption Do you wantto be Google or do you want to be Microsoft BingTheres also an argument to be made for IPv in data centers where direct connectivity to the outside world of IPv is not needed In these limited environmentsyou may indeed have the option to migrate to IPv and leave IPv behind therebysimplifying your infrastructure Internetfacing servers can speak IPv even as theyroute all internal and backend traffic over IPvA couple of points IPv has been productionready for a long time Implementation bugsarent a major concern Expect it to work as reliably as IPv From a hardware standpoint IPv support should be considered mandatory for all new device acquisitions Its doubtful that you could findany piece of enterprisegrade networking gear that doesnt support IPvthese days but a lot of consumergrade equipment remains IPvonlyIn this book we focus on IPv as the mainstream version of TCPIP IPvspecificmaterial is explicitly marked Fortunately for sysadmins IPv and IPv are highlyanalogous If you understand IPv you already know most of what you need toknow about IPv The main difference between the versions lies in their addressingschemes In addition to longer addresses IPv introduces a few additional addressing concepts and some new notation But thats about itPackets and encapsulationTCPIP supports a variety of physical networks and transport systems includingEthernet token ring MPLS Multiprotocol Label Switching wireless Ethernetand seriallinebased systems Hardware is managed within the link layer of theTCPIP architecture and higherlevel protocols do not know or care about thespecific hardware being usedData travels on a network in the form of packets bursts of data with a maximumlength imposed by the link layer Each packet consists of a header and a payloadThe header tells where the packet came from and where its going It can also include checksums protocolspecific information or other handling instructionsThe payload is the data to be transferredThe name of the primitive data unit depends on the layer of the protocol At the linklayer it is called a frame at the IP layer a packet and at the TCP layer a segment Inthis book we use packet as a generic term that encompasses these various casesAs a packet travels down the protocol stack from TCP or UDP transport to IP toEthernet to the physical wire in preparation for being sent each protocol adds itsown header information Each protocols finished packet becomes the payload partof the packet generated by the next protocol This nesting is known as encapsulation On the receiving machine the encapsulation is reversed as the packet travelsback up the protocol stackFor example a UDP packet being transmitted over Ethernet contains three different wrappers or envelopes On the Ethernet wire it is framed with a simple headerthat lists the source and nexthop destination hardware addresses the length ofthe frame and the frames checksum CRC The Ethernet frames payload is anIP packet the IP packets payload is a UDP packet and the UDP packets payloadis the data being transmitted Exhibit B shows the components of such a frameExhibit B A typical network packetEthernet frame bytesEthernetheaderIPvheaderUDPheader Application data bytes bytes bytesEthernetCRC bytes bytesIPv packet bytesUDP packet bytesEthernet framingOne of the main chores of the link layer is to add headers to packets and to putseparators between them The headers contain each packets linklayer addressinginformation and checksums and the separators ensure that receivers can tell whereone packet stops and the next one begins The process of adding these extra bits isknown generically as framingThe link layer is divided into two parts MAC the Media Access Control sublayer and LLC the Logical Link Control sublayer The MAC sublayer deals with themedia and transmits packets onto the wire The LLC sublayer handles the framingToday a single standard for Ethernet framing is in common use DIX Ethernet II Inthe distant past several slightly different standards based on IEEE were alsoused You might still run across vestigial references to framing choices in networkdocumentation but you can now ignore this issueMaximum transfer unitThe size of packets on a network can be limited both by hardware specifications andby protocol conventions For example the payload of a standard Ethernet frameis traditionally bytes The size limit is associated with the linklayer protocoland is called the maximum transfer unit or MTU Table shows some typicalvalues for the MTUTable MTUs for various types of networkNetwork type Maximum transfer unitEthernet bytes with framing aIPv all hardware At least bytes at the IP layerToken ring Configurable bPointtopoint WAN links T T Configurable often or bytesa See page for some comments on jumbo Ethernet packetsb Common values are and Sometimes to match EthernetIPv splits packets to conform to the MTU of a particular network link If a packetis routed through several networks one of the intermediate networks might havea smaller MTU than the network of origin In this case an IPv router that forwards the packet onto the smallMTU network further subdivides the packet in aprocess called fragmentationFragmentation of inflight packets is an unwelcome chore for a busy router so IPvlargely removes this feature Packets can still be fragmented but the originatinghost must do the work itself All IPv networks are required to support an MTU ofat least bytes at the IP layer so IPv senders also have the option of limitingthemselves to packets of this sizeIPv senders can discover the lowestMTU link through which a packet must passby setting the packets do not fragment flag If the packet reaches an intermediaterouter that cannot forward the packet without fragmenting it the router returnsan ICMP error message to the sender The ICMP packet includes the MTU of thenetwork thats demanding smaller packets and this MTU then becomes the governing packet size for communication with that destinationIPv path MTU discovery works similarly but since intermediate routers are never allowed to fragment IPv packets all IPv packets act as if they had a do notfragment flag enabled Any IPv packet thats too large to fit into a downstreampipe causes an ICMP message to be returned to the senderThe TCP protocol automatically does path MTU discovery even in IPv UDP isnot so nice and is happy to shunt extra work to the IP layerIPv fragmentation problems can be insidious Although path MTU discoveryshould automatically resolve MTU conflicts an administrator must occasionallyintervene If you are using a tunneled architecture for a virtual private network forexample you should look at the size of the packets that are traversing the tunnelThey are often bytes to start with but once the tunneling header is addedthey become bytes or so and must be fragmented Setting the MTU of thelink to a smaller value averts fragmentation and increases the overall performanceof the tunneled network Consult the ifconfig or iplink man page to see how toset an interfaces MTU Packet addressingLike letters or email messages network packets must be properly addressed to reachtheir destinations Several addressing schemes are used in combination MAC Media Access Control addresses for use by hardware IPv and IPv network addresses for use by software Hostnames for use by peopleHardware MAC addressingEach of a hosts network interfaces usually has one linklayer MAC address thatdistinguishes it from other machines on the physical network plus one or moreIP addresses that identify the interface on the global Internet This last part bearsrepeating IP addresses identify network interfaces not machines To users the distinction is irrelevant but administrators must know the truthThe lowest level of addressing is dictated by network hardware For example Ethernet devices are assigned a unique byte hardware address at the time of manufacture These addresses are traditionally written as a series of digit hex bytesseparated by colons for example dabdfToken ring interfaces have a similar address that is also six bytes long Some pointtopoint networks such as PPP need no hardware addresses at all the identity ofthe destination is specified as the link is establishedA byte Ethernet address is divided into two parts The first three bytes identifythe manufacturer of the hardware and the last three bytes are a unique serial number that the manufacturer assigns Sysadmins can sometimes identify the brand ofmachine that is trashing a network by looking up the byte identifier in a table ofvendor IDs The byte codes are actually IEEE Organizationally Unique IdentifiersOUIs so you can look up them up directly in the IEEEs database atstandardsieeeorgregauthouiOf course the relationships among the manufacturers of chipsets componentsand systems are complex so the vendor ID embedded in a MAC address can bemisleading tooIn theory Ethernet hardware addresses are permanently assigned and immutableHowever many network interfaces let you override the hardware address and setone of your own choosing This feature can be handy if you have to replace a broken machine or network card and for some reason must use the old MAC addresseg all your switches filter it or your DHCP server hands out addresses accordingto MAC addresses or your MAC address is also a software license key SpoofableMAC addresses are also helpful if you need to infiltrate a wireless network that usesMACbased access control But for simplicity its generally advisable to preservethe uniqueness of MAC addressesIP addressingAt the next level up from the hardware Internet addressing more commonly knownas IP addressing is used IP addresses are hardware independent Within any particular network context an IP address identifies a specific and unique destinationHowever its not quite accurate to say that IP addresses are globally unique becauseseveral special cases muddy the water NAT uses one interfaces IP address to handletraffic for multiple machines IP private address spaces are addresses that multiplesites can use at once as long as the addresses are not visible to the Internet anycastaddressing shares one IP address among several machinesThe mapping from IP addresses to hardware addresses is implemented at the linklayer of the TCPIP model On networks such as Ethernet that support broadcastingthat is networks that allow packets to be addressed to all hosts on this physicalnetwork senders use the ARP protocol to discover mappings without assistancefrom a system administrator In IPv an interfaces MAC address is often used aspart of the IP address making the translation between IP and hardware addressing virtually automaticHostname addressingIP addresses are sequences of numbers so they are hard for people to rememberOperating systems allow one or more hostnames to be associated with an IP addressso that users can type rfceditororg instead of This mapping can be setup in several ways ranging from a static file etchosts to the LDAP database system to DNS the worldwide Domain Name System Keep in mind that hostnamesare really just a convenient shorthand for IP addresses and as such they refer tonetwork interfaces rather than computersPortsIP addresses identify a machines network interfaces but they are not specific enoughto address individual processes or services many of which might be actively usingthe network at once TCP and UDP extend IP addresses with a concept known asa port a bit number that supplements an IP address to specify a particular communication channel Valid ports are in the range See page for moredetails about NAT andprivate address spacesSee page formore informationabout ARPSee Chapter for more information about DNSStandard services such as SMTP SSH and HTTP associate themselves with wellknown ports defined in etcservices Here are some typical entries from theservices filesmtp udp Simple Mail Transfersmtp tcp Simple Mail Transferdomain udp Domain Name Serverdomain tcp Domain Name Serverhttp udp www wwwhttp World Wide Web HTTPhttp tcp www wwwhttp World Wide Web HTTPkerberos udp Kerberoskerberos tcp KerberosThe services file is part of the infrastructure You should never need to modify italthough you can do so if you want to add a nonstandard service You can find afull list of assigned ports at ianaorgassignmentsportnumbersAlthough both TCP and UDP have ports and those ports have the same sets ofpotential values the port spaces are entirely separate and unrelated Firewalls mustbe configured separately for each of these protocolsTo help prevent impersonation of system services UNIX systems restrict programsfrom binding to port numbers under unless they are run as root or have anappropriate Linux capability Anyone can communicate with a server running on alow port number the restriction applies only to the program listening on the portToday the privileged port system is as much a nuisance as it is a bulwark againstmalfeasance In many cases its more secure to run standard services on unprivileged ports as nonroot users and to forward network traffic to these highnumberedports through a load balancer or some other type of network appliance This practice limits the proliferation of unnecessary root privileges and adds an additionallayer of abstraction to your infrastructureAddress typesThe IP layer defines several broad types of address some of which have direct counterparts at the link layer Unicast addresses that refer to a single network interface Multicast addresses that simultaneously target a group of hosts Broadcast addresses that include all hosts on the local subnet Anycast addresses that resolve to any one of a group of hostsMulticast addressing facilitates applications such as video conferencing for whichthe same set of packets must be sent to all participants The Internet Group Management Protocol IGMP constructs and manages sets of hosts that are treated asone multicast destinationMulticast is largely unused on todays Internet but its slightly more mainstream inIPv IPv broadcast addresses are really just specialized forms of multicast addressingAnycast addresses bring load balancing to the network layer by allowing packetsto be delivered to whichever of several destinations is closest in terms of networkrouting You might expect that theyd be implemented similarly to multicast addresses but in fact they are more like unicast addressesMost of the implementation details for anycast support are handled at the level ofrouting rather than through IP The novelty of anycast addressing is really just therelaxation of the traditional requirement that IP addresses identify unique destinations Anycast addressing is formally described for IPv but the same tricks can beapplied to IPv toofor example as is done for root DNS name servers IP addresses the gory detailsWith the exception of multicast addresses Internet addresses consist of a networkportion and a host portion The network portion identifies a logical network towhich the address refers and the host portion identifies a node on that networkIn IPv addresses are bytes long and the boundary between network and hostportions is set administratively In IPv addresses are bytes long and the network portion and host portion are always bytes eachIPv addresses are written as decimal numbers one for each byte separated byperiods for example The leftmost byte is the most significant andis always part of the network portionWhen is the first byte of an address it denotes the loopback network a fictitious network that has no real hardware interface and only one host The loopbackaddress always refers to the current host Its symbolic name is localhostThis is another small violation of IP address uniqueness since every host thinks is a different computerIPv addresses and their textformatted equivalents are a bit more complicatedTheyre discussed in the section IPv addressing starting on page An interfaces IP address and other parameters are set with the ip address Linuxor ifconfig FreeBSD command Details on configuring a network interface starton page IPv address classesHistorically IP addresses had an inherent class that depended on the first bits ofthe leftmost byte The class determined which bytes of the address were in the network portion and which were in the host portion Today an explicit mask identifiesthe network portion and the boundary can fall between any two adjacent bits notjust between bytes However the traditional classes are still used as defaults whenno explicit division is specifiedClasses A B and C denote regular IP addresses Classes D and E are multicastingand research addresses Table describes the characteristics of each class Thenetwork portion of an address is denoted by N and the host portion by HTable Historical IPv address classes Classst bytea Format CommentsA NHHH Very early networks or reserved for DoDB NNHH Large sites usually subnetted were hard to getC NNNH Were easy to get often obtained in setsD Multicast addresses not permanently assignedE Experimental addressesa The value is special and is not used as the first byte of regular IP addresses The value is reserved for the loopback addressIts unusual for a single physical network to have thousands of computers attachedto it so class A and class B addresses which allow for hosts and hosts per network respectively are really quite wasteful For example the classA networks use up half the available byte address space Who knew that IPv address space would become so preciousIPv subnettingTo make better use of these addresses you can now reassign part of the host portion to the network portion by specifying an explicit byte bit subnet maskor netmask in which the s correspond to the desired network portion and thes correspond to the host portion The s must be leftmost and contiguous At leasteight bits must be allocated to the network part and at least two bits to the host partErgo there are really only possible values for an IPv netmaskFor example the four bytes of a class B address would normally be interpreted asNNHH The implicit netmask for class B is therefore in decimal notation With a netmask of however the address would be interpretedas NNNH Use of the mask turns a single class B network address into distinctclassClike networks each of which can support hostsNetmasks are assigned with the ip or ifconfig command as each network interfaceis set up By default these commands use the inherent class of an address to figureout which bits are in the network part When you set an explicit mask you simplyoverride this behaviorNetmasks that do not end at a byte boundary can be annoying to decode and areoften written as XX where XX is the number of bits in the network portion of theaddress This is sometimes called CIDR Classless InterDomain Routing see page notation For example the network address refers to thefirst of four networks whose first bytes are The other three networkshave and as their fourth bytes The netmask associated with these networks is or xFFFFFFC in binary its ones followed by zerosExhibit C breaks out these numbers in a bit more detailExhibit C Netmask base conversionIP addressDecimal netmaskHex netmaskBinary netmask f f f f f f c A network has bits left to number hosts is so the networkhas potential host addresses However it can only accommodate actual hostsbecause the all and all host addresses are reserved they are the network andbroadcast addresses respectivelyIn our example the extra two bits of network address obtained bysubnetting can take on the values and The networkhas thus been divided into four networks in decimal is in binary in decimal is in binary in decimal is in binary in decimal is in binaryThe boldfaced bits of the last byte of each address are the bits that belong to thenetwork portion of that byteSee page formore informationabout ip and ifconfigTricks and tools for subnet arithmeticIts confusing to do all this bit twiddling in your head but some tricks can makeit simpler The number of hosts per network and the value of the last byte in thenetmask always add up to last netmask byte net sizeFor example which is the final byte of the netmask in the precedingexample Another arithmetic fact is that the last byte of an actual network addressas opposed to a netmask must be evenly divisible by the number of hosts per network We see this in action in the example where the last bytesof the networks are and all divisible by Given an IP address say we cannot tell without the associatednetmask what the network address and broadcast address will be Table showsthe possibilities for the default for a class B address a plausible valueand a reasonable value for a small networkTable Example IPv address decodingsIP address Netmask Network Broadcast The network address and broadcast address steal two hosts from each network so itwould seem that the smallest meaningful network would have four possible hoststwo real hostsusually at either end of a pointtopoint linkand the networkand broadcast addresses To have four values for hosts requires two bits in the hostportion so such a network would be a network with netmask or xFFFFFFFC However a network is treated as a special case see RFCand has no network or broadcast address both of its two addresses are used forhosts and its netmask is A handy web site called the IP Calculator by Krischan Jodies jodiesdeipcalc helpswith binaryhexmask arithmetic IP Calculator displays everything you might needto know about a network address and its netmask broadcast address hosts etcA commandline version of the tool ipcalc is also available Its in the standardrepositories for Debian Ubuntu and FreeBSDRed Hat and CentOS include a similar but unrelated program thats also called ipcalcHowever its relatively useless because it only understands default IP address classes Of course counts as being divisible by any numberRHELHeres some sample ipcalc output munged a bit to help with formatting ipcalc Address Netmask Wildcard Network HostMin HostMax Broadcast HostsNet Class AThe output includes both easytounderstand versions of the addresses and cutand paste versions Very usefulIf a dedicated IP calculator isnt available the standard utility bc makes a goodbackup utility since it can do arithmetic in any base Set the input and output baseswith the ibase and obase directives Set the obase first otherwise its interpretedrelative to the new ibaseCIDR Classless InterDomain RoutingLike subnetting of which it is a direct extension CIDR relies on an explicit netmaskto define the boundary between the network and host parts of an address But unlike subnetting CIDR allows the network portion to be made smaller than wouldbe implied by an addresss implicit class A short CIDR mask can have the effect ofaggregating several networks for purposes of routing Hence CIDR is sometimesreferred to as supernettingCIDR simplifies routing information and imposes hierarchy on the routing processAlthough CIDR was intended as only an interim solution along the road to IPvit has proved to be sufficiently powerful to handle the Internets growth problemsfor more than two decadesFor example suppose that a site has been given a block of eight class C addressesnumbered through in CIDR notation Internally the site could use them as network of length with hosts netmask networks of length with hosts each netmask networks of length with hosts each netmask networks of length with hosts each netmask and so on But from the perspective of the Internet its not necessary to have or even routing table entries for these addresses They all refer to the sameorganization and all the packets go to the same ISP A single routing entry for suffices CIDR makes it easy to suballocate portions of addressesand thus increases the number of available addresses manyfoldCIDR is definedin RFCInside your network you can mix and match regions of different subnet lengthsas long as all the pieces fit together without overlaps This is called variable lengthsubnetting For example an ISP with the allocation could definesome networks for pointtopoint customers some s for large customersand some s for smaller folksAll the hosts on a network must be configured with the same netmask You canttell one host that it is a and another host on the same network that it is a Address allocationOnly network numbers are formally assigned sites must define their own hostnumbers to form complete IP addresses You can subdivide the address space thathas been assigned to you into subnets however you likeAdministratively ICANN the Internet Corporation for Assigned Names andNumbers has delegated blocks of addresses to five regional Internet registries andthese regional authorities are responsible for doling out subblocks to ISPs withintheir regions These ISPs in turn divide up their blocks and hand out pieces to individual clients Only large ISPs should ever have to deal directly with one of theICANNsponsored address registriesTable lists the regional registration authoritiesTable Regional Internet registriesName Site Region coveredARIN arinnet North America part of the CaribbeanAPNIC apnicnet AsiaPacific region including Australia and New ZealandAfriNIC afrinicnet AfricaLACNIC lacnicnet Central and South America part of the CaribbeanRIPE NCC ripenet Europe and surrounding areasThe delegation from ICANN to regional registries and then to national or regionalISPs has allowed for further aggregation in the backbone routing tables ISP customers who have been allocated address space within the ISPs block do not needindividual routing entries on the backbone A single entry for the aggregated blockthat points to the ISP sufficesPrivate addresses and network address translation NATAnother factor that has mitigated the effect of the IPv address crisis is the use ofprivate IP address spaces described in RFC These addresses are used by yoursite internally but are never shown to the Internet or at least not intentionally Aborder router translates between your private address space and the address spaceassigned by your ISPRFC sets aside class A network class B networks and class C networksthat will never be globally allocated and can be used internally by any site Table shows the options The CIDR range column shows each range in the morecompact CIDR notation it does not add additional informationTable IP addresses reserved for private useIP class From To CIDR rangeClass A Class B Class C The original idea was that sites would choose an address class from among theseoptions to fit the size of their organizations But now that CIDR and subnetting areuniversal it probably makes the most sense to use the class A address subnettedof course for all new private networksTo allow hosts that use these private addresses to talk to the Internet the sites borderrouter runs a system called NAT Network Address Translation NAT interceptspackets addressed with these internal addresses and rewrites their source addresses using a valid external IP address and perhaps a different source port number Italso maintains a table of the mappings it has made between internal and externaladdressport pairs so that the translation can be performed in reverse when answering packets arrive from the InternetMany gardenvariety NAT gateways actually perform Port Address Translationaka PAT they use a single external IP address and multiplex connections for manyinternal clients onto the port space of that single address For example this is thedefault configuration for most of the massmarket routers used with cable modemsIn practice NAT and PAT are similar in terms of their implementation and bothsystems are commonly referred to as NATA site that uses NAT must still request a small section of address space from its ISPbut most of the addresses thus obtained are used for NAT mappings and are notassigned to individual hosts If the site later wants to choose another ISP only theborder router and its NAT configuration need be updated not the configurationsof the individual hostsLarge organizations that use NAT and RFC addresses must institute someform of central coordination so that all hosts independently of their departmentor administrative group have unique IP addresses The situation can become complicated when one company that uses RFC address space acquires or mergeswith another company thats doing the same thing Parts of the combined organization must often renumberIt is possible to have a UNIX or Linux box perform the NAT function but mostsites prefer to delegate this task to their routers or network connection devices Seethe vendorspecific sections later in this chapter for detailsAn incorrect NAT configuration can let privateaddressspace packets escape ontothe Internet The packets might get to their destinations but answering packetswont be able to get back CAIDA an organization that collects operational datafrom the Internet backbone finds that to of the packets on the backbonehave either private addresses or bad checksums This sounds like a tiny percentagebut it represents thousands of packets every minute on a busy circuit See caidaorgfor other interesting statistics and network measurement toolsOne issue raised by NAT is that an arbitrary host on the Internet cannot initiateconnections to your sites internal machines To get around this limitation NATimplementations let you preconfigure externally visible tunnels that connect tospecific internal hosts and portsAnother issue is that some applications embed IP addresses in the data portion ofpackets these applications are foiled or confused by NAT Examples include somemedia streaming systems routing protocols and FTP commands NAT sometimesbreaks VPNs tooNAT hides interior structure This secrecy feels like a security win but the securityfolks say NAT doesnt really help for security and does not replace the need for afirewall Unfortunately NAT also foils attempts to measure the size and topology ofthe Internet See RFC Local Network Protection for IPv for a good discussionof both the real and illusory benefits of NAT in IPvIPv addressingIPv addresses are bits long These long addresses were originally intendedto solve the problem of IP address exhaustion But now that theyre here they arebeing exploited to help with issues of routing mobility and locality of referenceThe boundary between the network portion and the host portion of an IPv address is fixed at so there can be no disagreement or confusion about how long Of course many routers now run embedded Linux kernels Even so these dedicated systems are stillgenerally more reliable and more secure than generalpurpose computers that also forward packets CAIDA pronounced kay duh is the Cooperative Association for Internet Data Analysis at the SanDiego Supercomputer Center on the UCSD campus caidaorg Many routers also support the Universal Plug and Play UPnP standards promoted by Microsoftone feature of which allows interior hosts to set up their own dynamic NAT tunnels This can be either a godsend or a security risk depending on your perspective You can easily disable the feature atthe router if you so desirean addresss network portion really is Stated another way true subnetting nolonger exists in the IPv world although the term subnet lives on as a synonymfor local network Even though network numbers are always bits long routersneednt pay attention to all bits when making routing decisions They can routepackets by prefix just as they do under CIDRIPv address notationThe standard notation for IPv addresses divides the bits of an address into groups of bits each separated by colons For examplefbae Each bit group is represented by hexadecimal digits This is different fromIPv notation in which each byte of the address is represented by a decimal base numberA couple of notational simplifications help limit the amount of typing needed torepresent IPv addresses First you neednt include leading zeros within a groupa in the third group above can be written simply as a and in the fourthgroup can be written as Groups with a value of should be represented as Application of this rule reduces the address above to the following stringfbaeSecond you can replace any number of contiguous zerovalued bit groupswith a double colonfbaeThe can be used only once within an address However it can appear as the firstor last component For example the IPv loopback address analogous to in IPv is which is equivalent to The original specification for IPv addresses RFC documented these notational simplifications but did not require their use As a result there can be multipleRFCcompliant ways to write a given IPv address as illustrated by the severalversions of the example address aboveThis polymorphousness makes searching and matching difficult because addressesmust be normalized before they can be compared Thats a problem we cant expectstandard datawrangling software such as spreadsheets scripting languages anddatabases to know about the details of IPv notation This is a real IPv address so dont use it on your own systems even for experimentation RFCsuggests that documentation and examples show IPv addresses within the prefix block dbBut we wanted to show a real example thats routed on the Internet backboneRFC updates RFC to make the notational simplifications mandatoryIt also adds a few more rules to ensure that every address has only a single textrepresentation Hex digits af must be represented by lowercase letters The element cannot replace a single bit group Just use If there is a choice of groups to replace with the must replace the longest possible sequence of zerosYou will still see RFCnoncompliant addresses out in the wild and nearly allnetworking software accepts them too However we strongly recommend followingthe RFC rules in your configurations recordkeeping and softwareIPv prefixesIPv addresses were not designed to be geographically clustered in the manner ofphone numbers or zip codes but clustering was added after the fact in the form ofthe CIDR conventions Of course the relevant geography is really routing spacerather than physical location CIDR was so technically successful that hierarchicalsubassignment of network addresses is now assumed throughout IPvYour IPv ISP obtains blocks of IPv prefixes from one of the regional registrieslisted in Table on page The ISP in turn assigns you a prefix that you prepend to the local parts of your addresses usually at your border router Organizations are free to set delegation boundaries wherever they wish within the addressspaces assigned to themWhenever an address prefix is represented in text form IPv adopts CIDR notationto represent the length of the prefix The general pattern isIPvaddressprefixlengthindecimalThe IPvaddress portion is as outlined in the previous section It must be a fulllength bit address In most cases the address bits beyond the prefix are set tozero However its sometimes appropriate to specify a complete host address alongwith a prefix length the intent and meaning are usually clear from contextThe IPv address shown in the previous section leads to a Google server The bitprefix thats routed on the North American Internet backbone isfbIn this case the address prefix was assigned by ARIN directly to Google as you canverify by looking up the prefix at arinnet There is no intervening ISP Google isresponsible for structuring the remaining bits of the network number as it sees In this case the prefix lengths of the ARINassigned address block and the backbone routing tableentry are the same but that is not always true The allocation prefix determines an administrativeboundary whereas the routing prefix relates to routespace localityfit Most likely several additional layers of prefixing are used within the GoogleinfrastructureAutomatic host numberingA machines bit interface identifier the host portion of the IPv address can beautomatically derived from the interfaces bit MAC hardware address with thealgorithm known as modified EUI documented in RFCSpecifically the interface identifier is just the MAC address with the two bytesxFFFE inserted in the middle and one bit complemented The bit you will flip isthe th most significant bit of the first byte in other words you XOR the first bytewith x For example on an interface with MAC address bec theautogenerated interface identifier would be bfffeec The underlineddigit is instead of because of the flipped bitThis scheme allows for automatic host numbering which is a nice feature for sysadmins since only the network portion of addresses need be managedThat the MAC address can be seen at the IP layer has both good and bad implicationsThe good part is that host number configuration can be completely automatic Thebad part is that the manufacturer of the interface card is encoded in the first half ofthe MAC address see page so you inevitably give up some privacy Pryingeyes and hackers with code for a particular architecture will be helped along TheIPv standards point out that sites are not required to use MAC addresses to derivehost IDs they can use whatever numbering system they wantVirtual servers have virtual network interfaces The MAC addresses associated withthese interfaces are typically randomized which all but guarantees uniquenesswithin a particular local contextStateless address autoconfigurationThe autogenerated host numbers described in the previous section combine witha couple of other simple IPv features to enable automatic network configurationfor IPv interfaces The overall scheme is known as SLAAC for StateLess AddressAutoConfigurationSLAAC configuring for an interface begins by assigning an address on the linklocalnetwork which has the fixed network address fe The host portion of theaddress is set from the MAC address of the interface as described aboveIPv does not have IPvstyle broadcast addresses per se but the linklocal networkserves roughly the same purpose it means this physical network Routers neverforward packets that were sent to addresses on this networkOnce the linklocal address for an interface has been set the IPv protocol stacksends an ICMP Router Solicitation packet to the all routers multicast addressRouters respond with ICMP Router Advertisement packets that list the IPv network numbers prefixes really in use on the networkIf one of these networks has its autoconfiguration OK flag set the inquiring hostassigns an additional address to its interface that combines the network portionadvertised by the router with the autogenerated host portion constructed withthe modified EUI algorithm Other fields in the Router Advertisement allow arouter to identify itself as an appropriate default gateway and to communicate thenetworks MTUThe end result is that a new host becomes a full citizen of the IPv network withoutthe need for any server other than the router to be running on the network andwithout any local configuration Unfortunately the system does not address theconfiguration of higherlevel software such as DNS so you may still want to run atraditional DHCPv server tooYou will sometimes see IPv network autoconfiguration associated with the nameNeighbor Discovery Protocol Although RFC is devoted to the Neighbor Discovery Protocol the term is actually rather vague It covers the use and interpretation of a variety of ICMPv packet types some of which are only peripherallyrelated to discovering network neighbors From a technical perspective the relationship is that the SLAAC procedure described above uses some but not all ofthe ICMPv packet types defined in RFC Its clearer to just call it SLAAC orIPv autoconfiguration and to reserve neighbor discovery for the IPtoMACmapping process described starting on page IPv tunnelingVarious schemes have been proposed to ease the transition from IPv to IPv mostly focusing on ways to tunnel IPv traffic through the IPv network to compensatefor gaps in IPv support The two tunneling systems in common use are called toand Teredo the latter named after a family of woodboring shipworms can be usedon systems behind a NAT deviceIPv information sourcesHere are some useful sources of additional IPv information worldipvlaunchcom A variety of IPv propaganda RFC IPv Global Unicast Address Format RFC IP Version Addressing Architecture RoutingRouting is the process of directing a packet through the maze of networks that standbetween its source and its destination In the TCPIP system it is similar to askingfor directions in an unfamiliar country The first person you talk to might pointSee page formore informationabout DHCPyou toward the right city Once you were a bit closer to your destination the nextperson might be able to tell you how to get to the right street Eventually you getclose enough that someone can identify the building youre looking forRouting information takes the form of rules routes such as To reach networkA send packets through machine C There can also be a default route that tells whatto do with packets bound for a network to which no explicit route existsRouting information is stored in a table in the kernel Each table entry has severalparameters including a mask for each listed network To route a packet to a particular address the kernel picks the most specific of the matching routesthat isthe one with the longest mask If the kernel finds no relevant route and no defaultroute then it returns a network unreachable ICMP error to the senderThe word routing is commonly used to mean two distinct things Looking up a network address in the routing table as part of the processof forwarding a packet toward its destination Building the routing table in the first placeIn this section we examine the forwarding function and look at how routes canbe manually added to or deleted from the routing table We defer the more complicated topic of routing protocols that build and maintain the routing table untilChapter Routing tablesYou can examine a machines routing table with ip route show on Linux or netstat ron FreeBSD Although netstat on Linux is on its way out it still exists and continues to work We use netstat for the examples below just to avoid having to showtwo different versions of the output The ip version contains similar content butits format is somewhat differentUse netstat rn to avoid DNS lookups and present all information numericallywhich is generally more useful Here is a short example of an IPv routing table togive you a better idea of what routes look likeredhat netstat rnDestination Genmask Gateway Fl MSS Iface U ethdefault UG eth U eth UG eth U loThis host has two network interfaces eth on the network and eth on the network The destination field is usually a network address although you can also addhostspecific routes their genmask is since all bits are consultedAn entrys gateway field must contain the full IP address of a local network interfaceor adjacent host on Linux kernels it can be to invoke the default gatewayFor example the fourth route in the table above says that to reach the network packets must be sent to the gateway through interface eth The second entry is a default route packets not explicitly addressedto any of the three networks listed or to the machine itself are sent to the defaultgateway host A host can route packets only to gateway machines that are reachable through a directly connected network The local hosts job is limited to moving packets one hopcloser to their destinations so it is pointless to include information about nonadjacent gateways in the local routing table Each gateway that a packet visits makesa fresh nexthop routing decision by consulting its own local routing databaseRouting tables can be configured statically dynamically or with a combination ofthe two approaches A static route is one that you enter explicitly with the ip Linuxor route FreeBSD command Static routes remain in the routing table as long asthe system is up they are often set up at boot time from one of the system startupscripts For example the Linux commandsip route add via dev ethip route add default via dev ethadd the fourth and second routes displayed by netstat rn above The first and thirdroutes in that display were added automatically when the eth and eth interfaceswere configured The equivalent FreeBSD commands are similarroute add net gw ethroute add default gw ethThe final route is also added at boot time It configures the loopback interface whichprevents packets sent from the host to itself from going out on the network Insteadthey are transferred directly from the network output queue to the network inputqueue inside the kernelIn a stable local network static routing is an efficient solution It is easy to manageand reliable However it requires that the system administrator know the topology of the network accurately at boot time and that the topology not change oftenMost machines on a local area network have only one way to get out to the rest ofthe network so the routing problem is easy A default route added at boot timesuffices to point toward the way out Hosts that use DHCP see page to gettheir IP addresses can also obtain a default route with DHCPFor more complicated network topologies dynamic routing is required Dynamicrouting is implemented by a daemon process that maintains and modifies the routingtable Routing daemons on different hosts communicate to discover the topology The IPv source routing feature is an exception to this rule see page of the network and to figure out how to reach distant destinations Several routingdaemons are available See Chapter IP Routing for detailsICMP redirectsAlthough IP generally does not concern itself with the management of routing information it does define a nave damage control feature called an ICMP redirectWhen a router forwards a packet to a machine on the same network from whichthe packet was originally received something is clearly wrong Since the senderthe router and the nexthop router are all on the same network the packet couldhave been forwarded in one hop rather than two The router can conclude that thesenders routing tables are inaccurate or incompleteIn this situation the router can notify the sender of its problem by sending an ICMPredirect packet In effect a redirect says You should not be sending packets forhost xxx to me you should send them to host yyy insteadIn theory the recipient of a redirect can adjust its routing table to fix the problemIn practice redirects contain no authentication information and are therefore untrustworthy Dedicated routers usually ignore redirects but most UNIX and Linux systems accept them and act on them by default Youll need to consider thepossible sources of redirects in your network and disable their acceptance if theycould pose a problemUnder Linux the variable acceptredirects in the proc hierarchy controls theacceptance of ICMP redirects See page for instructions on examining andresetting this variableOn FreeBSD the parameters netineticmpdropredirect and netineticmprediracceptcontrol the acceptance of ICMP redirects Set them to and respectively in thefile etcsysctlconf to ignore redirects To activate the new settings reboot or runsudo etcrcdsysctl reload IPv ARP and IPv neighbor discoveryAlthough IP addresses are hardwareindependent hardware addresses must stillbe used to actually transport data across a networks link layer IPv and IPv useseparate but eerily similar protocols to discover the hardware address associatedwith a particular IP addressIPv uses ARP the Address Resolution Protocol defined in RFC IPv uses partsof the Neighbor Discovery Protocol defined in RFC These protocols can beused on any kind of network that supports broadcasting or allnodes multicastingbut they are most commonly described in terms of EthernetIf host A wants to send a packet to host B on the same Ethernet it uses ARP or NDto discover Bs hardware address If B is not on the same network as A host A uses An exception is for pointtopoint links where the identity of the destination is sometimes implicitthe routing system to determine the nexthop router along the route to B and thenuses ARP or ND to find that routers hardware address These protocols can onlybe used to find the hardware addresses of machines that are directly connected tothe sending hosts local networksEvery machine maintains a table in memory called the ARP or ND cache whichcontains the results of recent queries Under normal circumstances many of theaddresses a host needs are discovered soon after booting so ARP and ND do notaccount for a lot of network trafficThese protocols work by broadcasting or multicasting a packet of the form Doesanyone know the hardware address for IP address X The machine being searched forrecognizes its own IP address and replies Yes thats the IP address assigned to oneof my network interfaces and the corresponding MAC address is fbaThe original query includes the IP and MAC addresses of the requester so that themachine being sought can reply without issuing a query of its own Thus the twomachines learn each others address mappings with only one exchange of packetsOther machines that overhear the requesters initial broadcast can record its address mapping tooOn Linux the ip neigh command examines and manipulates the caches created byARP and ND adds or deletes entries and flushes or prints the table ip neigh showdisplays the contents of the cachesOn FreeBSD the arp command manipulates the ARP cache and the ndp commandgives access to the ND cacheThese commands are generally useful only for debugging and for situations that involve special hardware For example if two hosts on a network are using the sameIP address one has the right ARP or ND table entry and one is wrong You can usethe cache information to track down the offending machineInaccurate cache entries can be a sign that someone with access to your local network is attempting to hijack network traffic This type of attack is known genericallyas ARP spoofing or ARP cache poisoning DHCP the Dynamic Host Configuration ProtocolWhen you plug a device or computer into a network it usually obtains an IP addressfor itself on the local network sets up an appropriate default route and connectsitself to a local DNS server The Dynamic Host Configuration Protocol DHCP isthe hidden Svengali that makes this magic happenThe protocol lets a DHCP client lease a variety of network and administrativeparameters from a central server that is authorized to distribute them The leasingparadigm is particularly convenient for PCs that are turned off when not in use andfor networks that must support transient guests such as laptopsDHCP is definedin RFCs and Leasable parameters include IP addresses and netmasks Gateways default routes DNS name servers Syslog hosts WINS servers X font servers proxy servers NTP servers TFTP servers for loading a boot imageThere are dozens moresee RFC for IPv and RFC for IPv Realworlduse of the more exotic parameters is rare howeverClients must report back to the DHCP server periodically to renew their leases Ifa lease is not renewed it eventually expires The DHCP server is then free to assignthe address or whatever was being leased to a different client The lease period isconfigurable but its usually quite long hours or daysEven if you want each host to have its own permanent IP address DHCP can save youtime and suffering because it concentrates configuration information on the DHCPserver rather than requiring it to be distributed to individual hosts Once the serveris up and running clients can use DHCP to obtain their network configurationsat boot time The clients neednt know that theyre receiving a static configurationDHCP softwareISC the Internet Systems Consortium maintains a nice open source referenceimplementation of DHCP Major versions and of ISCs software are all incommon use and all these versions work fine for basic service Version supportsbackup DHCP servers and version supports IPv Server client and relay agentsare all available from iscorgVendors all package some version of the ISC software although you may have toinstall the server portion explicitly The server package is called dhcp on Red Hatand CentOS iscdhcpserver on Debian and Ubuntu and iscdhcpserver onFreeBSD Make sure youre installing the software you intend as many systemspackage multiple implementations of both the server and client sides of DHCPIts best not to tamper with the client side of DHCP since that part of the code isrelatively simple and comes preconfigured and ready to use Changing the clientside of DHCP is not trivialHowever if you need to run a DHCP server we recommend the ISC package overvendorspecific implementations In a typical heterogeneous network environmentadministration is greatly simplified by standardizing on a single implementationThe ISC software is a reliable open source solution that builds without incidenton most systemsAnother option to consider is Dnsmasq a server that implements DHCP servicein combination with a DNS forwarder Its a tidy package that runs on pretty muchany system The project home page is thekelleysorgukdnsmasqdochtmlDHCP server software is also built into most routers Configuration is usuallymore painful than on a UNIX or Linuxbased server but reliability and availability might be higherIn the next few sections we briefly discuss the DHCP protocol explain how to setup the ISC server that implements it and review some client configuration issuesDHCP behaviorDHCP is a backwardcompatible extension of BOOTP a protocol originally devisedto help diskless workstations boot DHCP generalizes the parameters that can besupplied and adds the concept of a lease period for assigned valuesA DHCP client begins its interaction with a DHCP server by broadcasting a HelpWho am I message If a DHCP server is present on the local network it negotiates with the client to supply an IP address and other networking parameters Ifthere is no DHCP server on the local net servers on different subnets can receivethe initial broadcast message through a separate piece of DHCP software that actsas a relay agentWhen the clients lease time is half over it attempts to renew its lease The serveris obliged to keep track of the addresses it has handed out and this informationmust persist across reboots Clients are supposed to keep their lease state acrossreboots too although many do not The goal is to maximize stability in networkconfiguration In theory all software should be prepared for network configurations to change at a moments notice but some software still makes unwarrantedassumptions about the continuity of the networkISCs DHCP softwareISCs server daemon is called dhcpd and its configuration file is dhcpdconf usually found in etc or etcdhcp The format of the config file is a bit fragile leaveout a semicolon and you may receive a cryptic unhelpful error messageWhen setting up a new DHCP server you must also make sure that an empty leasedatabase file has been created Check the summary at the end of the man pagefor dhcpd to find the correct location for the lease file on your system Its usuallysomewhere underneath var IPv clients initiate conversations with the DHCP server by using the generic alls broadcast addressThe clients dont yet know their subnet masks and therefore cant use the subnet broadcast addressIPv uses multicast addressing instead of broadcastingTo set up the dhcpdconf file you need the following information The subnets for which dhcpd should manage IP addresses and the rangesof addresses to dole out A list of static IP address assignments you want to make if any alongwith the MAC hardware addresses of the recipients The initial and maximum lease durations in seconds Any other options the server should pass to DHCP clients netmask default route DNS domain name servers etcThe dhcpd man page outlines the configuration process and the dhcpdconf manpage covers the exact syntax of the config file In addition to setting up your configuration make sure dhcpd is started automatically at boot time See Chapter Booting and System Management Daemons for instructions Its helpful to makestartup of the daemon conditional on the existence of the dhcpdconf file if yoursystem doesnt automatically do this for youBelow is a sample dhcpdconf file from a Linux box with two interfaces one internal and one that connects to the Internet This machine performs NAT translationfor the internal network see page and leases out a range of IP addresseson this network as wellEvery subnet must be declared even if no DHCP service is provided on it so thisdhcpdconf file contains a dummy entry for the external interface It also includesa host entry for one particular machine that needs a fixed address global optionsoption domainname synacknetoption domainnameservers gwsynacknetoption subnetmask defaultleasetime maxleasetime subnet netmask range option broadcastaddress option routers gwsynacknetsubnet netmask host gandalf hardware ethernet fixedaddress gandalfsynacknetUnless you make static IP address assignments such as the one for gandalf aboveyou need to consider how your DHCP configuration interacts with DNS Theeasy option is to assign a generic name to each dynamically leased address egdhcpsynacknet and allow the names of individual machines to float along withtheir IP addresses Alternatively you can configure dhcpd to update the DNS database as it hands out addresses The dynamic update solution is more complicatedbut it has the advantage of preserving each machines hostnameISCs DHCP relay agent is a separate daemon called dhcrelay Its a simple programwith no configuration file of its own although vendors often add a startup harnessthat feeds it the appropriate commandline arguments for your site dhcrelay listens for DHCP requests on local networks and forwards them to a set of remoteDHCP servers that you specify Its handy both for centralizing the management ofDHCP service and for provisioning backup DHCP serversISCs DHCP client is similarly configuration free It stores status files for each connection in the directory varlibdhcp or varlibdhclient The files are named afterthe interfaces they describe For example dhclientethleases would contain allthe networking parameters that dhclient had set up on behalf of the eth interface Security issuesWe address the topic of security in a chapter of its own Chapter but severalsecurity issues relevant to IP networking merit discussion here In this section webriefly look at a few networking features that have acquired a reputation for causingsecurity problems and we recommend ways to minimize their impact The detailsof our example systems default behavior on these issues and the appropriate methods for changing them vary considerably and are discussed in the systemspecificmaterial starting on page IP forwardingA UNIX or Linux system that has IP forwarding enabled can act as a router That isit can accept third party packets on one network interface match them to a gatewayor destination host on another interface and retransmit the packetsUnless your system has multiple network interfaces and is actually supposed tofunction as a router its best to turn this feature off Hosts that forward packets cansometimes be coerced into compromising security by making external packets appear to have come from inside your network This subterfuge can help an intruderspackets evade network scanners and packet filtersIt is perfectly acceptable for a host to have network interfaces on multiple subnetsand to use them for its own traffic without forwarding third party packetsSee Chapter for more information about DNSICMP redirectsICMP redirects see page can maliciously reroute traffic and tamper withyour routing tables Most operating systems listen to ICMP redirects and followtheir instructions by default It would be bad if all your traffic were rerouted to acompetitors network for a few hours especially while backups were running Werecommend that you configure your routers and hosts acting as routers to ignoreand perhaps log ICMP redirect attemptsSource routingIPvs source routing mechanism lets you specify an explicit series of gateways fora packet to transit on the way to its destination Source routing bypasses the nexthop routing algorithm thats normally run at each gateway to determine how apacket should be forwardedSource routing was part of the original IP specification it was intended primarily to facilitate testing It can create security problems because packets are oftenfiltered according to their origin If someone can cleverly route a packet to makeit appear to have originated within your network instead of the Internet it mightslip through your firewall We recommend that you neither accept nor forwardsourcerouted packetsDespite the Internets dim view of IPv source routing it somehow managed to sneakits way into IPv as well However this IPv feature was deprecated by RFC in Compliant IPv implementations are now required to reject sourceroutedpackets and return an error message to the sender Linux and FreeBSD both follow the RFC behavior as do commercial routersBroadcast pings and other directed broadcastsPing packets addressed to a networks broadcast address instead of to a particularhost address are typically delivered to every host on the network Such packetshave been used in denialofservice attacks for example the socalled Smurf attacks The Smurf attacks Wikipedia article has detailsBroadcast pings are a form of directed broadcast in that they are packets sent tothe broadcast address of a distant network The default handling of such packetshas been gradually changing For example versions of Ciscos IOS up through xforwarded directed broadcast packets by default but IOS releases since do notIt is usually possible to convince your TCPIP stack to ignore broadcast packetsthat come from afar but since this behavior must be set on each interface the taskcan be nontrivial at a large site Even so IPv source routing may be poised to stage a minicomeback in the form of segment routing a feature that has now been integrated into the Linux kernel See lwnnetArticles for adiscussion of this technologyIP spoofingThe source address on an IP packet is normally filled in by the kernels TCPIPimplementation and is the IP address of the host from which the packet was sentHowever if the software creating the packet uses a raw socket it can fill in any sourceaddress it likes This is called IP spoofing and is usually associated with some kindof malicious network behavior The machine identified by the spoofed source IPaddress if it is a real address at all is often the victim in the scheme Error and return packets can disrupt or flood the victims network connections Packet spoofingfrom a large set of external machines is called a distributed denialofservice attackDeny IP spoofing at your border router by blocking outgoing packets whose sourceaddress is not within your address space This precaution is especially important ifyour site is a university where students like to experiment and might be temptedto carry out digital vendettasIf you are using private address space internally you can filter at the same time tocatch any internal addresses escaping to the Internet Such packets can never beanswered because they lack a backbone route and always indicate that your sitehas some kind of internal configuration errorIn addition to detecting outbound packets with bogus source addresses you mustalso protect against an attackers forging the source address on external packets tofool your firewall into thinking that they originated on your internal network Aheuristic known as unicast reverse path forwarding uRPF helps to address thisproblem It makes IP gateways discard packets that arrive on an interface differentfrom the one on which they would be transmitted if the source address were thedestination Its a quick sanity check that uses the normal IP routing table as a wayto validate the origin of network packets Dedicated routers implement uRPF butso does the Linux kernel On Linux its enabled by defaultIf your site has multiple connections to the Internet it might be perfectly reasonable for inbound and outbound routes to be different In this situation youll haveto turn off uRPF to make your routing work properly If your site has only one wayout to the Internet then turning on uRPF is usually safe and appropriateHostbased firewallsTraditionally a network packet filter or firewall connects your local network to theoutside world and controls traffic according to a sitewide policy UnfortunatelyMicrosoft has warped everyones perception of how a firewall should work with itsnotoriously insecure Windows systems The last few Windows releases all comewith their own personal firewalls and they complain bitterly if you try to turn offthe firewallOur example systems all include packet filtering software but you should not inferfrom this that every UNIX or Linux machine needs its own firewall The packetfiltering features are there primarily to allow these machines to serve as networkgatewaysHowever we dont recommend using a workstation as a firewall Even with meticulous hardening fullfledged operating systems are too complex to be fully trustworthy Dedicated network equipment is more predictable and more reliableevenif it secretly runs LinuxEven sophisticated software solutions like those offered by Check Point whoseproducts run on UNIX Linux and Windows hosts are not as secure as a dedicated device such as Ciscos Adaptive Security Appliance series The softwareonlysolutions are nearly the same price to bootA more thorough discussion of firewallrelated issues begins on page Virtual private networksMany organizations that have offices in several locations would like to have all thoselocations connected to one big private network Such organizations can use the Internet as if it were a private network by establishing a series of secure encryptedtunnels among their various locations A network that includes such tunnels isknown as a virtual private network or VPNVPN facilities are also needed when employees must connect to your private network from their homes or from the field A VPN system doesnt eliminate everypossible security issue relating to such ad hoc connections but its secure enoughfor many purposesSome VPN systems use the IPsec protocol which was standardized by the IETF in as a relatively lowlevel adjunct to IP Others such as OpenVPN implementVPN security on top of TCP by using Transport Layer Security TLS the successor to the Secure Sockets Layer SSL TLS is also on the IETFs standards trackalthough it hasnt yet been fully adopted as of this writing A variety of proprietary VPN implementations are also available These systemsgenerally dont interoperate with one another or with the standardsbased VPNsystems but thats not necessarily a major drawback if all the endpoints are underyour controlThe TLSbased VPN solutions seem to be the marketplace winners at this pointThey are just as secure as IPsec and considerably less complicated Having a freeimplementation in the form of OpenVPN doesnt hurt eitherFor users at home and at large a common paradigm is for them to download asmall Java or executable component through their web browser This componentthen implements VPN connectivity back to the enterprise network The mechanism is convenient for users but be aware that the browserbased systems differwidely in their implementations some offer VPN service through a pseudonetworkinterface while others forward only specific ports Still others are little morethan glorified web proxiesSee page formore informationabout IPsecBe sure you understand the underlying technology of the solutions youre considering and dont expect the impossible True VPN service that is full IPlayerconnectivity through a network interface requires administrative privileges andsoftware installation on the client whether that client is a Windows system or aLinux laptop Check browser compatibility too since the voodoo involved in implementing browserbased VPN solutions often doesnt translate among browsers Basic network configurationOnly a few steps are involved in adding a new machine to an existing local areanetwork but every system does it slightly differently Systems with a GUI installedtypically include a control panel for network configuration but these visual toolsaddress only simple scenarios On a typical server you just enter the network configuration directly into text filesBefore bringing up a new machine on a network that is connected to the Internetsecure it Chapter Security so that you are not inadvertently inviting attackers onto your local networkAdding a new machine to a local network goes like this Assign a unique IP address and hostname Configure network interfaces and IP addresses Set up a default route and perhaps fancier routing Point to a DNS name server to allow access to the rest of the InternetIf you rely on DHCP for basic provisioning most of the configuration chores for anew machine are performed on the DHCP server rather than on the new machineitself New OS installations typically default to configuration through DHCP so newmachines may require no network configuration at all Refer to the DHCP sectionstarting on page for general informationAfter any change that might affect startup always reboot to verify that the machinecomes up correctly Six months later when the power has failed and the machinerefuses to boot its hard to remember what change you made that might havecaused the problemThe process of designing and installing a physical network is touched on in Chapter Physical Networking If you are dealing with an existing network and have ageneral idea of how it is set up it may not be necessary for you to read too muchmore about the physical aspects of networking unless you plan to extend the existing networkIn this section we review the various issues involved in manual network configuration This material is general enough to apply to any UNIX or Linux system Inthe vendorspecific sections starting on page we address the unique twiststhat separate the various vendors systemsAs you work through basic network configuration on any machine youll find ithelpful to test your connectivity with basic tools such as ping and traceroute SeeNetwork troubleshooting starting on page for a description of these toolsHostname and IP address assignmentAdministrators have various heartfelt theories about how the mapping from hostnames to IP addresses is best maintained through the hosts file LDAP the DNSsystem or perhaps some combination of those options The conflicting goals arescalability consistency and maintainability versus a system that is flexible enoughto allow machines to boot and function when not all services are availableAnother consideration when youre designing your addressing system is the possibleneed to renumber your hosts in the future Unless you are using RFC privateaddresses see page your sites IP addresses might change when you switchISPs Such a transition becomes daunting if you must visit each host on the network to reconfigure its address To expedite renumbering you can use hostnamesin configuration files and confine address mappings to a few centralized locationssuch as the DNS database and your DHCP configuration filesThe etchosts file is the oldest and simplest way to map names to IP addressesEach line starts with an IP address and continues with the various symbolic namesby which that address is knownHere is a typical etchosts file for the host lollipop localhost localhost iplocalhostff ipallnodesff ipallrouters lollipopatrustcom lollipop loghost chimchimgwatrustcom chimchimgw nsatrustcom ns licensesatrustcom licenseserverA minimalist version would contain only the first three lines localhost is commonly the first entry in the etchosts file this entry is unnecessary on many systemsbut it doesnt hurt to include it You can freely intermix IPv and IPv addressesBecause etchosts contains only local mappings and must be maintained on eachclient system its best reserved for mappings that are needed at boot time eg thehost itself the default gateway and name servers Use DNS or LDAP to find mappings for the rest of the local network and the rest of the world You can also useetchosts to specify mappings that you do not want the rest of the world to knowabout and therefore do not publish in DNSThe hostname command assigns a hostname to a machine hostname is typically run at boot time from one of the startup scripts which obtains the name to be You can also use a split DNS configuration to achieve this goal see page See Chapter for more information about DNSassigned from a configuration file Of course each system does this slightly differently See the systemspecific sections beginning on page for details Thehostname should be fully qualified that is it should include both the hostnameand the DNS domain name such as anchorcscoloradoeduAt a small site you can easily dole out hostnames and IP addresses by hand Butwhen many networks and many different administrative groups are involved it helpsto have some central coordination to ensure uniqueness For dynamically assignednetworking parameters DHCP takes care of the uniqueness issuesNetwork interface and IP configurationA network interface is a piece of hardware that can potentially be connected to anetwork The actual hardware varies widely It can be an RJ jack with associatedsignaling hardware for wired Ethernet a wireless radio or even a virtual piece ofhardware that connects to a virtual networkEvery system has at least two network interfaces a virtual loopback interface andat least one real network card or port On PCs with multiple Ethernet jacks a separate network interface usually controls each jack These interfaces quite often havehardware different from that of each other as wellOn most systems you can see all the network interfaces with ip link show Linuxor ifconfig a FreeBSD whether or not the interfaces have been configured or arecurrently running Heres an example from an Ubuntu system ip link show lo LOOPBACKUPLOWERUP mtu qdisc noqueue state UNKNOWNmode DEFAULT group default qlen linkloopback brd enps BROADCASTMULTICASTUPLOWERUP mtu qdisc pfifofaststate UP mode DEFAULT group default qlen linkether cbfa brd ffffffffffffInterface naming conventions vary Current versions of Linux try to ensure thatinterface names dont change over time so the names are somewhat arbitrary egenps FreeBSD and older Linux kernels use a more traditional driver instancenumber scheme resulting in names like em or ethNetwork hardware often has configurable options that are specific to its media typeand have little to do with TCPIP per se One common example of this is modernday Ethernet wherein an interface card might support or even Mbs in either halfduplex or fullduplex mode Most equipment defaults toautonegotiation mode in which both the card and its upstream connection usually a switch port try to guess what the other wants to useHistorically autonegotiation worked about as well as a blindfolded cowpoke tryingto rope a calf Modern network devices play better together but autonegotiation isstill a possible source of failure High packet loss rates especially for large packetsare a common artifact of failed autonegotiationIf you manually configure a network link turn off autonegotiation on both sides Itmakes intuitive sense that you might be able to manually configure one side of thelink and then let the other side automatically adapt to those settings But alas thatis not how Ethernet autoconfiguration actually works All participants must agreethat the network is either automatically or manually configuredThe exact method by which hardware options like autonegotiation are set varieswidely so we defer discussion of those details to the systemspecific sections starting on page Above the level of interface hardware every network protocol has its own configuration IPv and IPv are the only protocols you might normally want to configure but its important to understand that configurations are defined per interfaceprotocol pair In particular IPv and IPv are completely separate worlds each ofwhich has its own configurationIP configuration is largely a matter of setting an IP address for the interface IPvalso needs to know the subnet mask netmask for the attached network so that itcan distinguish the network and host portions of addresses At the level of networktraffic in and out of an interface IPv does not use netmasks the network and hostportions of an IPv address are of fixed sizeIn IPv you can set the broadcast address to any IP address thats valid for thenetwork to which the host is attached Some sites have chosen weird values for thebroadcast address in the hope of avoiding certain types of denialofservice attacksthat use broadcast pings but this is risky and probably overkill Failure to properlyconfigure every machines broadcast address can lead to broadcast storms in whichpackets travel from machine to machine until their TTLs expireA better way to avoid problems with broadcast pings is to prevent your border routersfrom forwarding them and to tell individual hosts not to respond to them IPv nolonger has broadcasting at all its been replaced with various forms of multicastingYou can assign more than one IP address to an interface In the past it was sometimes helpful to do this to allow one machine to host several web sites howeverthe need for this feature has been superseded by the HTTP Host header and theSNI feature of TLS See page for details Broadcast storms occur because the same linklayer broadcast address must be used to transportpackets no matter what the IP broadcast address has been set to For example suppose that machineX thinks the broadcast address is A and machine Y thinks it is A If X sends a packet to address AY will receive the packet because the linklayer destination address is the broadcast address will seethat the packet is not for itself and also not for the broadcast address because Y thinks the broadcastaddress is A and may then forward the packet back to the net If two machines are in Ys state thepacket circulates until its TTL expires Broadcast storms can erode your bandwidth especially on alarge switched netRouting configurationThis books discussion of routing is divided among several sections in this chapterand Chapter IP Routing Although most of the basic information about routingis found here and in the sections about the ip route Linux and route FreeBSDcommands you might find it helpful to read the first few sections of Chapter if you need more informationRouting is performed at the IP layer When a packet bound for some other host arrives the packets destination IP address is compared with the routes in the kernelsrouting table If the address matches a route in the table the packet is forwarded tothe nexthop gateway IP address associated with that routeThere are two special cases First a packet may be destined for some host on a directly connected network In this case the nexthop gateway address in the routingtable is one of the local hosts own interfaces and the packet is sent directly to itsdestination This type of route is added to the routing table for you by the ifconfigor ip address command when you configure a network interfaceSecond it may be that no route matches the destination address In this case thedefault route is invoked if one exists Otherwise an ICMP network unreachableor host unreachable message is returned to the senderMany local area networks have only one way out so all they need is a single default route that points to the exit On the Internet backbone the routers do nothave default routes If there is no routing entry for a destination that destinationcannot be reachedEach ip route Linux or route FreeBSD command adds or removes one routeHere are two prototypical commandslinux ip route add via zulugwatrustnetfreebsd route add net zulugwatrustnetThese commands add a route to the network through the gateway router zulugwatrustnet which must be either an adjacent host or one of thelocal hosts own interfaces Naturally the hostname zulugwatrustnet must beresolvable to an IP address Use a numeric IP address if your DNS server is on theother side of the gatewayDestination networks were traditionally specified with separate IP addresses andnetmasks but all routingrelated commands now understand CIDR notation eg CIDR notation is clearer and relieves you of the need to fussover some of the systemspecific syntax issuesSome other tricks To inspect existing routes use the command netstat nr or netstat r ifyou want to see names instead of numbers Numbers are often better ifyou are debugging since the name lookup may be the thing that is broken An example of netstat output is shown on page On Linux iproute show is the officially blessed command for seeing routes Howeverwe find its output less clear than netstats Use the keyword default instead of an address or network name to setthe systems default route This mnemonic is identical to whichmatches any address and is less specific than any real routing destination Use ip route del Linux or route del FreeBSD to remove entries fromthe routing table Run ip route flush Linux or route flush FreeBSD to initialize therouting table and start over IPv routes are set up similarly to IPv routes include the option toroute to tell it that youre setting an IPv routing entry ip route can normally recognize IPv routes on its own by inspecting the format of theaddresses but it accepts the argument too etcnetworks maps names to network numbers much like the hosts filemaps hostnames to IP addresses Commands such as ip and route thatexpect a network number can accept a name if it is listed in the networksfile Network names can also be listed in DNS see RFCDNS configurationTo configure a machine as a DNS client you need only set up the etcresolvconffile DNS service is not strictly required but its hard to imagine a situation in whichyoud want to eliminate it completelyThe resolvconf file lists the DNS domains that should be searched to resolvenames that are incomplete that is not fully qualified such as anchor instead ofanchorcscoloradoedu and the IP addresses of the name servers to contact forname lookups A sample is shown here for more details see page search cscoloradoedu coloradoedunameserver nameserver nameserver etcresolvconf should list the closest stable name server first Servers are contacted in order and the timeout after which the next server in line is tried can bequite long You can have up to three nameserver entries If possible you shouldalways have more than oneIf the local host obtains the addresses of its DNS servers through DHCP the DHCPclient software stuffs these addresses into the resolvconf file for you when it obtains the leases Since DHCP configuration is the default for most systems yougenerally need not configure the resolvconf file manually if your DHCP serverhas been set up correctlyMany sites use Microsofts Active Directory DNS server implementation Thatworks fine with the standard UNIX and Linux resolvconf theres no need to doanything differentlySystemspecific network configurationOn early UNIX systems you configured the network by editing the system startup scripts and directly changing the commands they contained Modern systemshave readonly scripts they cover a variety of configuration scenarios and chooseamong them by reusing information from other system files or consulting configuration files of their ownAlthough this separation of configuration and implementation is a good idea every system does it a little bit differently The format and use of the etchosts andetcresolvconf files are relatively consistent among UNIX and Linux systems butthats about all you can count on for sureMost systems offer some sort of GUI interface for basic configuration tasks but themapping between the visual interface and the configuration files behind the scenesis often unclear In addition the GUIs tend to ignore advanced configurations andthey are relatively inconvenient for remote and automated administrationIn the next sections we pick apart some of the variations among our example systems describe whats going on under the hood and cover the details of networkconfiguration for each of our supported operating systems In particular we cover Basic configuration DHCP client configuration Dynamic reconfiguration and tuning Security firewalls filtering and NAT configurationKeep in mind that most network configuration happens at boot time so theres someoverlap between the information here and the information presented in Chapter Booting and System Management Daemons Linux networkingLinux developers love to tinker and they often implement features and algorithmsthat arent yet accepted standards One example is the Linux kernels addition ofpluggable congestioncontrol algorithms in release The several options include variations for lossy networks highspeed WANs with lots of packet loss satellite links and more The standard TCP reno mechanism slow start congestionavoidance fast retransmit and fast recovery is still used by default A variant mightbe more appropriate for your environment but probably notAfter any change to a file that controls network configuration at boot time you mayneed either to reboot or to bring the network interface down and then up again foryour change to take effect You can use ifdown interface and ifup interface for thispurpose on most Linux systemsNetworkManagerLinux support for mobile networking was relatively scattershot until the adventof NetworkManager in It consists of a service thats run continuously alongwith a system tray app for configuring individual network interfaces In addition tovarious kinds of wired network NetworkManager also handles transient wirelessnetworks wireless broadband and VPNs It continually assesses the available networks and shifts service to preferred networks as they become available Wirednetworks are most preferred followed by familiar wireless networksThis system represented quite a change for Linux network configuration In additionto being more fluid than the traditional static configuration its also designed to berun and managed by users rather than system administrators NetworkManagerhas been widely adopted by Linux distributions including all our examples but inan effort to avoid breaking existing scripts and setups its usually made availableas a sort of parallel universe of network configuration in addition to whatevertraditional network configuration was used in the pastDebian and Ubuntu run NetworkManager by default but keep the statically configured network interfaces out of the NetworkManager domain Red Hat and CentOSdont run NetworkManager by default at allNetworkManager is primarily of use on laptops since their network environmentmay change frequently For servers and desktop systems NetworkManager isntnecessary and may in fact complicate administration In these environments itshould be ignored or configured out See lwnnetArticles for some hints on when to consider the use of alternate congestion control algorithmsip manually configure a networkLinux systems formerly used the same basic commands for network configurationand status checking as traditional UNIX ifconfig route and netstat These arestill available on most distributions but active development has moved on to theiproute package which features the commands ip for most everyday networkconfiguration including routing and ss for examining the state of network sockets roughly replacing netstatIf youre accustomed to the traditional commands its worth the effort to transitionyour brain to ip The legacy commands wont be around forever and although theycover the common configuration scenarios they dont give access to the full featureset of the Linux networking stack ip is cleaner and more regularip takes a second argument for you to specify what kind of object you want toconfigure or examine There are many options but the common ones are ip linkfor configuring network interfaces ip address for binding network addresses tointerfaces and ip route for changing or printing the routing table Most objectsunderstand list or show to print out a summary of their current status so ip linkshow prints a list of network interfaces ip route show dumps the current routingtable and ip address show lists all assigned IP addressesThe man pages for ip are divided by subcommand For example to see detailedinformation about interface configuration run man iplink You can also run iplink help to see a short cheat sheetThe UNIX ifconfig command conflates the concept of interface configuration withthe concept of configuring the settings for a particular network protocol In factseveral protocols can run on a given network interface the prime example beingsimultaneous IPv and IPv and each of those protocols can support multipleaddresses so ips distinction between ip link and ip address is actually quite smartMost of what system administrators traditionally think of as interface configuration really has to do with setting up IPv and IPvip accepts a or argument to target IPv or IPv explicitly but its rarely necessary to specify these options ip guesses the right mode just by looking at theformat of the addresses you provideBasic configuration of an interface looks like this ip address add broadcast dev enpsIn this case the broadcast clause is superfluous because that would be the defaultvalue anyway given the netmask But this is how you would set it if you needed toOf course in daily life you wont normally be setting up network addresses by handThe next sections describe how our example distributions handle static configuration of the network from the perspective of configuration files You can abbreviate ip arguments so ip ad is the same as ip address We show full names for clarityDebian and Ubuntu network configurationAs shown in Table Debian and Ubuntu configure the network in etchostnameand etcnetworkinterfaces with a bit of help from the file etcnetworkoptionsTable Ubuntu network configuration files in etcFile Whats set therehostname Hostnamenetworkinterfaces IP address netmask default routeThe hostname is set in etchostname The name in this file should be fully qualified its value is used in a variety of contexts some of which require qualificationThe IP address netmask and default gateway are set in etcnetworkinterfacesA line starting with the iface keyword introduces each interface The iface linecan be followed by indented lines that specify additional parameters For exampleauto lo enpsiface lo inet loopbackiface enps inet staticaddress netmask gateway The ifup and ifdown commands read this file and bring the interfaces up or downby calling lowerlevel commands such as ip with the appropriate parametersThe auto clause specifies the interfaces to be brought up at boot time or wheneverifup a is runThe inet keyword in the iface line is the address family IPv To configure IPvas well include an inet configurationThe keyword static is called a method and specifies that the IP address and netmask for enps are directly assigned The address and netmask lines are requiredfor static configurations The gateway line specifies the address of the default network gateway and is used to install a default routeTo configure interfaces with DHCP just specify that in the interfaces fileiface enps inet dhcpRed Hat and CentOS network configurationRed Hat and CentOSs network configuration revolves around etcsysconfig Table on the next page shows the various configuration files RHELTable Red Hat network configuration files in etcsysconfigFile Whats set therenetwork Hostname default routenetworkscriptsifcfgifname Perinterface parameters IP address netmask etcnetworkscriptsrouteifname Perinterface routing arguments to ip routeYou set the machines hostname in etcsysconfignetwork which also containslines that specify the machines DNS domain and default gateway Essentially thisfile is where you specify all interfaceindependent network settingsFor example here is a network file for a host with a single Ethernet interfaceNETWORKINGyesNETWORKINGIPVnoHOSTNAMEredhattoadranchcomDOMAINNAMEtoadranchcom optionalGATEWAYInterfacespecific data is stored in etcsysconfignetworkscriptsifcfgifnamewhere ifname is the name of the network interface These configuration files setthe IP address netmask network and broadcast address for each interface Theyalso include a line that specifies whether the interface should be configured upat boot timeA generic machine has files for an Ethernet interface eth and for the loopbackinterface lo For exampleDEVICEethIPADDRNETMASKNETWORKBROADCASTMTUONBOOTyesandDEVICEloIPADDRNETMASKNETWORKBROADCASTONBOOTyesNAMEloopbackare the ifcfgeth and ifcfglo files for the machine redhattoadranchcom describedin the network file aboveA DHCPbased setup for eth is even simplerDEVICEethBOOTPROTOdhcpONBOOTyesAfter changing configuration information in etcsysconfig run ifdown ifname followed by ifup ifname for the appropriate interface If you reconfigure multiple interfaces at once you can use the command sysctl restart network to reset networkingLines in networkscriptsrouteifname are passed as arguments to ip route whenthe corresponding interface is configured For example the linedefault via sets a default route This isnt really an interfacespecific configuration but for clarity it should go in the file that corresponds to the interface on which you expectdefaultrouted packets to be transmittedLinux network hardware optionsThe ethtool command queries and sets a network interfaces mediaspecific parameters such as link speed and duplex It replaces the old miitool command but somesystems still include both If ethtool is not installed by default its usually includedin an optional package of its own also called ethtoolYou can query the status of an interface just by naming it For example this ethinterface a generic NIC on a PC motherboard has autonegotiation enabled andis currently running at full speed ethtool ethSettings for eth Supported ports TP MII Supported link modes baseTHalf baseTFull baseTHalf baseTFull baseTHalf baseTFull Supports autonegotiation Yes Advertised link modes baseTHalf baseTFull baseTHalf baseTFull baseTHalf baseTFull Advertised autonegotiation Yes Speed Mbs Duplex Full Port MII PHYAD Transceiver internal Autonegotiation on Supports Wakeon pumbg Wakeon g Current message level x Link detected yesTo lock this interface to Mbs full duplex use the command ethtool s eth speed duplex fullIf you are trying to determine whether autonegotiation is reliable in your environment you may also find ethtool r helpful It forces the parameters of the link tobe renegotiated immediatelyAnother useful option is k which shows what protocolrelated tasks are assignedto the network interface rather than being performed by the kernel Most interfacescan calculate checksums and some can assist with segmentation as well Unless youbelieve that a network interface is not doing these tasks reliably its always betterto offload them You can use ethtool K in combination with various suboptionsto force or disable specific types of offloading The k option shows current valuesand the K option sets themAny changes you make with ethtool are transient If you want them to be enforcedconsistently make sure that ethtool gets run as part of the systems network configuration Its best to do this as part of the perinterface configuration if you justarrange to have some ethtool commands run at boot time your configuration willnot properly cover cases in which the interfaces are restarted without a reboot ofthe systemOn Red Hat and CentOS systems you can include an ETHTOOLOPTS line in theconfiguration file for the interface underneath etcsysconfignetworkscripts Theifup command passes the entire line as arguments to ethtoolIn Debian and Ubuntu you can run ethtool commands directly from the configuration for a particular network in etcnetworkinterfacesLinux TCPIP optionsLinux puts a representation of each tunable kernel variable into the proc virtualfilesystem See Tuning Linux kernel parameters starting on page for generalinformation about the proc mechanismThe networking variables are under procsysnetipv and procsysnetipvWe formerly showed a complete list here but there are too many to list these daysThe ipv directory includes a lot more parameters than does the ipv directorybut thats mostly because IPversionindependent protocols such as TCP and UDPconfine their parameters to the ipv directory A prefix such as tcp or udp tellsyou which protocol the parameter relates toThe conf subdirectories within ipv and ipv contain parameters that are set perinterface They include subdirectories all and default and a subdirectory for eachinterface including the loopback Each subdirectory has the same set of filesRHELubuntu ls F procsysnetipvconfdefaultacceptlocal dropgratuitousarp proxyarpacceptredirects dropunicastinlmulticast proxyarppvlanacceptsourceroute forceigmpversion routelocalnetarpaccept forwarding rpfilterarpannounce igmpvunsolicitedreportinterval secureredirectsarpfilter igmpvunsolicitedreportinterval sendredirectsarpignore ignorerouteswithlinkdown sharedmediaarpnotify logmartians srcvalidmarkbootprelay mcforwarding tagdisablepolicy mediumiddisablexfrm promotesecondariesIf you change a variable in the confenps subdirectory for example your changeapplies to that interface only If you change the value in the confall directory youmight expect it to set the corresponding value for all existing interfaces but thisis not what happens Each variable has its own rules for accepting changes via allSome values are ORed with the current values some are ANDed and still othersare MAXed or MINed As far as we are aware there is no documentation for thisprocess except in the kernel source code so the whole debacle is probably bestavoided Just confine your modifications to individual interfacesIf you change a variable in the confdefault directory the new value propagatesto any interfaces that are later configured On the other hand its nice to keep thedefaults unmolested as reference information they make a nice sanity check if youwant to undo other changesThe procsysnetipvneigh and procsysnetipvneigh directories also contain a subdirectory for each interface The files in each subdirectory control ARPtable management and IPv neighbor discovery for that interface Here is the listof variables the ones starting with gc for garbage collection determine how ARPtable entries are timed out and discardedubuntu ls F procsysnetipvneighdefaultanycastdelay gcinterval locktime retranstimeappsolicit gcstaletime mcastresolicit retranstimemsbasereachabletime gcthresh mcastsolicit ucastsolicitbasereachabletimems gcthresh proxydelay unresqlendelayfirstprobetime gcthresh proxyqlen unresqlenbytesTo see the value of a variable use cat To set it you can use echo redirected to theproper filename but the sysctl command which is just a command interface tothe same variables is often easierFor example the commandubuntu cat procsysnetipvicmpechoignorebroadcastsshows that this variables value is meaning that broadcast pings are not ignoredTo set it to and avoid falling prey to Smurftype denial of service attacks runubuntu sudo sh c echo icmpechoignorebroadcasts from the procsysnet directory orubuntu sysctl netipvicmpechoignorebroadcastssysctl variable names are pathnames relative to procsys Dots are the traditionalseparator but sysctl also accepts slashes if you prefer themYou are typically logged in over the same network you are tweaking as you adjustthese variables so be careful You can mess things up badly enough to require areboot from the console to recover which might be inconvenient if the systemhappens to be in Point Barrow Alaska and its January Testtune these variableson your desktop system before you even think of tweaking a production machineTo change any of these parameters permanently or more accurately to reset themevery time the system boots add the appropriate variables to etcsysctlconf whichis read by the sysctl command at boot time For example the linenetipvipforwardin the etcsysctlconf file turns off IP forwarding on this hostSome of the options under proc are better documented than others Your best betis to look at the man page for the protocol in question in section of the manualsFor example man icmp documents six of the eight available options You musthave man pages for the Linux kernel installed to see man pages about protocolsYou can also look at the ipsysctltxt file in the kernel source distribution for somegood comments If you dont have kernel source installed just Google for ipsysctltxtto reach the same documentSecurityrelated kernel variablesTable shows Linuxs default behavior with regard to various touchy networkissues For a brief description of the implications of these behaviors see page We recommend that you verify the values of these variables so that you donot answer broadcast pings do not listen to routing redirects and do not acceptsourcerouted packets These should be the defaults on current distributions except for acceptredirects If you try this command in the form sudo echo icmpechoignorebroadcasts you just generate a permission denied messageyour shell attempts to open the output file before it runs sudoYou want the sudo to apply to both the echo command and the redirection Ergo you must create aroot subshell in which to execute the entire commandTable Default securityrelated network behaviors in LinuxFeature Host Gateway Control file in procsysnetipvIP forwarding off on ipforward for the whole systemconfinterfaceforwarding per interface aICMP redirects obeys ignores confinterfaceacceptredirectsSource routing off off confinterfaceacceptsourcerouteBroadcast ping ignores ignores icmpechoignorebroadcastsa The interface can be either a specific interface name or all FreeBSD networkingAs a direct descendant of the BSD lineage FreeBSD remains something of a reference implementation of TCPIP It lacks many of the elaborations that complicatethe Linux networking stack From the standpoint of system administrators FreeBSD network configuration is simple and directifconfig configure network interfacesifconfig enables or disables a network interface sets its IP address and subnet maskand sets various other options and parameters It is usually run at boot time withcommandline parameters taken from config files but you can also run it by handto make changes on the fly Be careful if you are making ifconfig changes and arelogged in remotelymany a sysadmin has been locked out this way and had todrive in to fix thingsAn ifconfig command most commonly has the formifconfig interface family address options For example the commandifconfig em upsets the IPv address and netmask associated with the interface em and readiesthe interface for useinterface identifies the hardware interface to which the command applies The loopback interface is named lo The names of real interfaces vary according to theirhardware drivers ifconfig a lists the systems network interfaces and summarizestheir current settingsThe family parameter tells ifconfig which network protocol address family youwant to configure You can set up multiple protocols on an interface and use themall simultaneously but they must be configured separately The main options hereare inet for IPv and inet for IPv inet is assumed if you omit the parameterThe address parameter specifies the interfaces IP address A hostname is also acceptable here but the hostname must be resolvable to an IP address at boot timeFor a machines primary interface this means that the hostname must appear inthe local hosts file since other name resolution methods depend on the networkhaving been initializedThe keyword up turns the interface on down turns it off When an ifconfig command assigns an IP address to an interface as in the example above the up parameter is implicit and need not be mentioned by nameFor subnetted networks you can specify a CIDRstyle netmask as shown in theexample above or you can include a separate netmask argument The mask canbe specified in dotted decimal notation or as a byte hexadecimal number beginning with xThe broadcast option specifies the IP broadcast address for the interface expressedin either hex or dotted quad notation The default broadcast address is one in whichthe host part is set to all s In the ifconfig example above the autoconfiguredbroadcast address is FreeBSD network hardware configurationFreeBSD does not have a dedicated command analogous to Linuxs ethertool Instead ifconfig passes configuration information down to the network interfacedriver through the media and mediaopt clauses The legal values for these optionsvary with the hardware To find the list read the man page for the specific driverFor example an interface named em uses the em driver man em shows thatthis is the driver for certain types of Intelbased wired Ethernet hardware To forcethis interface to gigabit mode using fourpair cabling the typical configurationthe command would be ifconfig em media baseT mediaopt fullduplexYou can include these media options along with other configuration clauses forthe interfaceFreeBSD boottime network configurationFreeBSDs static configuration system is mercifully simple All the network parameters live in etcrcconf along with other systemwide settings Heres a typicalconfigurationhostnamefreebeerifconfigeminet netmask defaultrouterEach network interface has its own ifconfig variable The value of the variable issimply passed to ifconfig as a series of commandline arguments The defaultrouterclause identifies a default network gatewayTo obtain the systems networking configuration from a DHCP server use the following tokenifconfigemDHCPThis form is magic and is not passed on to ifconfig which wouldnt know how tointerpret a DHCP argument Instead it makes the startup scripts run the commanddhclient em To modify the operational parameters of the DHCP system timeoutsand such set them in etcdhclientconf The default version of this file is emptyexcept for comments and you shouldnt normally need to modify itIf you modify the network configuration you can run service netif restart to repeatthe initial configuration procedure If you changed the defaultrouter parameteralso run service routing restartFreeBSD TCPIP configurationFreeBSDs kernellevel networking options are controlled similarly to those of Linuxsee page except that theres no proc hierarchy you can go rooting aroundin Instead run sysctl ad to list the available parameters and their oneline descriptions There are a lot of them on FreeBSD so you need to grep forlikely suspects such as redirect or netTable lists a selection of securityrelated parametersTable Default securityrelated network parameters in FreeBSDParameter Dfl What it does when set to netinetipforwarding Acts as a router for IPv packetsnetinetipforwarding Acts as a router for IPv packetsnetinettcpblackhole Disables unreachable messages for closed portsnetinetudpblackhole Does not send RST packets for closed TCP portsnetineticmpdropredirect Ignores IPv ICMP redirectsnetineticmprediraccept Accepts obeys IPv ICMP redirectsnetinetipacceptsourceroute Allows sourcerouted IPv packetsThe blackhole options are potentially useful on systems that you want to shieldfrom port scanners but they do change the standard behaviors of UDP and TCPYou might also want to disable acceptance of ICMP redirects for both IPv and IPvYou can set parameters in the running kernel with sysctl For example sudo sysctl netineticmpdropredirectTo have the parameter set at boot time list it in etcsysctlconfnetineticmpdropredirect Network troubleshootingSeveral good tools are available for debugging a network at the TCPIP layer Mostgive lowlevel information so you must understand the main ideas of TCPIP androuting to use themIn this section we start with some general troubleshooting strategy We then coverseveral essential tools including ping traceroute tcpdump and Wireshark Wedont discuss the arp ndp ss or netstat commands in this chapter though theytoo are useful debugging toolsBefore you attack your network consider these principles Make one change at a time Test each change to make sure that it had theeffect you intended Back out any changes that have an undesired effect Document the situation as it was before you got involved and documentevery change you make along the way Start at one end of a system or network and work through the systemscritical components until you reach the problem For example you mightstart by looking at the network configuration on a client work your wayup to the physical connections investigate the network hardware and finally check the servers physical connections and software configuration Or use the layers of the network to negotiate the problem Start at the topor bottom and work your way through the protocol stackThis last point deserves a bit more discussion As described on page the architecture of TCPIP defines several layers of abstraction at which componentsof the network can function For example HTTP depends on TCP TCP dependson IP IP depends on the Ethernet protocol and the Ethernet protocol dependson the integrity of the network cable You can dramatically reduce the amount oftime spent debugging a problem if you first figure out which layer is misbehavingAsk yourself questions like these as you work up or down the stack Do you have physical connectivity and a link light Is your interface configured properly Do your ARP tables show other hosts Is there a firewall on your local machine Is there a firewall anywhere between you and the destination If firewalls are involved do they pass ICMP ping packets and responses Can you ping the localhost address Can you ping other local hosts by IP address Is DNS working properly Can you ping other local hosts by hostname Can you ping hosts on another network Do highlevel services such as web and SSH servers work Did you really check the firewallsOnce youve identified where the problem lies and have a fix in mind step back toconsider the effect that your subsequent tests and prospective fixes will have onother services and hostsping check to see if a host is aliveThe ping command and its IPv twin ping are embarrassingly simple but inmany situations they are the only commands you need for network debuggingThey send an ICMP ECHOREQUEST packet to a target host and wait to see ifthe host answers backYou can use ping to check the status of individual hosts and to test segments of thenetwork Routing tables physical networks and gateways are all involved in processing a ping so the network must be more or less working for ping to succeedIf ping doesnt work you can be pretty sure that nothing more sophisticated willwork eitherHowever this rule does not apply to networks or hosts that block ICMP echo requests with a firewall Make sure that a firewall isnt interfering with your debugging before you conclude that the target host is ignoring a ping You might considerdisabling a meddlesome firewall for a short period of time to facilitate debuggingIf your network is in bad shape chances are that DNS is not working Simplify thesituation by using numeric IP addresses when pinging and use pings n option toprevent ping from attempting to do reverse lookups on IP addressesthese lookups also trigger DNS requestsBe aware of the firewall issue if youre using ping to check your Internet connectivity too Some wellknown sites answer ping packets and others dont Weve foundgooglecom to be a consistent responderMost versions of ping run in an infinite loop unless you supply a packet count argument Once youve had your fill of pinging type the interrupt character usuallyControlC to get out If a machine hangs at boot time boots very slowly or hangs on inbound SSH connections DNSshould be a prime suspect Most systems use an approach to name resolution thats configurable inetcnsswitchconf If the system runs nscd the name service caching daemon that component deserves some suspicion as well If nscd crashes or is misconfigured name lookups are affected Use thegetent command to check whether your resolver and name servers are working properly eg getenthosts googlecom Note that recent versions of Windows block ping requests by defaultHeres an examplelinux ping beastPING beast bytes of data bytes from beast icmpseq ttl timems bytes from beast icmpseq ttl timems bytes from beast icmpseq ttl timemsC beast ping statistics packets transmitted received packet loss time msrtt minavgmaxmdev msThe output for beast shows the hosts IP address the ICMP sequence number of eachresponse packet and the round trip travel time The most obvious thing that theoutput above tells you is that the server beast is alive and connected to the networkThe ICMP sequence number is a particularly valuable piece of information Discontinuities in the sequence indicate dropped packets Theyre normally accompaniedby a message for each missing packetDespite the fact that IP does not guarantee the delivery of packets a healthy network should drop very few of them Lostpacket problems are important to trackdown because they tend to be masked by higherlevel protocols The network mayappear to function correctly but it will be slower than it ought to be not only because of the retransmitted packets but also because of the protocol overhead neededto detect and manage themTo track down the cause of disappearing packets first run traceroute see the nextsection to discover the route that packets are taking to the target host Then pingthe intermediate gateways in sequence to discover which link is dropping packetsTo pin down the problem you need to send a fair number of packets The fault generally lies on the link between the last gateway you can ping without loss of packetsand the gateway beyond thatThe round trip time reported by ping can afford insight into the overall performanceof a path through a network Moderate variations in round trip time do not usually indicate problems Packets may occasionally be delayed by tens or hundredsof milliseconds for no apparent reason thats just the way IP works You shouldsee a fairly consistent round trip time for the majority of packets with occasionallapses Many of todays routers implement ratelimited or lowpriority responsesto ICMP packets which means that a router may defer responding to your ping ifit is already dealing with a lot of other trafficThe ping program can send echo request packets of any size so by using a packetlarger than the MTU of the network bytes for Ethernet you can force fragmentation This practice helps you identify media errors or other lowlevel issuessuch as problems with a congested network or VPN You specify the desired packetsize in bytes with the s flag ping s cuinfocornelleduNote that even a simple command like ping can have dramatic effects In thesocalled Ping of Death attack crashed large numbers of UNIX and Windows systems It was launched simply by transmission of an overly large ping packet Whenthe fragmented packet was reassembled it filled the receivers memory buffer andcrashed the machine The Ping of Death issue has long since been fixed but keepin mind several other caveats regarding pingFirst its hard to distinguish the failure of a network from the failure of a serverwith only the ping command In an environment where ping tests normally worka failed ping just tells you that something is wrongIts also worth noting that a successful ping does not guarantee much about the target machines state Echo request packets are handled within the IP protocol stackand do not require a server process to be running on the probed host A responseguarantees only that a machine is powered on and has not experienced a kernelpanic Youll need higherlevel methods to verify the availability of individual services such as HTTP and DNStraceroute trace IP packetstraceroute originally written by Van Jacobson uncovers the sequence of gatewaysthrough which an IP packet travels to reach its destination All modern operatingsystems come with some version of traceroute The syntax is simplytraceroute hostnameMost of the variety of options are not important in daily use As usual the hostnamecan be specified as either a DNS name or an IP address The output is simply a listof hosts starting with the first gateway and ending at the destination For examplea traceroute from our host jaguar to our host nubark produces the following output traceroute nubarktraceroute to nubark hops max byte packets labgw ms ms ms dmzgw ms ms ms nubark ms ms msFrom this output we can tell that jaguar is three hops away from nubark and we cansee which gateways are involved in the connection The round trip time for eachgateway is also shownthree samples for each hop are measured and displayed Atypical traceroute between Internet hosts often includes more than hops evenif the two sites are just across towntraceroute works by setting the timetolive field TTL actually hop count to liveof an outbound packet to an artificially low number As packets arrive at a gatewaytheir TTL is decreased When a gateway decreases the TTL to it discards thepacket and sends an ICMP time exceeded message back to the originating host Even Windows has it but the command is spelled tracert extra history points if you can guess whyThe first three traceroute packets above have their TTL set to The first gateway tosee such a packet labgw in this case determines that the TTL has been exceededand notifies jaguar of the dropped packet by sending back an ICMP message Thesenders IP address in the header of the error packet identifies the gateway andtraceroute looks up this address in DNS to find the gateways hostnameTo identify the secondhop gateway traceroute sends out a second round of packetswith TTL fields set to The first gateway routes the packets and decreases their TTLby At the second gateway the packets are then dropped and ICMP error messagesare generated as before This process continues until the TTL is equal to the numberof hops to the destination host and the packets reach their destination successfullyMost routers send their ICMP messages from the interface closest to the destination If you run traceroute backward from the destination host you may see different IP addresses being used to identify the same set of routers You might alsodiscover that packets flowing in the reverse direction take a completely differentpath a configuration known as asymmetric routingSince traceroute sends three packets for each value of the TTL field you may sometimes observe an interesting artifact If an intervening gateway multiplexes trafficacross several routes the packets might be returned by different hosts in this casetraceroute simply prints them allLets look at a more interesting example from a host in Switzerland to caidaorg atthe San Diego Supercomputer Centerlinux traceroute caidaorgtraceroute to caidaorg hops max byte packets gwoetikerinitnet ms ms ms rzurcoreinitnet ms ms ms rfracoreinitnet ms ms ms ramscoreinitnet ms ms ms rloncoreinitnet ms ms ms rlaxceinitnet ms ms ms ceniclaapnet ms ms ms dclaxcoregecenicnet ms ms dctusaggcoregecenicnet ms ms ms dcsdsctusdcgecenicnet ms ms ms pinotsdscedu ms ms ms rommiecaidaorg ms ms msThis output shows that packets travel inside Init Sevens network for a long timeSometimes we can guess the location of the gateways from their names Init Sevenscore stretches all the way from Zurich zur to Frankfurt fra Amsterdam amsLondon lon and finally Los Angeles lax Here the traffic transfers to cenicnetwhich delivers the packets to the caidaorg host within the network of the San Diego Supercomputer Center sdsc in La Jolla CA We removed a few fractions of milliseconds from the longer lines to keep them from foldingSee page for moreinformation aboutreverse DNS lookupsAt hop we see a star in place of one of the round trip times This notation meansthat no response error packet was received in response to the probe In this casethe cause is probably congestion but that is not the only possibility traceroute relies on lowpriority ICMP packets which many routers are smart enough to dropin preference to real traffic A few stars shouldnt send you into a panicIf you see stars in all the time fields for a given gateway no time exceeded messagescame back from that machine Perhaps the gateway is simply down Sometimes agateway or firewall is configured to silently discard packets with expired TTLs Inthis case you can still see through the silent host to the gateways beyond Anotherpossibility is that the gateways error packets are slow to return and that traceroutehas stopped waiting for them by the time they arriveSome firewalls block ICMP time exceeded messages entirely If such a firewalllies along the path you wont get information about any of the gateways beyond itHowever you can still determine the total number of hops to the destination because the probe packets eventually get all the way thereAlso some firewalls may block the outbound UDP datagrams that traceroutesends to trigger the ICMP responses This problem causes traceroute to report nouseful information at all If you find that your own firewall is preventing you fromrunning traceroute make sure the firewall has been configured to pass UDP ports as well as ICMP ECHO type packetsA slow link does not necessarily indicate a malfunction Some physical networkshave a naturally high latency UMTSEDGEGPRS wireless networks are a goodexample Sluggishness can also be a sign of high load on the receiving network Inconsistent round trip times would support such a hypothesisYou may occasionally see the notation N instead of a star or round trip time Thenotation indicates that the current gateway sent back a network unreachable errormeaning that it doesnt know how to route your packet Other possibilities includeH for host unreachable and P for protocol unreachable A gateway that returnsany of these error messages is usually the last hop you can get to That host oftenhas a routing problem possibly caused by a broken network link either its staticroutes are wrong or dynamic protocols have failed to propagate a usable route tothe destinationIf traceroute doesnt seem to be working for you or is working slowly it may betiming out while trying to resolve the hostnames of gateways through DNS If DNSis broken on the host you are tracing from use traceroute n to request numeric output This option disables hostname lookups it may be the only way to gettraceroute to function on a crippled networktraceroute needs root privileges to operate To be available to normal users it mustbe installed setuid root Several Linux distributions include the traceroute command but turn off the setuid bit Depending on your environment and needs youcan either turn the setuid bit back on or give interested users access to the command through sudoRecent years have seen the introduction of several new traceroutelike utilities thatcan bypass ICMPblocking firewalls See the PERTKB Wiki for an overview of thesetools at googlfXpMeu We especially like mtr which has a toplike interface andshows a sort of live traceroute NeatWhen debugging routing issues look at your site from the perspective of the outside world Several webbased route tracing services let you do this sort of inversetraceroute right from a browser window Thomas Kernen maintains a list of theseservices at tracerouteorgPacket snifferstcpdump and Wireshark belong to a class of tools known as packet sniffers Theylisten to network traffic and record or print packets that meet criteria of your choiceFor example you can inspect all packets sent to or from a particular host or TCPpackets related to one particular network connectionPacket sniffers are useful both for solving problems that you know about and fordiscovering entirely new problems Its a good idea to take an occasional sniff ofyour network to make sure the traffic is in orderPacket sniffers need to be able to intercept traffic that the local machine would notnormally receive or at least pay attention to so the underlying network hardwaremust allow access to every packet Broadcast technologies such as Ethernet workfine as do most other modern local area networksSince packet sniffers need to see as much of the raw network traffic as possible theycan be thwarted by network switches which by design try to limit the propagationof unnecessary packets However it can still be informative to try out a sniffer ona switched network You may discover problems related to broadcast or multicastpackets Depending on your switch vendor you may be surprised at how muchtraffic you can see Even if you dont see other systems network traffic a sniffercan be helpful when you are tracking down problems that involve the local hostIn addition to having access to all network packets the interface hardware musttransport those packets up to the software layer Packet addresses are normallychecked in hardware and only broadcastmulticast packets and those addressedto the local host are relayed to the kernel In promiscuous mode an interface letsthe kernel read all packets on the network even the ones intended for other hostsPacket sniffers understand many of the packet formats used by standard networkservices and they can print these packets in humanreadable form This capabilitymakes it easier to track the flow of a conversation between two programs Somesniffers print the ASCII contents of a packet in addition to the packet header andso are useful for investigating highlevel protocolsSee page for moreinformation aboutnetwork switchesSince some protocols send information and even passwords across the networkas cleartext take care not to invade the privacy of your users On the other handnothing quite dramatizes the need for cryptographic security like the sight of aplaintext password captured in a network packetSniffers read data from a raw network device so they must run as root Althoughthis root limitation serves to decrease the chance that normal users will listen inon your network traffic it is really not much of a barrier Some sites choose to remove sniffer programs from most hosts to reduce the chance of abuse If nothingelse you should check your systems interfaces to be sure they are not running inpromiscuous mode without your knowledge or consenttcpdump commandline packet sniffertcpdump yet another amazing network tool by Van Jacobson runs on most systems tcpdump has long been the industrystandard sniffer and most other network analysis tools read and write trace files in tcpdump format also known aslibpcap formatBy default tcpdump tunes in on the first network interface it comes across If itchooses the wrong interface you can force an interface with the i flag If DNS isbroken or you just dont want tcpdump doing name lookups use the n option Thisoption is important because slow DNS service can cause the filter to start droppingpackets before they can be dealt with by tcpdumpThe v flag increases the information you see about packets and vv gives you evenmore data Finally tcpdump can store packets to a file with the w flag and can readthem back in with the r flagNote that tcpdump w saves only packet headers by default This default makesfor small dumps but the most helpful and relevant information may be missingSo unless you are sure you need only headers use the s option with a value onthe order of actual values are MTUdependent to capture whole packetsfor later inspectionAs an example the following truncated output comes from the machine namednubark The filter specification host bull limits the display of packets to those thatdirectly involve the machine bull either as source or as destination sudo tcpdump host bull bull nubarkdomain A atrustcom DF nubarkdomain bull A DFThe first packet shows bull sending a DNS lookup request about atrustcom to nubarkThe response is the IP address of the machine associated with that name which is Note the time stamp on the left and tcpdumps understanding ofthe applicationlayer protocol in this case DNS The port number on bull is arbitrary and is shown numerically but since the server port number iswell known tcpdump shows its symbolic name domainPacket sniffers can produce an overwhelming amount of informationoverwhelming not only for you but also for the underlying operating system To avoid thisproblem on busy networks tcpdump lets you specify complex filters For examplethe following filter collects only incoming web traffic from one subnet sudo tcpdump src net and dst port The tcpdump man page contains several good examples of advanced filtering alongwith a complete listing of primitivesWireshark and TShark tcpdump on steroidstcpdump has been around since approximately the dawn of time but a newer opensource package called Wireshark formerly known as Ethereal has been gainingground rapidly Wireshark is under active development and incorporates morefunctionality than most commercial sniffing products Its an incredibly powerfulanalysis tool and should be included in every networking experts tool kit Its alsoan invaluable learning aidWireshark includes both a GUI interface wireshark and a commandline interface tshark Its available as a core package on most operating systems If its notin your systems core repository check wiresharkorg which hosts the source codeand a variety of precompiled binariesWireshark can read and write trace files in the formats used by many other packetsniffers Another handy feature is that you can click on any packet in a TCP conversation and ask Wireshark to reassemble splice together the payload data of allthe packets in the stream This feature is useful if you want to examine the datatransferred during a complete TCP exchange such as a connection on which anemail message is transmitted across the networkWiresharks capture filters are functionally identical to tcpdumps since Wiresharkuses the same underlying libpcap library Watch out thoughone important gotchawith Wireshark is the added feature of display filters which affect what you seerather than whats actually captured by the sniffer The display filter syntax is morepowerful than the libpcap syntax supported at capture time The display filters dolook somewhat similar but they are not the sameWireshark has builtin dissectors for a wide variety of network protocols includingmany used to implement SANs It breaks packets into a structured tree of information in which every bit of the packet is described in plain EnglishA note of caution regarding Wireshark although it has lots of neat features it has alsorequired many security updates over the years Run a current copy and do not leaveit running indefinitely on sensitive machines it might be a potential route of attackSee page formore informationabout SANs Network monitoringChapter Monitoring describes several generalpurpose platforms that canhelp structure the ongoing oversight of your systems and networks These systemsaccept data from a variety of sources summarize it in a way that illuminates ongoing trends and alert administrators to problems that require immediate attentionThe network is a key component of any computing environment so its often oneof the first parts of the infrastructure to benefit from systematic monitoring Ifyou dont feel quite ready to commit to a single monitoring platform for all youradministrative needs the packages outlined in this section are good options forsmallscale monitoring thats focused on the networkSmokePing gather ping statistics over timeEven healthy networks drop an occasional packet On the other hand networksshould not drop packets regularly even at a low rate because the impact on userscan be disproportionately severe Because highlevel protocols often function evenin the presence of packet loss you might never notice dropped packets unless youreactively monitoring for themSmokePing an open source tool by Tobias Oetiker can help you develop a morecomprehensive picture of your networks behavior SmokePing sends several pingpackets to a target host at regular intervals It shows the history of each monitoredlink through a web front end and can send alarms when things go amiss You canget a copy from ossoetikerchsmokepingExhibit D on the next page shows a SmokePing graph The vertical axis is the roundtrip time of pings and the horizontal axis is time weeks The black line fromwhich the gray spikes stick up indicates the median round trip time The spikesthemselves are the transit times of individual packets Since the gray in this graphappears only above the median line the great majority of packets must be travelingat close to the median speed with just a few being delayed This is a typical findingThe stairstepped shape of the median line indicates that the baseline transit timeto this destination has changed several times during the monitoring period Themost likely hypotheses to explain this observation are either that the host is reachable by several routes or that it is actually a collection of several hosts that have thesame DNS name but multiple IP addressesiPerf track network performancePingbased tools are helpful for verifying reachability but theyre not really powerful enough to analyze and track network performance Enter iPerf The latestversion iPerf has an extensive set of features that administrators can use to finetune network settings for maximum performanceExhibit D Sample SmokePing graphHere we look only at iPerf s throughput monitoring At the most basic level iPerfopens a connection TCP or UDP between two servers passes data between themand records how long the process tookOnce youve installed iperf on both machines start the server side iperf sServer listening on TCP port TCP window size KByte defaultThen on the machine you want to test from transfer some data as shown here iperf c Client connecting to TCP port TCP window size KByte default local port connected with port ID Interval Transfer Bandwidth sec GBytes GbitsseciPerf returns great instantaneous data for tracking bandwidth Its particularlyhelpful for assessing the effect of changes to kernel parameters that control thenetwork stack such as changes to the maximum transfer unit MTU see page for more detailsCacti collect and graph dataCacti available from cactinet offers several attractive features It uses a separatepackage RRDtool as its back end in which it stores monitoring data in the formof zeromaintenance statically sized databasesCacti stores only enough data to create the graphs you want For example Cacticould store one sample every minute for a day one sample every hour for a weekand one sample every week for a year This consolidation scheme lets you maintainimportant historical context without having to store unimportant details or spendtime on database administrationCacti can record and graph any SNMP variable see page as well as manyother performance metrics Youre free to collect whatever data you want Whencombined with the NETSNMP agent Cacti generates a historical perspective onalmost any system or network resourceExhibit E shows some examples of graphs created by Cacti These graphs show theload average on a device over a period of multiple weeks along with a days trafficon a network interfaceExhibit E Examples of Cacti graphsCacti sports easy webbased configuration as well as all the other builtin benefits ofRRDtool such as low maintenance and beautiful graphing See the RRDtool homepage at rrdtoolorg for links to the current versions of RRDtool and Cacti as wellas dozens of other monitoring tools Firewalls and NATWe do not recommend the use of Linux UNIX or Windows systems as firewallsbecause of the insecurity inherent in running a fullfledged generalpurpose operating system However all operating systems have firewall features and a hardenedsystem is a workable substitute for organizations that dont have the budget for ahighdollar firewall appliance Likewise a Linux or UNIX firewall is a fine optionfor a securitysavvy home user with a penchant for tinkeringIf you are set on using a generalpurpose computer as a firewall make sure that itsup to date with respect to security configuration and patches A firewall machineis an excellent place to put into practice all the recommendations found in Chapter Security The section that starts on page discusses packetfilteringfirewalls in general If you are not familiar with the basic concept of a firewall itwould probably be wise to read that section before continuingMicrosoft has largely succeeded in convincing the world that every computer needsits own builtin firewall However thats not really true In fact machinespecificfirewalls can lead to no end of inconsistent behavior and mysterious network problems if they are not managed in synchrony with sitewide standardsTwo main schools of thought deal with the issue of machinespecific firewalls Thefirst school considers them superfluous According to this view firewalls belong ongateway routers where they can protect an entire network through the applicationof one consistent and consistently applied set of rulesThe second school considers machinespecific firewalls an important componentof a defense in depth security plan Although gateway firewalls are theoreticallysufficient to control network traffic they can be compromised routed around oradministratively misconfigured Therefore its prudent to implement the same network traffic restrictions through multiple redundant firewall systemsIf you do choose to implement machinespecific firewalls you need a system fordeploying them in a consistent and easily updatable way The configuration management systems described in Chapter are excellent candidates for this taskDont rely on manual configuration its just too vulnerable to entropyLinux iptables rules chains and tablesVersion of the Linux kernel introduced an allnew packethandling engine calledNetfilter along with a commandline tool iptables to manage it That said many consumeroriented networking devices such as Linksyss router products use Linuxand iptables at their core An even newer system nftables has been available since kernel version from Its an elaboration of the Netfilter system thats configured with the nft command rather than the iptables command We dont discuss it in this book but its worth evaluating at sites that run current kernelsiptables configuration can be rather fiddly Debian and Ubuntu include a simplefront end ufw that facilitates common operations and configurations Its worthchecking out if your needs dont stray far from the mainstreamiptables applies ordered chains of rules to network packets Sets of chains makeup tables and are used for handling specific kinds of trafficFor example the default iptables table is named filter Chains of rules in this tableare used for packetfiltering network traffic The filter table contains three defaultchains FORWARD INPUT and OUTPUT Each packet handled by the kernel ispassed through exactly one of these chainsRules in the FORWARD chain are applied to all packets that arrive on one networkinterface and need to be forwarded to another Rules in the INPUT and OUTPUTchains are applied to traffic addressed to or originating from the local host respectively These three standard chains are usually all you need for firewalling betweentwo network interfaces If necessary you can define a custom configuration to support more complex accounting or routing scenariosIn addition to the filter table iptables includes the nat and mangle tables Thenat table contains chains of rules that control Network Address Translation herenat is the name of the iptables table and NAT is the name of the generic addresstranslation scheme The section Private addresses and network address translationNAT on page discusses NAT and an example of the nat table in action isshown on page Later in this section we use the nat tables PREROUTINGchain for antispoofing packet filteringThe mangle table contains chains that modify or alter the contents of network packets outside the context of NAT and packet filtering Although the mangle table ishandy for special packet handling such as resetting IP timetolive values it is nottypically used in most production environments We discuss only the filter and nattables in this section leaving the mangle table to the adventurousiptables rule targetsEach rule that makes up a chain has a target clause that determines what to dowith matching packets When a packet matches a rule its fate is in most cases sealedno additional rules are checked Although many targets are defined internally toiptables it is possible to specify another chain as a rules targetThe targets available to rules in the filter table are ACCEPT DROP REJECT LOGULOG REDIRECT RETURN MIRROR and QUEUE When a rule results inan ACCEPT matching packets are allowed to proceed on their way DROP andREJECT both drop their packets DROP is silent and REJECT returns an ICMPerror message LOG gives you a simple way to track packets as they match rulesand ULOG expands loggingREDIRECT shunts packets to a proxy instead of letting them go on their merryway For example you might use this feature to force all your sites web traffic togo through a web cache such as Squid RETURN terminates userdefined chainsand is analogous to the return statement in a subroutine call The MIRROR targetswaps the IP source and destination addresses before sending the packet FinallyQUEUE hands packets to local user programs through a kernel moduleiptables firewall setupBefore you can use iptables as a firewall you must enable IP forwarding and makesure that various iptables modules have been loaded into the kernel For more information on enabling IP forwarding see Linux TCPIP options on page orSecurityrelated kernel variables on page Packages that install iptables generally include startup scripts to achieve this enabling and loadingA Linux firewall is usually implemented as a series of iptables commands contained in an rc startup script Individual iptables commands usually take one ofthe following formsiptables F chainnameiptables P chainname targetiptables A chainname i interface j targetThe first form F flushes all prior rules from the chain The second form P setsa default policy aka target for the chain We recommend that you use DROP forthe default chain target The third form A appends the current specification tothe chain Unless you specify a table with the t argument your commands applyto chains in the filter table The i parameter applies the rule to the named interfaceand j identifies the target iptables accepts many other clauses some of which areshown in Table Table Commandline flags for iptables filtersClause Meaning or possible valuesp proto Matches by protocol tcp udp or icmps sourceip Matches host or network source IP address CIDR notation is OKd destip Matches host or network destination addresssport port Matches by source port note the double dashesdport port Matches by destination port note the double dashesicmptype type Matches by ICMP type code note the double dashes Negates a clauset table Specifies the table to which a command applies default is filterA complete exampleBelow we break apart a complete example We assume that the eth interface goesto the Internet and that the eth interface goes to an internal network The eth IPaddress is the eth IP address is and both interfaces havea netmask of This example uses stateless packet filtering to protectthe web server with IP address which is the standard method of protecting Internet servers Later in the example we show how to use stateful filtering toprotect desktop usersOur first set of rules initializes the filter table First all chains in the table are flushedthen the INPUT and FORWARD chains default target is set to DROP As with anyother network firewall the most secure strategy is to drop any packets you havenot explicitly allowediptables Fiptables P INPUT DROPiptables P FORWARD DROPSince rules are evaluated in order we put our busiest rules at the front The first ruleallows all connections through the firewall that originate from within the trustednet The next three rules in the FORWARD chain allow connections through thefirewall to network services on Specifically we allow SSH port HTTPport and HTTPS port through to our web serveriptables A FORWARD i eth p ANY j ACCEPTiptables A FORWARD d p tcp dport j ACCEPTiptables A FORWARD d p tcp dport j ACCEPTiptables A FORWARD d p tcp dport j ACCEPTThe only TCP traffic we allow to our firewall host is SSH which is usefulfor managing the firewall itself The second rule listed below allows loopback traffic which stays local to the host Administrators get nervous when they cant pingtheir default route so the third rule here allows ICMP ECHOREQUEST packetsfrom internal IP addressesiptables A INPUT i eth d p tcp dport j ACCEPTiptables A INPUT i lo d p ANY j ACCEPTiptables A INPUT i eth d p icmp icmptype j ACCEPTFor any IP host to work properly on the Internet certain types of ICMP packetsmust be allowed through the firewall The following eight rules allow a minimal setof ICMP packets to the firewall host as well as to the network behind itiptables A INPUT p icmp icmptype j ACCEPTiptables A INPUT p icmp icmptype j ACCEPTiptables A INPUT p icmp icmptype j ACCEPTiptables A INPUT p icmp icmptype j ACCEPTiptables A FORWARD d p icmp icmptype j ACCEPTiptables A FORWARD d p icmp icmptype j ACCEPTiptables A FORWARD d p icmp icmptype j ACCEPTiptables A FORWARD d p icmp icmptype j ACCEPT However you must be careful that reordering the rules for performance doesnt modify functionalityWe next add rules to the PREROUTING chain in the nat table Although the nattable is not intended for packet filtering its PREROUTING chain is particularlyuseful for antispoofing filtering If we put DROP entries in the PREROUTINGchain they need not be present in the INPUT and FORWARD chains since thePREROUTING chain is applied to all packets that enter the firewall host Its cleanerto put the entries in a single place rather than to duplicate themiptables t nat A PREROUTING i eth s j DROPiptables t nat A PREROUTING i eth s j DROPiptables t nat A PREROUTING i eth s j DROPiptables t nat A PREROUTING i eth s j DROPiptables t nat A PREROUTING i eth s j DROPFinally we end both the INPUT and FORWARD chains with a rule that forbidsall packets not explicitly permitted Although we already enforced this behaviorwith the iptables P commands the LOG target lets us see who is knocking on ourdoor from the Internetiptables A INPUT i eth j LOGiptables A FORWARD i eth j LOGOptionally we could set up IP NAT to disguise the private address space used onthe internal network See page for more information about NATOne of the most powerful features that Netfilter brings to Linux firewalling is statefulpacket filtering Instead of allowing specific incoming services a firewall for clientsconnecting to the Internet needs to allow incoming responses to the clients requestsThe simple stateful FORWARD chain below allows all traffic to leave our networkbut allows only incoming traffic thats related to connections initiated by our hostsiptables A FORWARD i eth p ANY j ACCEPTiptables A FORWARD m state state ESTABLISHEDRELATED j ACCEPTCertain kernel modules must be loaded to enable iptables to track complex networksessions such as those of FTP and IRC If these modules are not loaded iptablessimply disallows those connections Although stateful packet filters can increasethe security of your site they also add to the complexity of the network and canreduce performance Be sure you need stateful functionality before implementingit in your firewallPerhaps the best way to debug your iptables rulesets is to use iptables L v Theseoptions tell you how many times each rule in your chains has matched a packetWe often add temporary iptables rules with the LOG target when we want moreinformation about the packets that get matched You can often solve trickier problems by using a packet sniffer such as tcpdumpLinux NAT and packet filteringLinux traditionally implements only a limited form of Network Address Translation NAT that is more properly called Port Address Translation or PAT InsteadSee page formore informationabout IP spoofingof using a range of IP addresses as a true NAT implementation would PAT multiplexes all connections onto a single address The details and differences arent ofmuch practical importanceiptables implements NAT as well as packet filtering In earlier versions of Linuxthis functionality was a bit of a mess but iptables makes a much cleaner separation between the NAT and filtering features Of course if you use NAT to let localhosts access the Internet you must use a full complement of firewall filters as wellTo make NAT work enable IP forwarding in the kernel by setting the kernel variable procsysnetipvipforward to Additionally insert the appropriate kernel modules sudo modprobe iptablenat sudo modprobe ipconntrack sudo modprobe ipconntrackftpMany other modules track connections see the netnetfilter subdirectory underneath libmodules for a more complete list and enable the ones you needThe iptables command to route packets using NAT is of the formiptables t nat A POSTROUTING o eth j SNAT to This example is for the same host as the filtering example in the previous section soeth is the interface connected to the Internet The eth interface does not appeardirectly in the command line above but its IP address is the one that appears as theargument to to The eth interface is the one connected to the internal networkTo Internet hosts it appears that all packets from hosts on the internal network haveeths IP address The host that implements NAT receives incoming packets looksup their true destinations rewrites them with the appropriate internal network IPaddress and sends them on their merry wayIPFilter for UNIX systemsIPFilter an open source package developed by Darren Reed supplies NAT andstateful firewall services on a variety of systems including Linux and FreeBSDYou can use IPFilter as a loadable kernel module which is recommended by thedevelopers or include it statically in the kernelIPFilter is mature and featurecomplete The package has an active user communityand a history of continuous development It is capable of stateful tracking even forstateless protocols such as UDP and ICMPIPFilter reads filtering rules from a configuration file usually etcipfipfconf or etcipfconf rather than obliging you to run a series of commands as does iptablesAn example of a simple rule that could appear in ipfconf isblock in allThis rule blocks all inbound traffic ie network activity received by the system onall network interfaces Certainly secure but not particularly usefulTable shows some of the possible conditions that can appear in an ipf ruleTable Commonly used ipf conditionsCondition Meaning or possible valueson interface Applies the rule to the specified interfaceproto protocol Selects packet according to protocol tcp udp or icmpfrom sourceip Filters by source host network or anyto destip Filters by destination host network or anyport port Filters by port name from etcservices or number aflags flagspec Filters according to TCP header flags bitsicmptype number Filters by ICMP type and codekeep state Retains details about the flow of a session see belowa You can use any comparison operator etcIPFilter evaluates rules in the sequence in which they are presented in the configuration file The last match is binding For example inbound packets traversing thefollowing filter will always passblock in allpass in allThe block rule matches all packets but so does the pass rule and pass is the lastmatch To force a matching rule to apply immediately and make IPFilter skip subsequent rules use the quick keywordblock in quick allpass in allAn industrialstrength firewall typically contains many rules so liberal use of quickis important in order to maintain the performance of the firewall Without it everypacket is evaluated against every rule and this wastefulness is costlyPerhaps the most common use of a firewall is to control access to and from a specificnetwork or host often with respect to a specific port IPFilter has powerful syntaxto control traffic at this level of granularity In the following rules inbound traffic ispermitted to the network on TCP ports and and on UDP port block out quick allpass in quick proto tcp from any to port keep statepass in quick proto tcp from any to port keep statepass in quick proto udp from any to port keep stateblock in allThe keep state keywords deserve special attention IPFilter can keep track of connections by noting the first packet of new sessions For example when a new packetarrives addressed to port on IPFilter makes an entry in the state tableand allows the packet through It also allows the reply from the web server eventhough the first rule explicitly blocks all outbound traffickeep state is also useful for devices that offer no services but that must initiateconnections The following ruleset permits all conversations that are initiated by It blocks all inbound packets except those related to connectionsthat have already been initiatedblock in quick allpass out quick from to any keep stateThe keep state keywords work for UDP and ICMP packets too but since theseprotocols are stateless the mechanics are slightly more ad hoc IPFilter permitsresponses to a UDP or an ICMP packet for seconds after the inbound packet isseen by the filter For example if a UDP packet from port is addressed to port a UDP reply from will be permitted until seconds have passed Similarly an ICMP echo reply ping response ispermitted after an echo request has been entered in the state tableIPFilter uses the map keyword in place of pass and block to provide NAT servicesIn the following rule traffic from the network is mapped to the currentroutable address on the em interfacemap em The filter must be reloaded if the address of em changes as might happen if emleases a dynamic IP address through DHCP For this reason IPFilters NAT featuresare best used at sites that have a static IP address on the Internetfacing interfaceTable lists the commandline tools that come with the IPFilter packageTable IPFilter commandsCmd Functionipf Manages rules and filter listsipfstat Obtains statistics about packet filteringipmon Monitors logged filter informationipnat Manages NAT rulesOf the commands in Table ipf is the most commonly used ipf accepts a rulefile as input and adds correctly parsed rules to the kernels filter list ipf adds rulesto the end of the filter unless you use the Fa argument which flushes all existingSee page formore informationabout NATrules For example to flush the kernels existing set of filters and load the rules fromipfconf use the following syntax sudo ipf Fa f etcipfipfconfIPFilter relies on pseudodevice files in dev for access control and by default onlyroot can edit the filter list We recommend leaving the default permissions in placeand using sudo to maintain the filterUse ipfs v flag when loading the rules file to debug syntax errors and other problems in the configuration Cloud networkingOne of the interesting features of the cloud is that you get to define the networkingenvironment in which your virtual servers live Ultimately of course cloud serverslive on physical computers that are connected to real network hardware Howeverthat doesnt necessarily mean that virtual servers running on the same node arenetworked together The combination of virtualization technology and programmable network switching equipment gives platform providers great flexibility todefine the networking model they export to clientsAWSs virtual private cloud VPCVPC the softwaredefined network technology for Amazon Web Services createsprivate networks within the broader AWS network VPC was first introduced in as a bridge between an onpremises data center and the cloud opening upmany hybrid use cases for enterprise organizations Today VPC is a central featureof AWS and a default VPC is included for all accounts EC instances for newerAWS accounts must be created within a VPC and most new AWS services launchwith native VPC supportThe central features of VPC include An IPv address range selected from the RFC private address spaceexpressed in CIDR notation for example for the addresses Subnets to segment the VPC address space into smaller subnetworks Routing tables that determine where to send traffic Security groups that act as firewalls for EC instances Network Access Control Lists NACLs to isolate subnets from each otherYou can create as many VPCs as you need and no other AWS customer has accessto network traffic within your VPCs VPCs within the same region can be peered Longtime users gripe that AWS services are incomplete until they support VPC VPC has also recently added support for IPv Depending on the state of your account AWS may initially limit you to VPCs However you can request a higher limit if you need itcreating private routes between separate networks VPCs in different regions canbe connected with software VPN tunnels over the Internet or with expensive custom direct connections to AWS data centers over private circuits that you mustlease from a telcoVPCs can be as small as a network or as large as a Its important to planahead because the size cannot be adjusted after the VPC is created Choose an address space that is large enough to accommodate future growth but also ensure thatit does not conflict with other networks that you may wish to connectSubnets and routing tablesLike traditional networks VPCs are divided into subnets Public subnets are forservers that must talk directly to clients on the Internet They are akin to traditional DMZs Private subnets are inaccessible from the Internet and are intended fortrusted or sensitive systemsVPC routing is simpler than routing for a traditional hardware network because thecloud does not simulate physical topology Every accessible destination is reachablein one logical hopIn the world of physical networking every device has a routing table that tells ithow to route outbound network packets But in VPC routing tables are also anabstract entity thats defined through the AWS web console or its commandlineequivalent Every VPC subnet has an associated VPC routing table When instancesare created on a subnet their routing tables are initialized from the VPC templateThe simplest routing table contains only a default static route for reaching otherinstances within the same VPC You can add additional routes to access the Internet onpremises networks through VPN connections or other VPCs throughpeering connectionsA component called an Internet Gateway connects VPCs to the Internet This entityis transparent to the administrator and is managed by AWS However you need tocreate one and attach it to your VPC if instances are to have Internet connectivityHosts in public subnets can access the Internet Gateway directlyInstances in private subnets cannot be reached from the Internet even if they areassigned public IP addresses a fact that results in much confusion for new usersFor outbound access they must hop through a NAT gateway on a public subnetVPC offers a managed NAT feature which saves you the overhead of running yourown gateway but it incurs an additional hourly cost The NAT gateway is a potential bottleneck for applications that have high throughput requirements so itsbetter to locate the servers for such applications on public subnets avoiding NATAWSs implementation of IPv does not have NAT and all instances set up for IPvreceive public ie routable IPv addresses You make IPv subnets private byconnecting them through an egressonly Internet Gateway aka eigw which blocksSee page formore informationabout DMZsinbound connections The gateway is stateful so external hosts can talk to serverson the private IPv network as long as the AWS server initiates the connectionTo understand the network routing for an instance youll find it more informativeto review the VPC routing table for its subnet than to look at the instances actual routing table such as might be displayed by netstat r or ip route show whenlogged in to the instance The VPC version identifies gateways targets by theirAWS identifiers which makes the table easy to parse at a glanceIn particular you can easily distinguish public subnets from private subnets bylooking at the VPC routing table If the default gateway ie the target associatedwith the address is an Internet Gateway an entity named igwsomethingthen that subnet is public If the default gateway is a NAT device a route target prefixed by an instance ID isomething or natsomething then the subnet is privateTable shows an example routing table for a private subnetTable Example VPC routing table for a private subnetDestination Target Target type local Builtin route for the local VPC network nataed Internet access through a VPC NAT gateway pcxceb Peering connection to another VPC vgwed VPN gateway to an external networkVPCs are regional but subnets are restricted to a single availability zone To buildhighly available systems create at least one subnet per zone and distribute instancesevenly among all the subnets A typical design puts load balancers or other proxies inpublic subnets and restricts web application and database servers to private subnetsSecurity groups and NACLsSecurity groups are firewalls for EC instances Security group rules dictate whichsource addresses are allowed for ICMP UDP and TCP traffic ingress rules andwhich ports on other systems can be accessed by instances egress rules Securitygroups deny all connections by default so any rules you add allow additional trafficAll EC instances belong to at least one security group but they may be in as manyas five The more security groups an instance belongs to the more confusing itcan be to determine precisely what traffic is and is not allowed We prefer that eachinstance be in only one security group even if that configuration results in someduplicate rules among groups Security groups are actually associated with network interfaces and an instance can have more thanone network interface So to be perfectly correct we should say that the maximum number of security groups is the number of network interfaces times fiveWhen adding rules to security groups always consider the principle of least privilege Opening ports unnecessarily presents a security risk especially for systemsthat have public routable IP addresses For example a web server may only needports SSH used for management and control of the system HTTP and HTTPSIn addition all hosts should accept the ICMP packets used to implement path MTUdiscovery Failure to admit these packets can lower network bandwidth considerablyso we find puzzling AWSs decision to block them by default See googlWrETNqdeep link into docsawsamazoncom for the steps to enable these packetsMost security groups have granular inbound rules but allow all outbound traffic asshown in Table This configuration is convenient since you dont need to thinkabout what outside connectivity your systems have However its easier for attackersto set up shop if they can retrieve tools and communicate with their external controlsystems The most secure networks have both inbound and outbound restrictionsTable Typical security group rulesDirection Proto Ports CIDR NotesIngress TCP SSH from the internal networkIngress TCP HTTP from anywhereIngress TCP HTTPS from anywhereIngress ICMP na a Allow path MTU discoveryEgress ALL ALL Outbound traffic all OKa See googlWrETNq for detailed instructions this record is a bit tricky to set upMuch like access control lists on a firewall device NACLs control traffic amongsubnets Unlike security groups NACLs are stateless they dont distinguish betweennew and existing connections They are similar in concept to NACLs on a hardwarefirewall NACLs allow all traffic by default In the wild we see security groups usedfar more often than NACLsA sample VPC architectureExhibit F on the next page depicts two VPCs each with public and private subnetsNetwork hosts an Elastic Load Balancer in its public subnets The ELB acts as aproxy for some autoscaling EC instances that live in the private subnet and protectsthose instances from the Internet Service in Network may need access to Service hosted in Network and they can communicate privately through VPC peeringExhibit F Peered VPCs with public and private subnetsNAT NATVPC VPCVPC PeeringVPC VPC Autoscale groupSecurity GroupService Public netsPrivate netsElasticLoadBalancerAutoscale groupSecurity GroupService Internet usersPublic netsPrivate netsArchitecture diagrams like Exhibit F communicate dense technical details moreclearly than written prose We maintain diagrams like this one for every application we deployCreating a VPC with TerraformVPCs are composed of many resources each of which has its own settings and options The interdependencies among these objects are complex Its possible to createand manage almost everything by using the CLI or web console but that approachrequires that you keep all the minutiae in your head Even if you can keep all the moving parts straight during the initial setup its difficult to track your work over timeTerraform a tool from HashiCorp creates and manages cloud resources For exampleTerraform can create a VPC launch instances and then initialize those instances byrunning scripts or other configuration management tools Terraform configurationis expressed in HashiCorp Configuration Language HCL a declarative format thatlooks similar to JSON but adds variable interpolation and comments The file canbe tracked in revision control so its simple to update and adaptThe example below shows a Terraform configuration for a simple VPC with onepublic subnet We think its rather selfdocumenting intelligible even to a neophyte Specify the VPC address range as a variablevariable vpccidr default The address range for a public subnetvariable publicsubnetcidr default The VPCresource awsvpc default cidrblock varvpccidr enablednshostnames true Internet gateway to connect the VPC to the Internetresource awsinternetgateway default vpcid awsvpcdefaultid Public subnetresource awssubnet publicuswesta vpcid awsvpcdefaultid cidrblock varpublicsubnetcidr availabilityzone uswesta Route table for the public subnetresource awsroutetable publicuswesta vpcid awsvpcdefaultid route cidrblock gatewayid awsinternetgatewaydefaultid Associate the route table with the public subnetresource awsroutetableassociation publicuswesta subnetid awssubnetpublicuswestaid routetableid awsroutetablepublicuswestaidThe Terraform documentation is the authoritative syntax reference Youll findmany example configurations like this one in the Terraform GitHub repositoryand elsewhere on the InternetRun terraform apply to have Terraform create this VPC It examines the currentdirectory by default for tf files and processes each of them assembling an execution plan and then invoking API calls in the appropriate order You can set the AWSAPI credentials in the configuration file or through the AWSACCESSKEYID andAWSSECRETACCESSKEY environment variables as we have done here AWSACCESSKEYIDAKIAIOSFODNNEXAMPLE AWSSECRETACCESSKEYwJalrXUtnFEMIKMDENGbPxRfiCYEXAMPLEKEY time terraform applyawsvpcdefault Creating cidrblock defaultnetworkaclid computed defaultsecuritygroupid computed dhcpoptionsid computed enablednshostnames enablednssupport computed mainroutetableid computedawsvpcdefault Creation completeawsinternetgatewaydefault Creating vpcid vpcaebeccawssubnetpublicuswesta Creating availabilityzone uswesta cidrblock mappubliciponlaunch vpcid vpcaebeccawssubnetpublicuswesta Creation completeawsroutetablepublicuswesta Creation completesnipApply complete Resources added changed destroyedrealmsusermssys mstime measures how long it takes to create all the resources in the configurationabout seconds The computed values indicate that Terraform chose defaultsbecause we didnt specify those settings explicitlyThe state of all resources created by Terraform is saved in a file called terraformtfstateThis file must be preserved so that Terraform knows which resources are under itscontrol In the future Terraform will discover the managed resources on its ownWe can clean up the VPC just as easily terraform destroy forceawsvpcdefault Refreshing state ID vpcebeeawssubnetpublicuswesta Refreshing state ID subnetcabawsinternetgatewaydefault Refreshing state ID igwdcedbawsroutetablepublicuswesta Refreshing state ID rtbfcbawsroutetableassociationpublicuswesta Refreshing state IDrtbassocdabbeawsroutetableassociationpublicuswesta Destroyingawsroutetableassociationpublicuswesta Destruction completeawssubnetpublicuswesta Destroyingawsroutetablepublicuswesta Destroyingawsroutetablepublicuswesta Destruction completeawsinternetgatewaydefault Destroyingawssubnetpublicuswesta Destruction completeawsinternetgatewaydefault Destruction completeawsvpcdefault Destroyingawsvpcdefault Destruction completeApply complete Resources added changed destroyedTerraform is cloudagnostic so it can manage resources for AWS GCP DigitalOcean Azure Docker and other providersHow do you know when to use Terraform and when to use the CLI If youre building infrastructure for a team or project or if youll need to make changes and repeatthe build later use Terraform If you need to fire off a quick instance as a test ifyou need to inspect the details of a resource or if you need to access the API froma shell script use the CLIGoogle Cloud Platform networkingOn the Google Cloud Platform networking is functionally part of the platformas opposed to being represented as a distinct service GCP private networks areglobal an instance in the useast region can communicate with another instancein europewest over the private network a fact that makes it easy to build globalnetwork services Network traffic among instances in the same zone is free butthere is a fee for traffic between zones or regionsNew projects have a default network with the address range Youcan create up to five separate networks per project and instances are members ofexactly one network Many sites use this network architecture to isolate test anddevelopment from production systemsNetworks can be subdivided by region with subnetworks a relatively recent addition to GCP that functions differently from subnets on AWS The global networkdoes not need to be part of a single IPv prefix range and there can be multipleprefixes per region GCP configures all the routing for you so instances on different CIDR blocks within the same network can still reach each other Exhibit Gdemonstrates this topologyExhibit G A multiregion private GCP network with subnetworksInternal routinguseastbuseastuscentral uscentralf uscentralbThere is no concept of a subnetwork being public or private instead instances thatdont need to accept inbound traffic from the Internet can simply not have a public Internetfacing address Google offers static external IP addresses that you canborrow for use in DNS records without fear that they will be assigned to anothercustomer When an instance does have an external address you still wont see it ifyou run ip addr show Google handles the address translation for youBy default firewall rules in a GCP network apply to all instances To restrict rulesto a smaller set of instances you can tag instances and filter the rules accordingto the tags The default global firewall rules deny everything except the following ICMP traffic for RDP remote desktop for Windows TCP port for SSH TCP port for All ports and protocols for the internal network by defaultWhen it comes to decisions that impact security we always come back to the principle of least privilege In this case we recommend narrowing these default rules toblock RDP entirely allow SSH only from your own source IPs and further restricttraffic within the GCP network You might also want to block ICMP but be awarethat you need to allow ICMP packets of type code to enable path MTU discoveryDigitalOcean networkingDigitalOcean does not have a private network or at least not one similar to thoseof GCP and AWS Droplets can have private interfaces that communicate over aninternal network within the same region However that network is shared with allother DigitalOcean customers in the same region This is a slight improvement overusing the Internet but firewalls and intransit encryption become hard requirementsWe can examine a booted DigitalOcean droplet with the tugboat CLI tugboat info ulsahDroplet fuzzy name provided Finding droplet IDdone ulsahubuntuName ulsahubuntuID Status activeIP IP AEFDPrivate IP Region San Francisco sfoImage ubuntuxSize MBBackups Active falseThe output includes an IPv address in addition to public and private IPv addressesOn the instance we can further explore by looking at the addresses on the localinterfaces tugboat ssh ulsahubuntu ip address show eth eth BROADCASTMULTICASTUPLOWERUP mtu qdisc pfifofaststate UP group default qlen linkether d brd ffffffffffff inet brd scope global eth validlft forever preferredlft forever inet scope global eth validlft forever preferredlft forever inet fefffed scope link validlft forever preferredlft forever ip address show eth eth BROADCASTMULTICASTUPLOWERUP mtu qdisc pfifofaststate UP group default qlen linkether d brd ffffffffffff inet brd scope global eth validlft forever preferredlft forever inet fefffed scope link validlft forever preferredlft foreverThe public address is assigned directly to the eth interface not translated by theprovider as on other cloud platforms Each interface also has an IPv address soits possible to serve traffic through IPv and IPv simultaneously Recommended readingHistoryComer Douglas E Internetworking with TCPIP Volume Principles Protocolsand Architectures th Edition Upper Saddle River NJ Prentice Hall Doug Comers Internetworking with TCPIP series was for a long time the standardreference for the TCPIP protocols The books are designed as undergraduate textbooks and are a good introductory source of background materialSalus Peter H Casting the Net From ARPANET to INTERNET and Beyond Reading MA AddisonWesley Professional This is a lovely history of the ARPANET as it grew into the Internet written by ahistorian who has been hanging out with UNIX people long enough to sound likeone of themAn excellent collection of documents about the history of the Internet and its various technologies can be found at isocorginternethistoryClassics and biblesStevens W Richard UNIX Network Programming Upper Saddle River NJPrentice Hall Stevens W Richard Bill Fenner and Andrew M Rudoff UNIX NetworkProgramming Volume The Sockets Networking API rd Edition Upper SaddleRiver NJ AddisonWesley Stevens W Richard UNIX Network Programming Volume Interprocess Communications nd Edition Upper Saddle River NJ AddisonWesley The UNIX Network Programming books are the students bibles in networking classes that involve programming If you need only the Berkeley sockets interface theoriginal edition is still a fine reference If you need the STREAMS interface too thenthe third edition which includes IPv is a good bet All three are clearly writtenin typical Rich Stevens styleTanenbaum Andrew S and David J Wetherall Computer Networks th Edition Upper Saddle River NJ Prentice Hall PTR Computer Networks was the first networking text and it is still a classic It contains athorough description of all the nittygritty details going on at the physical and linklayers of the protocol stack The latest edition includes coverage of wireless networksgigabit Ethernet peertopeer networks voice over IP cellular networks and moreProtocolsFall Kevin R and W Richard Stevens TCPIP Illustrated Volume One TheProtocols nd Edition Reading MA AddisonWesley Wright Gary R and W Richard Stevens TCPIP Illustrated Volume Two TheImplementation Reading MA AddisonWesley The books in the TCPIP Illustrated series are an excellent and thorough guide tothe TCPIP protocol stackHunt Craig TCPIP Network Administration rd Edition Sebastopol CAOReilly Media Like other books in the nutshell series this book is directedat administrators of UNIX systems Half the book is about TCPIP and the restdeals with higherlevel UNIX facilities such as email and remote loginFarrel Adrian The Internet and Its Protocols A Comparative Approach San Francisco CA Morgan Kaufmann Publishers Kozierak Charles M The TCPIP Guide A Comprehensive Illustrated InternetProtocols Reference San Francisco CA No Starch Press Donahue Gary A Network Warrior Everything You Need to Know That Wasnton the CCNA Exam Sebastopol CA OReilly Media Regardless of whether your systems live in a data center a cloud or an old missilesilo one element they have in common is the need to communicate on a networkThe ability to move data quickly and reliably is essential in every environment Iftheres one area in which UNIX technology has touched human lives and influencedother operating systems its in the practical realization of largescale packetizeddata transportNetworks are following the same trail blazed by servers in that the physical and logical views of the network are increasingly separated by a virtualization layer that hasits own configuration That sort of setup is standard in the cloud but even physicaldata centers often include a layer of softwaredefined networking SDN these daysAdministrators interact with realworld network hardware less frequently than theyonce did but familiarity with traditional networking remains a crucial skill Virtualized networks closely emulate physical networks in their features terminologyarchitecture and topologyMany linklayer technologies have been promoted over the years but Ethernet hasemerged as the clear and decisive winner Now that Ethernet is found on everythingfrom game consoles to refrigerators a thorough understanding of this technologyis critical to success as a system administrator Physical NetworkingObviously the speed and reliability of your network have a direct effect on yourorganizations productivity But today networking has become so pervasive thatthe state of the network affects even such basic interactions as the ability to maketelephone calls A poorly designed network is a personal and professional embarrassment that can lead to catastrophic social effects It can also be expensive to fixAt least four major factors contribute to success Development of a reasonable network design Selection of highquality hardware Proper installation and documentation Competent ongoing operations and maintenanceThis chapter focuses on understanding installing and operating Ethernet networksin an enterprise environment Ethernet the Swiss Army knife of networkingHaving captured over of the worldwide local area network LAN marketEthernet can be found just about everywhere in its many forms It started as BobMetcalfes PhD thesis at MIT but is now described in a variety of IEEE standardsEthernet was originally specified at Mbs megabits per second but it moved to Mbs almost immediately Once a Mbs standard was finalized in itbecame clear that Ethernet would evolve rather than be replaced This realizationtouched off a race to build increasingly faster versions of Ethernet and that racecontinues today Table highlights the evolution of the various EthernetstandardsEthernet signalingThe underlying model used by Ethernet can be described as a polite dinner partyat which guests computers dont interrupt each other but rather wait for a lull inthe conversation no traffic on the network cable before speaking If two guestsstart to talk at once a collision they both stop excuse themselves wait a bit andthen one of them starts talking againThe technical term for this scheme is CSMACD Carrier Sense you can tell whether anyone is talking Multiple Access everyone can talk Collision Detection you know when you interrupt someone elseThe actual delay after a collision is somewhat random This convention avoids thescenario in which two hosts simultaneously transmit to the network detect thecollision wait the same amount of time and then start transmitting again thusflooding the network with collisions This was not always true We have omitted a few of the less popular Ethernet standards that cropped up along the wayToday the importance of the CSMACD conventions has been lessened by theadvent of switches which typically limit the number of hosts to two in a given collision domain To continue the dinner party analogy you might think of thisswitched variant of Ethernet as being akin to the scenes found in old movies wheretwo people sit at opposite ends of a long formal dining tableEthernet topologyThe Ethernet topology is a branching bus with no loops A packet can travel between two hosts on the same network in only one wayThree types of packets can be exchanged on a segment unicast multicast andbroadcast Unicast packets are addressed to only one host Multicast packets are addressed to a group of hosts Broadcast packets are delivered to all hosts on a segmentA broadcast domain is the set of hosts that receive packets destined for the hardware broadcast address Exactly one broadcast domain is defined for each logicalEthernet segment Under the early Ethernet standards and media eg BASETable The evolution of EthernetYear Speed Common name IEEE Dist Mediaa Mbs Xerox Ethernet Coax Mbs Ethernet m RG coax Mbs BASET m Cat UTP copper Mbs BASETX u m Cat UTP copper Gbs BASET gigabit ab m Cat e UTP copper Gbs GBASET gig an m Cat a a UTP Gbs GBASECRGBASESRPba mmUTP copperMM fiber Gbs GBASECRGBASESRPba mmUTP copperMM fiber b Gbs GBASEFRGBASELRbsc kmkmCWDM fiberCWDM fiber b Gbs GBASESRGBASEDRGBASEFRGBASELRbs mmkmkmMM fiber strandMM fiber strandCWDM fiberCWDM fiber b Tbs TbE TBD TBD TBDa MM Multimode SM Singlemode UTP Unshielded twisted pairCWDM Coarse wavelength division multiplexingb Industry projectionc Well give the benefit of the doubt and assume this lettering choice was an unfortunate coincidencephysical segments and logical segments were exactly the same because all thepackets traveled on one big cable with host interfaces strapped onto the side of itWith the advent of switches todays logical segments usually consist of many physical segments possibly dozens or hundreds to which only two devices are connected a switch port and a host The switches are responsible for escorting multicastand unicast packets to the physical or wireless segments on which the intendedrecipients reside Broadcast traffic is forwarded to all ports in a logical segmentA single logical segment can consist of physical or wireless segments that operateat different speeds Hence switches must have buffering and timing capabilities tolet them smooth over any potential timing conflictsUnshielded twistedpair cablingUnshielded twisted pair UTP has historically been the preferred cable medium forEthernet in most office environments Today wireless networking has displaced UTPin many situations The general shape of a UTP network is illustrated in Exhibit AExhibit A A UTP installationPUNISHER CPUNISHER CUTP switchWorkstation Workstationlink to backboneEthernet printerPowerUTP wire is commonly broken down into eight classifications The performancerating system was first introduced by Anixter a large cable supplier These standardswere formalized by the Telecommunications Industry Association TIA and areknown today as Category through Category with a few special variants suchas Category e and Category a thrown in for good measureThe International Organization for Standardization ISO has also jumped intothe exciting and highly profitable world of cable classification They promote standards that are exactly or approximately equivalent to the highernumbered TIA No kidding Attaching a new computer involved boring a hole into the outer sheath of the cable witha special drill to reach the center conductor A vampire tap that bit into the outer conductor wasthen clamped on with screws Wireless networks are another common type of logical Ethernet segment They behave more like thetraditional forms of Ethernet which share one cable among many hosts See page categories For example TIA Category cable is equivalent to ISO Class D cableFor the geeks in the audience Table illustrates the differences among the various modernday classifications This is good information to memorize so you canimpress your friends at partiesTable UTP cable characteristicsParameter UnitsCat bClass D Cat eCat Class ECat aClass EACat Class FCat aClass FACat Class IFrequency range MHz Attenuation dB NEXT a dB ELFEXT a dB Return loss dB Propagation delay ns a NEXT Nearend crosstalk ELFEXT Equal level farend crosstalkb Includes additional TIA and ISO requirements TSB and FDAM respectivelyCategory cable can support Mbs and is table stakes for network wiring today Category e Category and Category a cabling support Gbs and are themost common standard currently in use for data cabling Category a is the cable ofchoice for new installations because it is particularly resistant to interference fromolder Ethernet signaling standards eg BASET a problem that has plaguedsome Category e installations Category and Category a cable are intendedfor Gbs use and Category rounds out the family at GbsFaster standards require multiple pairs of UTP Having multiple conductors transports data across the link faster than any single pair can support BASETX requires two pairs of Category wire BASETX requires four pairs of Categorye or Category a wire and GBASETX requires four pairs of Category a or a wire All these standards are limited to meters in lengthBoth PVCcoated and Tefloncoated wire are available Your choice of jacketingshould depend on the environment in which the cable will be installed Enclosedareas that feed into the buildings ventilation system return air plenums typicallyrequire Teflon PVC is less expensive and easier to work with but produces toxicfumes if it catches fire hence the need to keep it out of air plenumsFor terminating the fourpair UTP cable at patch panels and RJ wall jacks wesuggest you use the TIAEIAA RJ wiring standard This standard which iscompatible with other uses of RJ eg RS is a convenient way to keep thewiring at both ends of the connection consistent regardless of whether you can easily access the cable pairs themselves Table on the next page shows the pinouts Check with your fire marshal or local fire department to determine the requirements in your areaTable TIAEIAA standard for wiring fourpair UTP to an RJ jackPair Colors Wired to Pair Colors Wired to WhiteBlue Pins WhiteGreen Pins WhiteOrange Pins WhiteBrown Pins Existing building wiring might or might not be suitable for network use dependingon how and when it was installedOptical fiberOptical fiber is used in situations where copper cable is inadequate Fiber carriessignals farther than copper and is also resistant to electrical interference In caseswhere fiber isnt absolutely necessary copper is normally preferred because its lessexpensive and easier to work withMultimode and single mode fiber are the two common types Multimode fiberis typically used for applications within a building or campus Its thicker than singlemode fiber and can carry multiple rays of light this feature permits the use ofless expensive electronics eg LEDs as a light sourceSinglemode fiber is most often found in longhaul applications such as intercity orinterstate connections It can carry only a single ray of light and requires expensiveprecision electronics on the endpointsA common strategy to increase the bandwidth across a fiber link is coarse wavelength division multiplexing CWDM Its a way to transmit multiple channels ofdata through a single fiber on multiple wavelengths colors of light Some of thefaster Ethernet standards use this scheme natively However it can also be employed to extend the capabilities of an existing dark fiber link through the use ofCWDM multiplexersTIAC recommends colorcoding the common types of fiber as shown in Table The key rule to remember is that everything must match The fiber that connects the endpoints the fiber crossconnect cables and the endpoint electronicsmust all be of the same type and size Note that although both OM and OM arecolored orange they are not interchangeablecheck the size imprint on the cablesto make sure they match You will experience no end of difficulttoisolate problems if you dont follow this ruleMore than types of connectors are used on the ends of optical fibers and thereis no real rhyme or reason as to which connectors are used where The connectorsyou need to use in a particular case will most often be dictated by your equipmentvendors or by your existing building fiber plant The good news is that conversionjumpers are fairly easy to obtainEthernet connection and expansionEthernets can be connected through several types of devices The options below areranked by approximate cost with the cheapest options first The more logic that adevice uses to move bits from one network to another the more hardware and embedded software the device needs to have and the more expensive it is likely to beHubsDevices from a bygone era hubs are also referred to as concentrators or repeatersThey are active devices that connect Ethernet segments at the physical layer Theyrequire external powerA hub retimes and retransmits Ethernet frames but does not interpret them it hasno idea where packets are going or what protocol they are using With the exceptionof extremely special cases hubs should no longer be used in enterprise networks wediscourage their use in residential consumer networks as well Switches make significantly more efficient use of network bandwidth and are just as cheap these daysSwitchesSwitches connect Ethernets at the link layer They join two physical networks in away that makes them seem like one big physical network Switches are the industrystandard for connecting Ethernet devices todaySwitches receive regenerate and retransmit packets in hardware Switches use adynamic learning algorithm They notice which source addresses come from oneport and which from another They forward packets between ports only when necessary At first all packets are forwarded but in a few seconds the switch has learnedthe locations of most hosts and can be more selectiveSince not all packets are forwarded among networks each segment of cable thatconnects to a switch is less saturated with traffic than it would be if all machineswere on the same cable Given that most communication tends to be localized theincrease in apparent bandwidth can be dramatic And since the logical model ofTable Attributes of standard optical fibersMode ISO nameaCorediameterCladdingdiameter ColorMulti OM m m OrangeMulti OM m m OrangeMulti OM mb m AquaSingle OS m m Yellowa According to ISO b OM is optimized for carrying laser lightthe network is not affected by the presence of a switch few administrative consequences result from installing oneSwitches can sometimes become confused if your network contains loops Theconfusion arises because packets from a single host appear to be on two or moreports of the switch A single Ethernet cannot have loops but as you connect several Ethernets with routers and switches the topology can include multiple pathsto a host Some switches can handle this situation by holding alternative routes inreserve in case the primary route goes down They prune the network they see untilthe remaining sections present only one path to each node on the network Someswitches can also handle duplicate links between the same two networks and routetraffic in a round robin fashionSwitches must scan every packet to determine if it should be forwarded Theirperformance is usually measured by both the packet scanning rate and the packetforwarding rate Many vendors do not mention packet sizes in the performancefigures they quote and thus actual performance might be less than advertisedAlthough Ethernet switching hardware is getting faster all the time it is still not areasonable technology for connecting more than a hundred hosts in a single logical segment Problems such as broadcast storms often plague large switched networks since broadcast traffic must be forwarded to all ports in a switched segmentTo solve this problem use a router to isolate broadcast traffic between switchedsegments thereby creating more than one logical EthernetChoosing a switch can be difficult The switch market is a highly competitive segment of the computer industry and its plagued with marketing claims that arenteven partially true When selecting a switch vendor rely on independent evaluations rather than on data supplied by vendors themselves In recent years it hasbeen common for one vendor to have the best product for a few months but thencompletely destroy its performance or reliability when trying to make improvementsthus elevating another manufacturer to the top of the heapIn all cases make sure that the backplane speed of the switch is adequatethats thenumber that really counts at the end of a long day A welldesigned switch shouldhave a backplane speed that exceeds the sum of the speeds of all its portsVLANcapable switchesLarge sites can benefit from switches that partition their ports through softwareconfiguration into subgroups called virtual local area networks or VLANs AVLAN is a group of ports that belong to the same logical segment as if the portswere connected to their own dedicated switch Such partitioning increases the ability of the switch to isolate traffic and that capability has beneficial effects on bothsecurity and performanceTraffic among VLANs is handled by a router or in some cases by a layer routingmodule or routing software layer within the switch An extension of this systemknown as VLAN trunking such as is specified by the IEEE Q protocol allows physically separate switches to service ports on the same logical VLANIts important to note that VLANs alone provide little additional security You mustfilter the traffic among VLANs to reap any potential security benefitRoutersRouters aka layer switches direct traffic at the network layer layer of theOSI network model They shuttle packets to their final destinations in accordancewith the information in the TCPIP protocol headers In addition to simply moving packets from one place to another routers can also perform other functionssuch as packet filtering for security prioritization for quality of service andbigpicture network topology discovery See Chapter for all the gory detailsof how routing actually worksRouters take one of two forms fixed configuration and modular Fixed configuration routers have network interfaces permanently installedat the factory They are usually suitable for small specialized applicationsFor example a router with a T interface and an Ethernet interface mightbe a good choice to connect a small company to the Internet Modular routers have a slot or bus architecture to which interfaces can beadded by the end user Although this approach is usually more expensiveit ensures greater flexibility down the roadDepending on your reliability needs and expected traffic load a dedicated routermight or might not be cheaper than a UNIX or Linux system configured to act asa router However a dedicated router usually achieves superior performance andreliability This is one area of network design in which its usually advisable to spendextra money up front to avoid headaches laterAutonegotiationWith the introduction of a variety of Ethernet standards came the need for devices to figure out how their neighbors were configured and to adjust their settingsaccordingly For example the network wont work if one side of a link thinks thenetwork is running at Gbs and the other side of the link thinks its running at Mbs The Ethernet autonegotiation feature of the IEEE standards is supposedto detect and solve this problem And in some cases it does In other cases it iseasily misapplied and simply compounds the problemThe two golden rules of autonegotiation are these You must use autonegotiation on all interfaces capable of Gbs or aboveIts required by the standard On interfaces limited to Mbs or below you must either configureboth ends of a link in autonegotiation mode or you must manually configure the speed and duplex half vs full on both sides If you configureonly one side in autonegotiation mode that side will not in most caseslearn how the other side has been configured The result will be a configuration mismatch and poor performanceTo see how to set a network interfaces autonegotiation policy see the systemspecific sections in the TCPIP Networking chapter they start on page Power over EthernetPower over Ethernet PoE is an extension of UTP Ethernet standardized as IEEEaf that transmits power to devices over the same UTP cable that carries theEthernet signal Its especially handy for Voice over IP VoIP telephones or wireless access points to name just two examples that need a relatively small amountof power in addition to a network connectionThe power supply capacity of PoE systems has been stratified into four classes thatrange from to watts Never satisfied the industry is currently working ona higher power standard bt that may provide more than watts Wont itbe convenient to operate an EasyBake Oven off the network port in the conference room PoE has two ramifications that are significant for sysadmins You need to be aware of PoE devices in your infrastructure so that youcan plan the availability of PoEcapable switch ports accordingly Theyare more expensive than nonPoE ports The power budget for data closets that house PoE switches must includethe wattage of the PoE devices Note that you dont have to budget thesame amount of extra cooling for the closet because most of the heat generated by the consumption of PoE power is dissipated outside the closetusually in an officeJumbo framesEthernet is standardized for a typical packet size of bytes with framing a value chosen long ago when networks were slow and memory for bufferswas scarce Today these byte packets look shrimpy in the context of a gigabit For those of you that are wondering yes it is possible to boot a small Linux system off a PoE portPerhaps the simplest option is a Raspberry Pi with an addon Pi PoE Switch HAT boardEthernet Because every packet consumes overhead and introduces latency networkthroughput can be higher if larger packet sizes are allowedUnfortunately the original IEEE standards for the various types of Ethernet forbidlarge packets because of interoperability concerns But just as highway traffic oftenmysteriously flows faster than the stated speed limit kingsize Ethernet packets area common sight on todays networks Egged on by customers most manufacturersof network equipment have built support for large frames into their gigabit productsTo use these socalled jumbo frames all you need do is bump up your networkinterfaces MTUs Throughput gains vary with traffic patterns but large transfersover TCP eg NFSv or SMB file service benefit the most Expect a modest butmeasurable improvement on the order of Be aware of these points though All network equipment on a subnet must support and use jumbo framesincluding switches and routers You cannot mix and match Because jumbo frames are nonstandard you usually have to enable themexplicitly Devices may accept jumbo frames by default but they probablywill not generate them Since jumbo frames are a form of outlawry theres no universal consensuson exactly how large a jumbo frame can or should be The most commonvalue is bytes or bytes with framing Youll have to investigateyour devices to determine the largest packet size they have in commonFrames larger than K or so are sometimes called super jumbo framesbut dont be scared off by the extremesounding name Larger is generallybetter at least up to K or soWe endorse the use of jumbo frames on gigabit Ethernets but be prepared to dosome extra debugging if things go wrong Its perfectly reasonable to deploy newnetworks with the default MTU and convert to jumbo frames later once the reliability of the underlying network has been confirmed Wireless Ethernet for nomadsA wireless network consists of wireless access points WAPs or simply APs andwireless clients WAPs can be connected to traditional wired networks the typical configuration or wirelessly connected to other access points a configurationknown as a wireless meshWireless standardsThe common wireless standards today are IEEE g n and acg operates in the GHz frequency band and affords LANlike access at upto Mbs Operating range varies from meters to kilometers dependingon equipment and terrainn delivers up to Mbs of bandwidth and can use both the GHz frequency band and the GHz band though GHz is recommended for deployment Typical operating range is approximately double that of g ac isan extension of n with support for up to Gbs of multistation throughputAll these standards are covered by the generic term WiFi In theory the WiFilabel is restricted to Ethernet implementations from the IEEE family However thats the only kind of wireless Ethernet hardware you can actually buy so allwireless Ethernet is WiFiToday g and n are commonplace The transceivers are inexpensiveand are built into most laptops Addin cards are widely and cheaply available fordesktop PCs tooWireless client accessYou can configure a UNIX or Linux box to connect to a wireless network as a clientif you have the right hardware and driver Since most PCbased wireless cards arestill designed for Microsoft Windows they might not come from the factory withFreeBSD or Linux driversWhen attempting to add wireless connectivity to a FreeBSD or Linux system youlllikely need these commands ifconfig configure a wireless network interface iwlist list available wireless access points iwconfig configure wireless connection parameters wpasupplicant authenticate to a wireless or wired x networkUnfortunately the industrys frantic scramble to sell lowcost hardware often meansthat getting a wireless adapter to work correctly under UNIX or Linux might require hours of trial and error Plan ahead or buy the same adapter that someoneelse on the Internet has had good luck with on the same OS version youre runningWireless infrastructure and WAPsEveryone wants wireless everything everywhere and a wide variety of products areavailable to provide wireless service But as with so many things you get what youpay for Inexpensive devices often meet the needs of home users but fail to scalewell in an enterprise environment The Mbs bandwidth of n is largely theoretical In practice bandwidth in the neighborhood of Mbs is a more realistic expectation for an optimized configuration The environmentand capabilities of client devices explain most of the difference between theoretical and reallifethroughput When it comes to wireless your mileage may vary always appliesWireless topologyWAPs are usually dedicated appliances that consist of one or more radios and someform of embedded network operating system often a strippeddown version ofLinux A single WAP can provide a connection point for multiple clients but notfor an unlimited number of clients A good rule of thumb is to serve no more thanforty simultaneous clients from a single enterprisegrade WAP Any device that communicates through a wireless standard supported by your WAPs can act as a clientWAPs are configured to advertise one or more service set identifiers aka SSIDsThe SSID acts as the name of a wireless LAN and must be unique within a particular area When a client wants to connect to a wireless LAN it listens to see whatSSIDs are being advertised and lets the user select from among these networksYou can choose to name your SSID something helpful and easy to remember such asThird Floor Public or you can get creative Some of our favorite SSID names are FBI Surveillance Van The Promised LAN IP Freely Get Off My LAN Virus Distribution Center Access DeniedNothing better than geeks at play In the simplest scenarios a WAP advertises asingle SSID your client connects to that SSID and voila Youre on the networkHowever few aspects of wireless networking are truly simple What if your houseor building is too big to be served by a single WAP Or what if you need to providedifferent networks to different groups of users such as employees vs guests Forthese cases you need to strategically structure your wireless networkYou can use multiple SSIDs to break up groups of users or functions Typically youmap them to separate VLANs which you can then route or filter as desired justlike wired networksThe frequency spectrum allocated to wireless is broken up into bands commonly called channels Left on its own a WAP selects a quiet radio channel to advertise an SSID Clients and the WAP then use that channel for communicationforming a single broadcast domain Nearby WAPs will likely choose other channelsso as to maximize available bandwidth and minimize interferenceThe theory is that as clients move around the environment they will dissociatefrom one WAP when its signal becomes weak and connect to a closer WAP witha stronger signal Theory and reality often dont cooperate however Many clientshold onto weak WAP signals with a death grip and ignore better optionsIn most situations you should allow WAPs to automatically select their favoritechannels If you must manually interfere with this process and are using bgnconsider selecting channel or The spectrum allocated to these channelsdoes not overlap so combinations of these channels create the greatest likelihoodof a wideopen wireless highway The default channels for aac dont overlapat all so just pick your favorite numberSome WAPs have multiple antennas and take advantage of multipleinputmultipleoutput technology MIMO This practice can increase available bandwidthby exploiting multiple transmitters and receivers to take advantage of signal offsetsresulting from propagation delay The technology can provide a slight performanceimprovement in some situations though probably not as much improvement asthe dazzling proliferation of antennas might lead you to expectIf you need a physically larger coverage area then deploy multiple WAPs If the areais completely open you can deploy them in a grid structure If the physical plantincludes walls and other obstructions you may want to invest in a professionalwireless survey The survey will identify the best options for WAP placement giventhe physical attributes of your spaceSmall money wirelessWe like the products made by Ubiquiti ubntcom for inexpensive highperforming home networks Google Wifi is a nice cloudmanaged solution great if yousupport remote family members Another option is to run a stripped down versionof Linux such as OpenWrt or LEDE on a commercial WAP See openwrtorg formore information and a list of compatible hardwareLiterally dozens of vendors are hawking wireless access points these days You canbuy them at Home Depot and even at the grocery store El cheapo access pointsthose in the range are likely to perform poorly when handling large file transfers or more than one active clientBig money wirelessBig wireless means big money Providing reliable highdensity wireless on a largescale think hospitals sports arenas schools cities is a challenge complicated byphysical plant constraints user density and the pesky laws of physics For situations like this you need enterprisegrade wireless gear thats aware of the locationand condition of each WAP and that actively adjusts the WAPs channels signalstrengths and client associations to yield the best results These systems usuallysupport transparent roaming which allows a clients association with a particularVLAN and session to seamlessly follow it as the client moves among WAPsOur favorite large wireless platforms are those made by Aerohive and Meraki thelatter now owned by Cisco These nextgeneration platforms are managed fromthe cloud allowing you to sip martinis on the beach as you monitor your networkthrough a browser You can even eject individual users from the wireless networkfrom the comfort of your beach chair Take that haterIf you are deploying a wireless network on a large scale youll probably need toinvest in a wireless network analyzer We highly recommend the analysis productsmade by AirMagnetWireless securityThe security of wireless networks has traditionally been poor Wired EquivalentPrivacy WEP is a protocol used in conjunction with legacy b networks toencrypt packets traveling over the airwaves Unfortunately this standard containsa fatal design flaw that makes it little more than a speed bump for snoopers Someone sitting outside your building or house can access your network directly andundetectably usually in under a minuteMore recently the WiFi Protected Access WPA security standards have engendered new confidence in wireless security Today WPA specifically WPA shouldbe used instead of WEP in all new installations Without WPA wireless networksshould be considered completely insecure and should never be found inside an enterprise firewall Dont even use WEP at homeTo remember that its WEP thats insecure and WPA thats secure just rememberthat WEP stands for Wired Equivalent Privacy The name is accurate WEP givesyou as much protection as letting someone connect directly to your wired networkThat is no protection at allat least at the IP level SDN softwaredefined networkingJust as with server virtualization the separation of physical network hardware fromthe functional architecture of the network can significantly increase flexibility andmanageability The best traction along this path is the softwaredefined networking SDN movementThe main idea of SDN is that the components managing the network the controlplane are physically separate from the components that forward packets the dataplane The data plane is programmable through the control plane so you canfinetune or dynamically configure data paths to meet performance security andaccessibility goalsAs with so many things in our industry SDN for enterprise networks has becomesomewhat of a marketing gimmick The original goal was to standardize vendorindependent ways to reconfigure network components Although some of this idealismhas been realized many vendors now offer proprietary enterprise SDN productsthat run somewhat counter to SDNs original purpose If you find yourself exploring the enterprise SDN space choose products that conform to open standards andare interoperable with other vendors productsFor large cloud providers SDN adds a layer of flexibility that reduces your need toknow or care where a particular resource is physically located Although thesesolutions may be proprietary they are tightly integrated into cloud providers platforms and can make configuring your virtual infrastructure effortlessSDN and its APIdriven configuration system offer you the sysadmin a temptingopportunity to integrate network topology management with other DevOpsstyletools for continuous integration and deployment Perhaps in some ideal world youalways have a next at bat production environment staged and ready to activatewith a single click As the new environment is promoted to production the networkinfrastructure magically morphs eliminating uservisible downtime and the needfor you to schedule maintenance windows Network testing and debuggingThe key to debugging a network is to break it down into its component parts andthen test each piece until youve isolated the offending device or cable The idiotlights on switches and hubs such as link status and packet traffic often holdimmediate clues to the source of the problem Topnotch documentation of yourwiring scheme is essential for making these indicator lights work in your favorAs with most tasks having the right tools for the job is a big part of being able toget the job done right and without delay The market offers two major types of network debugging tools although these are quickly converging into unified devicesThe first type of tool is the handheld cable analyzer This device can measure theelectrical characteristics of a given cable including its length with a groovy technology called time domain reflectrometry Usually these analyzers can also pointout simple faults such as broken or miswired cablesOur favorite product for LAN cable analysis is the Fluke LanMeter Its an allinoneanalyzer that can even perform IP pings across the network Highend versions havetheir own web server that can show you historical statistics For WAN telco circuitsthe TBERD line analyzer is the cats meow Its made by Viavi viavisolutionscomThe second type of debugging tool is the network sniffer A sniffer captures the bytesthat travel across the wire and disassembles network packets to look for protocolerrors misconfigurations and general snafus Sniffers operate at the link layer ofthe network rather than the electrical layer so they cannot diagnose cabling problems or electrical issues that might be affecting network interfacesCommercial sniffers are available but we find that the freely available programWireshark running on a fat laptop is usually the best option See the Packet snifferssection starting on page for details Like so many popular programs Wireshark is often the target of attacks by hackers Make sure youstay up to date with the most current version Building wiringIf youre embarking on a building wiring project the most important advice wecan give you is to do it right the first time This is not an area in which to skimpor cut corners Buying quality materials selecting a competent wiring contractorand installing extra connections drops will save you years of frustration andheartburn down the roadUTP cabling optionsCategory a wire typically offers the best price vs performance tradeoff in todaysmarket Its normal format is four pairs per sheath which is just right for a varietyof data connections from RS to gigabit EthernetCategory a specifications require that the twist be maintained to the point ofcontact Special training and termination equipment are necessary to satisfy thisrequirement You must use Category a jacks and patch panels Weve had the bestluck with parts manufactured by SiemonConnections to officesFor many years there has been an ongoing debate about how many connectionsshould be wired per office One connection per office is clearly not enough Butshould you use two or four With the advent of highbandwidth wireless we nowrecommend two for several reasons A nonzero number of wired connections is typically needed to supportvoice telephones and other specialty devices Most user devices can now be connected through wireless networkingand users prefer this to being chained down by cables Your network wiring budget is better spent on core infrastructure fiberto closets etc than on more drops to individual officesIf youre in the process of wiring your entire building you might consider installing a few outlets in hallways conference rooms lunch rooms bathrooms and ofcourse ceilings for wireless access points Dont forget to keep security in mindhowever and put publicly accessible ports on a guest VLAN that doesnt haveaccess to your internal network resources You can also secure public ports by implementing x authenticationWiring standardsModern buildings often require a large and complex wiring infrastructure to support all the various activities that take place inside Walking into the average telecommunications closet can be a shocking experience for the weak of stomach asidentically colored unlabeled wires often cover the wallsIn an effort to increase traceability and standardize building wiring the Telecommunications Industry Association in February released TIAEIA Administration Standard for Commercial Telecommunications Infrastructure later updatedto TIAEIAB in EIA specifies requirements and guidelines for the identification and documentation of telecommunications infrastructure Items covered by EIA include Termination hardware Cables Cable pathways Equipment spaces Infrastructure color coding Labeling requirements Symbols for standard componentsIn particular the standard specifies colors to be used for wiring Table showsthe detailsTable EIA color chartTermination type Color Codea CommentsDemarcation point Orange C Central office terminationsNetwork connections Green C Also used for aux circuit terminationsCommon equipment b Purple C Major switchingdata eqpt terminationsFirstlevel backbone White Cable terminationsSecondlevel backbone Gray C Cable terminationsStation Blue C Horizontal cable terminationsInterbuilding backbone Brown C Campus cable terminationsMiscellaneous Yellow C Maintenance alarms etcKey telephone systems Red C a Pantone Matching System color codeb PBXes hosts LANs muxes etcPantone sells software to map between the Pantone systems for inkonpaper textiledyes and colored plastic Hey you could colorcoordinate the wiring the uniformsof the installers and the wiring documentation On second thought Network design issuesThis section addresses the logical and physical design of networks Its targeted atmediumsized installations The ideas presented here scale up to a few hundredhosts but are overkill for three machines and inadequate for thousands We alsoassume that you have an adequate budget and are starting from scratch which isprobably only partially trueMost of network design consists of specifying The types of media that will be used The topology and routing of cables The use of switches and routersAnother key issue in network design is congestion control For example filesharingprotocols such as NFS and SMB tax the network quite heavily and so file servingon a backbone cable is undesirableThe issues presented in the following sections are typical of those that must be considered in any network designNetwork architecture vs building architectureNetwork architecture is usually more flexible than building architecture but thetwo must coexist If you are lucky enough to be able to specify the network before a building is constructed be lavish For most of us both the building and afacilitiesmanagement department already exist and are somewhat rigidIn existing buildings the network must use the building architecture not fight itModern buildings often contain utility raceways for data and telephone cables inaddition to highvoltage electrical wiring and water or gas pipes They often usedrop ceilings a boon to network installers Many campuses and organizations haveunderground utility tunnels that facilitate network installationThe integrity of firewalls must be maintained if you route a cable through a firewall the hole must be snug and filled in with a noncombustible substance Respectreturn air plenums in your choice of cable If you are caught violating fire codesyou might be fined and will be required to fix the problems you have created evenif that means tearing down the entire network and rebuilding it correctlyYour networks logical design must fit into the physical constraints of the buildingsit serves As you specify the network keep in mind that its easy to draw a logicallygood solution and then find that it is physically difficult or impossible to implementExpansionIts difficult to predict needs ten years into the future especially in the computerand networking fields Therefore design the network with expansion and increasedbandwidth in mind As you install cable especially in outoftheway hardtoreachplaces pull three to four times the number of pairs you actually need Rememberthe majority of installation cost is labor not materials This type of firewall is a concrete brick or flameretardant wall that prevents a fire from spreadingand burning down the building Although different from a network security firewall its probably justas importantEven if you have no plans to use fiber its wise to install some when wiring yourbuilding especially in situations where it will be hard to install cable later Runboth multimode and singlemode fiber The kind you need in the future is alwaysthe kind you didnt installCongestionA network is like a chain it is only as good as its weakest or slowest link The performance of Ethernet like that of many other network architectures degrades nonlinearly as the network becomes loadedOvertaxed switches mismatched interfaces and lowspeed links can all lead to congestion Its helpful to isolate local traffic by creating subnets and by using interconnection devices such as routers Subnets can also be used to cordon off machinesthat are used for experimentation Its difficult to run an experiment that involvesseveral machines if you cannot isolate those machines both physically and logicallyfrom the rest of the networkMaintenance and documentationWe have found that the maintainability of a network correlates highly with thequality of its documentation Accurate complete uptodate documentation isindispensableCables should be labeled at all termination points Its a good idea to post copies oflocal cable maps inside communications closets so that the maps can be updatedon the spot when changes are made Once every few weeks have someone copydown the changes for entry into a wiring databaseJoints between major population centers in the form of switches or routers canfacilitate debugging by allowing parts of the network to be isolated and debuggedseparately Its also helpful to put joints between political and administrative domains for similar reasons Management issuesIf a network is to work correctly some things must be centralized some distributed and some local Reasonable ground rules and good citizen guidelines mustbe formulated and agreed onA typical environment includes A backbone network among buildings Departmental subnets connected to the backbone Group subnets within a department Connections to the outside world Internet or field office VPNsSeveral facets of network design and implementation must have sitewide controlresponsibility maintenance and financing Networks with chargeback algorithmsfor each connection grow in bizarre but predictable ways as departments try tominimize their own local costs Prime targets for central control are The network design including the use of subnets routers switches etc The backbone network itself including the connections to it Host IP addresses hostnames and subdomain names Protocols mostly to ensure their interoperation Routing policy to the InternetDomain names IP addresses and network names are in some sense already controlled centrally by authorities such as ARIN the American Registry for InternetNumbers and ICANN However your sites use of these items must be coordinated locally as wellA central authority has an overall view of the network its design capacity and expected growth It can afford to own monitoring equipment and the staff to run itand to keep the backbone network healthy It can insist on correct network designeven when that means telling a department to buy a router and build a subnet toconnect to the campus backbone Such a decision might be necessary to ensure thata new connection does not adversely impact the existing networkIf a network serves many types of machines operating systems and protocols it isalmost essential to have a layer device as a gateway between networks Recommended vendorsIn the past years of installing networks around the world weve gotten burnedmore than a few times by products that didnt quite meet specs or were misrepresented overpriced or otherwise failed to meet expectations Below is a list of vendors in the United States that we still trust recommend and use ourselves todayCables and connectorsAMP part of Tyco Anixter Black Box Corporation ampcom anixtercom blackboxcomBelden Cable Siemon Newark Electronics siemoncom newarkcombeldencomTest equipmentFluke Siemon Viavi flukecom siemoncom viavisolutionscomRoutersswitchesCisco Systems Juniper Networks ciscocom junipernet Recommended readingANSITIAEIAA Commercial Building Telecommunications Cabling Standardand ANSITIAEIA Administration Standard for the Telecommunications Infrastructure of Commercial Buildings are the telecommunication industrys standardsfor building wiring Unfortunately they are not free See tiaonlineorgBarnett David David Groth and Jim McBee Cabling The Complete Guide toNetwork Wiring rd Edition San Francisco CA Sybex Goransson Paul and Chuck Black Software Defined Networks A Comprehensive Approach nd Edition Burlington MA Morgan Kaufman Spurgeon Charles and Joann Zimmerman Ethernet The Definitive Guide Designing and Managing Local Area Networks nd Edition Sebastopol CA OReilly More than billion IP addresses are available worldwide so getting packets tothe right place on the Internet is no easy task Chapter TCPIP Networkingbriefly introduced IP packet forwarding In this chapter we examine the forwardingprocess in more detail and investigate several network protocols that allow routersto automatically discover efficient routes Routing protocols not only lessen the daytoday administrative burden of maintaining routing information but they also allow network traffic to be redirected quickly if a router link or network should failIts important to distinguish between the process of actually forwarding IP packetsand the management of the routing table that drives this process both of whichare commonly called routing Packet forwarding is simple whereas route computation is tricky consequently the second meaning is used more often in practiceThis chapter describes only unicast routing multicast routing sending packets togroups of subscribers involves an array of very different problems and is beyondthe scope of this bookFor most cases the information covered in Chapter is all you need to knowabout routing If the appropriate network infrastructure is already in place you canset up a single static default route as described in the Routing section starting on See wapostworldip IP Routingpage and voil you have enough information to reach just about anywhereon the Internet If you must live within a complex network topology or if you areusing UNIX or Linux systems as part of your network infrastructure then this chapters information about dynamic routing protocols and tools can come in handyIP routing both for IPv and for IPv is next hop routing At any given pointthe system handling a packet needs to determine only the next host or router inthe packets journey to its final destination This is a different approach from that ofmany legacy protocols which determine the exact path a packet will travel beforeit leaves its originating host a scheme known as source routing Packet forwarding a closer lookBefore we jump into the management of routing tables we need a more detailedlook at how the tables are used Consider the network shown in Exhibit AExhibit A Example networknetwork network HostAHostRouter BRRouterR to the InternetFor simplicity we start this example with IPv for an IPv routing table see page Router R connects two networks and router R connects one of these nets tothe outside world A look at the routing tables for these hosts and routers lets usexamine some specific packet forwarding scenarios First host As routing tableA netstat rnDestination Gateway Genmask Flags MSS Window irtt Iface U lo U eth UG ethThe example above uses the venerable netstat tool to query the routing table Thistool is distributed with FreeBSD and is available for Linux as part of the nettools We do not recommend the use of UNIX or Linux systems as network routers in a production infrastructure Buy a dedicated router IP packets can also be sourceroutedat least in theorybut this is almost never done The feature isnot widely supported because of security considerationspackage nettools is no longer actively maintained and as a result it is considereddeprecated The less featureful ip route command is the officially recommendedway to obtain this information on LinuxA ip routedefault via dev eth onlink dev eth proto kernel scope link src The output from netstat rn is slightly easier to read so we use that for subsequentexamples and for the following exploration of Exhibit AHost A has the simplest routing configuration of the four machines The first tworoutes describe the machines own network interfaces in standard routing termsThese entries exist so that forwarding to directly connected networks need not behandled as a special case eth is host As Ethernet interface and lo is the loopbackinterface a virtual interface emulated in software Entries such as these are normallyadded automatically when a network interface is configuredThe default route on host A forwards all packets not addressed to the loopback address or to the network to the router R whose address on this networkis Gateways must be only one hop awaySuppose a process on A sends a packet to B whose address is TheIP implementation looks for a route to the target network but noneof the routes match The default route is invoked and the packet is forwarded to RExhibit B shows the packet that actually goes out on the Ethernet The addresses inthe Ethernet header are the MAC addresses of As and Rs interfaces on the netExhibit B Ethernet packetETHERNET FRAMEEthernetheader IP header UDP header and dataIP PACKETUDP PACKETFromToTypeUDPFromToTypeARIPThe Ethernet destination address is that of router R but the IP packet hidden withinthe Ethernet frame does not mention R at all When R inspects the packet it hasreceived it sees from the IP destination address that it is not the ultimate destination of the packet It then uses its own routing table to forward the packet to host Bwithout rewriting the IP header the header still shows the packet coming from ASee the discussionof netmasks starting on page See page formore informationabout addressingHeres the routing table for host RR netstat rnDestination Gateway Genmask Flags MSS Window irtt Iface U lo U eth U eth UG ethThis table is similar to that of host A except that it shows two physical network interfaces The default route in this case points to R since thats the gateway throughwhich the Internet can be reached Packets bound for either of the networkscan be delivered directlyLike host A host B has only one real network interface However B needs an additional route to function correctly because it has direct connections to two differentrouters Traffic for the net must travel through R but other trafficshould go out to the Internet through RB netstat rnDestination Gateway Genmask Flags MSS Window irtt Iface U lo U eth U eth UG ethIn theory you can configure host B with initial knowledge of only one gateway andrely on help from ICMP redirects to eliminate extra hops For example here is onepossible initial configuration for host BB netstat rnDestination Gateway Genmask Flags MSS Window irtt Iface U lo U eth UG ethIf B then sends a packet to host A no route matches and the packetis forwarded to R for delivery R which being a router presumably has completeinformation about the network sends the packet on to R Since R and B are onthe same network R also sends an ICMP redirect notice to B and B enters a hostroute for A into its routing table UGHD ethThis route sends all future traffic for A directly through R However it does notaffect routing for other hosts on As network all of which have to be routed by separate redirects from RSome sites use ICMP redirects this way as a sort of lowrent routing protocol thinking that this approach is dynamic Unfortunately systems and routers all handleredirects differently Some hold on to them indefinitely Others remove them fromSee page foran explanation ofICMP redirectsthe routing table after a relatively short period minutes Still others ignorethem entirely which is probably the correct approach from a security perspectiveRedirects have several other potential disadvantages increased network load increased load on R routing table clutter and dependence on extra servers to namea few Therefore we dont recommend their use In a properly configured networkredirects should never appear in the routing tableIf you are using IPv addresses the same model applies Heres a routing table froma FreeBSD host that is running IPv netstat rnDestination Gateway Flags Netif Expiredefault b UGS reb link U refe UGRS lofere link U reAs in IPv the first route is a default thats used when no morespecific entriesmatch The next line contains a route to the global IPv network where the hostlives b The final two lines are special they represent a routeto the reserved IPv network fe known as the linklocal unicast network Thisnetwork is used for traffic that is scoped to the local broadcast domain typicallythe same physical network segment It is most often used by network services thatneed to find each other on a unicast network such as OSPF Dont use linklocaladdresses for normal networking purposes Routing daemons and routing protocolsIn simple networks such as the one shown in Exhibit A it is perfectly reasonable toconfigure routing by hand At some point however networks become too complicated to be managed this way Instead of having to explicitly tell every computer onevery network how to reach every other computer and network it would be nice ifthe computers could just cooperate and figure it all out This is the job of routingprotocols and the daemons that implement themRouting protocols have a major advantage over static routing systems in that theycan react and adapt to changing network conditions If a link goes down then therouting daemons can discover and propagate alternative routes to the networksserved by that link if any such routes existRouting daemons collect information from three sources configuration files theexisting routing tables and routing daemons on other systems This information ismerged to compute an optimal set of routes and the new routes are then fed backinto the system routing table and possibly fed to other systems through a routingprotocol Because network conditions change over time routing daemons mustperiodically check in with one another for reassurance that their routing information is still currentThe exact manner in which routes are computed depends on the routing protocolTwo general types of protocols are in common use distancevector protocols andlinkstate protocolsDistancevector protocolsDistancevector aka gossipy protocols are based on the general idea If router X is five hops away from network Y and Im adjacent to router X then I mustbe six hops away from network Y You announce how far you think you are fromthe networks you know about If your neighbors dont know of a better way to getto each network they mark you as being the best gateway If they already know ashorter route they ignore your advertisement Over time everyones routing tablesare supposed to converge to a steady stateThis is a really elegant idea If it worked as advertised routing would be relativelysimple Unfortunately the basic algorithm does not deal well with changes in topology In some cases infinite loops eg router X receives information from routerY and sends it on to router Z which sends it back to router Y can prevent routesfrom converging at all Realworld distancevector protocols must avoid such problems by introducing complex heuristics or by enforcing arbitrary restrictions suchas the RIP Routing Information Protocol notion that any network more than hops away is unreachableEven in nonpathological cases it can take many update cycles for all routers to reacha steady state Therefore to guarantee that routing does not jam for an extended period the cycle time must be made short and for this reason distancevector protocolsas a class tend to be talkative For example RIP requires that routers broadcast alltheir routing information every seconds EIGRP sends updates every secondsOn the other hand BGP the Border Gateway Protocol transmits the entire tableonce and then transmits changes as they occur This optimization substantially reduces the potential for chatty and mostly unnecessary trafficTable lists the distancevector protocols in common use todayTable Common distancevector routing protocolsName Long name ApplicationRIP Routing Information Protocol Internal LANs if thatRIPng Routing Information Protocol next generation IPv LANsEIGRP a Enhanced Interior Gateway Routing Protocol WANs corporate LANsBGP Border Gateway Protocol Internet backbone routinga This protocol EIGRP is proprietary to Cisco The problem is that changes in topology can lengthen the optimal routes Some DV protocols such asEIGRP maintain information about multiple possible routes so that they always have a fallback planThe exact details are not importantLinkstate protocolsLinkstate protocols distribute information in a relatively unprocessed form Therecords traded among routers are of the form Router X is adjacent to router Yand the link is up A complete set of such records forms a connectivity map of thenetwork from which each router can compute its own routing table The primaryadvantage that linkstate protocols offer over distancevector protocols is the ability to quickly converge on an operational routing solution after a catastrophe occurs The tradeoff is that maintaining a complete map of the network at each noderequires memory and CPU power that would not be needed by a distancevectorrouting systemBecause the communications among routers in a linkstate protocol are not part ofthe actual routecomputation algorithm they can be implemented in such a waythat transmission loops do not occur Updates to the topology database propagateacross the network efficiently at a lower cost in network bandwidth and CPU timeLinkstate protocols tend to be more complicated than distancevector protocolsbut this complexity can be explained in part by the fact that linkstate protocolsmake it easier to implement advanced features such as typeofservice routing andmultiple routes to the same destinationThe only true linkstate protocol in general use is OSPFCost metricsFor a routing protocol to determine which path to a network is shortest the protocol has to define what is meant by shortest Is it the path involving the fewestnumber of hops The path with the lowest latency The largest minimal intermediate bandwidth The lowest financial costFor routing the quality of a link is represented by a number called the cost metricA path cost is the sum of the costs of each link in the path In the simplest systemsevery link has a cost of leading to hop counts as a path metric But any of theconsiderations mentioned above can be converted to a numeric cost metricRouting protocol designers have labored long and hard to make the definition ofcost metrics flexible and some protocols even allow different metrics to be usedfor different kinds of network traffic Nevertheless in of cases all this hardwork can be safely ignored The default metrics for most systems work just fineYou might encounter situations in which the actual shortest path to a destinationis not a good default route for political or financial reasons To handle these casesyou can artificially boost the cost of the critical links to make them seem less appealing Leave the rest of the routing configuration aloneInterior and exterior protocolsAn autonomous system AS is a group of networks under the administrative control of a single entity The definition is vague realworld autonomous systems canbe as large as a worldwide corporate network or as small as a building or a singleacademic department It all depends on how you want to manage routing The general tendency is to make autonomous systems as large as you can This conventionsimplifies administration and makes routing as efficient as possibleRouting within an autonomous system is somewhat different from routing betweenautonomous systems Protocols for routing among ASs exterior protocols mustoften handle routes for many networks eg the entire Internet and they must dealgracefully with the fact that neighboring routers are under other peoples controlExterior protocols do not reveal the topology inside an autonomous system so ina sense they can be thought of as a second level of routing hierarchy that deals withcollections of nets rather than individual hosts or cablesIn practice small and mediumsized sites rarely need to run an exterior protocolunless they are connected to more than one ISP With multiple ISPs the easy division of networks into local and Internet domains collapses and routers must decidewhich route to the Internet is best for any particular address However that is notto say that every router must know this information Most hosts can stay stupid androute their default packets through an internal gateway that is better informedAlthough exterior protocols are not much different from their interior counterpartsthis chapter concentrates on the interior protocols and the daemons that supportthem If your site must use an external protocol as well see the recommended reading list on page for some suggested references Protocols on paradeSeveral routing protocols are in common use In this section we introduce the major players and summarize their main advantages and weaknessesRIP and RIPng Routing Information ProtocolRIP is an old Xerox protocol that was adapted for IP networks The IP version wasoriginally specified in RFC circa The protocol has existed in three versions RIP RIPv and the IPvonly RIPng next generationAll versions of RIP are simple distancevector protocols that use hop counts as a costmetric Because RIP was designed in an era when computers were expensive andnetworks small RIPv considers any host or more hops away to be unreachableLater versions of RIP have maintained the hopcount limit mostly to encourage theadministrators of complex sites to migrate to more sophisticated routing protocolsRIPv is a minor revision of RIP that distributes netmasks along with nexthopaddresses so its support for subnetted networks and CIDR is better than that ofRIPv A vague gesture toward increasing the security of RIP was also includedSee page formore informationabout CIDRRIPv can be run in a compatibility mode that preserves most of its new featureswithout entirely abandoning vanilla RIP receivers In most respects RIPv is identical to the original protocol and should be used in preference to itRIPng is a restatement of RIP in terms of IPv It is an IPvonly protocol and RIPremains IPvonly If you want to route both IPv and IPv with RIP youll needto run RIP and RIPng as separate protocolsAlthough RIP is known for its profligate broadcasting it does a good job when anetwork is changing often or when the topology of remote networks is not knownHowever it can be slow to stabilize after a link goes downIt was originally thought that the advent of more sophisticated routing protocolssuch as OSPF would make RIP obsolete However RIP continues to fill a need fora simple easytoimplement protocol that doesnt require much configuration andit works well on lowcomplexity networksRIP is widely implemented on nonUNIX platforms A variety of common devicesfrom printers to SNMPmanageable network components can listen to RIP advertisements to learn about network gateways In addition some form of RIP client isavailable for all versions of UNIX and Linux so RIP is a de facto lowestcommondenominator routing protocol Often RIP is used for LAN routing and a morefeatureful protocol is used for widearea connectivitySome sites run passive RIP daemons usually routed or Quaggas ripd that listenfor routing updates on the network but do not broadcast any information of theirown The actual route computations are performed with a more efficient protocolsuch as OSPF see the next section RIP is used only as a distribution mechanismOSPF Open Shortest Path FirstOSPF is the most popular linkstate protocol Shortest path first refers to themathematical algorithm that calculates routes open is used in the sense of nonproprietary RFC defines the basic protocol OSPF version and RFCextends it to include support for IPv OSPF version OSPF version is obsolete and is not usedOSPF is an industrialstrength protocol that works well for large complicated topologies It offers several advantages over RIP including the ability to manage severalpaths to a single destination and the ability to partition the network into sectionsareas that share only highlevel routing information The protocol itself is complex and hence only worthwhile at sites of significant size where routing protocolbehavior really makes a difference To use OSPF effectively your sites IP addressingscheme should be reasonably hierarchicalThe OSPF protocol specification does not mandate any particular cost metric Ciscos implementation uses a bandwidthrelated value by defaultSee page fordetails on IPvEIGRP Enhanced Interior Gateway Routing ProtocolEIGRP is a proprietary routing protocol that runs only on Cisco routers Its predecessor IGRP was created to address some of the shortcomings of RIP before robuststandards like OSPF existed IGRP has now been deprecated in favor of EIGRPwhich accommodates CIDR masks IGRP and EIGRP are configured similarly despite being quite different in their underlying protocol designEIGRP supports IPv but as with other routing protocols the IPv world and IPvworld are configured separately and act as separate though parallel routing domainsEIGRP is a distancevector protocol but its designed to avoid the looping andconvergence problems found in other DV systems Its widely regarded as the mostevolved distancevector protocol For most purposes EIGRP and OSPF are equallyfunctionalBGP Border Gateway ProtocolBGP is an exterior routing protocol that is a protocol that manages traffic amongautonomous systems rather than among individual networks There were onceseveral exterior routing protocols in common use but BGP has outlasted them allBGP is now the standard protocol used for Internet backbone routing As of mid the Internet routing table contains about prefixes It should be clearfrom this number that backbone routing has scaling requirements very differentfrom those for local routing Routing protocol multicast coordinationRouters need to talk to each other to learn how to get to places on the network butto get to places on the network they need to talk to a router This chickenandeggproblem is most commonly solved through multicast communication This is thenetworking equivalent of agreeing to meet your friend on a particular street cornerif you get separated The process is normally invisible to system administrators butyou might occasionally see this multicast traffic in your packet traces or when doing other kinds of network debugging Table lists the agreedon multicast addresses for various routing protocols Routing strategy selection criteriaRouting for a network can be managed at essentially four levels of complexity No routing Static routes only Mostly static routes but clients listen for RIP updates Dynamic routing everywhereThe topology of the overall network has a dramatic effect on each individual segments routing requirements Different nets might need very different levels of routing support The following rules of thumb can help you choose a strategy A standalone network requires no routing If a network has only one way out clients nongateway machines on thatnetwork should have a static default route to the lone gateway No otherconfiguration is necessary except perhaps on the gateway itself A gateway with a small number of networks on one side and a gatewayto the world on the other side can have explicit static routes pointingto the former and a default route to the latter However dynamic routingis advisable if both sides have more than one routing choice If networks cross political or administrative boundaries use dynamicrouting at those points even if the complexity of the networks involvedwould not otherwise suggest the use of a routing protocol RIP works OK and is widely supported Dont reject it out of hand justbecause its an older protocol with a reputation for chattinessThe problem with RIP is that it doesnt scale indefinitely an expandingnetwork will eventually outgrow it That fact makes RIP something ofa transitional protocol with a narrow zone of applicability That zone isbounded on one side by networks too simple to require any routing protocol and on the other side by networks too complicated for RIP If yournetwork plans include continued growth its probably reasonable to skipover the RIP zone entirely Even when RIP isnt a good choice for your global routing strategy itsstill a good way to distribute routes to leaf nodes But dont use it whereits not needed systems on a network that has only one gateway neverneed dynamic updatesTable Routing protocol multicast addressesDescription IPv IPvAll systems on this subnet ff All routers on this subnet ff Unassigned ff DVMRP Routers ff OSPF Routers ff OSPF DR Routers ff RIP Routers ff EIGRP Routers ff EIGRP and OSPF are about equally functional but EIGRP is proprietaryto Cisco Cisco makes excellent and costcompetitive routers nevertheless standardizing on EIGRP limits your choices for future expansion Routers connected to the Internet through multiple upstream providersmust use BGP However most routers have only one upstream path andcan therefore use a simple static default routeA good default strategy for a mediumsized site with a relatively stable local structure and a connection to someone elses net is to use a combination of static anddynamic routing Routers within the local structure that do not lead to externalnetworks can use static routing forwarding all unknown packets to a default machine that understands the outside world and does dynamic routingA network that is too complicated to be managed with this scheme should rely ondynamic routing Default static routes can still be used on leaf nets but machineson networks with more than one router should run routed or some other RIP receiver in passive mode Routing daemonsYou should not use UNIX and Linux systems as routers for production networksDedicated routers are simpler more reliable more secure and faster even if theyare secretly running a Linux kernel That said its nice to be able to set up a newsubnet with only a network card and a switch Thats a reasonable approachfor lightly populated test and auxiliary networksSystems that act as gateways to such subnets dont need any help managing their ownrouting tables Static routes are perfectly adequate both for the gateway machineand for the machines on the subnet itself However if you want the subnet to bereachable by other systems at your site you need to advertise the subnets existenceand to identify the router to which packets bound for that subnet should be sentThe usual way to do this is to run a routing daemon on the gatewayUNIX and Linux systems can participate in most routing protocols through variousrouting daemons The notable exception is EIGRP which as far as we are awarehas no widely available UNIX or Linux implementationBecause routing daemons are uncommon on production systems we dont describetheir use and configuration in detail However the following sections outline thecommon software options and point to detailed configuration informationrouted obsolete RIP implementationrouted was for a long time the only standard routing daemon and its still included on a few systems routed speaks only RIP and poorly at that even support forRIPv is scattershot routed does not speak RIPng implementation of that protocolbeing confined to modern daemons such as QuaggaWhere available routed is useful chiefly for its quiet mode q in which it listens for routing updates but does not broadcast any information of its own Asidefrom the commandline flag routed normally does not require configuration Itsan easy and cheap way to get routing updates without having to deal with muchconfiguration hasslerouted adds its discovered routes to the kernels routing table Routes must be reheard at least every four minutes or they will be removed However routed knowswhich routes it has added and does not remove static routes that were installedwith the route or ip commandsQuagga mainstream routing daemonQuagga quagganet is a development fork of Zebra a GNU project started byKunihiro Ishiguro and Yoshinari Yoshikawa to implement multiprotocol routingwith a collection of independent daemons instead of a single monolithic application In real life the quaggaa subspecies of zebra last photographed in isextinct but in the digital realm it is Quagga that survives and Zebra that is no longer under active developmentQuagga currently implements RIP all versions OSPF versions and and BGPIt runs on Linux FreeBSD and several other platforms Quagga is either installedby default or is available as an optional package through the systems standard software repositoryIn the Quagga system the core zebra daemon acts as a central clearinghouse forrouting information It manages the interaction between the kernels routing tableand the daemons for individual routing protocols ripd ripngd ospfd ospfdand bgpd It also controls the flow of routing information among protocols Eachdaemon has its own configuration file in the etcquagga directoryYou can connect to any of the Quagga daemons through a commandline interfacevtysh to query and modify its configuration The command language itself is designed to be familiar to users of Ciscos IOS operating system see the section onCisco routers below for some additional details As in IOS you use enable to entersuperuser mode config term to enter configuration commands and write to saveyour configuration changes back to the daemons configuration fileThe official documentation at quagganet is available in HTML or PDF form Although complete its for the most part a workmanlike catalog of options and doesnot provide much of an overview of the system The real documentation action isat quagganetdocs Look there for wellcommented example configurations FAQsand tipsAlthough the configuration files have a simple format youll need to understandthe protocols youre configuring and have some idea of which options you wantto enable or configure See the recommended reading list on page for somegood books on routing protocolsSee page for moreabout manual maintenance of routing tablesXORP router in a boxXORP the eXtensible Open Router Platform project was started at around the sametime as Zebra but its ambitions are more general Instead of focusing on routingXORP aims to emulate all the functions of a dedicated router including packet filtering and traffic management Check it out at xorporgOne interesting aspect of XORP is that in addition to running under several operatingsystems Linux FreeBSD macOS and Windows Server its also available as a liveCD that runs directly on PC hardware The live CD is secretly based on Linux but itdoes go a long way toward turning a generic PC into a dedicated routing appliance Cisco routersRouters made by Cisco Systems Inc are the de facto standard for Internet routingtoday Having captured over of the router market Ciscos products are wellknown and staff that know how to operate them are relatively easy to find BeforeCisco UNIX boxes with multiple network interfaces were often used as routersToday dedicated routers are the favored gear to put in datacom closets and aboveceiling tiles where network cables come togetherMost of Ciscos router products run an operating system called Cisco IOS which isproprietary and unrelated to UNIX Its command set is rather large the full documentation set fills up about feet of shelf space We could never fully cover CiscoIOS here but knowing a few basics can get you a long wayBy default IOS defines two levels of access user and privileged both of whichare password protected By default you can simply ssh to a Cisco router to enteruser modeYou are prompted for the userlevel access password ssh acmegwacmecomPassword passwordUpon entering the correct password you receive a prompt from Ciscos EXECcommand interpreteracmegwacmecomAt this prompt you can enter commands such as show interfaces to see the routersnetwork interfaces or show to list the other things you can seeTo enter privileged mode type enable and when asked type the privileged passwordOnce you have reached the privileged level your prompt ends in a acmegwacmecomBe carefulyou can do anything from this prompt including erasing the routersconfiguration information and its operating system When in doubt consult Ciscosmanuals or one of the comprehensive books published by Cisco PressYou can type show running to see the current running configuration of the router and show config to see the current nonvolatile configuration Most of the timethese are the sameHeres a typical configurationacmegwacmecom show runningCurrent configurationversion hostname acmegwenable secret xxxxxxxxip subnetzerointerface Ethernetdescription Acme internal network ip address no ip directedbroadcastinterface Ethernetdescription Acme backbone network ip address no ip directedbroadcastip classlessline con transport input noneline aux transport input telnetline vty password xxxxxxxxloginendThe router configuration can be modified in a variety of ways Cisco offers graphicaltools that run under some versions of UNIXLinux and Windows Real network administrators never use these the command prompt is always the sure bet You canalso scp a config file to or from a router so you can edit it with your favorite editorTo modify the configuration from the command prompt type config termacmegwacmecom config termEnter configuration commands one per line End with CNTLZacmegwconfigYou can then type new configuration commands exactly as you want them to appearin the show running output For example if you wanted to change the IP addressof the Ethernet interface in the configuration above you could enterinterface Ethernetip address When youve finished entering configuration commands press ControlZ to return to the regular command prompt If youre happy with the new configurationenter write mem to save the configuration to nonvolatile memoryHere are some tips for a successful Cisco router experience Name the router with the hostname command This precaution helpsprevent accidents caused by configuration changes to the wrong routerThe hostname always appears in the command prompt Always keep a backup router configuration on hand You can scp or tftpthe running configuration to another system each night for safekeeping Its often possible to store a copy of the configuration in NVRAM or on aremovable jump drive Do so Control access to the router command line by putting access lists on therouters VTYs VTYs are like PTYs on a UNIX system This precautionprevents unwanted parties from trying to break into your router Control the traffic flowing through your networks and possibly to theoutside world by setting up access lists on each router interface Keep routers physically secure Its easy to reset the privileged passwordif you have physical access to a Cisco boxIf you have multiple routers and multiple router wranglers check out the free toolRANCID from shrubberynet With a name like RANCID it practically marketsitself but heres the elevator pitch RANCID logs into your routers every night toretrieve their configuration files It diffs the configurations and lets you know aboutanything thats changed It also automatically keeps the configuration files underrevision control see page Recommended readingPerlman Radia Interconnections Bridges Routers Switches and InternetworkingProtocols nd Edition Reading MA AddisonWesley This is the definitivework in this topic area If you buy just one book about networking fundamentalsthis should be it Also dont ever pass up a chance to hang out with Radiashes alot of fun and holds a shocking amount of knowledge in her brainEdgeworth Brad Aaron Foss and Ramiro Garza Rios IP Routing on CiscoIOS IOS XE and IOS XR An Essential Guide to Understanding and ImplementingIP Routing Protocols Indianapolis IN Cisco Press Huitema Christian Routing in the Internet nd Edition Upper Saddle RiverNJ Prentice Hall PTR This book is a clear and wellwritten introduction torouting from the ground up It covers most of the protocols in common use andalso some advanced topics such as multicastingThere are many routingrelated RFCs Table shows the main onesTable Routingrelated RFCsRFC Title Authors ICMP Router Discovery Messages Deering RIP Version MIB Extension Malkin Baker RIPng for IPv Malkin Minnear OSPF Version Moy Routing Information Protocol Version Malkin A Border Gateway Protocol BGP Rekhter Li et al AuthenticationConfidentiality for OSPFv Gupta Melam RIPv Cryptographic Authentication Atkinson Fanto Neighbor Discovery for IPv Narten et al IPv Router Advertisement Flags Option Haberman Hinden Routing IPv with ISIS Hopps OSPF for IPv Coltun et al Management Information Base for OSPFv Joyal Manral et alThe Internet delivers instant access to resources all over the world and each of thosecomputers or sites has a unique name eg googlecom However anyone whohas tried to find a friend or a lost child in a crowded stadium knows that simplyknowing a name and yelling it loudly is not enough Essential to finding anythingor anyone is an organized system for communicating updating and distributingnames and their locationsUsers and userlevel programs like to refer to resources by name eg amazoncombut lowlevel network software understands only IP addresses eg Mapping between names and addresses is the best known and arguably most important function of DNS the Domain Name System DNS includes other elements andfeatures but almost without exception they exist to support this primary objectiveOver the history of the Internet DNS has been both praised and criticized Its initial elegance and simplicity encouraged adoption in the early years and enabled theInternet to grow quickly with little centralized management As needs for additionalfunctionality grew so did the DNS system Sometimes these functions were bolted on in a way that looks ugly today Naysayers point out weaknesses in the DNSinfrastructure as evidence that the Internet is on the verge of collapse DNS The Domain Name SystemSay what you will but the fundamental concepts and protocols of DNS have so farwithstood growth from a few hundred hosts in a single country to a worldwidenetwork that supports over billion users across more than billion hosts Nowhere else can we find an information system that has grown to this scale with sofew issues Without DNS the Internet would have failed long ago DNS architectureDNS is a distributed database Under this model one site stores the data for computers it knows about another site stores the data for its own set of computers andthe sites cooperate and share data when one site needs to look up the others dataFrom an administrative point of view the DNS servers you have configured foryour domain answer queries from the outside world about names in your domainthey also query other domains servers on behalf of your usersQueries and responsesA DNS query consists of a name and a record type The answer returned is a set ofresource records RRs that are responsive to the query or alternatively a responseindicating that the name and record type you asked for do not existResponsive doesnt necessarily mean dispositive DNS servers are arranged intoa hierarchy and it might be necessary to contact servers at several layers to answera particular query see page Servers that dont know the answer to a queryreturn resource records that help the client locate a server that doesThe most common query is for an A record which returns the IP address associatedwith a name Exhibit A illustrates a typical scenarioExhibit A A simple name lookupHuman Web browserSystem libraryand resolver Name servergethostbyname facebookcomTake me tofacebookcomFind the systemthat hostsfacebookcomWhat is theA record forfacebookcomThe A record forfacebookcom is User statistics are from internetlivestatscominternetusers Host statistics are from statistacom Name servers typically receive queries on UDP port First a human types the name of a desired site into a web browser The browser thencalls the DNS resolver library to look up the corresponding address The resolver library constructs a query for an A record and sends it to a name server whichreturns the A record in its response Finally the browser opens a TCP connectionto the target host through the IP address returned by the name serverDNS service providersYears ago one of the core tasks of every system administrator was to set up andmaintain a DNS server for their organization Today the landscape has changed Ifan organization maintains a DNS server at all it is frequently for internal use onlyEvery organization still needs an externalfacing DNS server but its now commonto use one of the many commercial managed DNS providers for this functionThese services offer a GUI management interface and highly available secure DNSinfrastructure for only pennies or dollars a day Amazon Route CloudFlareGoDaddy DNS Made Easy and Rackspace are just a few of the major providersOf course you can still set up and maintain your own DNS server internal or external if you wish You have dozens of DNS implementations to choose from butthe Berkeley Internet Name Domain BIND system still dominates the InternetOver of DNS servers run some form of itRegardless of which path you choose as a system administrator you need to understand the basic concepts and architecture of DNS The first few sections of thischapter focus on that important foundational knowledge Starting on page we show some specific configurations for BIND DNS for lookupsRegardless of whether you run your own name server use a managed DNS serviceor have someone else providing DNS service for you youll certainly want to configure all of your systems to look up names in DNSTwo steps are needed to make this happen First you configure your systems asDNS clients Second you tell the systems when to use DNS as opposed to othername lookup methods such as a static etchosts fileresolvconf client resolver configurationEach host on the network should be a DNS client You configure the clientsideresolver in the file etcresolvconf This file lists the name servers to which thehost can send queries Microsofts Active Directory system includes an integrated DNS server that meshes nicely with theother Microsoftflavored services found in corporate environments However Active Directory issuitable only for internal use It should never be used as an external Internetfacing DNS server because of potential security concerns According to the July ISC Internet Domain SurveyIf your host gets its IP address and network parameters from a DHCP server theetcresolvconf file is normally set up for you automatically Otherwise you mustedit the file by hand The format issearch domainname nameserver ipaddrUp to three name servers can be listed Heres a complete examplesearch atrustcom booklabatrustcomnameserver nsnameserver nsThe search line lists the domains to query if a hostname is not fully qualified Forexample if a user issues the command ssh coraline the resolver completes thename with the first domain in the search list and looks for coralineatrustcom Ifno such name exists the resolver also tries coralinebooklabatrustcom The number of domains that can be specified in a search directive is resolverspecific mostallow between six and eight with a limit of charactersThe name servers listed in resolvconf must be configured to allow your host tosubmit queries They must also be recursive that is they must answer queries tothe best of their ability and not try to refer you to other name servers see page DNS servers are contacted in order As long as the first one continues to answerqueries the others are ignored If a problem occurs the query eventually timesout and the next name server is tried Each server is tried in turn up to four timesThe timeout interval increases with each failure The default timeout interval is fiveseconds which seems like forever to impatient usersnsswitchconf who do I ask for a nameBoth FreeBSD and Linux use a switch file etcnsswitchconf to specify how hostnametoIPaddress mappings should be performed and whether DNS should betried first last or not at all If no switch file is present the default behavior ishosts dns UNAVAILreturn filesThe UNAVAIL clause means that if DNS is available but a name is not found therethe lookup attempt should fail rather than continuing to the next entry in this casethe etchosts file If no name server is running as might be the case during bootthe lookup process does consult the hosts fileOur example distributions all provide the following default nsswitchconf entryhosts files dnsThis configuration gives precedence to the etchosts file which is always checkedDNS is consulted only for names that are unresolvable through etchostsThere is really no best way to configure lookupsit depends on how your site ismanaged In general we prefer to keep as much host information as possible inSee page formore informationabout DHCPDNS but always preserve the ability to fall back to the static hosts file during theboot process if necessaryIf name service is provided for you by an outside organization you might be donewith DNS configuration after setting up resolvconf and nsswitchconf If so youcan skip the rest of this chapter or read on to learn more The DNS namespaceThe DNS namespace is organized into a tree that contains both forward mappingsand reverse mappings Forward mappings map hostnames to IP addresses andother records and reverse mappings map IP addresses to hostnames Every complete hostname eg nubarkatrustcom is a node in the forward branch of the treeand in theory every IP address is a node in the reverse branch Exhibit B showsthe general layout of the naming treeExhibit B DNS zone tree inaddrarpa rootcom org net de au amazon atrust www www nubark Reverse zones Forward zonesTo allow the same DNS system to manage both names which have the most significant information on the right and IP addresses which have the most significantpart on the left the IP branch of the namespace is inverted by listing the octets ofthe IP address backwards For example if host nubarkatrustcom has IP address the corresponding node of the forward branch of the naming tree isnubarkatrustcom and the node of the reverse branch is inaddrarpaBoth of these names end with a dot just as the full pathnames of files always startwith a slash That makes them fully qualified domain names or FQDNs for short The inaddrarpa portion of the name is a fixed suffixOutside the context of DNS names like nubarkatrustcom without the final dotare sometimes referred to as fully qualified hostnames but this is a colloquialismWithin the DNS system itself the presence or absence of the trailing dot is of crucial importanceTwo types of toplevel domains exist country code domains ccTLDs and generictoplevel domains gTLDs ICANN the Internet Corporation for Assigned Namesand Numbers accredits various agencies to be part of its shared registry projectfor registering names in the gTLDs such as com net and org To register for accTLD name check the IANA Internet Assigned Numbers Authority web pageianaorgcctld to find the registry in charge of a particular countrys registrationRegistering a domain nameTo obtain a secondlevel domain name such as blazedgoatcom you must applyto a registrar for the appropriate toplevel domain To complete the domain registration forms you must choose a name that is not already taken and identify atechnical contact person an administrative contact person and at least two hoststhat will be name servers for your domain Fees vary among registrars but thesedays they are all generally quite inexpensiveCreating your own subdomainsThe procedure for creating a subdomain is similar to that for creating a secondleveldomain except that the central authority is now local or more accurately withinyour own organization Specifically the steps are as follows Choose a name that is unique in the local context Identify two or more hosts to be servers for your new domain Coordinate with the administrator of the parent domainParent domains should check to be sure that a child domains name servers are upand running before performing the delegation If the servers are not working alame delegation results and you might receive nasty email asking you to clean upyour DNS act Page covers lame delegations in more detail How DNS worksName servers around the world work together to answer queries Typically theydistribute information maintained by whichever administrator is closest to thequery target Understanding the roles and relationships of name servers is important both for daytoday operations and for debugging The twoormoreservers rule is a policy not a technical requirement You make the rules in yourown subdomains so you can get away with a single server if you wantName serversA name server performs several chores It answers queries about your sites hostnames and IP addresses It asks about both local and remote hosts on behalf of your users It caches the answers to queries so that it can answer faster next time It communicates with other local name servers to keep DNS data synchronizedName servers deal with zones where a zone is essentially a domain minus its subdomains You will often see the term domain used where a zone is whats actuallymeant even in this bookName servers can operate in several different modes The distinctions among themfall along several axes so the final categorization is often not tidy To make thingseven more confusing a single server can play different roles with respect to different zones Table lists some of the adjectives used to describe name serversTable Name server taxonomyType of server Descriptionauthoritative Officially represents a zonemaster The master server for a zone gets its data from a disk fileprimary Another name for the master serverslave Copies its data from the mastersecondary Another name for a slave serverstub Like a slave but copies only name server data not host datadistribution A server advertised only within a domain aka stealth servernonauthoritative a Answers a query from cache doesnt know if the data is still validcaching Caches data from previous queries usually has no local zonesforwarder Performs queries on behalf of many clients builds a large cacherecursive Queries on your behalf until it returns either an answer or an errornonrecursive Refers you to another server if it cant answer a querya Strictly speaking nonauthoritative is an attribute of a DNS query response not a serverThese categorizations vary according to the name servers source of data authoritative caching master slave the type of data saved stub the query path forwarder the completeness of answers handed out recursive nonrecursive and finallythe visibility of the server distribution The next few sections provide additionaldetails on the most important of these distinctions the others are described elsewhere in this chapterAuthoritative and cachingonly serversMaster slave and cachingonly servers are distinguished by two characteristicswhere the data comes from and whether the server is authoritative for the domainEach zone typically has one master name server The master server keeps the official copy of the zones data on disk The system administrator changes the zonesdata by editing the master servers data filesA slave server gets its data from the master server through a zone transfer operation A zone can have several slave name servers and must have at least one Astub server is a special kind of slave that loads only the NS name server recordsfrom the master Its fine for the same machine to be both a master server for somezones and a slave server for other zonesA cachingonly name server loads the addresses of the servers for the root domainfrom a startup file and accumulates the rest of its data by caching answers to thequeries it resolves A cachingonly name server has no data of its own and is notauthoritative for any zone except perhaps the localhost zoneAn authoritative answer from a name server is guaranteed to be accurate a nonauthoritative answer might be out of date However a very high percentage of nonauthoritative answers are perfectly correct Master and slave servers are authoritative for their own zones but not for information they may have cached about otherdomains Truth be told even authoritative answers can be inaccurate if a sysadminchanges the master servers data but forgets to propagate the changes eg doesntchange the zones serial numberAt least one slave server is required for each zone Ideally there should be at leasttwo slaves one of which is in a location that does not share common infrastructure with the master Onsite slaves should live on different networks and differentpower circuits When name service stops all normal network access stops tooRecursive and nonrecursive serversName servers are either recursive or nonrecursive If a nonrecursive server has theanswer to a query cached from a previous transaction or is authoritative for the domain to which the query pertains it provides an appropriate response Otherwiseinstead of returning a real answer it returns a referral to the authoritative serversof another domain that are more likely to know the answer A client of a nonrecursive server must be prepared to accept and act on referralsAlthough nonrecursive servers might seem lazy they usually have good reason notto take on extra work Authoritativeonly servers eg root servers and topleveldomain servers are all nonrecursive but since they may process tens of thousandsof queries per second we can excuse them for cutting corners Some sites use multiple masters or even no masters we describe the singlemaster caseSee page formore informationabout zone transfersA recursive server returns only real answers and error messages It follows referralsitself relieving clients of this responsibility In other respects the basic procedurefor resolving a query is essentially the sameFor security an organizations externally accessible name servers should always benonrecursive Recursive name servers that are visible to the world can be vulnerable to cache poisoning attacksNote well resolver libraries do not understand referrals Any local name serverlisted in a clients resolvconf file must be recursiveResource recordsEach site maintains one or more pieces of the distributed database that makes upthe worldwide DNS system Your piece of the database consists of text files thatcontain records for each of your hosts these are known as resource records Eachrecord is a single line consisting of a name usually a hostname a record type andsome data values The name field can be omitted if its value is the same as that ofthe previous lineFor example the linesnubark IN A IN MX mailserveratrustcomin the forward file called atrustcom and the line IN PTR nubarkatrustcomin the reverse file called rev associate nubarkatrustcom with theIP address The MX record routes email addressed to this machine tothe host mailserveratrustcomThe IN fields denote the record classes In practice this field is always IN for InternetResource records are the lingua franca of DNS and are independent of the configuration files that control the operation of any given DNS server implementationThey are also the pieces of data that flow around the DNS system and becomecached at various locationsDelegationAll name servers read the identities of the root servers from a local config file orhave them built into the code The root servers know the name servers for comnet edu fi de and other toplevel domains Farther down the chain edu knowsabout coloradoedu berkeleyedu and so on Each domain can delegate authorityfor its subdomains to other serversLets inspect a real example Suppose we want to look up the address for the machinevangoghcsberkeleyedu from the machine laircscoloradoedu The host lair asksSee page formore informationabout MX recordsits local name server nscscoloradoedu to figure out the answer The followingillustration Exhibit C shows the subsequent eventsExhibit C DNS query process for vangoghcsberkeleyeduRecursive Nonrecursivelair nscscoloradoedu eduroot csberkeleyeduberkeleyedu Query Answer ReferralQARQAQRQRQRA QSTARTThe numbers on the arrows between servers show the order of events and a letterdenotes the type of transaction query referral or answer We assume that noneof the required information was cached before the query except for the names andIP addresses of the servers of the root domainThe local server doesnt know vangoghs address In fact it doesnt know anythingabout csberkeleyedu or berkeleyedu or even edu It does know servers for the rootdomain however so it queries a root server about vangoghcsberkeleyedu and receives a referral to the servers for eduThe local name server is a recursive server When the answer to a query consists ofa referral to another server the local server resubmits the query to the new serverIt continues to follow referrals until it finds a server that has the data its looking forIn this case the local name server sends its query to a server of the edu domainasking as always about vangoghcsberkeleyedu and gets back a referral to theservers for berkeleyedu The local name server then repeats this same query on aberkeleyedu server If the Berkeley server doesnt have the answer cached it returns a referral to the servers for csberkeleyedu The csberkeleyedu server is authoritative for the requested information looks the answer up in its zone files andreturns vangoghs addressWhen the dust settles nscscoloradoedu has cached vangoghs address It has alsocached data on the servers for edu berkeleyedu and csberkeleyeduYou can view the query process in detail with dig trace or drill T dig and drill are DNS query tools dig from the BIND distribution and drill from NLnet LabsCaching and efficiencyCaching increases the efficiency of lookups a cached answer is almost free and isusually correct because hostnametoaddress mappings change infrequently Ananswer is saved for a period of time called the time to live TTL which is specified by the owner of the data record in questionMost queries are for local hosts and can be resolved quickly Users also inadvertentlyhelp with efficiency because they repeat many queries after the first instance of aquery the repeats are more or less freeUnder normal conditions your sites resource records should use a TTL that issomewhere between an hour and a day The longer the TTL the less network trafficwill be consumed by Internet clients obtaining fresh copies of the recordIf you have a specific service that is loadbalanced across logical subnets often calledglobal server load balancing you may be required by your loadbalancing vendorto choose a shorter TTL such as seconds or minute The short TTL lets the loadbalancer react quickly to inoperative servers and denial of service attacks The system still works correctly with short TTLs but your name servers have to work hardIn the vangogh example above the TTLs were days for the roots days for edu days for berkeleyedu and day for vangoghcsberkeleyedu These are reasonablevalues If you are planning a massive renumbering change the TTLs to a shortervalue well before you startDNS servers also implement negative caching That is they remember when a queryfails and do not repeat that query until the negative caching TTL value has expiredNegative caching can potentially save answers of the following types No host or domain matches the name queried The type of data requested does not exist for this host The server is not responding The server is unreachable because of network problemsThe BIND implementation caches the first two types of negative data and allowsthe negative cache times to be configuredMultiple answers and round robin DNS load balancingA name server often receives multiple records in response to a query For example theresponse to a query for the name servers of the root domain would list all serversYou can take advantage of this balancing effect for your own servers by assigningseveral different IP addresses for different machines to a single hostnamewww IN A IN A IN A Most name servers return multirecord sets in a different order each time they receive a query rotating them in round robin fashion When a client receives a response with multiple records the most common behavior is to try the addressesin the order returned by the DNS serverThis scheme is commonly referred to as round robin DNS load balancing However it is a crude solution at best Large sites use loadbalancing software such asHAProxy see page or dedicated loadbalancing appliancesDebugging with query toolsFive commandline tools that query the DNS database are distributed with BINDnslookup dig host drill and delv nslookup and host are simple and have prettyoutput but you need dig or drill to get all the details drill is better for followingDNSSEC signature chains The name drill is a pun on dig the Domain Information Groper implying you can get even more info from DNS with drill than youcan with dig delv is new to BIND and will eventually replace drill for DNSSEC debuggingBy default dig and drill query the name servers configured in etcresolvconf Anameserver argument makes either command query a specific name server Theability to query a particular server lets you check to be sure that any changes youmake to a zone have been propagated to secondary servers and to the outside worldThis feature is especially useful if you use views split DNS and need to verify thatyou have configured them correctlyIf you specify a record type dig and drill query for that type only The pseudotypeany is a bit sneaky instead of returning all data associated with a name it returnsall cached data associated with the name So to get all records you might have todo dig domain NS followed by dig nsdomain domain any Authoritative datacounts as cached in this contextdig has about options and drill about half that many Either command accepts anh flag to list the various options Youll probably want to pipe the output throughless For both tools x reverses the bytes of an IP address and does a reverse query The trace flag to dig or T to drill shows the iterative steps in the resolutionprocess from the roots downdig and drill include the notation aa in the output flags if an answer is authoritative ie it comes directly from a master or slave server of that zone The code adindicates that an answer was authenticated by DNSSEC When testing a new configuration be sure that you look up data for both local and remote hosts If you canaccess a host by IP address but not by name DNS is probably the culpritThe most common use of dig is to determine what records are currently being returned for a particular name If only an AUTHORITY response is returned you have However this behavior is not required Some clients may behave differentlySee page formore informationabout DNSSECSee page formore informationabout split DNSbeen referred to another name server If an ANSWER response is returned your question has been directly answered and other information may be included as wellIts often useful to follow the delegation chain manually from the root servers toverify that everything is in the right place Below we look at an example of thatprocess for the name wwwviawestcom First we query a root server to see whois authoritative for viawestcom by requesting the startofauthority SOA record dig arootserversnet viawestcom soa DiG P arootserversnet viawestcom soa server found global options cmd Got answer HEADER opcode QUERY status NOERROR id flags qr rd QUERY ANSWER AUTHORITY ADDITIONAL WARNING recursion requested but not available QUESTION SECTIONviawestcom IN SOA AUTHORITY SECTIONcom IN NS cgtldserversnetcom IN NS bgtldserversnetcom IN NS agtldserversnet ADDITIONAL SECTIONcgtldserversnet IN A bgtldserversnet IN A bgtldserversnet IN AAAA dagtldserversnet IN A Query time msec SERVER WHEN Wed Feb MSG SIZE rcvd Note that the status returned is NOERROR That tells us that the query returned a response without notable errors Other common status values are NXDOMAIN whichindicates the name requested doesnt exist or isnt registered and SERVFAIL whichusually indicates a configuration error on the name server itselfThis AUTHORITY SECTION tells us that the global toplevel domain gTLD serversare the next link in the authority chain for this domain So we pick one at randomand repeat the same query dig cgtldserversnet viawestcom soa DiG P cgtldserversnet viawestcom soa server found global options cmd Got answer HEADER opcode QUERY status NOERROR id flags qr rd QUERY ANSWER AUTHORITY ADDITIONAL WARNING recursion requested but not available QUESTION SECTIONviawestcom IN SOA AUTHORITY SECTIONviawestcom IN NS nsviawestnetviawestcom IN NS nsviawestnet ADDITIONAL SECTIONnsviawestnet IN A nsviawestnet IN A Query time msec SERVER WHEN Wed Feb MSG SIZE rcvd This response is much more succinct and we now know that the next server toquery is nsviawestcom or nsviawestcom dig nsviawestnet viawestcom soa DiG P nsviawestnet viawestcom soa server found global options cmd Got answer HEADER opcode QUERY status NOERROR id flags qr aa rd QUERY ANSWER AUTHORITY ADDITIONAL WARNING recursion requested but not available QUESTION SECTIONviawestcom IN SOA ANSWER SECTIONviawestcom IN SOA mvecviawestnet hostmasterviawestnet AUTHORITY SECTIONviawestcom IN NS nsviawestnet ADDITIONAL SECTIONnsviawestnet IN A Query time msec SERVER WHEN Wed Feb MSG SIZE rcvd This query returns an ANSWER for the viawestcom domain We now know an authoritative name server and can query for the name we actually want wwwviawestcom dig nsviawestnet wwwviawestcom any DiG P nsviawestnet wwwviawestcom any server found global options cmd Got answer HEADER opcode QUERY status NOERROR id flags qr aa rd QUERY ANSWER AUTHORITY ADDITIONAL WARNING recursion requested but not available QUESTION SECTIONwwwviawestcom IN ANY ANSWER SECTIONwwwviawestcom IN CNAME hmdebfaviathreatxio AUTHORITY SECTIONviawestcom IN NS nsviawestnet ADDITIONAL SECTIONnsviawestnet IN A Query time msec SERVER WHEN Wed Feb MSG SIZE rcvd This final query shows us that wwwviawestcom has a CNAME record pointed athmdebfaviathreatxio meaning that it is another name for the threatx host ahost operated by a cloudbased distributed denialofservice providerOf course if you query a recursive name server it will follow the entire delegationchain on your behalf But when debugging its typically more useful to investigatethe chain link by link The DNS databaseA zones DNS database is a set of text files maintained by the system administratoron the zones master name server These text files are often called zone files Theycontain two types of entries parser commands things like ORIGIN and TTL andresource records Only the resource records are really part of the database the parsercommands just provide some shorthand ways to enter recordsParser commands in zone filesCommands can be embedded in zone files to make the zone files more readable andeasier to maintain The commands either influence the way the parser interpretssubsequent records or they expand into multiple DNS records themselves Once azone file has been read and interpreted none of these commands remain a part ofthe zones data at least not in their original formsZone file commandsare standardized inRFCs and Three commands ORIGIN INCLUDE and TTL are standard for all DNS implementations and a fourth GENERATE is found only in BIND Commands must startin column one and occur on a line by themselvesZone files are read and parsed from top to bottom in a single pass As the nameserver reads a zone file it adds the default domain or origin to any names thatare not already fully qualified The origin defaults to the domain name specified inthe name servers configuration file However you can set the origin or change itwithin a zone file by using the ORIGIN directiveORIGIN domainnameThe use of relative names where fully qualified names are expected saves lots oftyping and makes zone files much easier to readMany sites use the INCLUDE directive in their zone database files to separate overhead records from data records to separate logical pieces of a zone file or to keepcryptographic keys in a file with restricted permissions The syntax isINCLUDE filename originThe specified file is read into the database at the point of the INCLUDE directive Iffilename is not an absolute path it is interpreted relative to the home directory ofthe running name serverIf you supply an origin value the parser acts as if an ORIGIN directive precedes thecontents of the file being read Watch out the origin does not revert to its previousvalue after the INCLUDE has been executed Youll probably want to reset the origineither at the end of the included file or on the line following the INCLUDE statementThe TTL directive sets a default value for the timetolive field of the records thatfollow it It must be the first line of the zone file The default units for the TTL value are seconds but you can also qualify numbers with h for hours m for minutesd for days or w for weeks For example the linesTTL TTL hTTL dall set the TTL to one dayResource recordsEach zone of the DNS hierarchy has a set of resource records associated with it Thebasic format of a resource record isname ttl class type dataFields are separated by whitespace tabs or spaces and can contain the specialcharacters shown in Table on the next pageTable Special characters in resource recordsCharacter Meaning Introduces a comment The current zone name Allows data to span lines Wild card name field onlyaa See page for some cautionary statementsThe name field identifies the entity usually a host or domain that the record describes If several consecutive records refer to the same entity the name can beomitted after the first record as long as the subsequent records begin with whitespace If present the name field must begin in column oneA name can be either relative or absolute Absolute names end with a dot and arecomplete Internally the software deals only with absolute names it appends thecurrent origin and a dot to any name that does not already end in a dot This feature allows names to be shorter but it also invites mistakesFor example if cscoloradoedu were the current domain the name anchor wouldbe interpreted as anchorcscoloradoedu If by mistake you entered the name asanchorcscoloradoedu the lack of a final dot would still imply a relative nameresulting in the name anchorcscoloradoeducscoloradoedu This kind of mistake is commonThe ttl time to live field specifies the length of time in seconds that the recordcan be cached and still be considered valid It is often omitted except in the rootserver hints file It defaults to the value set by the TTL directive which must be thefirst line of the zone data fileIncreasing the value of the ttl parameter to about a week substantially reduces network traffic and DNS load However once records have been cached outside yourlocal network you cannot force them to be discarded If you plan a massive renumbering and your old ttl was a week lower the TTL value eg to one hour atleast a week before your intended renumbering This preparatory step makes surethat records with weeklong ttls are expired and replaced with records that haveonehour ttls You can then be certain that all your updates will propagate together within an hour Set the ttls back to their original value after youve completedyour update campaignSome sites set the TTL on the records for Internetfacing servers to a low value so thatif a server experiences problems network failure hardware failure denialofserviceattack etc the administrators can respond by changing the servers nametoIPaddress mapping Because the original TTLs were low the new values will propagatequickly For example the name googlecom has a fiveminute TTL but Googlesname servers have a TTL of four days secondsgooglecom IN A googlecom IN NS nsgooglecomnsgooglecom IN A We used dig to obtain these records we truncated the outputThe class specifies the network type IN for Internet is the defaultMany different types of DNS records are defined but fewer than are in common use IPv adds a few more We divide the resource records into four groups Zone infrastructure records which identify domains and their name servers Basic records which map between names and addresses and route mail Security records which add authentication and signatures to zone files Optional records which provide extra information about hosts or domainsThe contents of the data field depend on the record type A DNS query for a particular domain and record type returns all matching resource records from the zonefile Table lists the common record typesTable DNS record typesType Name FunctionZoneSOA Start Of Authority Defines a DNS zoneNS Name Server Identifies servers delegates subdomainsBasicsA IPv Address Nametoaddress translationAAAA IPv Address NametoIPvaddress translationPTR Pointer Addresstoname translationMX Mail Exchanger Controls email routingSecurityDS Delegation Signer Hash of signed child zones keysigning keyDNSKEY Public Key Public key for a DNS nameNSEC Next Secure Used with DNSSEC for negative answersNSEC Next Secure v Used with DNSSEC for negative answersRRSIG Signature Signed authenticated resource record setOptionalCNAME Canonical Name Nicknames or aliases for a hostSRV Service Gives locations of a wellknown serviceTXT Text Comments or untyped informationSome record types are obsolete experimental or not widely used See your nameservers implementation documentation for a complete list Most records are maintained by hand by editing text files or by entering them in a web GUI but the se MX mail routing records fit in both the zone infrastructure pile and the basic records pile becausethey can refer to entire zones as well as individual hostscurity resource records require cryptographic processing and so must be managedwith software tools These records are described in the DNSSEC section beginningon page The order of resource records in the zone file is arbitrary but traditionally the SOArecord is first followed by the NS records The records for each host are usuallykept together Its common practice to sort by the name field although some sitessort by IP address so that its easier to identify unused addressesAs we describe each type of resource record in detail in the next sections we inspect some sample records from the atrustcom domains data files The defaultdomain in this context is atrustcom so a host specified as bark really meansbarkatrustcomThe format and interpretation of each type of resource record is specified by theIETF in the RFC series In the upcoming sections we list the specific RFCs relevantto each record type along with their years of origin in a margin noteThe SOA recordAn SOA Start of Authority record marks the beginning of a zone a group of resource records located at the same place within the DNS namespace The data fora DNS domain usually includes at least two zones one for translating hostnamesto IP addresses called the forward zone and others that map IP addresses back tohostnames called reverse zonesEach zone has exactly one SOA record The SOA record includes the name of thezone the primary name server for the zone a technical contact and various timeout values Comments are introduced by a semicolon Heres an example Start of authority record for atrustcomatrustcom IN SOA nsatrustcom hostmasteratrustcom Serial number Refresh hours Retry minutes Expire days Minimum hourThe name field of the SOA record atrustcom in this example often contains thesymbol which is shorthand for the name of the current zone The value of isthe domain name specified in the zone statement of namedconf This value can bechanged from within the zone file with the ORIGIN parser directive see page This example has no ttl field The class is IN for Internet the type is SOA and theremaining items form the data field The numerical parameters in parentheses aretimeout values and are often written on one line without commentsnsatrustcom is the zones master name server Actually any name server for the zone can be listed in the SOA record unless you are using dynamicDNS In that case the SOA record must name the master serverSee page formore informationabout RFCsSOA recordsare specified inRFC hostmasteratrustcom was originally intended to be the email address of thetechnical contact in the format userhost rather than the standard userhostUnfortunately due to spam concerns and other reasons most sites do not keepthis contact info updatedThe parentheses continue the SOA record over several linesThe first numeric parameter is the serial number of the zones configuration data Theserial number is used by slave servers to determine when to get fresh data It can beany bit integer and should be incremented every time the data file for the zoneis changed Many sites encode the files modification date in the serial number Forexample would be the first change to the zone on November Serial numbers need not be continuous but they must increase monotonically Ifby accident you set a really large value on the master server and that value is transferred to the slaves then correcting the serial number on the master will not workThe slaves request new data only if the masters serial number is larger than theirsYou can fix this problem in two ways One fix is to exploit the properties of the sequence space in which the serial numbers live This procedure involves adding a large value to thebloated serial number letting all the slave servers transfer the data andthen setting the serial number to just what you want This weird arithmetic with explicit examples is covered in detail in the OReilly book titledDNS and BIND RFC describes the sequence space A sneaky but more tedious way to fix the problem is to change the serialnumber on the master kill the slave servers remove the slaves backup datafiles so they are forced to reload from the master and restart the slaves Itdoes not work to just remove the files and reload you must kill and restart the slave servers This method gets hard if you follow bestpracticesadvice and have your slave servers geographically distributed especiallyif you are not the sysadmin for those slave serversIts a common mistake to change the data files but forget to update the serial numberYour name server will punish you by failing to propagate your changes to slave serversThe next four entries in the SOA record are timeout values in seconds that controlhow long data can be cached at various points throughout the worldwide DNSdatabase Times can also be expressed in units of minutes hours days or weeks byaddition of a suffix of m h d or w respectively For example hm means hourand minutes Timeout values represent a tradeoff between efficiency its cheaper to use an old value than to fetch a new one and accuracy new values are moreaccurate The four timeout fields are called refresh update expire and minimumThe refresh timeout specifies how often slave servers should check with the masterto see if the serial number of the zones configuration has changed Whenever thezone changes slaves must update their copy of the zones data The slave comparesthe serial numbers if the masters serial number is larger the slave requests a zonetransfer to update the data Common values for the refresh timeout range from oneto six hours to secondsInstead of just waiting passively for slave servers to time out master servers forBIND notify their slaves every time a zone changes However its possible for anupdate notification to be lost because of network congestion so the refresh timeoutshould still be set to a reasonable valueIf a slave server tries to check the masters serial number but the master does notrespond the slave tries again after the retry timeout period has elapsed Our experience suggests that minutes seconds is a good valueIf a master server is down for a long time slaves will try to refresh their data manytimes but always fail Each slave should eventually decide that the master is nevercoming back and that its data is surely out of date The expire parameter determineshow long the slaves will continue to serve the domains data authoritatively in theabsence of a master The system should be able to survive if the master server isdown for a few days so this parameter should have a longish value We recommend a month or twoThe minimum parameter in the SOA record sets the time to live for negative answersthat are cached The default for positive answers ie actual records is specified atthe top of the zone file with the TTL directive Experience suggests values of severalhours to a few days for TTL and an hour to a few hours for the minimum BINDsilently discards any minimum values greater than hoursThe TTL expire and minimum parameters eventually force everyone that usesDNS to discard old data values The initial design of DNS relied on the fact thathost data was relatively stable and did not change often However DHCP mobilehosts and the Internet explosion have changed the rules Name servers are desperately trying to cope with the dynamic update and incremental zone transfermechanisms described laterNS recordsNS name server records identify the servers that are authoritative for a zone thatis all the master and slave servers and delegate subdomains to other organizationsNS records are usually placed directly after a zones SOA recordThe format iszone ttl IN NS hostnameFor example NS nsatrustcom NS nsatrustcombooklab NS ubuntubooklabatrustcom NS nsatrustcomNS records arespecified inRFC The first two lines define name servers for the atrustcom domain No name is listedbecause it is the same as the name field of the SOA record that precedes the recordsthe name can therefore be left blank The class is also not listed because IN is thedefault and does not need to be stated explicitlyThe third and fourth lines delegate a subdomain called booklabatrustcom to thename servers ubuntubooklabatrustcom and nsatrustcom These records are actually part of the booklab subdomain but they must also appear in the parent zoneatrustcom in order for the delegation to work In a similar fashion NS records foratrustcom are stored in the com zone file to define the atrustcom subdomain andidentify its servers The com servers refer queries about hosts in atrustcom to theservers listed in NS records for atrustcom within the com domainThe list of name servers in the parent zone should be kept up to date with those inthe zone itself if possible Nonexistent servers listed in the parent zone can delayname service although clients will eventually stumble onto one of the functioningname servers If none of the name servers listed in the parent exist in the child asocalled lame delegation results see page Extra servers in the child are OK as long as at least one of the childs servers still hasan NS record in the parent Check your delegations occasionally with dig or drillto be sure they specify an appropriate set of servers see page A recordsA address records are the heart of the DNS database They provide the mappingfrom hostnames to IP addresses A host usually has one A record for each of itsnetwork interfaces The format ishostname ttl IN A ipaddrFor examplens IN A In this example the name field is not dotterminated so the name server adds thedefault domain to it to form the fully qualified name nsatrustcom The recordassociates that name with the IP address AAAA recordsAAAA records are the IPv equivalent of A records Records are independent ofthe transport protocol used to deliver them publishing IPv records in your DNSzones does not mean that you must answer DNS queries over IPvThe format of an AAAA record ishostname ttl IN AAAA ipaddrFor examplefrootserversnet IN AAAA ffSee page formore informationabout delegationA records are specifiedin RFC AAAA recordsare specified inRFC Each colonseparated chunk of the address represents four hex digits with leading zeros usually omitted Two adjacent colons stand for enough zeros to fill outthe bits of a complete IPv address An address can contain at most one suchdouble colonPTR recordsPTR pointer records map from IP addresses back to hostnames As described inThe DNS namespace starting on page reverse mapping records live under theinaddrarpa domain and are named with the bytes of the IP address in reverse orderFor example the zone for the subnet in this example is inaddrarpaThe general format of a PTR record isaddr ttl IN PTR hostnameFor example the PTR record in the inaddrarpa zone that correspondsto nss A record above is IN PTR nsatrustcomThe name does not end in a dot and therefore is relative But relative to whatNot atrustcomfor this sample record to be accurate the default zone has to beinaddrarpaYou can set the zone by putting the PTR records for each subnet in their own fileThe default domain associated with the file is set in the name server configurationfile Another way to do reverse mappings is to include records such as IN PTR nsatrustcomwith a default domain of inaddrarpa Some sites put all reverse records inthe same file and use ORIGIN directives see page to specify the subnet Notethat the hostname nsatrustcom ends with a dot to prevent the default domaininaddrarpa from being appended to its nameSince atrustcom and inaddrarpa are different regions of the DNSnamespace they constitute two separate zones Each zone must have its own SOArecord and resource records In addition to defining an inaddrarpa zone for eachreal network you should also define one that takes care of the loopback network at least if you run BIND See page for an exampleThis all works fine if subnets are defined on byte boundaries But how do you handle the reverse mappings for a subnet such as where that last bytecan be in any of four subnets or An elegant hackdefined in RFC exploits CNAME resource records to accomplish this featIts important that A records match their corresponding PTR records Mismatchedand missing PTR records cause authentication failures that can slow your system toa crawl This problem is annoying in itself it can also facilitate denialofservice attacks against any application that requires the reverse mapping to match the A recordPTR recordsare specified inRFC See page for more detailsabout subnettingFor IPv the reverse mapping information that corresponds to an AAAA addressrecord is a PTR record in the iparpa toplevel domainThe nibble format reverses an AAAA address record by expanding each colonseparated address chunk to the full hex digits and then reversing the order of thosedigits and tacking on iparpa at the end For example the PTR record that corresponds to our sample AAAA record on page would beffiparpaPTR frootserversnetThis line has been folded to fit the page Its unfortunately not very friendly for asysadmin to have to type or debug or even read Of course in your actual DNS zonefiles the ORIGIN statement could hide some of the complexityMX recordsThe mail system uses mail exchanger MX records to route mail more efficientlyAn MX record preempts the destination specified by the sender of a message inmost cases directing the message to a hub at the recipients site This feature putsthe flow of mail into a site under the control of local sysadmins instead of sendersThe format of an MX record isname ttl IN MX preference host The records below route mail addressed to usersomehostatrustcom to the machinemailserveratrustcom if it is up and accepting email If mailserver is not availablemail goes to mailrelayatrustcom If neither machine named in the MX recordsis accepting mail the fallback behavior is to deliver the mail as originally addressedsomehost IN MX mailserveratrustcom IN MX mailrelayatrustcomHosts with low preference values are tried first is the most desirable and is as bad as it getsMX records are useful in many situations When you have a central mail hub or service provider for incoming mail When you want to filter mail for spam or viruses before delivering it When the destination host is down When the destination host isnt directly reachable from the Internet When the local sysadmin knows where mail should be sent better thanyour correspondents do ie alwaysA machine that accepts email on behalf of another host may need to configure itsmail transport program to enable this function See pages and for a discussion of how to set up this configuration on sendmail and Postfix email serversrespectivelyMX recordsare specified inRFC Wild card MX records are also sometimes seen in the DNS database IN MX mailserveratrustcomAt first glance this record seems like it would save lots of typing and add a defaultMX record for all hosts But wild card records dont quite work as you might expectThey match anything in the name field of a resource record that is not already listedas an explicit name in another resource recordThus you cannot use a star to set a default value for all your hosts But perverselyyou can use it to set a default value for names that are not your hosts This setupcauses lots of mail to be sent to your hub only to be rejected because the hostnamematching the star does not in fact belong to your domain Ergo avoid wild cardMX recordsCNAME recordsCNAME records assign additional names to a host These nicknames are commonlyused either to associate a function with a host or to shorten a long hostname The realname is sometimes called the canonical name hence CNAME Some examplesftp IN CNAME anchorkb IN CNAME kibblesnbitsThe format of a CNAME record isnickname ttl IN CNAME hostnameWhen DNS software encounters a CNAME record it stops its query for the nickname and requeries for the real name If a host has a CNAME record other records A MX NS etc for that host must refer to its real name not its nicknameCNAME records can nest eight deep That is a CNAME record can point to anotherCNAME and that CNAME can point to a third CNAME and so on up to seventimes the eighth target must be the real hostname If you use CNAMEs the PTRrecord should point to the real name not a nicknameYou can avoid CNAMEs altogether by publishing A records for both a hosts realname and its nicknames This configuration makes lookups slightly faster becausethe extra layer of indirection is not neededRFC requires the apex of a zone sometimes called the root domain ornaked domain to resolve to one or more A andor AAAA records The use of aCNAME record is forbidden In other words you can do thiswwwyourdomaincom CNAME somenamesomecloudcom This rule for CNAMEs was explicitly relaxed for DNSSEC which adds digital signatures to each DNSresource record set The RRSIG record for the CNAME refers to the nicknameCNAME recordsare specified inRFC but not thisyourdomaincom CNAME somenamesomecloudcomThis restriction is potentially vexatious especially when you want the apex to pointsomewhere within a cloud providers network and the servers IP address is subjectto change In this situation a static A record is not a reliable optionTo fix the problem youll need to use a managed DNS provider such as AWS Route or CloudFlare that has developed some kind of system for hacking around theRFC requirement Typically these systems let you specify your apex recordsin a manner similar to a CNAME but they actually serve A records to the outsideworld The DNS provider does the work of keeping the A records synchronized tothe actual targetSRV recordsAn SRV record specifies the location of services within a domain For example anSRV record lets you query a remote domain for the name of its FTP server BeforeSRV you had to hope the remote sysadmins had followed the prevailing customand added a CNAME for ftp to their servers DNS recordsSRV records make more sense than CNAMEs for this application and are certainlya better way for sysadmins to move services around and control their use However SRV records must be explicitly sought and parsed by clients so they are notused in all the places they probably should be They are used extensively by Windows howeverSRV records resemble generalized MX records with fields that let the local DNS administrator steer and loadbalance connections from the outside world The format isserviceprotoname ttl IN SRV pri weight port targetwhere service is a service defined in the IANA assigned numbers database seethe list at ianaorgnumbershtm proto is either tcp or udp name is the domainto which the SRV record refers pri is an MXstyle priority weight is a weight usedfor load balancing among several servers port is the port on which the serviceruns and target is the hostname of the server that provides the service To avoid asecond round trip DNS servers usually return the A record of the target with theanswer to a SRV queryA value of for the weight parameter means that no special load balancing shouldbe done A value of for the target means that the service is not run at this siteSRV recordsare specified inRFC Here is an example snitched from RFC and adapted for atrustcomftptcp SRV ftpserveratrustcom of the connections to old box to the new onesshtcp SRV oldslowboxatrustcom SRV newfastboxatrustcom main server on port backup on new box port httptcp SRV wwwserveratrustcom SRV newfastboxatrustcom so both httpwwwatrustcom and httpatrustcom workhttptcpwww SRV wwwserveratrustcom SRV newfastboxatrustcom block all other services target tcp SRV udp SRV This example illustrates the use of both the weight parameter for SSH and thepriority parameter HTTP Both SSH servers are used with the work being splitbetween them The backup HTTP server is only used when the principal server isunavailable All other services are blocked both for TCP and UDP However thefact that other services do not appear in DNS does not mean that they are not actually running just that you cant locate them through DNSTXT recordsA TXT record adds arbitrary text to a hosts DNS records For example some siteshave a TXT record that identifies them IN TXT Applied Trust Engineering Boulder CO USAThis record directly follows the SOA and NS records for the atrustcom zone andso inherits the name field from themThe format of a TXT record isname ttl IN TXT info All info items must be quoted Be sure the quotes are balancedmissing quoteswreak havoc with your DNS data because all the records between the missing quoteand the next occurrence of a quote mysteriously disappearAs with other resource records servers return TXT records in random order Toencode long items such as addresses use long text lines rather than a collection ofseveral TXT recordsBecause TXT records have no particular format they are sometimes used to addinformation for other purposes without requiring changes to the DNS system itselfTXT recordsare specified inRFC SPF DKIM and DMARC recordsSPF Sender Policy Framework DKIM DomainKeys Identified Mail and DMARCDomainbased Message Authentication Reporting and Conformance are standards that attempt to stem the Internets everincreasing flow of unsolicited commercial email aka UCE or spam Each of these systems distributes spamfightinginformation through DNS in the form of TXT records so they are not true DNSrecord types For that reason we cover these systems in Chapter ElectronicMail See the material that starts on page DNSSEC recordsFive resource record types are currently associated with DNSSEC the cryptographically secured version of DNSDS and DNSKEY records store various types of keys and fingerprints RRSIGs containthe signatures of other records in the zone record sets really Finally NSEC andNSEC records give DNS servers a way to sign nonexistent records thus extendingcryptographic security to negative query responses These six records differ frommost in that they are generated by software tools rather than being typed in by handDNSSEC is a big topic in its own right so we discuss these records and their usein the DNSSEC section which begins on page The BIND softwareBIND the Berkeley Internet Name Domain system is an open source softwarepackage from the Internet Systems Consortium ISC that implements DNS forLinux UNIX macOS and Windows systems There have been three main flavorsof BIND BIND BIND and BIND with BIND currently under development by ISC We cover only BIND in this bookComponents of BINDThe BIND distribution has four major components A name server daemon called named that answers queries A resolver library that queries DNS servers on behalf of users Commandline interfaces to DNS nslookup dig and host A program to remotely control named called rndcThe hardest BINDrelated sysadmin chore is probably sorting through all the myriad options and features that BIND supports and determining which ones makesense for your situation This is a little bit of a lie There is a defined DNS record type for SPF however the TXT record version is preferredConfiguration filesA complete configuration for named consists of the config file namedconf thezone data files that contain address mappings for each host and the root nameserver hints file Authoritative servers need namedconf and zone data files foreach zone for which they are the master server caching servers need namedconfand the root hints filenamedconf has its own format all the other files are collections of individual DNSdata records whose formats were discussed in The DNS database starting on page The namedconf file specifies the roles of this host master slave stub or cachingonly and the manner in which it should obtain its copy of the data for each zoneit serves Its also the place where options are specifiedboth global options relatedto the overall operation of named and server or zonespecific options that applyto only a portion of the DNS trafficThe config file consists of a series of statements whose syntax we describe as theyare introduced in subsequent sections The format is unfortunately quite fragileamissing semicolon or unbalanced quote can wreak havocComments can appear anywhere that whitespace is appropriate C C and shellstyle comments are all understood This is a comment that can span lines Everything to the end of the line is a comment Everything to the end of the line is a commentEach statement begins with a keyword that identifies the type of statement Therecan be more than one instance of each type of statement except for options andlogging Statements and parts of statements can also be left out invoking defaultbehavior for the missing itemsTable lists the available statements the Page column points to our discussionof each statement in the upcoming sectionsBefore describing these statements and the way they are used to configure namedwe need to describe a data structure that is used in many of the statements theaddress match list An address match list is a generalization of an IP address thatcan include the following items An IP address either IPv or IPv eg or febfffee An IP network specified with a CIDR netmask eg The name of a previously defined access control list see page The name of a cryptographic authentication key The character to negate things CIDR netmasks are described starting on page Table Statements used in namedconfStatement Page Functioninclude Interpolates a fileoptions Sets global configuration optionsdefaultsacl Defines access control listskey Defines authentication informationserver Specifies perserver optionsmasters Defines a list of masters for stub and slave zoneslogging Specifies logging categories and their destinationsstatisticschannels Outputs realtime statistics in XML formatzone Defines a zone of resource recordscontrols Defines channels used to control named with rndcview Defines a view of the zone datalwres Specifies that named should be a resolver tooAddress match lists are used as parameters to many statements and options Someexamples The first of these lists excludes the host but includes the rest of the network the second defines the networks assigned to the University of ColoradoThe braces and final semicolons are not really part of the address match lists butare included here for illustration they would be part of the enclosing statementsof which the address match lists are a partWhen an IP address or network is compared to a match list the list is searched inorder until a match is found This first match algorithm makes the ordering ofentries important For example the first address match list above would not havethe desired effect if the two entries were reversed because would succeedin matching and the negated entry would never be encounteredNow on to the statements Some are short and sweet others almost warrant achapter unto themselvesThe include statementTo break up or better organize a large configuration you can put different portionsof the configuration in separate files Subsidiary files are brought into namedconfwith an include statementinclude pathIf the path is relative it is interpreted with respect to the directory specified in thedirectory optionA common use of include is to incorporate cryptographic keys that should not beworldreadable Rather than forbidding read access to the entire namedconf filesome sites keep keys in files with restrictive permissions that only named can readThose files are then included into namedconfMany sites put zone statements in a separate file and use the include statement topull them in This configuration helps separate the parts of the configuration thatare relatively static from those that are likely to change frequentlyThe options statementThe options statement specifies global options some of which might later be overridden for particular zones or servers The general format isoptions optionoptionIf no options statement is present in namedconf default values are usedBIND has a lot of optionstoo many in fact The release has more than which is a lot for sysadmins to wrap their heads around Unfortunately as soon asthe BIND folks think about removing some of the options that were a bad idea orthat are no longer necessary they get pushback from sites who use and need thoseobscure options We do not cover the whole gamut of BIND options here we havebiased our coverage and discuss only the ones whose use we recommend We alsoasked the BIND developers for their suggestions on which options to cover andwe followed their adviceFor more complete coverage of the options see one of the books on DNS and BINDlisted at the end of this chapter You can also refer to the documentation shippedwith BIND The ARM document in the doc directory of the distribution describeseach option and shows both syntax and default values The file docmiscoptionsalso contains a complete list of optionsAs we wind our way through about a quarter of the possible options we have addeda margin note as a mini index entry The default values are listed in square brackets beside each option For most sites the default values are just fine Options arelisted in no particular orderdirectory path directory where the server was startedkeydirectory path same as directory entryThe directory statement causes named to cd to the specified directory Whereverrelative pathnames appear in nameds configuration files they are interpreted relaFile locationstive to this directory The path should be an absolute path Any output files debugging statistics etc are also written in this directory The keydirectory is wherecryptographic keys are stored it should not be worldreadableWe like to put all the BINDrelated configuration files other than namedconf andresolvconf in a subdirectory beneath var or wherever you keep your configuration files for other programs We use varnamed or vardomainversion string real version number of the serverhostname string real hostname of the serverserverid string noneThe version string identifies the version of the name server software running onthe server The hostname string identifies the server itself as does the serveridstring These options let you lie about the true values Each of them puts data intoCHAOSclass as opposed to INclass the default TXT records where curiousonlookers can search for them with the dig commandThe hostname and serverid parameters are additions motivated by the use of anycast routing to duplicate instances of the root and gTLD serversnotify yes masteronly explicit no yesalsonotify serveripaddrs emptyallownotify addressmatchlist emptyThe notify and alsonotify clauses apply only to master servers allownotifyapplies only to slave serversEarly versions of BIND synchronized zone files between master and slave serversonly when the refresh timeout in the zones SOA record had expired These daysthe master named automatically notifies its peers whenever the corresponding zonedatabase has been reloaded as long as notify is set to yes The slave servers canthen rendezvous with the master to see if the file has changed and if so to updatetheir copies of the zone dataYou can use notify both as a global option and as a zonespecific option It makesthe zone files converge much more quickly after you make changes By defaultevery authoritative server sends updates to every other authoritative server a system termed splattercast by Paul Vixie Setting notify to masteronly curbs thischatter by sending notifications only to slave servers of zones for which this serveris the master If the notify option is set to explicit then named only notifies theservers listed in the alsonotify clausenamed normally figures out which machines are slave servers of a zone by lookingat the zones NS records If alsonotify is specified a set of additional servers thatare not advertised with NS records can also be notified This tweak is sometimesnecessary when your site has internal serversName server identityZone synchronizationSee page formore informationabout stub zonesThe target of an alsonotify is a list of IP addresses and optionally ports Youmust use the allownotify clause in the secondaries namedconf files if you wanta name server other than the master to notify themFor servers with multiple network interfaces additional options specify the IP address and port to use for outgoing notificationsrecursion yes no yesallowrecursion addressmatchlist all hostsThe recursion option specifies whether named should process queries recursivelyon behalf of your users You can enable this option on an authoritative server ofyour zones data but thats frowned upon The bestpractice recommendation is tokeep authoritative servers and caching servers separateIf this name server should be recursive for your clients set recursion to yes andinclude an allowrecursion clause so that named can distinguish queries thatoriginate at your site from remote queries named will act recursively for the formerand nonrecursively for the latter If your name server answers recursive queries foreveryone it is called an open resolver and can become a reflector for certain kindsof attacks see RFCrecursiveclients number maxcachesize number unlimitedIf your server is handling an extraordinary amount of traffic you may need to tweakthe recursiveclients and maxcachesize options recursiveclients controlsthe number of recursive lookups the server will process simultaneously each requires about KiB of memory maxcachesize limits the amount of memory theserver will use for caching answers to queries If the cache grows too large nameddeletes records before their TTLs expire to keep memory use under the limitusevudpports range begin end range usevudpports range begin end range avoidvudpports portlist emptyavoidvudpports portlist emptyquerysource vaddress port any CAUTION dont use portquerysourcev vaddress port any CAUTION dont use portSource ports have become important in the DNS world because of a DNS protocol weakness discovered by Dan Kaminsky one that allows DNS cache poisoningwhen name servers use predictable source ports and query IDs The use andavoid options for UDP ports together with changes to the named software havemitigated this attackSome sysadmins formerly set a specific outgoing port number so they could configure their firewalls to recognize it and accept UDP packets only for that port However this configuration is no longer safe in the postKaminsky era Dont use theQuery recursionCache memory useIP port utilizationquerysource address options to specify a fixed outgoing port for DNS queries oryou will forfeit the Kaminsky protection that a large range of random ports providesThe defaults for the use ranges are fine and you shouldnt need to change themBut be aware of the implications queries go out from random highnumberedports and the answers come back to those same ports Ergo your firewall must beprepared to accept UDP packets on random highnumbered portsIf your firewall blocks certain ports in this range for example port for RPCthen you have a small problem When your name server sends a query and uses oneof the blocked ports as its source the firewall blocks the answer The name servereventually stops waiting and sends out the query again Not fatal but annoying tothe user caught in the crossfireTo forestall this problem use the avoid options to make BIND stay away fromthe blocked ports Any highnumbered UDP ports blocked by your firewall shouldbe included in the list If you update your firewall in response to some threatenedattack be sure to update the port list here tooThe querysource options let you specify the IP address to be used on outgoingqueries For example you might need to use a specific IP address to get throughyour firewall or to distinguish between internal and external viewsforwarders inaddr inaddr empty listforward only first firstInstead of having every name server perform its own external queries you can designate one or more servers as forwarders A runofthemill server can look in itscache and in the records for which it is authoritative If it doesnt find the answer itslooking for it can then send the query on to a forwarder host That way the forwarders build up caches that benefit the entire site The designation is implicitnothingin the configuration file of the forwarder explicitly says Hey youre a forwarderThe forwarders option lists the IP addresses of the servers you want to use as forwarders They are queried in turn The use of a forwarder circumvents the normalDNS procedure of starting at a root server and following the chain of referrals Becareful not to create forwarding loopsA forwardonly server caches answers and queries forwarders but it never queriesanyone else If the forwarders do not respond queries fail A forwardfirst serverprefers to deal with forwarders but if they do not respond the forwardfirst serverwill complete queries on its ownSince the forwarders option has no default value forwarding does not occur unlessit has been specifically configured Some firewalls are stateful and may be smart enough to recognize the DNS answer as being pairedwith the corresponding query of a second ago Such firewalls dont need help from this optionForwardingYou can turn on forwarding either globally or within individual zone statementsallowquery addressmatchlist all hostsallowquerycache addressmatchlist all hostsallowtransfer addressmatchlist all hostsallowupdate addressmatchlist noneblackhole addressmatchlist emptyThese options specify which hosts or networks can query your name server or itscache request block transfers of your zone data or dynamically update your zonesThese match lists are a lowrent form of security and are susceptible to IP addressspoofing so theres some risk in relying on them Its probably not a big deal if someone tricks your server into answering a DNS query but avoid the allowupdateand allowtransfer clauses use cryptographic keys insteadThe blackhole address list identifies servers that you never want to talk to nameddoes not accept queries from these servers and will never ask them for answersednsudpsize number maxudpsize number All machines on the Internet must be capable of reassembling a fragmented UDPpacket of bytes or fewer Although this conservative requirement made sensein the s it is laughably small by modern standards Modern routers and firewalls can handle much larger packets but it only takes one bad link in the IP chainto spoil the whole pathSince DNS by default uses UDP for queries and since DNS responses are oftenlarger than bytes DNS administrators have to worry about large UDP packetsbeing dropped If a large reply gets fragmented and your firewall only lets the firstfragment through the receiver gets a truncated answer and retries the query withTCP TCP is a more expensive protocol than UDP and busy servers at the root orTLDs dont need increased TCP traffic because of everybodys broken firewallsThe ednsudpsize option sets the reassembly buffer size that the name server advertises through EDNS the extended DNS protocol The maxudpsize optionsets the maximum packet size that the server will actually send Both sizes are inbytes Reasonable values are in the byte rangednssecenable yes no yesdnssecvalidation yes no yesdnssecmustbesecure domain yes no noneThese options configure support for DNSSEC See the sections starting on page for a general discussion of DNSSEC and a detailed description of how to setup DNSSEC at your siteAn authoritative server needs the dnssecenable option turned on A recursiveserver needs the dnssecenable and dnssecvalidation options turned onPermissionsPacket sizesDNSSEC controldnssecenable and dnssecvalidation are turned on by default which has various implications An authoritative server of a signed zone answering a query with theDNSSECaware bit turned on answers with the requested resource records and their signatures An authoritative server of a signed zone answering a query with theDNSSECaware bit not set answers with just the requested resource records as in the preDNSSEC era An authoritative server of an unsigned zone answers queries with just therequested resource records there are no signatures to include A recursive server sends queries on behalf of users with the DNSSECawarebit set A recursive server validates the signatures included with signed repliesbefore returning data to the userThe dnssecmustbesecure option allows you to specify that you will only accept secure answers from particular domains or alternatively that you dont careand that insecure answers are OK For example you might say yes to the domainimportantstuffmybankcom and no to the domain marketingmybankcomzonestatistics yes no noThis option causes named to maintain perzone statistics as well as global statisticsRun rndc stats to dump the statistics to a fileclientsperquery int Clients waiting on same querymaxclientsperquery int Max clients before server dropsdatasize int unlimited Max memory server may usefiles int unlimited Max of concurrent open fileslamettl int min Secs to cache lame server datamaxacachesize int Cache size for additional datamaxcachesize int Max memory for cached answersmaxcachettl int week Max TTL for caching positive datamaxjournalsize int Max size of transaction journalmaxncachettl int hrs Max TTL for caching negative datatcpclients int Max simultaneous TCP clientsThis long list of options can be used to tune named to run well on your hardwareWe dont describe them in detail but if you are having performance problems theseoptions may suggest a starting point for your tuning effortsWhew we are finally done with the options Lets get on to the rest of the configuration languageStatisticsPerformance tuningThe acl statementAn access control list is just an address match list with a nameacl aclname addressmatchlistYou can use an aclname anywhere an address match list is called forAn acl must be a toplevel statement in namedconf so dont try sneaking it inamid your other option declarations Also keep in mind that namedconf is readin a single pass so access control lists must be defined before they are used Fourlists are predefined any all hosts localnets all hosts on the local networks localhost the machine itself none nothingThe localnets list includes all of the networks to which the host is directly attachedIn other words its a list of the machines network addresses modulo their netmasksThe TSIG key statementThe key statement defines a shared secret that is a password that authenticatescommunication between two servers for example between the master server anda slave for a zone transfer or between a server and the rndc process that controls itBackground information about BINDs support for cryptographic authenticationis given in the DNS security issues section starting on page Here we touchbriefly on the mechanics of the processTo build a key record you specify both the cryptographic algorithm that you wantto use and the shared secret represented as a baseencoded string see page for detailskey keyid algorithm stringsecret stringAs with access control lists the keyid must be defined with a key statement beforeit is used To associate the key with a particular server just include keyid in thekeys clause of that servers server statement The key is used both to verify requestsfrom that server and to sign the responses to those requestsThe shared secret is sensitive information and should not be kept in a worldreadable file Use an include statement to bring it into the namedconf fileThe server statementnamed can potentially talk to many servers not all of them running current software and not all of them even nominally sane The server statement tells namedabout the characteristics of its remote peers The server statement can overridedefaults for a particular server its not required unless you want to configure keysfor zone transfersserver ipaddr bogus yes no noprovideixfr yes no yesrequestixfr yes no yeskeys keyid keyid nonetransfersource ipaddress port closest interfacetransfersourcev ipvaddress port closest interfaceYou can use a server statement to override the values of global configuration optionsfor individual servers Just list the options for which you want nondefault behaviorWe have not shown all the serverspecific options just the ones we think you mightneed See the BIND documentation for a complete listIf you mark a server as being bogus named wont send any queries its way Thisdirective should be reserved for servers that are in fact bogus bogus differs fromthe global option blackhole in that it suppresses only outbound queries By contrast the blackhole option completely eliminates all forms of communication withthe listed serversA BIND name server acting as master for a dynamically updated zone performsincremental zone transfers if provideixfr is set to yes Likewise a server actingas a slave requests incremental zone transfers from the master if requestixfr isset to yes Dynamic DNS is discussed in detail on page The keys clause identifies a key ID that has been previously defined in a key statement for use with transaction signatures see page Any requests sent to theremote server are signed with this key Requests originating at the remote serverare not required to be signed but if they are the signature will be verifiedThe transfersource clauses give the IPv or IPv address of the interface andoptionally the port that should be used as a source address port for zone transferrequests This clause is only needed when the system has multiple interfaces andthe remote server has specified a specific IP address in its allowtransfer clausethe addresses must matchThe masters statementThe masters statement lets you name a set of one or more master servers by specifying their IP addresses and cryptographic keys You can then use this defined name inthe masters clause of zone statements instead of repeating the IP addresses and keysHow can there bemore than one master See page The masters facility is helpful when multiple slave or stub zones get their data fromthe same remote servers If the addresses or cryptographic keys of the remote servers change you can update the masters statement that introduces them rather thanchanging many different zone statementsThe syntax ismasters name ipaddr port ipport key key The logging statementnamed is the current holder of the most configurable logging system on Earthaward Syslog put the prioritization of log messages into the programmers handsand the disposition of those messages into the sysadmins hands But for a givenpriority the sysadmin had no way to say I care about this message but not aboutthat message BIND added categories that classify log messages by type and channels that broaden the choices for the disposition of messages Categories are determined by the programmer and channels by the sysadminSince logging requires quite a bit of explanation and is somewhat tangential wediscuss it in the debugging section beginning on page The statisticschannels statementThe statisticschannels statement lets you connect to a running named witha browser to view statistics as they are accumulated Since the stats of your nameserver might be sensitive you should restrict access to this data to trusted hosts atyour own site The syntax isstatisticschannels inet ipaddr port port allow addressmatchlist You can include multiple inetportallow sequences The defaults are open so becareful The IP address defaults to any the port defaults to port normal HTTPand the allow clause defaults to letting anyone connect To use statistics channelsnamed must have been compiled with libxmlThe zone statementzone statements are the heart of the namedconf file They tell named about thezones for which it is authoritative and set the options that are appropriate for managing each zone A zone statement is also used by a caching server to preload theroot server hints that is the names and addresses of the root servers which bootstrap the DNS lookup processThe exact format of a zone statement varies depending on the role that named isto play with respect to that zone The possible zone types are master slave hintforward stub and delegationonly We do not describe stub zones used byBIND only or delegationonly zones used to stop the use of wild card recordsin toplevel zones to advertise a registrars services The following brief sectionsdescribe the other zone typesMany of the global options covered earlier can become part of a zone statement andoverride the previously defined values We have not repeated those options hereexcept to mention certain ones that are frequently usedConfiguring the master server for a zoneHeres the format you need for a zone of which this named is the master serverzone domainname type masterfile pathThe domainname in a zone specification must always appear in double quotesThe zones data is kept on disk in a humanreadable and humaneditable file Thefilename has no default so you must provide a file statement when declaring amaster zone A zone file is just a collection of DNS resource records in the formatsdescribed starting on page Other serverspecific attributes are also frequently specified within the zone statement For exampleallowquery addressmatchlist anyallowtransfer addressmatchlist anyallowupdate addressmatchlist nonezonestatistics yes no noThe access control options are not required but its a good idea to use them Theyaccept any kind of address match list so you can configure security either in terms ofIP addresses or in terms of TSIG encryption keys As usual encryption keys are saferIf dynamic updates are used for this zone the allowupdate clause must be present with an address match list that limits the hosts from which updates can occurDynamic updates apply only to master zones the allowupdate clause cannot beused for a slave zone Be sure that this clause includes just your own machines egDHCP servers and not the whole InternetThe zonestatistics option makes named keep track of queryresponse statisticssuch as the number and percentage of responses that were referrals that resultedin errors or that demanded recursionWith all these zonespecific options and about more we have not covered theconfiguration is starting to sound complicated However a master zone declaration You also need ingress filtering at your firewall see page Better yet use TSIG for authenticationSee page for moreinformation aboutdynamic updatesconsisting of nothing but a pathname to the zone file is perfectly reasonable Hereis an example slightly modified from the BIND documentationzone examplecom type masterfile forwardexamplecomallowquery any allowtransfer myslaves Here myslaves would be an access control list you had previously definedConfiguring a slave server for a zoneThe zone statement for a slave is similar to that of a masterzone domainname type slavefile pathmasters ipaddr port ipport key keyname allowquery addressmatchlist anySlave servers normally maintain a complete copy of their zones database The filestatement specifies a local file in which the replicated database can be stored Eachtime the server fetches a new copy of the zone it saves the data in this file If theserver crashes and reboots the file can then be reloaded from the local disk without being transferred across the networkYou shouldnt edit this cache file since its maintained by named However it can beinteresting to inspect if you suspect you have made an error in the master serversdata file The slaves disk file shows you how named has interpreted the original zonedata In particular relative names and ORIGIN directives have all been expandedIf you see a name in the data file that looks like one of thesecscoloradoeduanchorcscoloradoeducscoloradoeduyou can be pretty sure that you forgot a trailing dot somewhereThe masters clause lists the IP addresses of one or more machines from which thezone database can be obtained It can also contain the name of a list of masters defined by a previous masters statementWe said that only one machine can be the master for a zone so why is it possible tolist more than one address Two reasons First the master machine might have morethan one network interface and therefore more than one IP address Its possiblefor one interface to become unreachable because of network or routing problemswhile others are still accessible Therefore its a good practice to list all the masterservers topologically distinct addressesSecond named doesnt care where the zone data comes from It can pull the database just as easily from a slave server as from the master You could use this featureto allow a wellconnected slave server to serve as a sort of backup master sincethe IP addresses are tried in order until a working server is found In theory youcan also set up a hierarchy of servers with one master serving several secondlevelservers which in turn serve many thirdlevel serversSetting up the root server hintsAnother form of zone statement points named toward a file from which it can preload its cache with the names and addresses of the root name serverszone type hintfile pathThe hints are a set of DNS records that list servers for the root domain Theyreneeded to give a recursive caching instance of named a place to start searching forinformation about other sites domains Without them named would only knowabout the domains it serves and their subdomainsWhen named starts it reloads the hints from one of the root servers Ergo youllbe fine as long as your hints file contains at least one valid reachable root serverAs a fallback the root server hints are also compiled into namedThe hints file is often called rootcache It contains the response you would get ifyou queried any root server for the name server records in the root domain In factyou can generate the hints file in exactly this way with dig For example dig frootserversnet ns rootcacheMind the dot If frootserversnet is not responding you can run the query withoutspecifying a particular server dig ns rootcacheThe output will be similar however you will be obtaining the list of root servers fromthe cache of a local name server not from an authoritative source That should bejust fineeven if you have not rebooted or restarted your name server for a year ortwo it has been refreshing its root server records periodically as their TTLs expireSetting up a forwarding zoneA zone of type forward overrides nameds default query path ask the root first thenfollow referrals as described on page for a particular domainzone domainname type forwardforward only firstforwarders ipaddr ipaddr You might use a forward zone if your organization had a strategic working relationship with some other group or company and you wanted to funnel traffic directlyto that companys name servers bypassing the standard query pathThe controls statement for rndcThe controls statement limits the interaction between the running named processand rndc the program a sysadmin can use to signal and control it rndc can startand stop named dump its state put it in debug mode etc rndc operates over thenetwork and with improper configuration it might let anyone on the Internet messwith your name server The syntax iscontrols inet addr port port allow addressmatchlist keys keylist rndc talks to named on port if you dont specify a different portAllowing your name server to be controlled remotely is both handy and dangerousStrong authentication through a key entry in the allow clause is required keys inthe address match list are ignored and must be explicitly stated in the keys clauseof the controls statementYou can use the rndcconfgen command to generate an authentication key for usebetween rndc and named There are essentially two ways to set up use of the keyyou can have both named and rndc consult the same configuration file to learn thekey eg etcrndckey or you can include the key in both the rndcs and namedsconfiguration files etcrndcconf for rndc and etcnamedconf for named Thelatter option is more complicated but its necessary when named and rndc will berunning on different computers rndcconfgen a sets up keys for localhost accessWhen no controls statement is present BIND defaults to the loopback addressfor the address match list and looks for the key in etcrndckey Because strongauthentication is mandatory the rndc command cannot control named if no keyexists This precaution may seem draconian but consider even if rndc workedonly from and this address was blocked from the outside world at yourfirewall you would still be trusting all local users not to tamper with your nameserver Any user could telnet to the control port and type stopquite an effectivedenialofservice attackHere is an example of the output to standard out from rndcconfgen when abit key is requested We chose bits because it fits on the page you wouldnormally choose a longer key and redirect the output to etcrndcconf The comments at the bottom of the output show the lines you need to add to namedconfto make named and rndc play together rndcconfgen b Start of rndcconfkey rndckey algorithm hmacmdsecret orZuzamkUnEpzlHxDcdhACldOGsGelPdvIYoptions defaultkey rndckeydefaultserver defaultport End of rndcconf Use the following in namedconf adjusting the allow list as needed key rndckey algorithm hmacmd secret orZuzamkUnEpzlHxDcdhACldOGsGelPdvIY controls inet port allow keys rndckey End of namedconf Split DNS and the view statementMany sites want the internal view of their network to be different from the viewseen from the Internet For example you might reveal all of a zones hosts to internal users but restrict the external view to a few wellknown servers Or you mightexpose the same set of hosts in both views but supply additional or different records to internal users For example the MX records for mail routing might pointto a single mail hub machine from outside the domain but point to individualworkstations from the perspective of internal usersA split DNS configuration is especially useful for sites that use RFC private IPaddresses on their internal networks For example a query for the hostname associated with IP address can never be answered by the global DNS systembut it is meaningful within the context of the local network Of the queries arrivingat the root name servers are either from an IP address in one of the privateaddress ranges or about one of these addresses Neither can be answered both arethe result of misconfiguration either of BINDs split DNS or of Microsofts domainsSee page for moreinformation aboutprivate address spacesThe view statement packages up a couple of access lists that control which clientssee which view some options that apply to all the zones in the view and finally thezones themselves The syntax isview viewname matchclients addressmatchlist anymatchdestinations addressmatchlist anymatchrecursiveonly yes no noviewoption zonestatement Views have always had a matchclients clause that filters on queries source IPaddresses It typically serves internal and external views of a sites DNS data Forfiner control you can now also filter on the query destination address and can require recursive queriesThe matchdestinations clause looks at the destination address to which a querywas sent Its useful on multihomed machines that is machines with more than onenetwork interface when you want to serve different DNS data depending on theinterface on which the query arrived The matchrecursiveonly clause requiresqueries to be recursive as well as to originate at a permitted client Iterative querieslet you see what is in a sites cache this option prevents itViews are processed in order so put the most restrictive views first Zones in different views can have the same names but take their data from different files Views arean allornothing proposition if you use them all zone statements in your namedconfiguration file must appear in the context of a viewHere is a simple example from the BIND documentation The two views definethe same zone but with different dataview internal matchclients ournets Only internal networksrecursion yes Internal clients onlyzone examplecom Complete view of zone type masterfile exampleinternaldbview external matchclients any Allow all queriesrecursion no But no recursionzone examplecom Only public hosts type masterfile exampleexternaldbIf the order of the views were reversed no one would ever see the internal view Internal hosts would match the any value in the matchclients clause of the externalview before they reached the internal viewOur second DNS configuration example starting on page provides an additional example of views BIND configuration examplesNow that we have explored the wonders of namedconf lets look at two completeconfiguration examples The localhost zone A small security company that uses split DNSThe localhost zoneThe IPv address refers to a host itself and should be mapped to the namelocalhost Some sites map the address to localhostlocaldomain and some doboth The corresponding IPv address is If you forget to configure the localhost zone your site may end up querying theroot servers for localhost information The root servers receive so many of thesequeries that the operators are considering adding a generic mapping between localhost and at the root level Other unusual names in the popular bogusTLD category are lan home localdomain and domainThe forward mapping for the name localhost can be defined in the forward zone filefor the domain with an appropriate ORIGIN statement or in its own file Each server even a caching server is usually the master for its own reverse localhost domainHere are the lines in namedconf that configure localhostzone localhost localhost forward zonetype masterfile localhostallowupdate none zone inaddrarpa localhost reverse zonetype masterfile allowupdate none Actually the whole class A network refers to localhost but most folks just use The corresponding forward zone file localhost contains the following linesTTL d localhost IN SOA localhost postmasterlocalhost Serial Refresh Retry Expiration Minimum NS localhost A The reverse file containsTTL d inaddrarpa IN SOA localhost postmasterlocalhost Serial Refresh Retry Expiration Minimum NS localhost PTR localhostThe mapping for the localhost address never changes so the timeouts canbe large Note the serial number which encodes the date the file was last changedin Also note that only the master name server is listed for the localhost domain The meaning of here is inaddrarpaA small security companyOur second example is for a small company that specializes in security consultingThey run BIND on a recent version of Red Hat Enterprise Linux and use viewsto implement a split DNS system in which internal and external users see different host data They also use private address space internally queries about thoseaddresses should never escape to the Internet to clutter up the global DNS systemHere is their namedconf file reformatted and commented a bitoptions directory vardomainversion rootatrustcomallowtransfer listenon include atrustkey Mode filecontrols inet allow keys atkey view internal matchclients recursion yesinclude infrastructurezones Root hints localhost forw revzone atrustcom Internal forward zone type masterfile internalatrustcomzone inaddrarpa Internal reverse zone type masterfile internalrev allowupdate none Many zones omittedinclude internaltmarkzones atrustnet atrustorg slaves End of internal viewview world External viewmatchclients any recursion nozone atrustcom External forward zone type masterfile worldatrustcom allowupdate none zone inaddrarpa External reverse zone type masterfile worldrev allowupdate none include worldtmarkzones atrustnet atrustorg masterszone admincom Master zones only in world view type masterfile worldadmincom allowupdate none Lots of masterslave zones omitted End of external viewThe file atrustkey defines the key named atkeykey atkey algorithm hmacmdsecret shared secret key goes hereThe file tmarkzones includes variations on the name atrustcom both in differenttoplevel domains net org us info etc and with different spellings appliedtrustcometc The file infrastructurezones contains the root hints and localhost filesZones are organized by view internal or world and type master or slave and thenaming convention for zone data files reflects this scheme This server is recursivefor the internal view which includes all local hosts including many that use privateaddressing The server is not recursive for the external view which contains onlyselected hosts at atrustcom and the external zones for which they provide eithermaster or slave DNS serviceSnippets of the files internalatrustcom and worldatrustcom are shown belowFirst the internal file atrustcom internal fileTTL ORIGIN atrustcom SOA nsatrustcom trentatrustcom NS NSatrustcom NS NSatrustcom MX mailserveratrustcom A ns A ns A www A mailserver A exchange A secure A You can see from the IP address ranges that this site is using RFC private addresses internally Note also that instead of assigning nicknames to hosts throughCNAMEs this site has multiple A records that point to the same IP addressesThis approach works fine but each IP address should have only one PTR recordin the reverse zone A records were at one time potentially faster to resolve than CNAMEs because they relieved clientsof the need to perform a second DNS query to obtain the address of a CNAMEs target These daysDNS servers are smarter and automatically include an A record for the target in the original query response if they know itHere is the external view of that same domain from the file worldatrustcom atrustcom external fileTTL ORIGIN atrustcom SOA nsatrustcom trentatrustcom NS NSatrustcom NS NSatrustcom MX mailserveratrustcom A nsatrustcom A nsatrustcom A www A mailserver A secure A reverse mapsexterior A PTR exterioratrustcomexterior A PTR exterioratrustcomAs in the internal view nicknames are implemented with A records Only a fewhosts are actually visible in the external view although thats not immediately apparent from these truncated excerpts Machines that appear in both views forexample ns have RFC private addresses internally but publicly registeredand assigned addresses externallyThe TTL in these zone files is set to hours seconds For internal zonesthe TTL is one day seconds Zone file updatingTo change a domains data eg to add or delete a host you update the zone datafiles on the master server You must also increment the serial number in the zonesSOA record Finally you must get your name server software to pick up and distribute your changesThis final step varies depending on your software For BIND just run rndc reloadto signal named to pick up the changes You can also kill and restart named butif your server is both authoritative for your zone and recursive for your users thisoperation discards cached data from other domainsUpdated zone data is propagated to slave servers of BIND masters right away because the notify option is on by default If notifications are not turned on yourslave servers will not pick up the changes until after refresh seconds as set in thezones SOA record typically an hour laterIf you have the notify option turned off you can force BIND slaves to updatethemselves by running rndc reload on each slave This command makes the slavecheck with the master see that the data has changed and request a zone transferDont forget to modify both the forward and reverse zones when you change ahostname or IP address Forgetting the reverse files leaves sneaky errors somecommands work and some dontChanging a zones data but forgetting to change the serial number makes the changestake effect on the master server after a reload but not on the slavesDo not edit data files on slave servers These files are maintained by the name server and sysadmins should not meddle with them Its fine to look at the BIND datafiles as long as you dont make changesZone transfersDNS servers are synchronized through a mechanism called a zone transfer A zonetransfer can include the entire zone called AXFR or be limited to incrementalchanges called IXFR By default zone transfers use the TCP protocol on port BIND logs transferrelated information with category xferin or xferoutA slave wanting to refresh its data must request a zone transfer from the masterserver and make a backup copy of the zone data on disk If the data on the masterhas not changed as determined by a comparison of the serial numbers not theactual data no update occurs and the backup files are just touched That is theirmodification times are set to the current timeBoth the sending and receiving servers remain available to answer queries duringa zone transfer Only after the transfer is complete does the slave begin to use thenew dataWhen zones are huge like com or dynamically updated see the next sectionchanges are typically small relative to the size of the entire zone With IXFR onlythe changes are sent unless they are larger than the complete zone in which casea regular AXFR transfer is done The IXFR mechanism is analogous to the patchprogram in that it makes changes to an old database to bring it into conformitywith a new databaseIn BIND IXFR is the default for any zones configured for dynamic update andnamed maintains a transaction log called zonenamejnl You can set the optionsprovideixfr and requestixfr in the server statements for individual peers Theprovideixfr option enables or disables IXFR service for zones for which thisserver is the master The requestixfr option requests IXFRs for zones for whichthis server is a slaveprovideixfr yes In BIND server statementrequestixfr yes In BIND server statementIXFRs work for zones that are edited by hand too Use the BIND zone option calledixfrfromdifferences to enable this behavior IXFR requires the zone file to besorted in a canonical order An IXFR request to a server that does not support itautomatically falls back to the standard AXFR zone transferDynamic updatesDNS was originally designed under the assumption that nametoaddress mappings are relatively stable and do not change frequently However a site that usesDHCP to dynamically assign IP addresses as machines boot and join the networkbreaks this rule constantly Two basic solutions are available either add genericand static entries to the DNS database or provide some way to make small frequent changes to zone dataThe first solution should be familiar to anyone who has looked up the PTR recordfor the IP address assigned to them by a massmarket home ISP The DNS configuration usually looks something like thisdhcphostdomain IN A dhcphostdomain IN A Although this is a simple solution it means that hostnames are permanently associated with particular IP addresses and that computers therefore change hostnameswhenever they receive new IP addresses Hostnamebased logging and securitymeasures become very difficult in this environmentThe dynamic update feature outlined in RFC offers an alternative solution Itextends the DNS protocol to include an update operation thereby allowing entitiessuch as DHCP daemons to notify name servers of the address assignments theymake Dynamic updates can add delete or modify resource recordsWhen dynamic updates are enabled in BIND named maintains a journal of dynamic changes zonenamejnl that it can consult in the event of a server crashnamed recovers the inmemory state of the zone by reading the original zone filesand then replaying the changes from the journalYou cannot handedit a dynamically updated zone without first stopping the dynamic update stream rndc freeze zone or rndc freeze zone class view will do thetrick These commands sync the journal file to the master zone file on disk andthen delete the journal You can then edit the zone file by hand Unfortunately theoriginal formatting of the zone file will have been destroyed by nameds monkeyingthe file will look like those maintained by named for slave serversDynamic update attempts are refused while a zone is frozen To reload the zone filefrom disk and reenable dynamic updates use rndc thaw with the same argumentsyou used to freeze the zoneThe nsupdate program supplied with BIND comes with a commandline interfacefor making dynamic updates It runs in batch mode accepting commands from theSee page formore informationabout DHCPkeyboard or a file A blank line or the send command signals the end of an updateand sends the changes to the server Two blank lines signify the end of input Thecommand language includes a primitive if statement to express constructs suchas if this hostname does not exist in DNS add it As predicates for an nsupdateaction you can require a name to exist or not exist or require a resource recordset to exist or not existFor example here is a simple nsupdate script that adds a new host and also addsa nickname for an existing host if the nickname is not already in use The anglebracket prompt is produced by nsupdate and is not part of the command script nsupdate update add newhostcscoloradoedu A prereq nxdomain gypsycscoloradoedu update add gypsycscoloradoedu CNAME evilaptopcscoloradoeduDynamic updates to DNS are scary They can potentially provide uncontrolledwrite access to your important system data Dont try to use IP addresses for accesscontrolthey are too easily forged TSIG authentication with a sharedsecret keyis better its available and is easy to configure BIND supports both nsupdate k keydirkeyfileor nsupdate y keynamesecretkeySince the password goes on the command line in the y form anyone running wor ps at the right moment can see it For this reason the k form is preferred Formore details on TSIG see the section starting on page Dynamic updates to a zone are enabled in namedconf with an allowupdateor updatepolicy clause allowupdate grants permission to update any recordsin accordance with IP or keybased authentication updatepolicy is a BIND extension that allows finegrained control for updates according to the hostnameor record type It requires keybased authentication Both forms can be used onlywithin zone statements and they are mutually exclusive within a particular zoneA good default for zones with dynamic hosts is to use updatepolicy to allow clients to update their A or PTR records but not to change the SOA record NS records or KEY recordsThe syntax of an updatepolicy rule of which there can be several isgrantdeny identity nametype name types The identity is the name of the cryptographic key needed to authorize the updateThe nametype has one of four values name subdomain wildcard or self Theself option is particularly prized because it allows hosts to update only their ownrecords Use it if your situation allowsThe name is the zone to be updated and the types are the resource record types thatcan be updated If no types are specified all types except SOA NS RRSIG andNSEC or NSEC can be updatedHeres a complete exampleupdatepolicy grant dhcpkey subdomain dhcpcscoloradoedu A This configuration allows anyone who knows the key dhcpkey to update addressrecords in the dhcpcscoloradoedu subdomain This statement would appear in themaster servers namedconf file within the zone statement for dhcpcscoloradoeduThere would be a key statement somewhere to define dhcpkey as wellThe snippet below from the namedconf file at the computer science departmentat the University of Colorado uses the updatepolicy statement to allow studentsin a system administration class to update their own subdomains but not to messwith the rest of the DNS environmentzone saclassnet type masterfile saclasssaclassnetupdatepolicy grant feanormroe subdomain saclassnet grant mojomroe subdomain saclassnet grant dawdlemroe subdomain saclassnet grant piratemroe subdomain saclassnet DNS security issuesDNS started out as an inherently open system but it has steadily grown more andmore secureor at least securable By default anyone on the Internet can investigate your domain with individual queries from tools such as dig host nslookupand drill In some cases they can dump your entire DNS databaseTo address such vulnerabilities name servers support various types of access controlthat key off of host and network addresses or cryptographic authentication Table on the next page summarizes the security features that can be configured innamedconf The Page column shows where in this book to look for more informationBIND can run in a chrooted environment under an unprivileged UID to minimizesecurity risks It can also use transaction signatures to control communication betweenmaster and slave servers and between the name servers and their control programsTable Security features in BINDFeature Context Page What it specifiesacl Various Access control listsallowquery options zone Who can query a zone or serverallowrecursion options Who can make recursive queriesallowtransfer options zone Who can request zone transfersallowupdate zone Who can make dynamic updatesblackhole options Servers to completely ignorebogus server Servers never to queryupdatepolicy zone Who can make dynamic updatesAccess control lists in BIND revisitedACLs are named addressmatch lists that can appear as arguments to statementssuch as allowquery allowtransfer and blackhole Their basic syntax wasdescribed on page ACLs can help beef up DNS security in a variety of waysEvery site should at least have one ACL for bogus addresses and one ACL for localaddresses For exampleacl bogusnets ACL for bogus networks Default wild card addresses Reserved addresses Reserved addresses Linklocal delegated addresses Sample addresses like examplecom Multicast address space Private address space RFC Private address space RFC Private address space RFC acl cunets ACL for University of Colorado networks Main campus network In the global options section of your config file you could then includeallowrecursion cunets blackhole bogusnets Dont make private addresses bogus if you use them and are configuring your internal DNS serversIts also a good idea to restrict zone transfers to legitimate slave servers An ACLmakes things nice and tidyacl ourslaves anchor acl measurements Bill mannings measurements v address Bill mannings measurements v address The actual restriction is implemented with a line such asallowtransfer ourslaves measurements Here transfers are limited to our own slave servers and to the machines of an Internet measurement project that walks the reverse DNS tree to determine the sizeof the Internet and the percentage of misconfigured servers Limiting transfers inthis way makes it impossible for other sites to dump your entire database with atool such as dig see page Of course you should still protect your network at a lower level through routeraccess control lists and standard security hygiene on each host If those measuresare not possible you can refuse DNS packets except to a gateway machine that youmonitor closelyOpen resolversAn open resolver is a recursive caching name server that accepts and answers queries from anyone on the Internet Open resolvers are bad Outsiders can consumeyour resources without your permission or knowledge and if they are bad guysthey might be able to poison your resolvers cacheWorse open resolvers are sometimes used by miscreants to amplify distributeddenial of service attacks The attacker sends queries to your resolver with a fakedsource address that points back to the victim of the attack Your resolver dutifullyanswers the queries and sends some nice fat packets to the victim The victim didntinitiate the queries but it still has to route and process the network traffic Multiplyby a bunch of open resolvers and its real trouble for the victimStatistics show that between and of caching name servers are currentlyopen resolversyikes The site dnsmeasurementfactorycomtools can help youtest your site Go there select the open resolver test and type in the IP addressesof your name servers Alternatively you can enter a network number or WHOISidentifier to test all the associated serversUse access control lists in namedconf to limit your caching name servers to answering queries from your own usersRunning in a chrooted jailIf hackers compromise your name server they can potentially gain access to thesystem under the guise of the user as whom it runs To limit the damage that someone could do in this situation you can run the server in a chrooted environmentrun it as an unprivileged user or bothFor named the commandline flag t specifies the directory to chroot to and theu flag specifies the UID under which named should run For example sudo named u initially starts named as root but after named completes its rootly chores it relinquishes its root privileges and runs as UID Many sites dont bother to use the u and t flags but when a new vulnerability isannounced they must be faster to upgrade than the hackers are to attackThe chroot jail cannot be empty since it must contain all the files the name servernormally needs to run devnull devrandom the zone files configuration fileskeys syslog target files UNIX domain socket for syslog var etc It takes a bit ofwork to set all this up The chroot system call is performed after libraries have beenloaded so you need not copy shared libraries into the jailSecure servertoserver communication with TSIG and TKEYDuring the time when DNSSEC covered in the next section was being developedthe IETF developed a simpler mechanism called TSIG RFC to allow securecommunication among servers through the use of transaction signatures Access control through transaction signatures is more secure than access control byIP source addresses alone TSIG can secure zone transfers between a master serverand its slaves and can also secure dynamic updatesThe TSIG seal on a message authenticates the peer and verifies that the data has notbeen tampered with Signatures are checked at the time a packet is received andare then discarded they are not cached and do not become part of the DNS dataTSIG uses symmetric encryption That is the encryption key is the same as thedecryption key This single key is called the shared secret The TSIG specificationallows multiple encryption methods and BIND implements quite a few Use a different key for each pair of servers that want to communicate securelyTSIG is much less expensive computationally than public key cryptography butbecause it requires manual configuration it is only appropriate for a local networkon which the number of pairs of communicating servers is small It does not scaleto the global InternetSetting up TSIG for BINDFirst use BINDs dnsseckeygen utility to generate a sharedsecret host key for thetwo servers say master and slave dnsseckeygen a HMACSHA b n HOST masterslaveKmasterslaveThe b flag tells dnsseckeygen to create a bit key We use bits here justto keep the keys short enough to fit on our printed pages In real life you mightwant to use a longer key bits is the maximum allowedThis command produces the following two filesKmasterslaveprivateKmasterslavekeyThe represents the SHA algorithm and is a number used as a keyidentifier in case you have multiple keys for the same pair of servers Both filesinclude the same key but in different formatsThe private file looks like thisPrivatekeyformat vAlgorithm HMACSHAKey owKtZWOlugaVFkwOqGxABits AAACreated Publish Activate and the key file like thismasterslave IN KEY owKtZWOlugaVFkwOqGxANote that dnsseckeygen added a dot to the end of the key names in both the filenames and the contents of the key file The motivation for this convention is thatwhen dnsseckeygen is used for DNSSEC keys that are added to zone files thekey names must be fully qualified domain names and must therefore end in a dotThere should probably be two tools one that generates sharedsecret keys and onethat generates publickey key pairsYou dont actually need the key fileits an artifact of dnsseckeygens being usedfor two different jobs Just delete it The in the KEY record is not the key lengthbut rather a flag bit that identifies the record as a DNS host keyAfter all this complication you may be disappointed to learn that the generatedkey is really just a long random number You could generate the key manually bywriting down an ASCII string of the right length divisible by and pretendingthat its a base encoding of something or you could use mmencode to encode The number looks random but it is actually just a hash of the TSIG keya random string The way you create the key is not important it just has to existon both machinesCopy the key from the private file to both master and slave with scp or cut andpaste it Do not use telnet or ftp to copy the key even internal networks mightnot be secureThe key must be included in both machines namedconf files Since namedconf isusually worldreadable and keys should not be put the key in a separate file that isincluded in namedconf The key file should have mode and should be ownedby the named userFor example you could put the snippetkey masterslave algorithm hmacmd secret sharedkeyyougenerated in the file masterslavetsig In the namedconf file add the lineinclude masterslavetsig near the topThis part of the configuration simply defines the keys For them to actually be usedto sign and verify updates the master needs to require the key for transfers andthe slave needs to identify the master with a server statement and keys clause Forexample you might add the lineallowtransfer key masterslave to the zone statement on the master server and the lineserver mastersIPaddress keys masterslave to the slaves namedconf file If the master server allows dynamic updates it canalso use the key in its allowupdate clause in the zone statementOur example key name is pretty generic If you use TSIG keys for many zones youmight want to include the name of the zone in the key name to help you keep everything straightWhen you first turn on transaction signatures run named at debug level seepage for information about debug mode for a while to see any error messages that are generatedWhen using TSIG keys and transaction signatures between master and slave serverskeep the clocks of the servers synchronized with NTP If the clocks are too far apartmore than about minutes signature verification will not work This problemcan be very hard to identifyscp is part of theOpenSSH suite Seepage for detailsTKEY is a BIND mechanism that lets two hosts generate a sharedsecret key automatically without phone calls or secure copies to distribute the key It uses analgorithm called the DiffieHellman key exchange in which each side makes up arandom number does some math on it and sends the result to the other side Eachside then mathematically combines its own number with the transmission it received to arrive at the same key An eavesdropper might overhear the transmissionbut will be unable to reverse the mathMicrosoft servers use TSIG in a nonstandard way called GSSTSIG that exchangesthe shared secret through TKEY If you need a Microsoft server to communicatewith BIND use the tkeydomain and tkeygssapicredential optionsSIG is another mechanism for signing transactions between servers or betweendynamic updaters and the master server It uses public key cryptography see RFCs and for detailsDNSSECDNSSEC is a set of DNS extensions that authenticate the origin of zone data andverify its integrity by using public key cryptography That is the extensions allowDNS clients to ask the questions Did this DNS data really come from the zonesowner and Is this really the data sent by that ownerDNSSEC relies on a cascading chain of trust The root servers validate informationfor the toplevel domains the toplevel domains validate information for the secondlevel domains and so onPublic key cryptosystems use two keys one to encrypt sign and a different oneto decrypt verify Publishers sign their data with the secret private key Anyonecan verify the validity of a signature with the matching public key which is widelydistributed If a public key correctly decrypts a zone file then the zone must havebeen encrypted with the corresponding private key The trick is to make sure thatthe public keys you use for verification are authentic Public key systems allow oneentity to sign the public key of another thereby vouching for the legitimacy of thekey hence the term chain of trustThe data in a DNS zone is too voluminous to be encrypted with public key cryptographythe encryption would be too slow Instead since the data is not secreta secure hash is run on the data and the results of the hash are signed encryptedby the zones private key The results of the hash are like a fingerprint of the dataand are called a digital signature The signatures are appended to the data they authenticate as RRSIG records in the signed zone fileTo verify the signature you decrypt it with the public key of the signer run the datathrough the same secure hash algorithm and compare the computed hash value The math involved is called the discrete log problem and relies on the fact that for modular arithmetic taking powers is easy but taking logs to undo the powers is close to impossiblewith the decrypted hash value If they match you have authenticated the signer andverified the integrity of the dataIn the DNSSEC system each zone has its own public and private keys In fact ithas two sets of keys a zonesigning key pair and a keysigning key pair The privatezonesigning key signs each RRset that is each set of records of the same type forthe same host The public zonesigning key verifies the signatures and is includedin the zones data in the form of a DNSKEY resource recordParent zones contain DS records that are hashes of the child zones selfsigned keysigningkey DNSKEY records A name server verifies the authenticity of a childzones DNSKEY record by checking it against the parent zones signature To verifythe authenticity of the parent zones key the name server can check the parentsparent and so on back to the root The public key for the root zone is widely published and is included in the root hints fileThe DNSSEC specifications require that if a zone has multiple keys each is trieduntil the data is validated This behavior is required so that keys can be rolled overchanged without interruptions in DNS service If a DNSSECaware recursivename server queries an unsigned zone the unsigned answer that comes back isaccepted as valid But problems occur when signatures expire or when parent andchild zones do not agree on the childs current DNSKEY recordDNSSEC policyBefore you begin deployment of DNSSEC you should nail down or at least thinkabout a few policies and procedures For example What size keys will you use Longer keys are more secure but they makefor larger packets How often will you change keys in the absence of a security incidentWe suggest that you keep a key log that records the date you generated each key thehardware and operating system used the key tag assigned the version of the keygenerator software the algorithm used the key length and the signature validityperiod If a cryptographic algorithm is later compromised you can check your logto see if you are vulnerableDNSSEC resource recordsDNSSEC uses five resource record types that were referred to in the DNS databasesection back on page but were not described in detail DS DNSKEY RRSIGNSEC and NSEC We describe them here in general and then outline the stepsinvolved in signing a zone Each of these records is created by DNSSEC tools ratherthan by being typed into a zone file with a text editorThe DS Designated Signer record appears only in the parent zone and indicates thata subzone is secure signed It also identifies the key used by the child to selfsignits own KEY resource record set The DS record includes a key identifier a fivedigitnumber a cryptographic algorithm a digest type and a digest of the public keyrecord allowed or used to sign the childs key resource record Heres an exampleexamplecom IN DS DCFFCEAADDBCEEThe question of how to change existing keys in the parent and child zones hasbeen a thorny one that seemed destined to require cooperation and communication between parent and child The creation of the DS record the use of separatekeysigning and zonesigning keys and the use of multiple key pairs have helpedaddress this problemKeys included in a DNSKEY resource record can be either keysigning keys KSKsor zonesigning keys ZSKs A flag called SEP for secure entry point distinguishesthem Bit of the flags field is set to for KSKs and to for ZSKs This convention makes the flags field of KSKs odd and of ZSKs even when they are treated asdecimal numbers The values are currently and respectivelyMultiple keys can be generated and signed so that a smooth transition from one keyto the next is possible The child can change its zonesigning keys without notifying the parent it must only coordinate with the parent if it changes its keysigningkey As keys roll over both the old key and the new key are valid for a certain interval Once cached values on the Internet have expired the old key can be retiredAn RRSIG record is the signature of a resource record set that is the set of all records of the same type and name within a zone RRSIG records are generated byzonesigning software and added to the signed version of the zone fileAn RRSIG record contains a wealth of information The type of record set being signed The signature algorithm used encoded as a small integer The number of labels dotseparated pieces in the name field The TTL of the record set that was signed The time the signature expires as yyyymmddhhssss The time the record set was signed also yyyymmddhhssss A key identifier a digit number The signers name domain name And finally the digital signature itself baseencodedHeres an example RRSIG NS examplecom pMKZwaPVTbIguEQNUojNVlVewHaup NSEC or NSEC records are also produced as a zone is signed Rather than signingrecord sets they certify the intervals between record set names and so allow for a In this section baseencoded hashes and keys have all been truncated to save space and better illustrate the structure of the recordssigned answer of no such domain or no such resource record set For examplea server might respond to a query for A records named borkatrustcom with anNSEC record that certifies the nonexistence of any A records between barkatrustcomand bundtatrustcomUnfortunately the inclusion of the endpoint names in NSEC records allows someone to walk through the zone and obtain all of its valid hostnames NSEC fixesthis feature by including hashes of the endpoint names rather than the endpointnames themselves but it is more expensive to compute more security less performance NSEC and NSEC are both in current use and you can choose betweenthem when you generate your keys and sign your zonesUnless protecting against a zone walk is critically important for your site we recommend that you use NSEC for nowTurning on DNSSECTwo separate workflows are involved in deploying signed zones a first that createskeys and signs zones and a second that serves the contents of those signed zonesThese duties need not be implemented on the same machine In fact it is better toquarantine the private key and the CPUintensive signing process on a machinethat is not publicly accessible from the Internet Of course the machine that actually serves the data must be visible to the InternetThe first step in setting up DNSSEC is to organize your zone files so that all thedata files for a zone are in a single directory The tools that manage DNSSEC zonesexpect this organizationNext enable DNSSEC on your servers with the namedconf optionsoptions dsnsecenable yesfor authoritative servers andoptions dsnsecenable yesdnssecvalidation yesfor recursive servers The dnssecenable option tells your authoritative servers toinclude DNSSEC record set signatures in their responses when answering queriesfrom DNSSECaware name servers The dnssecvalidation option makes namedverify the legitimacy of signatures it receives in responses from other serversKey pair generationYou must generate two key pairs for each zone you want to sign a zonesigningZSK pair and a keysigning KSK pair Each pair consists of a public key and aprivate key The KSKs private key signs the ZSK and creates a secure entry point forthe zone The ZSKs private key signs the zones resource records The public keysare then published to allow other sites to verify your signaturesThe commands dnsseckeygen a RSASHA b n ZONE examplecomKexamplecom dnsseckeygen a RSASHA b n ZONE f KSK examplecomKexamplecomgenerate for examplecom a bit ZSK pair that uses the RSA and SHA algorithms and a corresponding bit KSK pair The outstanding issue of UDPpacket size limits suggests that its best to use short zonesigning keys and to changethem often You can use longer keysigning keys to help recover some securityIt can take a whileminutesto generate these keys The limiting factor is typically not CPU power but the entropy available for randomization On Linux youcan install the haveged daemon to harvest entropy from additional sources andthereby speed up key generationdnsseckeygen prints to standard out the base filename of the keys it has generated In this example examplecom is the name of the key is the identifier ofthe RSASHA algorithm suite and and are hashes called the keyidentifiers key footprints or key tags As when generating TSIG keys each run ofdnsseckeygen creates two files key and privateKexamplecomkey Public zonesigning keyKexamplecomprivate Private zonesigning keySeveral encryption algorithms are available each with a range of possible key lengthsYou can run dnsseckeygen with no arguments to see the current list of supportedalgorithms BIND can also use keys generated by other softwareDepending on the version of your software some of the available algorithm namesmight have NSEC appended or prepended to them If you want to use NSECrecords instead of NSEC records for signed negative answers you must generateNSECcompatible keys with one of the NSECspecific algorithms see the manpage for dnsseckeygenThe key files each contain a single DNSKEY resource record for examplecom Forexample here is the zonesigning public key truncated to fit the page You can tellits a ZSK because the flags field is rather than for a KSKexamplecom IN DNSKEY AwEAAcyLrgENtOJPIQivZhWwSviAThese public keys must be INCLUDEd or inserted into the zone file either at the endor right after the SOA record To copy the keys into the zone file you can appendthem with cat or paste them in with a text editor bits is surely overkill many sites use or fewer Use a command like cat Kexamplecomkey zonefile The appends to the zonefile ratherthan replacing it entirely as would Dont mess this one upIdeally the private key portion of any key pair would be kept offline or at least ona machine that is not on the public Internet This precaution is impossible for dynamically updated zones and impractical for zonesigning keys but it is perfectlyreasonable for keysigning keys which are presumably quite longlived Considera hidden master server that is not accessible from outside for the ZSKs Print outthe private KSK or write it to a USB memory stick and then lock it in a safe untilyou need it againWhile youre locking away your new private keys its also a good time to enter thenew keys into your key log file You dont need to include the keys themselves justthe IDs algorithms date purpose and so onThe default signature validity periods are one month for RRSIG records ZSKsignatures of resource record sets and three months for DNSKEY records KSKsignatures of ZSKs Current best practice suggests ZSKs of length that areused for three months to a year and KSKs of length that are used for a yearor two Since the recommended key retention periods are longer than the defaultsignature validity periods you must either specify a longer validity period whensigning zones or periodically resign the zones even if the key has not changedZone signingNow that youve got keys you can sign your zones with the dnssecsignzone command which adds RRSIG and NSEC or NSEC records for each resource recordset These commands read your original zone file and produce a separate signedcopy named zonefilesignedThe syntax isdnssecsignzone o zone N increment k KSKfile zonefile ZSKfilewhere zone defaults to zonefile and the key files default to the filenames producedby dnsseckeygen as outlined aboveIf you name your zone data files after the zones and maintain the names of theoriginal key files the command reduces todnssecsignzone N increment zonefileThe N increment flag automatically increments the serial number in the SOA record so that you cant forget You can also specify the value unixtime to update theserial number to the current UNIX time seconds since January or the valuekeep to prevent dnssecsignzone from modifying the original serial number Theserial number is incremented in the signed zone file but not in the original zone fileHeres a spelledout example that uses the keys generated above sudo dnssecsignzone o examplecom N incrementk Kexamplecom examplecom Kexamplecom The web site keylengthcom tabulates a variety of organizations recommendations regarding the suggested lengths of cryptographic keysThe signed file is sorted in alphabetical order and includes the DNSKEY records weadded by hand and the RRSIG and NSEC records generated during signing Thezones serial number has been incrementedIf you generated your keys with an NSECcompatible algorithm you would signthe zone as above but with a salt flag Table shows some other useful optionsTable Useful options for dnssecsignzoneOption Functiong Generates DS records to be included in the parent zones starttime Sets the time at which the signatures become valide endtime Sets the time at which the signatures expiret Prints statisticsThe dates and times for signature validity can be expressed as absolute times in theformat yyyymmddhhmmss or as times relative to now in the format N where N isin seconds The default signature validity period is from an hour in the past to days in the future Here is an example in which we specify that signatures shouldbe valid until the end of the calendar year sudo dnssecsignzone N increment e examplecomSigned zone files are typically four to ten times larger than the original zone andall your nice logical ordering is lost A line such asmailrelay A becomes several linesmailrelayexamplecom A RRSIG A examplecom YsjDWYuuXvozeUzGRdFClrzUcLiwoev ITGfLlbhsRgJfkpEYFVRUBkKVRNguEYwk dRSkDJQzRQw NSEC mailrelayexamplecom A RRSIG NSEC RRSIG NSEC examplecom QrXPvpoChsGPseProBMZtwfeSWKO WNsNhFnotymRxZRIZypqWzLIPBZAUJR HPhLfBDoqmZYw In practical terms a signed zone file is no longer humanreadable and it cannot beedited by hand because of the RRSIG and NSEC or NSEC records No userserviceable parts insideWith the exception of DNSKEY records each resource record set resource recordsof the same type for the same name gets one signature from the ZSK DNSKEYresource records are signed by both the ZSK and the KSK so they have two RRSIGs The base representation of a signature ends in however many equal signsare needed to make the length a multiple of Once your zones are signed all that remains is to point your name server at thesigned versions of the zone files If youre using BIND look for the zone statementthat corresponds to each zone in namedconf and change the file parameter fromexamplecom to examplecomsignedFinally restart the name server daemon telling it to reread its configuration filewith sudo rndc reconfig followed by sudo rndc flushYou are now serving a DNSSEC signed zone To make changes you can edit eitherthe original unsigned zone or the signed zone and then resign the zone Editinga signed zone is something of a logistical nightmare but it is much quicker thanresigning the entire zone Be sure to remove the RRSIG records that correspondto any records that you change You probably want to make identical changes tothe unsigned zone to avoid version skewIf you pass a signed zone as the argument to dnssecsignzone any unsigned recordsare signed and the signatures of any records that are close to expiring are renewedClose to expiring is defined as being threequarters of the way through the validity period Resigning typically results in changes so make sure you increment thezones serial number by hand or use dnssecsignzone N increment to automatically increment the zones serial numberThats all there is to the local part of DNSSEC configuration Whats left is the thornyproblem of getting your island of secure DNS connected to other trusted signedparts of the DNS archipelagoThe DNSSEC chain of trustContinuing with our example DNSSEC setup examplecom is now signed andits name servers have DNSSEC enabled This means that when querying they useEDNS the extended DNS protocol and set the DNSSECaware option in the DNSheader of the packet When answering a query that arrives with that bit set theyinclude the signature data with their answerA client that receives signed answers can validate the response by checking the records signatures with the appropriate public key But it gets this key from the zonesown DNSKEY record which is rather suspicious if you think about it Whats to stopan impostor from serving up both fake records and a fake key that validates themThe canonical solution is that you give your parent zone a DS record to include inits zone file By virtue of coming from the parent zone the DS record is certifiedby the parents private key If the client trusts your parent zone it should then trustthat the parent zones DS record accurately reflects your zones public keyThe parent zone is in turn certified by its parent and so on back to the rootDNSSEC key rolloverKey rollover has always been a troublesome issue in DNSSEC In fact the originalspecifications were changed specifically to address the issue of the communicationneeded between parent and child zones whenever keys were created changed ordeleted The new specifications are called DNSSECbisZSK rollover is relatively straightforward and does not involve your parent zone orany trust anchor issues The only tricky part is the timing Keys have an expirationtime so rollover must occur well before that time However keys also have a TTLdefined in the zone file To illustrate assume that the TTL is one day and that keysdont expire for another week The following steps are then involved Generate a new ZSK Include it in the zone file Sign or resign the zone with the KSK and the old ZSK Signal the name server to reload the zone the new key is now there Wait hours the TTL now everyone has both the old and new keys Sign the zone again with the KSK and the new ZSK Signal the name server to reload the zone Wait another hours now everyone has the new signed zone Remove the old ZSK at your leisure eg the next time the zone changesThis scheme is called prepublishing Obviously you must start the process at leasttwo TTLs before the point at which you need to have everyone using the new keyThe waiting periods guarantee that any site with cached values always has a cachedkey that corresponds to the cached dataAnother variable that affects this process is the time it takes for your slowest slaveserver to update its copy of your zone when notified by the master server So dontwait until the last minute to start your rollover process or to resign zones whosesignatures are expiring Expired signatures do not validate so sites that verify DNSSEC signatures will not be able to do DNS lookups for your domainThe mechanism to roll over a KSK is called double signing and its also prettystraightforward However you will need to communicate your new DS record toyour parent Make sure you have positive acknowledgement from the parent beforeyou switch to just the new key Here are the steps Create a new KSK Include it in the zone file Sign the zone with both old and new KSKs and the ZSK Signal the name server to reload the zone Wait hours the TTL now everyone has the new key After confirmation delete the old KSK record from the zone Resign the zone with the new KSK and ZSKDNSSEC toolsWith the advent of BIND comes a new debugging tool The Domain EntityLookup and Validation engine DELV looks much like dig but has a better understanding of DNSSEC In fact delv checks the DNSSEC validation chain with thesame code that is used by the BIND named itselfIn addition to the DNSSEC tools that come with BIND four other deployment andtesting toolsets might be helpful ldns DNSSECTools formerly Sparta RIPE andOpenDNSSEC opendnssecorgldns tools nlnetlabsnlprojectsldnsldns from the folks at NLnet Labs is a library of routines for writing DNS tools anda set of example programs that use this library We list the tools and what each onedoes below The tools are all in the examples directory except for drill which hasits own directory in the distribution Man pages can be found with the commandsThe toplevel README file gives very brief installation instructions ldnschaos shows the name server ID info stored in the CHAOS class ldnscomparezones shows the differences between two zone files ldnsdpa analyzes DNS packets in tcpdump trace files ldnskeyds converts a DNSKEY record to a DS record ldnskeyfetcher fetches DNSSEC public keys for zones ldnskeygen generates TSIG keys and DNSSEC key pairs ldnsnotify makes a zones slave servers check for updates ldnsnsechash prints the NSEC hash for a name ldnsreadzone reads a zone and prints it in various formats ldnsrevoke sets the revoke flag on a DNSKEY key RR RFC ldnsrrsig prints humanreadable expiration dates from RRSIGs ldnssignzone signs a zone file with either NSEC or NSEC ldnsupdate sends a dynamic update packet ldnsverifyzone makes sure RRSIG NSEC and NSEC records are OK ldnswalk walks through a zone by following the DNSSEC NSEC records ldnszcat reassembles zone files split with ldnszsplit ldnszsplit splits a zone into chunks so it can be signed in parallelMany of these tools are simple and do only one tiny DNS chore They were writtenas example uses of the ldns library and demonstrate how simple the code becomeswhen the library does all the hard bits for youdnssectoolsorgDNSSectools builds on the BIND tools and includes the following commands dnspktflow traces the flow of DNS packets during a queryresponse sequence captured by tcpdump and produces a cool diagram donuts analyzes zone files and finds errors and inconsistencies donutsd runs donuts at intervals and warns of problems mapper maps zone files showing secure and insecure portions rollerd rollctl and rollinit automate key rollovers by using the prepublishing scheme for ZSKs and the double signature method for KSKs Seepage for the details of these schemes trustman manages trust anchors and includes an implementation ofRFC key rollover validate validates signatures from the commandline zonesigner generates keys and signs zonesThe web site contains good documentation and tutorials for all of these tools Thesource code is available for download and is covered by the BSD licenseRIPE tools ripenetRIPEs tools act as a front end to BINDs DNSSEC tools and focus on key management They have friendlier messages since they run and package up the manyarguments and commands into more intuitive formsOpenDNSSEC opendnssecorgOpenDNSSEC is a set of tools that takes unsigned zones adds the signatures andother records for DNSSEC and passes it on to the authoritative name servers forthat zone This automation greatly simplifies the initial setup of DNSSECDebugging DNSSECDNSSEC interoperates with both signed and unsigned zones and with both DNSSECaware and DNSSECoblivious name servers Ergo incremental deploymentis possible and it usually just works But not alwaysDNSSEC is a distributed system with lots of moving parts Servers resolvers andthe paths among them can all experience problems A problem seen locally mayoriginate far away so tools like SecSpider and Vantages that monitor the distributedstate of the system can be helpful Those tools the utilities mentioned in the previous section and your name server log files are your primary debugging weaponsMake sure that you route the DNSSEC logging category in namedconf to a file onthe local machine Its helpful to separate out the DNSSECrelated messages so thatyou dont route any other logging categories to this file Here is an example loggingspecification for namedchannel dnsseclog file varlognameddnsseclog versions size m printtime yes printcategory yes printseverity yes severity debug category dnssec dnsseclog In BIND set the debugging level to or higher to see the validation steps taken bya recursive BIND server trying to validate a signature This logging level producesabout two pages of logging output per signature verified If you are monitoring abusy server log data from multiple queries will likely be interleaved Sorting throughthe mess can be challenging and tediousdrill has two particularly useful flags T to trace the chain of trust from the rootto a specified host and S to chase the signatures from a specified host back to theroot Heres some madeup sample output from drill S snitched from the DNSSECHOWTO at NLnet Labs drill S k kskkeyfile examplenet SOADNSSEC Trust treeexamplenet SOAexamplenet DNSKEY keytag examplenet DNSKEY keytag examplenet DS keytag net DNSKEY keytag net DNSKEY keytag net DS keytag DNSKEY keytag DNSKEY keytag Chase successfulIf a validating name server cannot verify a signature it returns a SERVFAIL indication The underlying problem could be a configuration error by someone at oneof the zones in the chain of trust bogus data from an interloper or a problem inthe setup of the validating recursive server itself Try drill to chase the signaturesalong the chain of trust and see where the problem liesIf all the signatures are verified try querying the troublesome site with dig andthen with dig cd The cd flag turns off validation Try this at each of the zonesin the chain of trust to see if you can find the problem You can work your way upor down the chain of trust The likely result will be an expired trust anchor or expired signatures BIND debuggingBIND provides three basic debugging tools logging described below a controlprogram described starting on page and a commandline query tool described on page Logging in BINDnameds logging facilities are flexible enough to make your hair stand on end BINDoriginally just used syslog to report error messages and anomalies Recent versionsgeneralize the syslog concepts by adding another layer of indirection and supportfor logging directly to files Before you dive in check the miniglossary of BINDlogging terms shown in Table See Chapter for more information about syslogTable A BIND logging lexiconTerm What it meanscategory A class of messages that named can generate for example messagesabout dynamic updates or messages about answering queriesmodule The name of the source module that generates a messageseverity The badness of an error message what syslog refers to as a prioritychannel A place where messages can go syslog a file or devnull afacility A syslog facility name DNS does not have its own specific facility butyou have your pick of all the standard onesa devnull is a pseudodevice that throws away all inputYou configure BIND logging with a logging statement in namedconf You firstdefine channels the possible destinations for messages You then direct variouscategories of message to go to particular channelsWhen a message is generated it is assigned a category a module and a severity atits point of origin It is then distributed to all the channels associated with its category and module Each channel has a severity filter that tells what severity levela message must have to get through Channels that lead to syslog stamp messageswith the designated facility name Messages that go to syslog are also filtered according to the rules in etcsyslogconf Heres the outline of a logging statementlogging channeldefchanneldefcategory categoryname channelname channelname ChannelsA channeldef looks slightly different according to whether the channel is a filechannel or a syslog channel You must choose file or syslog for each channel achannel cant be both at the same timechannel channelname file path versions numvers unlimited size sizespecsyslog facilityseverity severityprintcategory yes noprintseverity yes noprinttime yes noFor a file channel numvers tells how many backup versions of a file to keep andsizespec specifies how large the file should be allowed to grow examples k m unlimited default before it is automatically rotated If you name a filechannel mylog the rotated versions are mylog mylog and so onIn the syslog case facility names the syslog facility under which to log the messageIt can be any standard facility In practice only daemon and local through localare reasonable choicesThe rest of the statements in a channeldef are optional severity can have the valuesin descending order critical error warning notice info or debug with anoptional numeric level eg severity debug The value dynamic is also recognized and matches the servers current debug levelThe various print options add or suppress message prefixes Syslog prepends thetime and reporting host to each message logged but not the severity or the category The source filename module that generated the message is also available asa print option It makes sense to enable printtime only for file channelssyslogadds its own time stamps so theres no need to duplicate themThe four channels listed in Table are predefined by default These defaultsshould be fine for most installationsTable Predefined logging channels in BINDChannel name What it doesdefaultsyslog Sends to syslog with facility daemon severity infodefaultdebug Logs to the file namedrun with severity set to dynamicdefaultstderr Sends to standard error of the named process with severity infonull Discards all messagesCategoriesCategories are determined by the programmer at the time the code is written Theyorganize log messages by topic or functionality instead of just by severity Table shows the current list of message categoriesLog messagesThe default logging configuration islogging category default defaultsyslog defaultdebug See page for a listof syslog facility namesTable BIND logging categoriesCategory What it includesclient Client requestsconfig Configuration file parsing and processingdatabase Messages about database operationsdefault Default for categories without specific logging optionsdelegationonly Queries forced to NXDOMAIN by delegationonly zonesdispatch Dispatching of incoming packets to server modulesdnssec DNSSEC messagesednsdisabled Info about broken serversgeneral Catchall for unclassified messageslameservers Servers that are supposed to be serving a zone but arent anetwork Network operationsnotify Messages about the zone changed notification protocolqueries A short log message for every query the server receives resolver DNS resolution eg recursive lookups for clientssecurity Approvedunapproved requestsunmatched Queries named cannot classify bad class no viewupdate Messages about dynamic updatesupdatesecurity Approval or denial of update requestsxferin Zone transfers that the server is receivingxferout Zone transfers that the server is sendinga Either the parent zone or the child zone could be at fault hard to tell without investigatingYou should watch the log files when you make major changes to BIND and perhapsincrease the logging level Later reconfigure to preserve only serious messages onceyou have verified that named is stableQuery logging can be quite educational You can verify that your allow clauses areworking see who is querying you identify broken clients etc Its a good check toperform after major reconfigurations especially if you have a good sense of whatyour query load looked like before the changesTo start query logging just direct the queries category to a channel Writing tosyslog is less efficient than writing directly to a file so use a file channel on a local disk when you are logging every query Have lots of disk space and be ready toturn query logging off once you obtain enough data rndc querylog dynamicallytoggles query logging on and offViews can be pesky to debug but fortunately the view that matched a particularquery is logged along with the querySome common log messages are listed below Lame server resolving xxx If you get this message about one of your ownzones you have configured something incorrectly The message is harmless if its about some zone out on the Internet its someone elses problemA good one to throw away by directing it to the null channel query cache xxx denied This can be either misconfiguration of theremote site abuse or a case in which someone has delegated a zone toyou but you have not configured it Too many timeouts resolving xxx disabling EDNS This message can resultfrom a broken firewall not admitting UDP packets over bytes long ornot admitting fragments It can also be a sign of problems at the specifiedhost Verify that the problem is not your firewall and consider redirectingthese messages to the null channel Unexpected RCODE SERVFAIL resolving xxx This can be an attack ormore likely a sign of something repeatedly querying a lame zone Bad referral This message indicates a miscommunication among a zonesname servers Not authoritative for A slave server is unable to get authoritative data fora zone Perhaps its pointing to the wrong master or perhaps the masterhad trouble loading the zone in question Rejected zone named rejected a zone file because it contained errors No NS RRs found A zone file did not include NS records after the SOA record It could be that the records are missing or it could be that they dontstart with a tab or other whitespace In the latter case the records are notattached to the zone of the SOA record and are therefore misinterpreted No default TTL set The preferred way to set the default TTL for resourcerecords is with a TTL directive at the top of the zone file This error message indicates that the TTL is missing it is required in BIND No root name server for class Your server is having trouble finding the rootname servers Check your hints file and the servers Internet connectivity Address already in use The port on which named wants to run is alreadybeing used by another process probably another copy of named If youdont see another named around it might have crashed and left an rndccontrol socket open that youll have to track down and remove A goodway to fix the problem is to stop the named process with rndc and thenrestart named sudo rndc stop sudo usrsbinnamed updating zone xxx update unsuccessful A dynamic update for a zonewas attempted but refused most likely because of the allowupdate orupdatepolicy clause in namedconf for this zone This is a common errormessage and often is caused by misconfigured Windows boxesSample BIND logging configurationThe following snippet from the ISC namedconf file for a busy TLD name serverillustrates a comprehensive logging regimenlogging channel defaultlog Default channel to a filefile lognamedlog versions size m printtime yes printcategory yes printseverity yes severity infochannel xferlog Zone transfers channel to a filefile logxferlog versions size m printcategory yes printseverity yes printtime yes severity infochannel dnsseclog DNSSEC channel to a filefile logdnsseclog versions size M severity debug printseverity yes printtime yescategory default defaultlog defaultdebug category dnssec dnsseclog category xferin xferlog category xferout xferlog category notify xferlog Debug levels in BINDnamed debug levels are denoted by integers from to The higher the numberthe more verbose the output Level turns debugging off Levels and are finefor debugging your configuration and database Levels beyond about are appropriate for the maintainers of the codeYou invoke debugging on the named command line with the d flag For example sudo named dwould start named at debug level By default debugging information is written tothe file namedrun in the current working directory from which named is startedThe namedrun file grows fast so dont go out for a beer while debugging or youmight have bigger problems when you returnYou can also turn on debugging while named is running with rndc trace whichincrements the debug level by or with rndc trace level which sets the debuglevel to the value specified rndc notrace turns debugging off completely You canalso enable debugging by defining a logging channel that includes a severity specification such asseverity debug which sends all debugging messages up to level to that particular channel Otherlines in the channel definition specify the destination of those debugging messagesThe higher the severity level the more information is loggedWatching the logs or the debugging output illustrates how often DNS is misconfigured in the real world That pesky little dot at the end of names or rather the lackthereof accounts for an alarming amount of DNS trafficName server control with rndcTable shows some of the options accepted by rndc Typing rndc with no arguments lists the available commands and briefly describes what they do Earlierincantations of rndc used signals but with over commands the BIND folks ranout of signals long ago Commands that produce files put them in whatever directory is specified as nameds home in namedconfrndc reload makes named reread its configuration file and reload zone files Thereload zone command is handy when only one zone has changed and you dontwant to reload all the zones especially on a busy server You can also specify a classand view to reload only the selected view of the zones dataNote that rndc reload is not sufficient to add a completely new zone that requiresnamed to read both the namedconf file and the new zone file For new zones userndc reconfig which rereads the config file and loads any new zones without disturbing existing zonesrndc freeze zone stops dynamic updates and reconciles the journal of dynamic updates to the data files After freezing the zone you can edit the zone data by handAs long as the zone is frozen dynamic updates are refused Once youve finishedediting use rndc thaw zone to start accepting dynamic updates againrndc dumpdb instructs named to dump its database to nameddumpdb Thedump file is big and includes not only local data but also any cached data the nameserver has accumulatedTable rndc commands aCommand Functiondumpdb Dumps the DNS database to nameddumpdbflush view Flushes all caches or those for a specified viewflushname name view Flushes the specified name from the servers cachefreeze zone class view Suspends updates to a dynamic zonethaw zone class view Resumes updates to a dynamic zonehalt Halts named without writing pending updatesquerylog Toggles tracing of incoming queriesnotify zone class view Resends notification messages for zonenotrace Turns off debuggingreconfig Reloads the config file and loads any new zonesrecursing Dumps queries currently recursing namedrecursingrefresh zone class view Schedules maintenance for a zonereload Reloads namedconf and zone filesreload zone class view Reloads only the specified zone or viewretransferzone class view Recopies the data for zone from the master serverstats Dumps statistics to namedstatsstatus Displays the current status of the running namedstop Saves pending updates and then stops namedtrace Increments the debug level by trace level Changes the debug level to the value levelvalidation newstate Enablesdisables DNSSEC validation on the flya The class argument here is the same as for resource records typically IN for InternetYour versions of named and rndc must match or you will see an error messageabout a protocol version mismatch Theyre normally installed together on individual machines but version skew can be an issue when you are trying to controla named on another computerCommandline querying for lame delegationsWhen you apply for a domain name you are asking for a part of the DNS namingtree to be delegated to your name servers and your DNS administrator If you never use the domain or you change the name servers or their IP addresses withoutcoordinating with your parent zone a lame delegation resultsThe effects of a lame delegation can be really bad If one of your servers is lame yourDNS system is less efficient If all the name servers for a domain are lame no onecan reach you All queries start at the root unless answers are cached so lame servers and lazy software that doesnt do negative caching of SERVFAIL errors increasethe load of everyone on the path from the root to the lame domainThe doc domain obscenity control command can help you identify lame delegations but you can also find them just by reviewing your log files Heres anexample log messageJul nubark named lame server resolving wwcom inwwcom Digging for name servers for wwcom at one of the com gTLD servers yields theresults below We have truncated the output to tame digs verbosity the short flagto dig limits the output even more dig egtldserversnet wwcom ns ANSWER SECTIONwwcom IN NS nsnameservicesnetwwcom IN NS nsnameservicesnetIf we query each of these servers in turn we get an answer from ns but not from ns dig nsnameservicesnet wwcom ns ANSWER SECTIONwwcom IN NS nsnameservicesnetwwcom IN NS nsnameservicesnet dig nsnameservicesnet wwcom ns QUESTION SECTIONwwcom IN NS AUTHORITY SECTIONcom IN NS MGTLDSERVERSNETcom IN NS IGTLDSERVERSNETcom IN NS EGTLDSERVERSNETThe server nsnameservicesnet has been delegated responsibility for wwcomby the com servers but it does not accept that responsibility It is misconfiguredresulting in a lame delegation Clients trying to look up wwcom will experience slow service If wwcom is paying nameservicesnet for DNS service theydeserve a refundSometimes when you dig at an authoritative server in an attempt to find lamenessdig returns no information Try the query again with the norecurse flag so thatyou can see exactly what the server in question knows Recommended readingDNS and BIND are described by a variety of sources including the documentation that comes with the distributions chapters in several books on Internet topicsbooks in the OReilly Nutshell series books from other publishers and variousonline resources Many sites point their lameservers logging channel to devnull and dont fret about other peopleslame delegations Thats fine as long as your own domain is squeaky clean and is not itself a source orvictim of lame delegationsBooks and other documentationThe Nominum and ISC BIND Development Teams BIND Administrator Reference Manual This manual is included in the BIND distribution docarm fromiscorg and is also available separately from the same site It outlines the administration and management of BIND Liu Cricket and Paul Albitz DNS and BIND th Edition Sebastopol CAOReilly Media This is pretty much the BIND bible although its getting abit long in the toothLiu Cricket DNS BIND Cookbook Sebastopol CA OReilly Media Thisbaby version of the OReilly DNS book is task oriented and gives clear instructionsand examples for various name server chores Dated but still usefulLiu Cricket DNS and BIND on IPv Sebastopol CA OReilly Media This is an IPvfocused addendum to DNS and BIND Its short and includes onlyIPvrelated materialLucas Michael W DNSSEC Mastery Securing the Domain Name System withBIND Grosse Point Woods MI Tilted Windmill Press Online resourcesThe web sites iscorg dnsoarcnet ripenet and nlnetlabsnl contain a wealth of DNSinformation research measurement results presentations and other good stuffAll the nittygritty details of the DNS protocol resource records and the like aresummarized at ianaorgassignmentsdnsparameters This document contains anice mapping from a DNS fact to the RFC that specifies itThe DNSSEC HOWTO a tutorial in disguise by Olaf Kolkman is a page document that covers the ins and outs of deploying and debugging DNSSEC Get it atnlnetlabsnldnssechowtodnssechowtopdfThe RFCsThe RFCs that define the DNS system are available from rfceditororg We formerly listed a page or so of the most important DNSrelated RFCs but there are nowso many more than with another Internet drafts that you are better offsearching rfceditororg to access the entire archiveRefer to the docrfc and docdraft directories of the current BIND distribution tosee the entire complement of DNSrelated RFCsBoth users and system administrators would like account information to magicallypropagate to all an environments computers so that a user can log in to any system with the same credentials The common term for this feature is single signonSSO and the need for it is universalSSO involves two core security concepts identity and authentication A user identity is the abstract representation of an individual who needs access to a system oran application It typically includes attributes such as a username password userID and email address Authentication is the act of proving that an individual is thelegitimate owner of an identityThis chapter focuses on SSO as a component of UNIX and Linux systems within asingle organization For interorganizational SSO such as might be needed to integrate your systems with a SoftwareasaService provider several standardsbasedand commercial SSO solutions are available For those cases we recommend learningabout Security Assertion Markup Language SAML as a first step on your journey Single SignOn Core SSO elementsAlthough there are many ways to set up SSO four elements are typically requiredin every scenario A centralized directory store that contains user identity and authorizationinformation The most common solutions are directory services basedon the Lightweight Directory Access Protocol LDAP In environmentsthat mix Windows UNIX and Linux systems the everpopular Microsoft Active Directory service is a good choice Active Directory includesa customized nonstandard LDAP interface A tool for managing user information in the directory For native LDAPimplementations we recommend phpLDAPadmin or Apache DirectoryStudio Both are easytouse webbased tools that let you import addmodify and delete directory entries If youre a Microsoft Active Directory fanperson you can use the Windowsnative MMC snapin ActiveDirectory Users and Computers to manage information in the directory A mechanism for authenticating user identities You can authenticate users directly against an LDAP store but its also common to use the Kerberos ticketbased authentication system originally developed at MIT InWindows environments Active Directory supplies LDAP access to useridentities and uses a customized version of Kerberos for authenticationAuthentication on modern UNIX and Linux systems goes through thePluggable Authentication Module system aka PAM You can use the System Security Services Daemon sssd to aggregate access to user identityand authentication services then point PAM at sssd Centralizedidentityandauthenticationaware versions of the C libraryroutines that look up user attributes These routines eg getpwent historically read flat files such as etcpasswd and etcgroup and answeredqueries from their contents These days the data sources are configuredin the name service switch file etcnsswitchconfExhibit A on the next page illustrates the highlevel relationships of the variouscomponents in a typical configuration This example uses Active Directory as thedirectory server Note that both time synchronization NTP and hostname mapping DNS are critical for environments that use Kerberos because authenticationtickets are time stamped and have a limited validity periodIn this chapter we cover core LDAP concepts and introduce two specific LDAPservers for UNIX and Linux We then discuss the steps needed to make a machineuse a centralized directory service to process logins The security community is divided over whether authentication is most secure when performedthrough LDAP or Kerberos The road of life is paved with flat squirrels that couldnt decide Pick anoption and dont look backExhibit A SSO componentssssdLDAP glueKerberos glueActive DirectoryserverpamsssntpdDNS resolverLocal system External serversAuth providerID providerPAMnsssssNTP serverDNS serverlogin gettywindow serverC librarygetpwentgetpwnam LDAP lightweight directory servicesA directory service is just a database but one that makes a few assumptions Anykind of data that matches the assumptions is a candidate for inclusion in the directory The basic assumptions are as follows Data objects are relatively small The database will be widely replicated and cached The information is attributebased Data are read often but written infrequently Searching is a common operationThe current IETF standardstrack protocol that fills this role is the LightweightDirectory Access Protocol LDAP LDAP was originally a gateway protocol thatallowed TCPIP clients to talk to an older directory service called X which isnow obsoleteMicrosofts Active Directory is the most common instantiation of LDAP and manysites use Active Directory for both Windows and UNIXLinux authentication Forenvironments in which Active Directory isnt a candidate the OpenLDAP packageopenldaporg has become the standard implementation The cleverly named Directory Server formerly known as the Fedora Directory Server and the NetscapeDirectory Server is also open source and can be found at portorgUses for LDAPUntil youve had some experience with it LDAP can be a slippery fish to grab holdof LDAP by itself doesnt solve any specific administrative problem Today the most Ironically LDAP is anything but lightweight TCP port is the default port for all LDAP implementationscommon use of LDAP is to act as a central repository for login names passwordsand other account attributes However LDAP can be used in many other ways LDAP can store additional directory information about users such asphone numbers home addresses and office locations Most mail systemsincluding sendmail Exim and Postfixcan drawa large part of their routing information from LDAP See page formore information about using LDAP with sendmail LDAP makes it easy for applications even those written by other teamsand departments to authenticate users without having to worry aboutthe exact details of account management LDAP is well supported by common scripting languages such as Perland Python through code libraries Ergo LDAP can be an elegant wayto distribute configuration information for locally written scripts andadministrative utilities LDAP is well supported as a public directory service Most major emailclients can read user directories stored in LDAP Simple LDAP searchesare also supported by many web browsers through an LDAP URL typeThe structure of LDAP dataLDAP data takes the form of property lists which are known in the LDAP worldas entries Each entry consists of a set of named attributes such as descriptionor uid along with those attributes values Every attribute can have multiple values Windows users might recognize this structure as being similar to that of theWindows registryAs an example heres a typical but simplified etcpasswd line expressed as anLDAP entrydn uidghopperouPeopledcnavydcmilobjectClass topobjectClass personobjectClass organizationalPersonobjectClass inetOrgPersonobjectClass posixAccountobjectClass shadowAccountuid ghoppercn Grace HopperuserPassword cryptpZaGARLMPDJocafuhHYykHQFploginShell binbashuidNumber gidNumber homeDirectory homeghopperThis notation is a simple example of LDIF the LDAP Data Interchange Formatwhich is used by most LDAPrelated tools and server implementations The factthat LDAP data can be easily converted back and forth from plain text is part ofthe reason for its successEntries are organized into a hierarchy through the use of distinguished names attribute name dn that form a sort of search path As in DNS the most significant bitgoes on the right In the example above the DNS name navymil has structured thetop levels of the LDAP hierarchy It has been broken down into two domain components dcs navy and mil but this is only one of several common conventionsEvery entry has exactly one distinguished name Entries are entirely separate fromone another and have no hierarchical relationship except as is implicitly defined bythe dn attributes This approach enforces uniqueness and gives the implementationa hint as to how to efficiently index and search the data Various LDAP consumersuse the virtual hierarchy defined by dn attributes but thats more a datastructuringconvention than an explicit feature of the LDAP system There are however provisions for symbolic links between entries and for referrals to other serversLDAP entries are typically schematized through the use of an objectClass attribute Object classes specify the attributes that an entry can contain some of whichmay be required for validity The schemata also assign a data type to each attributeObject classes nest and combine in the traditional objectoriented fashion The toplevel of the object class tree is the class named top which specifies merely that anentry must have an objectClass attributeTable shows some common LDAP attributes whose meanings might not beimmediately apparent These attributes are caseinsensitiveTable Some common attribute names found in LDAP hierarchiesAttribute Stands for What it iso Organization Often identifies a sites toplevel entry aou Organizational unit A logical subdivision eg marketingcn Common name The most natural name to represent the entrydc Domain component Used at sites that model their hierarchy on DNSobjectClass Object class Schema to which this entrys attributes conforma Typically not used by sites that model their LDAP hierarchy on DNSOpenLDAP the traditional open source LDAP serverOpenLDAP is an extension of work originally done at the University of Michiganit now continues as an open source project Its shipped with most Linux distributions although it is not necessarily included in the default installation The documentation is perhaps best described as briskIn the OpenLDAP distribution slapd is the standard LDAP server daemon In anenvironment with multiple OpenLDAP servers slurpd runs on the master server and handles replication by pushing changes out to slave servers A selection ofcommandline tools enable the querying and modification of LDAP dataSetup is straightforward First create an etcopenldapslapdconf file by copyingthe sample installed with the OpenLDAP server These are the lines you need topay attention todatabase bdbsuffix dcmydomain dccomrootdn cnadmin dcmydomain dccomrootpw cryptabJnggxhByWIdirectory varlibldapThe database format defaults to Berkeley DB which is fine for data that will livewithin the OpenLDAP system You can use a variety of other back ends includingad hoc methods such as scripts that create the data on the flysuffix is your LDAP basename Its the root of your portion of the LDAP namespace similar in concept to your DNS domain name In fact this example illustratesthe use of a DNS domain name as an LDAP basename which is a common practicerootdn is your administrators name and rootpw is the administrators hashed password Note that the domain components leading up to the administrators namemust also be specified You can use slappasswd to generate the value for this fieldjust copy and paste its output into the fileBecause of the presence of this password hash make sure that the slapdconf fileis owned by root and that its permissions are Edit etcopenldapldapconf to set the default server and basename for LDAPclient requests Its pretty straightforwardjust set the argument of the host entryto the hostname of your server and set the base to the same value as the suffixin the slapdconf file Make sure both lines are uncommented Heres an examplefrom atrustcomBASE dcatrustdccomURI ldapatlanticatrustcomAt this point you can start up slapd simply by running it with no arguments Directory Server alternative open source LDAP serverLike OpenLDAP the Directory Server portorg is an extension of the workdone at the University of Michigan However it spent some years in the commercialworld at Netscape before returning as an open source projectThere are several reasons to consider the Directory Server as an alternativeto OpenLDAP but its superior documentation is one clear advantage The Directory Server comes with several professional grade administration and useguides including detailed installation and deployment instructionsA few other key features of the Directory Server are Multimaster replication for fault tolerance and superior write performance Active Directory user and group synchronization A graphical console for all facets of user group and server management Online zerodowntime LDAPbased update of schema configurationmanagement and intree Access Control Information ACIs Directory Server has a much more active development community than doesOpenLDAP We generally recommend it over OpenLDAP for new installationsFrom an administrative standpoint the structure and operation of the two opensource servers are strikingly similar This fact is perhaps not too surprising sinceboth packages were built on the same original code baseLDAP QueryingTo administer LDAP you need to be able to see and manipulate the contents ofthe database The phpLDAPadmin tool mentioned earlier is one of the nicer freetools for this purpose because it gives you an intuitive pointandclick interfaceIf phpLDAPadmin isnt an option ldapsearch distributed with both OpenLDAPand Directory Server is an analogous commandline tool that produces outputin LDIF format ldapsearch is especially good for use in scripts and for debuggingenvironments in which Active Directory is acting as the LDAP serverThe following example query uses ldapsearch to look up directory informationfor every user whose cn starts with ned In this case theres only one result Themeanings of the various commandline flags are discussed below ldapsearch h atlanticatrustcom p x D cntrentcnusersdcboulderdcatrustdccom Wb cnusersdcboulderdcatrustdccom cnnedEnter LDAP Password password LDAPv base cnusersdcboulderdcatrustdccom with scope sub filter cnned requesting ALL ned Users boulderatrustcomdn cnnedcnUsersdcboulderdcatrustdccomobjectClass topobjectClass personobjectClass organizationalPersonobjectClass usercn nedsn McClaintelephoneNumber givenName NeddistinguishedName cnnedcnUsersdcboulderdcatrustdccomdisplayName Ned McClainmemberOf cnUserscnBuiltindcboulderdcatrustdccommemberOf cnEnterprise AdminscnUsersdcboulderdcatrustdccomname nedsAMAccountName neduserPrincipalName nedboulderatrustcomlastLogonTimestamp mail nedatrustcomldapsearchs h and p flags specify the host and port of the LDAP server you wantto query respectivelyYou usually need to authenticate yourself to the LDAP server In this case the xflag requests simple authentication as opposed to SASL The D flag identifies thedistinguished name of a user account that has the privileges needed to execute thequery and the W flag makes ldapsearch prompt for the corresponding passwordThe b flag tells ldapsearch where in the LDAP hierarchy to start the search Thisparameter is known as the baseDN hence the b By default ldapsearch returns allmatching entries below the baseDN You can tweak this behavior with the s flagThe last argument is a filter which is a description of what youre searching for Itdoesnt require an option flag This filter cnned returns all LDAP entries thathave a common name that starts with ned The filter is quoted to protect the starfrom shell globbingTo extract all entries below a given baseDN just use objectClass as the searchfilteror leave the filter out since this is the defaultAny arguments that follow the filter select specific attributes to return For example if you added mail givenName to the command line above ldapsearch wouldreturn only the values of matching attributesConversion of passwd and group files to LDAPIf you are moving to LDAP and your existing user and group information is storedin flat files you may want to migrate your existing data RFC defines the standard mapping from traditional UNIX data sets such as the passwd and groupfiles into the LDAP namespace Its a useful reference document for sysadminswho want to use LDAP in a UNIX environment at least in theory In practice thespecifications are a lot easier for computers to read than for humans youre betteroff looking at examplesPadl Software offers a free set of Perl scripts that migrate existing flat files or NISmaps to LDAP Its available from padlcomOSSMigrationToolshtml and thescripts are straightforward to run They can be used as filters to generate LDIF orthey can be run against a live server to upload the data directly For example themigrategroup script converts this line from etcgroupcsstaffxevimatthewtrentto the following LDIFdn cncsstaffouGroupdcdomainnamedccomcn csstaffobjectClass posixGroupobjectClass topuserPassword cryptxgidNumber memberuid evimemberuid matthewmemberuid trent Using directory services for loginOnce you have a directory service set up complete the following configurationchores so your system can enter SSO paradise If youre planning to use Active Directory with Kerberos configure Kerberos and join the system to the Active Directory domain Configure sssd to communicate with the appropriate identity and authentication stores LDAP Active Directory or Kerberos Configure the name service switch nsswitchconf to use sssd as a sourceof user group and password information Configure PAM to service authentication requests through sssdWe walk through these procedures belowKerberosKerberos is a ticketbased authentication system that uses symmetric key cryptography Its recent popularity has been driven primarily by Microsoft which uses itas part of Active Directory and Windows authentication For SSO purposes wedescribe how to integrate with an Active Directory Kerberos environment on bothLinux and FreeBSD If youre using an LDAP server other than Active Directoryor if you want to authenticate against Active Directory through the LDAP pathrather than the Kerberos path you can skip to the discussion of sssd on page Some software uses the traditional getpwent family of library routines to look up user informationwhereas modern services often directly call the PAM authentication routines Configure both PAMand nsswitchconf to ensure a fully functional environmentSee page forgeneral informationabout KerberosLinux Kerberos configuration for AD integrationSysadmins often want their Linux systems to be members of an Active Directorydomain In the past the complexity of this configuration drove some of those sysadmins to drink Fortunately the debut of realmd has made this task much simplerrealmd acts as a configuration tool for both sssd and KerberosBefore attempting to join an Active Directory domain verify the following realmd is installed on the Linux system youre joining to the domain sssd is installed see below ntpd is installed and running You know the correct name of your AD domain You have credentials for an AD account that is allowed to join systemsto the domain This action results in a Kerberos ticketgranting ticketTGT being issued to the system so that it can perform authenticationoperations going forward without access to an administrators passwordFor example if your AD domain name is ULSAHCOM and the AD account trentis allowed to join systems to the domain you can use the following command tojoin your system to the domain sudo realm join usertrent ULSAHCOMYou can then verify the result realm listulsahcom type kerberos realmname ULSAHCOM domainname ulsahcom configured kerberosmember serversoftware activedirectory clientsoftware sssd requiredpackage sssd requiredpackage adcli requiredpackage sambacommon loginformats Uulsahcom loginpolicy allowreal loginsFreeBSD Kerberos configuration for AD integrationKerberos is infamous for its complex configuration process especially on the serverside Unfortunately FreeBSD has no slick tool akin to Linuxs realmd that configures Kerberos and joins an Active Directory domain in one step However you needto set up only the client side of Kerberos The configuration file is etckrbconfFirst doublecheck that the systems fully qualified domain name has been included in etchosts and that NTP is configured and working Then edit krbconf toadd the realm as shown in the following example Substitute the name of your sitesAD domain for ULSAHCOMloggingdefault FILEvarlogkrbloglibdefaultsclockskew defaultrealm ULSAHCOMkdctimesync ccachetype forwardable trueproxiable truerealmsULSAHCOM kdc dculsahcom adminserver dculsahcom defaultdomain ULSAHdomainrealmulsahcom ULSAHCOMulsahcom ULSAHCOMSeveral values are of interest in the example above A minute clock skew is allowed even though the time is set through NTP This leeway allows the system tofunction even in the event of an NTP problem The default realm is set to the ADdomain and the key distribution center or KDC is configured as an AD domaincontroller krblog might come in handy for debuggingRequest a ticket from the Active Directory controller by running the kinit command Specify a valid domain user account The administrator account is usuallya good test but any account will do When prompted type the domain password kinit administratorULSAHCOMPassword for administratorULSAHCOM passwordUse klist to show the Kerberos ticket klistTicket cache FILEtmpkrbccDefault principal administratorULSAHCOMValid starting Expires Service principal krbtgtULSAHCOMULSAHCOM renew until Kerberos ticket cache tmptktklist You have no tickets cachedIf a ticket is displayed authentication was successful In this case the ticket is validfor hours and can be renewed for hours You can use the kdestroy commandto invalidate the ticketThe last step is to join the system to the domain as shown below The administratoraccount used in this case trent must have the appropriate privileges on the ActiveDirectory server to join systems to the domain net ads join U trentEnter trents password passwordUsing short domain ULSAHJoined exampleulsahcom to domain ULSAHCOMSee the man page for krbconf for additional configuration optionssssd the System Security Services DaemonThe UNIX and Linux road to SSO nirvana has been a rough one Years ago it wascommon to set up independent authentication for every service or application Thisapproach often resulted in a morass of separate configurations and undocumenteddependencies that were impossible to manage over time Users passwords wouldwork with one application but not another causing frustration for everyoneMicrosoft formerly published extensions originally called Services for UNIXthen Windows Security and Directory Services for UNIX and finally IdentityManagement for UNIX in Windows Server that facilitated the housing ofUNIX users and groups within Active Directory Putting the authority for managingthese attributes in a nonUNIX system was an unnatural fit however To the reliefof many Microsoft discontinued this feature as of Windows Server These issues needed some kind of comprehensive solution and thats just what wegot with sssd the System Security Services Daemon Available for both Linux andFreeBSD sssd is a onestop shop for user identity wrangling authentication andaccount mapping It can also cache credentials offline which is useful for mobile devices sssd supports authentication both through native LDAP and through KerberosYou configure sssd through the sssdconf file Heres a basic example for an environment that uses Active Directory as the directory servicesssdservices nss pamdomains ULSAHCOMdomainULSAHCOMidprovider adaccessprovider ad If you are using a nonAD LDAP server your sssdconf file might look more like thissssdservices nss pamdomains LDAPdomainLDAPidprovider ldapauthprovider ldapldapuri ldapldapulsahcomldapusersearchbase dculsahdccomtlsreqcert demandldaptlscacert etcpkitlscertscabundlecrtFor obvious security reasons sssd does not allow authentication over an unencrypted channel so the use of LDAPSTLS is required Setting the tlsreqcert attributeto demand in the example above forces sssd to validate the server certificate as anadditional check sssd drops the connection if the certificate is found to be deficientOnce sssd is up and running you must tell the system to use it as the source foridentity and authentication information Configuring the name service switch andconfiguring PAM are the next steps in this processnsswitchconf the name service switchThe name service switch NSS was developed to ease selection among various configuration databases and name resolution mechanisms All the configuration goesinto the etcnsswitchconf fileThe syntax is simple for a given type of lookup you simply list the sources in theorder they should be consulted The systems local passwd and group files shouldalways be consulted first specified by files but you can then punt to Active Directory or another directory service by way of sssd specified by sss These entriesdo the trickpasswd files sssgroup files sssshadow files sssOnce youve configured the nsswitchconf file you can test the configuration withthe command getent passwd This command prints the user accounts defined byall sources in etcpasswd format getent passwdrootxrootrootbinbashdaemonxdaemonusrsbinbinshbwhaleyxhomebwhaleybinshguestGuesthomeULSAHguestbinbashbenBen WhaleyhomeULSAHbenbinbashkrbtgtkrbtgthomeULSAHkrbtgtbinbashThe only way to distinguish local users from domain accounts is by user ID and bythe path of the home directory as seen in the last three entries abovePAM cooking spray or authentication wonderPAM stands for pluggable authentication modules The PAM system relieves programmers of the chore of implementing direct connections to authentication systems and gives sysadmins flexible modular control over the systems authenticationmethods Both the concept and the term come from Sun Microsystems now partof Oracle sniff and from a paper by Samar and Lai of SunSoftIn the distant past commands like login included hardwired authentication codethat prompted the user for a password tested the password against the encrypted version obtained from etcshadow etcpasswd at that time and rendered ajudgment as to whether the two passwords matched Of course other commandseg passwd contained similar code It was impossible to change authenticationmethods without source code and administrators had little or no control overdetails such as whether the system should accept password as a valid passwordPAM changed all thatPAM puts the systems authentication routines into a shared library that login andother programs can call By separating authentication functions into a discretesubsystem PAM makes it easy to integrate new advances in authentication and encryption into the computing environment For instance multifactor authenticationis supported without changes to the source code of login and passwdFor the sysadmin setting the right level of security for authentication has becomea simple configuration task Programmers win too since they no longer have towrite tedious authentication code More importantly their authentication systemsare implemented correctly on the first try PAM can authenticate all sorts of activities user logins other forms of system access use of protected web sites even theconfiguration of applicationsPAM configurationPAM configuration files are a series of oneliners each of which names a particularPAM module to be used on the system The general format ismoduletype controlflag modulepath arguments Fields are separated by whitespaceThe order in which modules appear in the PAM configuration file is important Forexample the module that prompts the user for a password must come before themodule that checks that password for validity One module can pass its output tothe next by setting either environment variables or PAM variablesThe moduletype parameterauth account session or passworddetermineswhat the module is expected to do auth modules identify the user and grant groupmemberships Modules that do account chores enforce restrictions such as limiting logins to particular times of day limiting the number of simultaneous usersor limiting the ports on which logins can occur For example you would use anaccounttype module to restrict root logins to the console session chores includetasks that are done before or after a user is granted access for example mountingthe users home directory Finally password modules change a users password orpassphraseThe controlflag specifies how the modules in the stack should interact to produce anultimate result for the stack Table on the next page shows the common valuesTable PAM control flagsFlagStop onfailureStop onsuccess Commentsinclude Includes another file at this point in the stackoptional No No Significant only if this is the lone modulerequired No No Failure eventually causes the stack to failrequisite Yes No Same as required but fails stack immediatelysufficient No Yes The name is kind of a lie see comments belowIf PAM could simply return a failure code as soon as the first individual module ina stack failed the controlflags system would be simpler Unfortunately the systemis designed so that most modules get a chance to run regardless of their siblingmodules success or failure and this fact causes some subtleties in the flow of control The intent is to prevent an attacker from learning which module in the PAMstack caused the failurerequired modules are required to succeed a failure of any one of them guaranteesthat the stack as a whole will eventually fail However the failure of a module thatis marked required doesnt immediately stop execution of the stack If you wantthat behavior use the requisite control flag instead of requiredThe success of a sufficient module aborts the stack immediately However the ultimate result of the stack isnt guaranteed to be a success because sufficient modulescant override the failure of earlier required modules If an earlier required module has already failed a successful sufficient module aborts the stack and returnsfailure as the overall resultBefore you modify your systems security settings make sure you understand thesystem thoroughly and that you doublecheck the particulars You wont configure PAM every day How long will you remember which version is requisite andwhich is requiredPAM exampleAn example etcpamdlogin file from a Linux system running sssd is reproducedbelow We expanded the included files to form a more coherent exampleauth requisite pamnologinsoauth userunknownignore successok ignoreignore autherrdie defaultbad pamsecurettysoauth required pamenvsoauth sufficient pamunixsoauth sufficient pamsssso usefirstpassaccount required pamunixsoaccount defaultbad successok userunknownignore pamssssopassword requisite pampwcheckso nullok cracklibpassword required pamunixso useauthtok nullokpassword sufficient pamsssso useauthtoksession required pamloginuidsosession required pamlimitssosession required pamunixsosession sufficient pamssssosession optional pamumasksosession required pamlastlogso nowtmpsession optional pammailso standardsession optional pamckconnectorsoThe auth stack includes several modules On the first line the pamnologin modulechecks for the existence of the etcnologin file If it exists the module aborts thelogin immediately unless the user is root The pamsecuretty module ensures thatroot can log in only on terminals listed in etcsecuretty This line uses an alternative Linux syntax described in the pamconf man page In this case the requestedbehavior is similar to that of the required control flag pamenv sets environmentvariables from etcsecuritypamenvconf then pamunix checks the users credentials by performing standard UNIX authentication If the user doesnt have alocal UNIX account pamsss attempts authentication through sssd If any of thesemodules fail the auth stack returns an errorThe account stack includes only the pamunix and pamsss modules In thiscontext they assess the validity of the account itself The modules return an errorif for example the account has expired or the password must be changed In thelatter case the relevant module collects a new password from the user and passesit on to the password modulesThe pampwcheck line checks the strength of proposed new passwords by callingthe cracklib library It returns an error if a new password does not meet the requirements However it also allows empty passwords because of the nullok flagThe pamunix and pamsss lines update the actual passwordFinally the session modules perform several housekeeping chores pamloginuidsets the kernels loginuid process attribute to the users UID pamlimits readsresource usage limits from etcsecuritylimitsconf and sets the correspondingprocess parameters that enforce them pamunix and pamsss log the users accessto the system and pamumask sets an initial file creation mode The pamlastlogmodule displays the users last login time as a security check and the pammailmodule prints a note if the user has new mail Finally pamckconnector notifiesthe ConsoleKit daemon a systemwide daemon that manages login sessions ofthe new loginAt the end of the process the user has been successfully authenticated and PAMreturns control to login Alternative approachesAlthough LDAP is currently the most popular method for centralizing user identity and authentication information within an organization many other approacheshave emerged over the decades Two older options NIS and rsync are still in usein some isolated pocketsNIS the Network Information ServiceNIS released by Sun in the s was the first prime time administrative database It was originally called the Sun Yellow Pages but eventually had to be renamed for legal reasons NIS commands still begin with the letters yp so its hardto forget the original name NIS was widely adopted and is still supported in bothFreeBSD and LinuxThese days however NIS is an old gray mare NIS should not be used for new deployments and existing deployments should be migrated to a modern day alternative such as LDAPrsync transfer files securelyrsync written by Andrew Tridgell and Paul Mackerras is a bit like a soupedupversion of scp that is scrupulous about preserving links modification times andpermissions It is network efficient because it looks inside individual files and attempts to transmit only the differences between versionsOne quickanddirty approach to distributing files such as etcpasswd and etcgroupis to set up a cron job to rsync them from a master server Although this scheme iseasy to set up and might be useful in a pinch it requires that all changes be applieddirectly to the master including user password changesAs an example the command rsync gopt e ssh etcpasswd etcshadow lollipopetctransfers the etcpasswd and etcshadow files to the machine lollipop The goptoptions preserve the permissions ownerships and modification times of the filersync uses ssh as the transport and so the connection is encrypted However sshdon lollipop must be configured not to require a password if you want to run thiscommand from a script Of course such a setup has significant security implications Coder bewareWith the include and exclude flags you can specify a list of regular expressionsto match against filenames so you can set up a sophisticated set of transfer criteriaIf the command line gets too unwieldy you can read the patterns from separatefiles with the includefile and excludefile optionsConfiguration management tools such as Ansible are another common way todistribute files among systems See Chapter Configuration Management formore details Recommended readingA good general introduction to LDAP is LDAP for Rocket Scientists which coversLDAP architecture and protocol Find it online at zytraxcombooksldap Anothergood source of information is the LDAPrelated RFCs which are numerous andvaried As a group they tend to convey an impression of great complexity whichis somewhat unrepresentative of average use Table list some of the most important of these RFCsTable Important LDAPrelated RFCsRFC Title An Approach for Using LDAP as a Network Information Service Access Control Requirements for LDAP LDAP Data Interchange Format LDIFTechnical Specification LDAP Authentication Password Schema Subentries in the Lightweight Directory Access Protocol LDAP LDAP The Protocol LDAP Directory Information Models LDAP Authentication Methods and Security Mechanisms LDAP String Representation of Distinguished Names LDAP String Representation of Search Filters LDAP Uniform Resource Locator LDAP Syntaxes and Matching Rules LDAP Schema for User ApplicationsIn addition there are a couple of oldiebutgoodie books on LDAPCarter Gerald LDAP System Administration Sebastopol CA OReillyMedia Voglmaier Reinhard The ABCs of LDAP How to Install Run and AdministerLDAP Services Boca Raton FL Auerbach Publications Theres also a decent book focused entirely on PAMLucas Michael PAM Mastery North Charleston SC CreateSpace Finally the OReilly book on Active Directory is excellentDesmond Brian Joe Richards Robbie Allen and Alistair G LoweNorrisActiveDirectory Designing Deploying and Running Active Directory Sebastopol CA OReillyMedia Decades ago cooking a chicken dinner involved not just frying the chicken butselecting a tender young chicken out of the coop terminating it with a kill signalplucking the feathers etc Today most of us just buy a package of chicken at thegrocery store or butcher shop and skip the messEmail has evolved in a similar way Ages ago it was common for organizations tohandcraft their email infrastructure sometimes to the point of predeterminingexact mail routing Today many organizations use packaged cloudhosted emailservices such as Google Gmail or Microsoft Office Even if your email system runs in the cloud you will still have occasion to understand support and interact with it as an administrator If your site uses local emailservers the workload expands even further to include configuration monitoringand testing choresIf you find yourself in one of these more handson scenarios this chapter is for youOtherwise skip this material and spend your email administration time respondingto messages from wealthy foreigners who need help moving millions of dollars inexchange for a large reward Just kidding of course Electronic Mail Mail system architectureA mail system consists of several distinct components A mail user agent MUA or UA that lets users read and compose mail A mail submission agent MSA that accepts outgoing mail from anMUA grooms it and submits it to the transport system A mail transport agent MTA that routes messages among machines A delivery agent DA that places messages in a local message store An optional access agent AA that connects the user agent to the message store eg through the IMAP or POP protocolNote that these functional divisions are somewhat abstract Realworld mail systems break out these roles into somewhat different packagesAttached to some of these functions are tools for recognizing spam viruses andoutbound internal company secrets Exhibit A illustrates how the various piecesfit together as a message winds its way from sender to receiverExhibit A Mail system componentsHost A sender Host B receiver User agent Submission agent Transport agent Delivery agent Access agentUAMSAMTADAAAThunderbirdUAsendmailEximMS ExchangePostxport MTAsendmailEximMS ExchangePostxport MTAlocalmaillocalport DAto local user agentsUW imapdAACyrusAAport MSAInternetMS OutlookUAmacOS MailUAAlpineUAbinmailUAUser agentsEmail users run a user agent sometimes called an email client to read and compose messages Email messages originally consisted only of text but a standardknown as Multipurpose Internet Mail Extensions MIME now encodes text formats and attachments including viruses into email It is supported by most useragents Since MIME generally does not affect the addressing or transport of mailwe do not discuss it further The receiving users mailboxes or sometimes a databasebinmail was the original user agent and it remains the good ol standby forreading text email messages at a shell prompt Since email on the Internet hasmoved far beyond the text era textbased user agents are no longer practical formost users But we shouldnt throw binmail away its still a handy interface forscripts and other programsOne of the elegant features illustrated in Exhibit A is that a user agent doesnt necessarily need to be running on the same systemor even on the same platformasthe rest of your mail system Users can reach their email from a Windows laptop orsmartphone through access agent protocols such as IMAP and POPSubmission agentsMSAs a late addition to the email pantheon were invented to offload some of thecomputational tasks of MTAs MSAs make it easy for mail hub servers to distinguish incoming from outbound email when making decisions about allowing relaying for example and give user agents a uniform and simple configuration foroutbound mailThe MSA is a sort of receptionist for new messages being injected into the systemby local user agents An MSA sits between the user agent and the transport agentand takes over several functions that were formerly a part of the MTAs job AnMSA implements secure encrypted and authenticated communication with useragents and often does minor header rewriting and cleanup on incoming messagesIn many cases the MSA is really just the MTA listening on a different port with adifferent configuration appliedMSAs speak the same mail transfer protocol used by MTAs so they appear to beMTAs from the perspective of user agents However they typically listen for connections on port rather than port the MTA standard For this scheme towork user agents must connect on port instead of port If your user agentscannot be taught to use port you can still run an MSA on port but youmust do so on a system other than the one that runs your MTA only one processat a time can listen on a particular portIf you use an MSA be sure to configure your transport agent so that it doesntduplicate any of the rewriting or header fixup work done by the MSA Duplicateprocessing wont affect the correctness of mail handling but it does represent useless extra workSince your MSA uses your MTA to relay messages the MSA and MTA must useSMTPAUTH to authenticate each other Otherwise you create a socalled openrelay that spammers can exploit and that other sites will blacklist you forTransport agentsA transport agent must accept mail from a user agent or submission agent understand the recipients addresses and somehow get the mail to the correct hosts forSee page for moreinformation aboutSMTP authenticationdelivery Transport agents speak the Simple Mail Transport Protocol SMTP whichwas originally defined in RFC but has now been superseded and extended byRFC The extended version is called ESMTPAn MTAs list of chores as both a mail sender and receiver includes Receiving email messages from remote mail servers Understanding the recipients addresses Rewriting addresses to a form understood by the delivery agent Forwarding the message to the next responsible mail server or passing itto a local delivery agent to be saved to a users mailboxThe bulk of the work involved in setting up a mail system relates to the configuration of the MTA In this book we cover three open source MTAs sendmail Eximand PostfixLocal delivery agentsA delivery agent sometimes called a local delivery agent LDA accepts mail from atransport agent and delivers it to the appropriate recipients mailboxes on the localmachine As originally specified email can be delivered to a person to a mailinglist to a file or even to a program However the last two types of recipients canweaken the security and safety of your systemMTAs usually include a builtin local delivery agent for easy deliveries procmailprocmailorg and Maildrop couriermtaorgmaildrop are LDAs that can filteror sort mail before delivering it Some access agents AAs also have builtin LDAsthat do both delivery and local housekeeping choresMessage storesA message store is the final resting place of an email message once it has completedits journey across the Internet and been delivered to recipientsMail has traditionally been stored in either mbox format or Maildir format Theformer stores all mail in a single file typically varmailusername with individualmessages separated by a special From line Maildir format stores each message ina separate file A file for each message is more convenient but creates directorieswith many many small files some filesystems may not be amusedFlat files in mbox or Maildir format are still widely used but ISPs with thousandsor millions of email clients have typically migrated to other technologies for theirmessage stores usually databases Unfortunately that means that message storesare becoming more opaqueAccess agentsTwo protocols access message stores and download email messages to a local device workstation laptop smartphone etc Internet Message Access Protocolversion IMAP and Post Office Protocol version POP Earlier versions ofthese protocols had security issues Be sure to use a version IMAPS or POPSthat incorporates SSL encryption and hence does not transmit passwords in cleartext over the InternetIMAP is significantly better than POP It delivers your mail one message at a timerather than all at once which is kinder to the network especially on slow linksand better for someone who travels from location to location IMAP is especiallygood at dealing with the giant attachments that some folks like to send you canbrowse the headers of your messages and not download the attachments until youare ready to deal with them Anatomy of a mail messageA mail message has three distinct parts Envelope Headers Body of the messageThe envelope determines where the message will be delivered or if the message cantbe delivered to whom it should be returned The envelope is invisible to users andis not part of the message itself its used internally by the MTAEnvelope addresses generally agree with the From and To lines of the header whenthe sender and recipient are individuals The envelope and headers might not agreeif the message was sent to a mailing list or was generated by a spammer who istrying to conceal his identityHeaders are a collection of propertyvalue pairs as specified in RFC updated by RFC They record all kinds of information about the message such asthe date and time it was sent the transport agents through which it passed on itsjourney and who it is to and from The headers are a bona fide part of the mailmessage but user agents typically hide the less interesting ones when displayingmessages for the userThe body of the message is the content to be sent It usually consists of plain textalthough that text often represents a mailsafe encoding for various types of binaryor richtext contentDissecting mail headers to locate problems within the mail system is an essentialsysadmin skill Many user agents hide the headers but there is usually a way to seethem even if you have to use an editor on the message storeBelow are most of the headers with occasional truncations indicated by froma typical nonspam message We removed another half page of headers that Gmailuses as part of its spam filteringDeliveredTo sailingevigmailcomReceived by with SMTP id Fri May PDTReceived by with SMTP id Fri May PDTReturnPath davidschweikertchReceived from mailrelayatrustcommailrelayatrustcom by mxgooglecom withESMTP id sipxi Fri May PDTReceivedSPF fail googlecom domain of davidschweikertch does notdesignate as permitted sender clientipAuthenticationResults mxgooglecom spfhardfail googlecom domainof davidschweikertch does not designate as permittedsender smtpmaildavidschweikertchReceived from mailschweikertch nigelschweikertch by mailrelayatrustcom with ESMTP id nGFEDKAfor eviatrustcom Fri May Received from localhost localhostlocaldomain by mailschweikertch Postfix with ESMTP id DA Fri May CESTXVirusScanned Debian amavisdnew at mailschweikertchReceived from mailschweikertch by localhost mailschweikertch amavisdnew port with ESMTP iddVBpTrhJKC Fri May CESTReceived by mailschweikertch Postfix from userid id ADB Fri May CESTDate Fri May From David Schweikert davidschweikertchTo eviatrustcomCc Garth Snyder garthgarthsnydercomSubject Email chapter commentsHi eviI just finished reading the email chapter draft and I was pleased to seeTo decode this beast start reading the Received lines but start from the bottomsender side This message went from David Schweikerts home machine in theschweikertch domain to his mail server mailschweikertch where it was scannedfor viruses It was then forwarded to the recipient eviatrustcom However thereceiving host mailrelayatrustcom sent it on to sailingevigmailcom where itentered Evis mailboxMidway through the headers you see an SPF Sender Policy Framework validation failure an indication that the message has been flagged as spam This failurehappened because Google checked the IP address of mailrelayatrustcom andcompared it with the SPF record at schweikertch of course it doesnt match This In memory of Evi who originally owned this chapter this historical example has been kept intactSee page for moreinformation about SPFis an inherent weakness of relying on SPF records to identify forgeriesthey dontwork for mail that has been relayedYou can often see the MTAs that were used Postfix at schweikertch sendmail at atrustcom and in this case you can also see that virus scanning was performedthrough amavisdnew on port on a machine running Debian Linux You canfollow the progress of the message from the Central European Summer Time zoneCEST to Colorado and on to the Gmail server PDT thenumbers are the differences between local time and UTC Coordinated UniversalTime A lot of info is stashed in the headersHere are the headers again truncated from a spam messageDeliveredTo sailingevigmailcomReceived by with SMTP id Fri Oct Received by with SMTP id Fri Oct ReturnPath smotheringlshermandpuaReceived from mailrelayatrustcom mailrelayatrustcom ReceivedSPF neutral googlecom is neitherpermitted nor denied by best guess record for domain ofsmotheringlshermandpua clientipAuthenticationResults mxgooglecom spfneutral googlecom is neither permitted nor denied by bestguess record for domain of smotheringlshermandpuasmtpmailsmotheringlshermandpuaReceived from SpeedTouchlan dsltelespnetbr may be forged by mailrelayatrustcom Received from by relaytriflenet Fri Oct From alertatrustcom alertatrustcomTo nedatrustcomSubject A new settings file for the nedatrustcom mailboxDate Fri Oct According to the From header this messages sender is alertatrustcom But according to the ReturnPath header which contains a copy of the envelope senderthe originator was smotheringlshermandpua an address in the Ukraine Thefirst MTA that handled the message is at IP address which is inBrazil Sneaky spammersThe SPF check at Google fails again this time with a neutral result because thedomain shermandpua does not have an SPF record with which to compare the IPaddress of mailrelayatrustcom Its important to note that many of the lines in the header including the Received lines may havebeen forged Use this data with extreme cautionThe recipient information is also at least partially untrue The To header says themessage is addressed to nedatrustcom However the envelope recipient addresses must have included eviatrustcom in order for the message to be forwarded tosailingevigmailcom for delivery The SMTP protocolThe Simple Mail Transport Protocol SMTP and its extended version ESMTPhave been standardized in the RFC series RFC updated by RFC andare used for most message handoffs among the various pieces of the mail system UAtoMSA or MTA as a message is injected into the mail system MSAtoMTA as the message starts its delivery journey MTA or MSAtoantivirus or antispam scanning programs MTAtoMTA as a message is forwarded from one site to another MTAtoDA as a message is delivered to the local message storeBecause the format of messages and the transfer protocol are both standardized myMTA and your MTA dont have to be the same or even know each others identitythey just have to both speak SMTP or ESMTP Your various mail servers can rundifferent MTAs and interoperate just fineTrue to its name SMTP issimple An MTA connects to your mail server and saysHeres a message please deliver it to useryourdomain Your MTA says OKRequiring strict adherence to the SMTP protocol has become a technique for fighting spam and malware so its important for mail administrators to be somewhat familiar with the protocol The language has only a few commands Table showsthe most important onesTable SMTP commandsCommand FunctionHELO hostname Identifies the connecting host if speaking SMTPEHLO hostname Identifies the connecting host if speaking ESMTPMAIL FROM revpath Initiates a mail transaction envelope senderRCPT TO fwdpath a Identifies envelope recipientsVRFY address Verifies that address is valid deliverableEXPN address Shows expansion of aliases and forward mappingsDATA Begins the message body preceded by headers bQUIT Ends the exchange and closes the connectionRSET Resets the state of the connectionHELP Prints a summary of SMTP commandsa There can be multiple RCPT commands for a messageb You terminate the body by entering a dot on its own lineYou had me at EHLOESMTP speakers start conversations with EHLO instead of HELO If the processat the other end understands and responds with an OK then the participants negotiate supported extensions and agree on a lowest common denominator for theexchange If the peer returns an error in response to the EHLO then the ESMTPspeaker falls back to SMTP But today almost everything uses ESMTPA typical SMTP conversation to deliver an email message goes as follows HELOor EHLO MAIL FROM RCPT TO DATA and QUIT The sender does mostof the talking with the recipient contributing error codes and acknowledgmentsSMTP and ESMTP are both textbased protocols so you can use them directlywhen debugging the mail system Just telnet to TCP port or and start entering SMTP commands See the example on page SMTP error codesAlso specified in the RFCs that define SMTP are a set of temporary and permanenterror codes These were originally threedigit codes eg with each digit beinginterpreted separately A first digit of indicated success a signified a temporaryerror and a indicated a permanent errorThe threedigit error code system did not scale so RFC updated by RFCs and restructured it to create more flexibility It defined anexpanded error code format known as a delivery status notification or DSN DSNshave the format XXX instead of the old XXX and each of the individual Xs canbe a multidigit number The initial X must still be or The second digit specifies a topic and the third provides the details The new system uses the secondnumber to distinguish host errors from mailbox errors Table lists a few of theDSN codes RFCs Appendix A shows them allSMTP authenticationRFC updated by RFC defines an extension to the original SMTP protocolthat allows an SMTP client to identify and authenticate itself to a mail server Theserver might then let the client relay mail through it The protocol supports severaldifferent authentication mechanisms The exchange is as follows The client says EHLO announcing that it speaks ESMTP The server responds and advertises its authentication mechanisms The client says AUTH and names a specific mechanism that it wants touse optionally including its authentication data The server accepts the data sent with AUTH or starts a challenge andresponse sequence with the client The server either accepts or denies the authentication attemptTable RFC delivery status notificationsTemporary Permanent Meaning Mailbox is disabled Mailbox is full Message is too long No answer from host Unable to route Too many recipients Delivery not authorized message refused Site policy violationTo see what authentication mechanisms a server supports you can telnet to port and say EHLO For example here is a truncated conversation with the mail servermailrelayatrustcom the commands we typed are in bold telnet mailrelayatrustcom Trying Connected to mailrelayatrustcomEscape character is mailrelayatrustcom ESMTP AT Mail Service Mon Sep ehlo booklabatrustcommailrelayatrustcom Hello pleased to meet youENHANCEDSTATUSCODESPIPELININGBITMIMESIZEDSNETRNAUTH LOGIN PLAINDELIVERBY HELPIn this case the mail server supports the LOGIN and PLAIN authentication mechanisms sendmail Exim and Postfix all support SMTP authentication details ofconfiguration are covered on pages and respectively Spam and malwareSpam is the jargon word for junk mail also known as unsolicited commercial emailor UCE It is one of the most universally hated aspects of the Internet Once upona time system administrators spent many hours each week handtuning block listsand adjusting decision weights in homegrown spam filtering tools Unfortunatelyspammers have become so crafty and commercialized that these measures are nolonger an effective use of system administrators timeIn this section we cover the basic antispam features of each MTA However theresa certain futility to any attempt to fight spam as a lone vigilante You should reallypay for a cloudbased spamfighting service such as McAfee SaaS Email ProtectionGoogle G Suite or Barracuda and leave the spam fighting to the professionals wholove that stuff They have better intelligence about the state of the global emailsphereand can react far more quickly to new information than you canSpam has become a serious problem because although the absolute response rateis low the responses per dollar spent is high A list of million email addressescosts about If it didnt work for the spammers it wouldnt be such a problemSurveys show that of all mail is spamThere are even venturecapitalfunded companies whose entire mission is to deliver spam less expensively and more efficiently although they typically call it marketing email rather than spam If you work at or buy services from one of thesecompanies were not sure how you sleep at nightIn all cases advise your users to simply delete the spam they receive Many spammessages contain instructions that purport to explain how recipients can be removedfrom the mailing list If you follow those instructions however the spammers mayremove you from the current list but they immediately add you to several otherlists with the annotation reaches a real human who reads the message Your emailaddress is then worth even moreForgeriesForging email is trivial many user agents let you fill in the senders address withanything you want MTAs can use SMTP authentication between local servers butthat doesnt scale to Internet sizes Some MTAs add warning headers to outgoinglocal messages that they think might be forgedAny user can be impersonated in mail messages Be careful if email is your organizations authorization vehicle for things like door keys access cards and moneyThe practice of targeting users with forged email is commonly called phishingYou should warn administrative users of this fact and suggest that if they see suspicious mail that appears to come from a person in authority they should verifythe validity of the message Caution is doubly appropriate if the message asks thatunreasonable privileges be given to an unusual personSPF and Sender IDThe best way to fight spam is to stop it at its source This sounds simple and easy butin reality its almost an impossible challenge The structure of the Internet makesit difficult to track the real source of a message and to verify its authenticity Thecommunity needs a surefire way to verify that the entity sending an email is actually who or what it claims to be Many proposals have addressed this problem butSPF and Sender ID have achieved the most tractionSPF or Sender Policy Framework has been described by the IETF in RFCSPF defines a set of DNS records through which an organization can identify itsofficial outbound mail servers MTAs can then refuse email purporting to be fromthat organizations domain if the email does not originate from one of these official sources Of course the system only works well if the majority of organizationspublish SPF recordsSender ID and SPF are virtually identical in form and function However key partsof Sender ID are patented by Microsoft and hence it has been the subject of muchcontroversy As of this writing Microsoft is still trying to strongarm theindustry into adopting its proprietary standards The IETF chose not to chooseand published RFC on Sender ID and RFC on SPF Organizations thatimplement this type of spam avoidance strategy typically use SPFMessages that are relayed break both SPF and Sender ID which is a serious flaw inthese systems The receiver consults the SPF record for the original sender to discover its list of authorized servers However those addresses wont match any relaymachines that were involved in transporting the message Be careful what decisionsyou make in response to SPF failuresDKIMDKIM DomainKeys Identified Mail is a cryptographic signature system for emailmessages It lets the receiver verify not only the senders identity but also the factthat a message has not been tampered with in transit The system uses DNS records to publish a domains cryptographic keys and messagesigning policy DKIMis supported by all the MTAs described in this chapter but realworld deploymenthas been extremely rare Message privacy and encryptionBy default all mail is sent unencrypted Educate your users that they should neversend sensitive data through email unless they make use of an external encryptionpackage or your organization has provided a centralized encryption solution foremail Even with encryption electronic communication can never be guaranteedto be secure You pays your money and you takes your chancesHistorically the most common external encryption packages have been Pretty GoodPrivacy PGP its GNUified clone GPG and SMIME Both SMIME and PGP aredocumented in the RFC series with SMIME being on the standards track Mostcommon user agents support plugins for both solutions Computer security expert Donald J Trump I dont believe in it email because I think it can behacked for one thing But when I send an emailif I send oneI send one almost never Im just nota believer in email Wise wordsThese standards offer a basis for email confidentiality authentication messageintegrity assurance and nonrepudiation of origin But although PGPGPG andSMIME are potentially viable solutions for techsavvy users who care about privacy they have proved too cumbersome for unsophisticated users Both requiresome facility with cryptographic key management and an understanding of theunderlying encryption strategyMost organizations that handle sensitive data in email especially ones that communicate with the public such as health care institutions opt for a centralized servicethat uses proprietary technology to encrypt messages Such systems can use eitheronpremises solutions such as Ciscos IronPort that you deploy in your data center or cloudbased services such as Zix zixcorpcom that can be configured toencrypt outbound messages according to their contents or other rules Centralizedemail encryption is one category of service for which its best to use a commercialsolution rather than rolling your ownAt least in the email realm data loss prevention DLP is a kissing cousin to centralized encryption DLP systems seek to avoidor at least detectthe leakage ofproprietary information into the stream of email leaving your organization Theyscan outbound email for potentially sensitive content Suspicious messages can beflagged blocked or returned to their senders Our recommendation is that youchoose a centralized encryption platform that also includes DLP capability its oneless platform to manageIn addition to encrypting transport between MTAs its important to ensure thatuseragenttoaccessagent communication is always encrypted especially becausethis channel typically employs some form of user credentials to connect Makesure that only the secure TLSusing versions of the IMAP and POP protocols areallowed by access agents These are known as IMAPS and POPS respectively Mail aliasesAnother concept that is common to all MTAs is the use of aliases Aliases allow mailto be rerouted either by the system administrator or by individual users Aliasescan define mailing lists forward mail among machines or allow users to be referredto by more than one name Alias processing is recursive so its legal for an alias topoint to other destinations that are themselves aliasesSysadmins often use role or functional aliases eg printersexamplecom to routeemail about a particular issue to whatever person is currently handling that issueOther examples might include an alias that receives the results of a nightly securityscan or an alias for the postmaster in charge of email Pro tip If you use PGPGPG or SMIME you can increase your odds of remaining secure by ensuring that your public key or certificate is expired and replaced frequently Longterm use of a key increases the likelihood that it will be compromised without your awareness Technically aliases are configured only by sysadmins A users control of mail routing through the useof a forward file is not really aliasing but we have lumped them together hereSee page formore information about TLSThe most common method for configuring aliases is to use a simple flat file such asthe etcmailaliases file discussed later in this section This method was originallyintroduced by sendmail but Exim and Postfix support it tooMost user agents also provide some sort of aliasing feature usually called mygroups my mailing lists or something of that nature However the user agentexpands such aliases before mail ever reaches an MSA or MTA These aliases areinternal to the user agent and dont require support from the rest of the mail systemAliases can also be defined in a forwarding file in the home directory of each userusually forward These aliases which use a slightly nonstandard syntax applyto all mail delivered to that particular user Theyre often used to forward mail to adifferent account or to implement automatic Im on vacation responsesMTAs look for aliases in the global aliases file etcmailaliases or etcaliasesand then in recipients forwarding files Aliasing is applied only to messages thatthe transport agent considers to be localThe format of an entry in the aliases file islocalname recipientrecipientwhere localname is the original address to be matched against incoming messagesand the recipient list contains either recipient addresses or the names of other aliasesIndented lines are considered continuations of the preceding linesFrom mails point of view the aliases file supersedes etcpasswd so the entrydavid davidsomewhereelseeduwould prevent the local user david from ever receiving any mail Therefore administrators and adduser tools should check both the passwd file and the aliases filewhen selecting new usernamesThe aliases file should always contain an alias named postmaster that forwards mailto whoever maintains the mail system Similarly an alias for abuse is appropriatein case someone outside your organization needs to contact you regarding spamor suspicious network behavior that originates at your site An alias for automaticmessages from the MTA must also be present its usually called MailerDaemonand is often aliased to postmasterSadly the mail system is so commonly abused these days that some sites configuretheir standard contact addresses to throw mail away instead of forwarding it to ahuman user Entries such as Basic system aliases these MUST be presentmailerdaemon postmasterpostmaster devnullare common We dont recommend this practice because humans who are havingtrouble reaching your site by email do sometimes write to the postmaster addressA better paradigm might be Basic system aliases these MUST be presentmailerdaemon devnullpostmaster rootYou should redirect roots mail to your sites sysadmins or to someone who logs inevery day The bin sys daemon nobody and hostmaster accounts and any othersitespecific pseudouser accounts you set up should all have similar aliasesIn addition to a list of users aliases can refer to A file containing a list of addresses A file to which messages should be appended A command to which messages should be given as inputThese last two targets should push your What about security button because thesender of a message totally determines its content Being able to append that content to a file or deliver it as input to a command sounds pretty scary Many MTAseither disallow these alias targets or severely limit the commands and file permissions that are acceptableAliases can cause mail loops MTAs try to detect loops that would cause mail to beforwarded back and forth forever and return the errant messages to the sender Todetermine when mail is looping an MTA can count the number of Received linesin a messages header and stop forwarding it when the count reaches a preset limitusually Each visit to a new machine is called a hop in email jargon returninga message to the sender is known as bouncing it So a more typically jargonizedsummary of loop handling would be Mail bounces after hops Another wayMTAs can detect mail loops is by adding a DeliveredTo header for each host towhich a message is forwarded If an MTA finds itself wanting to send a messageto a host thats already mentioned in a DeliveredTo header it knows the messagehas traveled in a loopGetting aliases from filesThe include directive in the aliases file or a users forward file allows the list oftargets for the alias to be taken from the specified file It is a great way to let usersmanage their own local mailing lists The included file can be owned by the userand changed without involving a system administrator However such an alias canalso become a tasty and effective spam expander so dont let email from outsideyour site be directed there In this chapter we sometimes call a returned message a bounce and sometimes call it an errorWhat we really mean is that a delivery status notification DSN a specially formatted email messagehas been generated Such a notification usually means that a message was undeliverable and is therefore being returned to the senderWhen setting up a list to use include the sysadmin must enter the alias into theglobal aliases file create the included file and chown the included file to the userthat is maintaining the mailing list For example the aliases file might containsabook includeusrlocalmailulsahauthorsThe file ulsahauthors should be on a local filesystem and should be writable onlyby its owner To be complete we should also include aliases for the mailing listsowner so that errors bounces are sent to the owner of the list and not to the senderof a message addressed to the listownersabook eviMailing to filesIf the target of an alias is an absolute pathname messages are appended to the specified file The file must already exist For examplecronstatus usrlocaladmincronstatusmessagesIf the pathname includes special characters it must be enclosed in double quotesIts useful to be able to send mail to files but this feature arouses the interest of thesecurity police and is therefore restricted This syntax is only valid in the aliasesfile and in a users forward file or in a file thats interpolated into one of these fileswith the include directive A filename is not understood as a normal addressso mail addressed to etcpasswdexamplecom would bounceIf the destination file is referenced from the aliases file it must be worldwritablenot advisable setuid but not executable or owned by the MTAs default user Theidentity of the default user is set in the MTAs configuration fileIf the file is referenced in a forward file it must be owned and writable by the original message recipient who must be a valid user with an entry in the passwd fileand a valid shell thats listed in etcshells For files owned by root use mode or setuid but not executableMailing to programsAn alias can also route mail to the standard input of a program This behavior isspecified with a line such asautolog usrlocalbinautologgerIts even easier to create security holes with this feature than with mailing to a fileso once again it is only permitted in aliases forward or include files and oftenrequires the use of a restricted shellBuilding the hashed alias databaseSince entries in the aliases file are unordered it would be inefficient for the MTA tosearch this file directly Instead a hashed version is constructed with the Berkeley DBsystem Hashing significantly speeds alias lookups especially when the file gets largeThe file derived from etcmailaliases is called aliasesdb If you are running Postfixor sendmail you must rebuild the hashed database with the newaliases commandevery time you change the aliases file Exim detects changes to the aliases file automatically Save the error output if you run newaliases automaticallyyou mighthave introduced formatting errors in the aliases file Email configurationThe heart of an email system is its MTA or mail transport agent sendmail is theoriginal UNIX MTA written by Eric Allman while he was a graduate student manyyears ago Since then a host of other MTAs have been developed Some of them arecommercial products and some are open source implementations In this chapterwe cover three open source mailtransport agents sendmail Postfix by WietseVenema of IBM Research and Exim by Philip Hazel of the University of CambridgeConfiguration of the MTA can be a significant sysadmin chore Fortunately thedefault or sample configurations that ship with MTAs are often close to what theaverage site needs You need not start from scratch when configuring your MTASecuritySpace securityspacecom does a survey monthly to determine the marketshare of the various MTAs In their June survey million out of millionMTAs surveyed replied with a banner that identified the MTA software in use Table shows these results as well as the SecuritySpace results for and some values from a different surveyTable Mail transport agent market shareMTA Source Default MTA onMarket share Exim eximorg Debian Postfix postfixorg Red Hat Ubuntu Exchange microsoftcomexchange sendmail sendmailorg FreeBSD All others ea ea eaThe trend is clearly away from sendmail and toward Exim and Postfix with Microsoft dropping to almost nothing Keep in mind that this data includes only MTAsthat are directly exposed to the InternetFor each of the MTAs we cover we include details on the common areas of interest Configuration of simple clients Configuration of an Internetfacing mail server Control of both inbound and outbound mail routing Stamping of mail as coming from a central server or the domain itself Security DebuggingIf you are implementing a mail system from scratch and have no site politics or biases to deal with you may find it hard to choose an MTA sendmail is largely outof vogue with the possible exception of pure FreeBSD sites Exim is powerful andhighly configurable but suffers in complexity Postfix is simpler faster and was designed with security as a primary goal If your site or your sysadmins have a historywith a particular MTA its probably not worth switching unless you need featuresthat are not available from your old MTAsendmail configuration is covered in the next section Exim configuration beginson page and Postfix configuration on page sendmailThe sendmail distribution is available in source form from sendmailorg but itsrarely necessary to build sendmail from scratch these days If you must do so referto the toplevel INSTALL file for instructions To tweak some of the build defaultslook up sendmails assumptions in devtoolsOSyourOSname Add features byediting devtoolsSitesiteconfigmsendmail uses the m macro preprocessor not only for compilation but also forconfiguration An m configuration file is usually named hostnamemc and is thentranslated from a slightly userfriendly syntax into a totally inscrutable lowlevellanguage in the file hostnamecf which is in turn installed as etcmailsendmailcfTo see what version of sendmail is installed on your system and how it was compiled try the following commandlinux usrsbinsendmail d bt devnullVersion Compiled with DNSMAP HESIOD HESGETMAILHOST LDAPMAP LOG MAPREGEXMATCHGECOS MILTER MIMETO MIMETO NAMEDBIND NETINET NETINETNETUNIX NEWDB NIS PIPELINING SASLv SCANF SOCKETMAP STARTTLSTCPWRAPPERS USERDB USELDAPINIT SYSTEM IDENTITY after readcf short domain name w ross canonical domain name j rossatrustcom subdomain name m atrustcom node name k rossatrustcom As of October sendmail is supported and distributed by Proofpoint Inc a public companyThis command puts sendmail in address test mode bt and debug mode dbut gives it no addresses to test devnull A side effect is that sendmail tellsus its version and the compiler flags it was built with Once you know the versionnumber you can look at the sendmailorg web site to see if any known securityvulnerabilities are associated with that releaseTo find the sendmail files on your system look at the beginning of the installedetcmailsendmailcf file The comments there mention the directory in whichthe configuration was built That directory should in turn lead you to the mc filethat is the original source of the configurationMost vendors that ship sendmail include not only the binary but also the cf directory from the distribution tree which they hide somewhere among the operatingsystem files Table will help you find itTable Config directory locationsSystem DirectoryUbuntu usrsharesendmailDebian usrsharesendmailRed Hat etcmailCentOS etcmailFreeBSD etcmailThe switch fileMost systems have a service switch configuration file etcnsswitchconf thatenumerates the methods that can satisfy various standard queries such as user andhost lookups If more than one resolution method is listed for a given type of query the service switch file also determines the order in which the various methodsare consultedThe existence of the service switch is normally transparent to software Howeversendmail likes to exert finegrained control over its lookups so it currently ignoresthe system switch file and instead uses its own internal service configuration fileetcmailserviceswitchTwo fields in the switch file impact the mail system aliases and hosts The possible values for the hosts service are dns nis nisplus and files For aliases thepossible values are files nis nisplus and ldap Support for the mechanisms youuse except files must be compiled into sendmail before the service can be usedThe service switch iscovered in more detailstarting on page Starting sendmailsendmail should not be controlled by inetd or systemd so it must be explicitlystarted at boot time See Chapter Booting and System Management Daemonsfor startup detailsThe flags that sendmail is started with determine its behavior You can run it inseveral different modes selected with the b flag b stands for be or becomeand is always used with another flag that determines the role sendmail will playTable lists the legal values and also includes the A flag which selects betweenMTA and MSA behaviorTable Commandline flags for sendmails major modesFlag MeaningAc Uses the submitcf config file and acts as an MSAAm Uses the sendmailcf config file and acts as an MTAba Runs in ARPANET mode expects CRLF at the ends of linesbd Runs in daemon mode and listens for connections on port bD Runs in daemon mode but in the foreground rather than the backgroundabh Views recent connection info same as hoststatbH Purges disk copy of outdated connection info same as purgestatbi Initializes hashed aliases same as newaliasesbm Runs as a mailer delivers mail in the usual way defaultbp Prints the mail queue same as mailqbP Prints the number of entries in queues via shared memorybs Enters SMTP server mode on standard input not port bt Enters address test modebv Verifies mail addresses only doesnt send maila Use this mode for debugging so you can see the error and debugging messagesIf you are configuring a server that will accept incoming mail from the Internetrun sendmail in daemon mode bd In this mode sendmail listens on networkport and waits for work You will usually specify the q flag tooit sets theinterval at which sendmail processes the mail queue For example qm runs thequeue every thirty minutes and qh runs it every hoursendmail normally tries to deliver messages immediately saving them in the queueonly momentarily to guarantee reliability But if your host is too busy or the destination machine is unreachable sendmail queues messages and tries to send themagain later sendmail uses persistent queue runners that are usually started at boottime It does locking so multiple simultaneous queue runs are safe You can use The ports that sendmail listens on are determined by DAEMONOPTIONS port is the defaultthe queue groups configuration feature to facilitate delivery of large mailing listsand queuessendmail reads its configuration file sendmailcf only when it starts up Thereforeyou must either kill and restart sendmail or send it a HUP signal when you changethe config file sendmail creates a sendmailpid file that contains its process IDand the command that started it You should start sendmail with an absolute pathbecause it reexecs itself on receipt of the HUP signal The sendmailpid file allowsthe process to be HUPed with the command sudo kill HUP head sendmailpidThe location of the PID file is OS dependent Its usually varrunsendmailpid oretcmailsendmailpid but can be set in the config file with the confPIDFILE optiondefineconfPIDFILE varrunsendmailpidMail queuessendmail uses at least two queues varspoolmqueue when acting as an MTAon port and varspoolclientmqueue when acting as an MSA on port All messages make at least a brief stop in the queue before being sent on their wayA queued message is saved in pieces in several different files Table shows thesix possible pieces Each filename has a twoletter prefix that identifies the piecefollowed by a random ID built from sendmails process IDTable Prefixes for files in the mail queuePrefix File contentsqf The message header and control filedf The body of the messagetf A temporary version of the qf file while the qf file is being updatedTf A notice that or more failed locking attempts have occurredQf A notice that the message bounced and could not be returnedxf Temporary transcript file of error messages from mailersIf subdirectories qf df or xf exist in a queue directory then those pieces of themessage are put in the proper subdirectory The qf file contains not only the message header but also the envelope addresses the date at which the message shouldbe returned as undeliverable the messages priority in the queue and the reasonthe message is in the queue Each line begins with a singleletter code that identifies the rest of the line sendmail can use multiple queues beneath mqueue to increase performanceEach message that is queued must have a qf and df file All the other prefixes areused by sendmail during attempted delivery When a machine crashes and rebootsthe startup sequence for sendmail should delete the tf xf and Tf files from eachqueue If you are the sysadmin responsible for mail check occasionally for Qf filesin case local configuration is causing the bounces An occasional glance at the queuedirectories lets you spot problems before they become disastersThe mail queue opens up several opportunities for things to go wrong For example the filesystem can fill up avoid putting varspoolmqueue and varlog onthe same partition the queue can become clogged or orphaned mail messagescan get stuck in the queue sendmail has configuration options to help with performance on busy machinessendmail configurationsendmail is controlled by a single configuration file typically called etcmailsendmailcffor a sendmail running as an MTA or etcmailsubmitcf for a sendmail actingas an MSA The flags with which sendmail is started determine which config file ituses bm bs and bt use submitcf if it exists and all other modes use sendmailcfYou can change these names with commandline flags or config file options butit is best not toThe raw config file format was designed to be easy to parse by machines not humansThe m source mc file from which the cf file is generated is an improvement butits picky and rigid syntax isnt going to win any awards for user friendliness eitherFortunately many of the paradigms you might want to set up have already beenhammered out by others with similar needs and are supplied in the distributionas prepackaged featuressendmail configuration involves several steps Determine the role of the machine you are configuring client serverInternetfacing mail receiver etc Choose the features needed to implement that role and build an mc filefor the configuration Compile the mc file with m to produce a cf config fileWe cover the features commonly used for sitewide Internetfacing servers and forlittle desktop clients For more detailed coverage we refer you to two key pieces ofdocumentation on the care and feeding of sendmail the OReilly book sendmailby Bryan Costales et al and the file cfREADME from the distributionThe m preprocessorm originally intended as a front end for programming languages lets users writemore readable or perhaps more cryptic programs m is powerful enough to beuseful in many input transformation situations and it works nicely for sendmailconfiguration filesm macros have the formnamearg arg argnThere cannot be any space between the name and the opening parenthesis Left andright single quotes that is backticks and normal single quotes designate stringsas arguments ms quote conventions are weird since the left and right quotes aredifferent characters Quotes nest toom has some builtin macros and users can also define their own Table liststhe most common builtin macros that are used in sendmail configurationTable m macros commonly used with sendmailMacro Functiondefine Defines a macro named arg with value argdivert Manages output streamsdnl Discards characters up to and including the next newlineinclude Includes interpolates the file named argundefine Discards a previous definition of macro named argThe sendmail configuration piecesThe sendmail distribution includes a cf subdirectory beneath which are all thepieces necessary for m configuration Table on page shows the locationof the cf directory if you did not install the sendmail source but relied on yourvendor The README file found in the cf directory is sendmails configurationdocumentation The subdirectories listed in Table contain examples and snippets you can include in your own configurationThe cfcf directory contains examples of mc files In fact it contains so many examples that yours may get lost in the clutter We recommend that you keep yourown mc files separate from those in the distributed cf directory Either create anew directory named for your site cfsitename or move the cf directory aside tocfexamples and create a new cf directory If you do this copy the Makefile andBuild script over to your new directory so the instructions in the README filestill work Alternatively you can copy all your own configuration mc files to a central location rather than leaving them inside the sendmail distribution The Buildscript uses relative pathnames so youll have to modify it if you want to build a cffile from an mc file and are not in the sendmail distribution hierarchyThe files in the cfostype directory configure sendmail for each specific operatingsystem Many are predefined but if you have moved things around on your systemTable sendmail configuration subdirectoriesDirectory Contentscf Sample mc master configuration filesdomain Sample m files for various domains at Berkeleyfeature Fragments that implement various featureshack Special features of dubious value or implementationm The basic config file and other core filesmailer m files that describe common mailers delivery agentsostype OSdependent file locations and quirkssh Shell scripts used by myou might have to modify one or create a new one Copy one that is close to realityfor your system and give it a new nameThe cffeature directory is where you shop for any configuration pieces you mightneed There is a feature for just about anything that any site running sendmail hasfound usefulThe other directories beneath cf are pretty much boilerplate and do not need to betweaked or even understoodjust use themA configuration file built from a sample mc fileBefore we take off into the wilds of the various configuration macros features andoptions you might use in a sendmail configuration we shall put the cart before thehorse and devise a no frills configuration to illustrate the general process Ourexample is for a leaf node myhostexamplecom the master configuration file iscalled myhostmc Heres the complete mc filedivert basic mc file for examplecomdivertVERSIONIDIdOSTYPElinuxMAILERlocalMAILERsmtpExcept for the diversions and comments each line invokes a prepackaged macroThe first four lines are boilerplate they insert comments in the compiled file to notethe version of sendmail the directory the configuration was built in etc The OSTYPEmacro includes the ostypelinuxm file The MAILER lines allow for local deliveryto users with accounts on myhostexamplecom and for delivery to Internet sitesTo build the real configuration file just run the Build command you copied overto the new cf directory Build myhostcfFinally install myhostcf in the right spotnormally etcmailsendmailcf butsome vendors move it Favorite vendor hiding places are etc and usrlibAt a larger site you might want to create a separate m file to hold sitewide defaultsput it in the cfdomain directory Individual hosts can then include the contentsof this file with the DOMAIN macro Not every host needs a separate config file buteach group of similar hosts same architecture and same role server client etcwill probably need its own configurationThe order of the macros in the mc file is not arbitrary It should beVERSIONIDOSTYPEDOMAINFEATURElocal macro definitionsMAILEREven with sendmails easy m configuration system you still have to make severalconfiguration decisions for your site As you read about the features described below think about how they might fit into your sites organization A small site willprobably have only a hub node and leaf nodes and thus will need only two versionsof the config file A larger site might need separate hubs for incoming and outgoingmail and perhaps a separate POPIMAP serverWhatever the complexity of your site and whatever face it shows to the outside worldexposed behind a firewall or on a virtual private network for example its likelythat the cf directory contains some appropriate readymade configuration snippetsjust waiting to be customized and put to workConfiguration primitivessendmail configuration commands are case sensitive By convention the names ofpredefined macros are all caps eg OSTYPE m commands are all lower case egdefine and configurable option names usually start with lowercase conf and endwith an allcaps variable name eg confFASTSPLIT Macros usually refer to anm file called macronameargm For example the reference OSTYPElinuxcauses the file ostypelinuxm to be includedTables and databasesBefore we plunge into specific configuration primitives we must first discuss tablessometimes called maps or databases which sendmail can use to perform mailrouting or address rewriting Most are used in conjunction with the FEATURE macroA table is a cache usually a text file of routing aliasing policy or other information that is converted to a database format with the makemap command and thenused as an information source for one or more of sendmails various lookup operations Although the data usually starts as a text file data for sendmail tables cancome from DNS LDAP or other sources The use of a centralized IMAP server relieves sendmail of the chore of chasing down users and obsoletes some of its tablessendmail defines three database map types dbm legacy uses an extensible hashing algorithm dbmndbm hash uses a standard hashing scheme DB btree uses a Btree data structure DBFor most table applications in sendmail the hash database typethe defaultisthe best Use the makemap command to build the database file from a text fileyou specify the database type and the output file base name The text version of thedatabase should appear on makemaps standard input For example sudo makemap hash etcmailaccess etcmailaccessAt first glance this command looks like a mistake that would cause the input file tobe overwritten by an empty output file However makemap tacks on an appropriatesuffix so the actual output file is etcmailaccessdb and in fact no conflict occursEach time the text file is changed the database file must be rebuilt with makemapbut sendmail need not be HUPdComments can appear in the text files from which maps are produced They beginwith and continue until the end of the lineIn most circumstances the longest possible match is used for database keys Aswith any hashed data structure the order of entries in the input text file is not significant Some FEATUREs expect a database file as a parameter they default to hashas the database type and etcmailtablenamedb as the filename for the databaseGeneric macros and featuresTable on the next page lists common configuration primitives whether theyare typically used yes no maybe and a brief description of what they doOSTYPE macroAn OSTYPE file packages a variety of vendorspecific information such as the expected locations of mailrelated files paths to commands that sendmail needs flagsto mailer programs etc See cfREADME for a list of all the variables that can bedefined in an OSTYPE fileDOMAIN macroThe DOMAIN directive lets you specify sitewide generic information in one placecfdomainfilenamem and then include it in each hosts config file withDOMAINfilename So where is the OSTYPE macro itself defined In a file in the cfm directory which is magically prepended to your config file when you run the Build scriptTable sendmail generic configuration primitivesPrimitive Used a DescriptionOSTYPE Yes Includes OSspecific paths and mailer flagsDOMAIN No Includes sitespecific configuration detailsMAILER Yes Enables mailers typically smtp and localFEATURE Maybe Enables a variety of sendmail features usecwfile Yes S Lists hosts for which you accept mail redirect Maybe S Bounces mail nicely when users move alwaysadddomain Yes Fully qualifies hostnames if UA didnt accessdb Maybe S Sets database of hosts to relay mail for virtusertable Maybe S Turns on domain aliasing virtual domains ldaprouting Maybe S Routes incoming mail using LDAPMASQUERADEAS Yes Makes all mail seem to come from one placeEXPOSEDUSER Yes Lists users who shouldnt be masqueradedMAILHUB Yes S Specifies mail server for incoming mailSMARTHOST Yes C Specifies mail server for outgoing maila S servers C clientsMAILER macroYou must include a MAILER macro for every delivery agent you want to enable Youllfind a complete list of supported mailers in the directory cfmailers but typically youneed only local and smtp MAILER lines are generally the last thing in the mc fileFEATURE macroThe FEATURE macro enables a whole host of common scenarios at last countby including m files from the feature directory The syntax isFEATUREkeyword arg arg where keyword corresponds to a file keywordm in the cffeature directory and theargs are passed to it There can be at most nine arguments to a featureusecwfile featureThe sendmail internal class w hence the name cw contains the names of all localhosts for which this host accepts and delivers mail This feature specifies that mailbe accepted for the hosts listed one per line in etcmaillocalhostnames Theconfiguration lineFEATUREusecwfileinvokes the feature A client machine does not really need this feature unless it hasnicknames but your incoming mail hub machine does The localhostnames fileshould include any local hosts and virtual domains for which you accept email including sites whose backup MX records see page point to youWithout this feature sendmail delivers mail locally only if it is addressed to themachine on which sendmail is runningIf you add a new host at your site you must add it to the localhostnames file andsend a HUP signal to sendmail to make your changes take effectredirect featureWhen people leave your organization you usually either forward their mail or letmail to them bounce back to the sender with an error The redirect feature provides support for a more elegant way of bouncing mailIf Joe Smith has graduated from oldsiteedu login smithj to newsitecom loginjoe then enabling redirect withFEATUREredirectand adding the linesmithj joenewsitecomREDIRECTto the aliases file at oldsiteedu causes mail to smithj to be returned to the senderwith an error message suggesting that the sender try the address joenewsitecominstead The message itself is not automatically forwardedalwaysadddomain featureThe alwaysadddomain feature makes all email addresses fully qualified It shouldalways be usedaccessdb featureThe accessdb feature controls relaying and other policy issues Typically the rawdata that drives this feature either comes from LDAP or is kept in a text file calledetcmailaccess In the latter case the text file must be converted to some kindof indexed format with the makemap command as described on page Touse the flat file use FEATUREaccessdb in the configuration file for the LDAPversion use FEATUREaccessdb LDAPThe key field in the access database is an IP network or a domain name with anoptional tag such as Connect To or From The value field specifies what to dowith the messageThe most common values are OK to accept the message RELAY to allow it to be relayed REJECT to reject it with a generic error indication or ERRORerror code and This form uses the default LDAP schema defined in the file cfsendmailschema if you want a different schema file use additional arguments in your FEATURE statementmessage to reject it with a specific message Other possible values allow for finergrained control Here is a snippet from a sample etcmailaccess filelocalhost RELAY RELAY RELAY RELAY OKfaxcom OK ERROR We dont accept mail from spammers ERROR We dont accept mail from spammersvirtusertable featureThe virtusertable feature supports domain aliasing for incoming mail through amap stored in etcmailvirtusertable This feature lets one machine host multiplevirtual domains and is used frequently at webhosting sites The key field of thetable contains either an email address userhostdomain or a domain specification domain The value field is a local or external email address If the key is adomain the value can either pass the user field along as the variable or routethe mail to a different user Here are some examplesappliedtrustcom atrustcomunixbookadmincom sabookauthorsatrustcomlinuxbookadmincom sabookauthorsatrustcomwebmasterexamplecom billyqzakowskicoloradoeduinfotestdomainnet ausernamehotmailcomAll the host keys on the left side of the data mappings must be listed in the cw fileetcmaillocalhostnames or be included in the VIRTUSERDOMAIN list If theyare not sendmail will not know to accept the mail locally and will try to find thedestination host on the Internet But DNS MX records will point sendmail backto this same server and you will get a local configuration error message in theresulting bounce message Unfortunately sendmail cannot tell that the error message for this instance should in fact be virtusertable key not in cw fileldaprouting featureLDAP the Lightweight Directory Access Protocol can be a source of data for aliasesor mail routing information as well as general tabular data as described earlier ThecfREADME file has a long section on LDAP with lots of examplesTo use LDAP in this way you must have built sendmail to include LDAP supportIn your mc file add the linesdefineconfLDAPDEFAULTSPEC h server b searchbaseFEATUREldaproutingLDAPROUTEDOMAINmydomainSee page forgeneral information about LDAPThose lines tell sendmail that you want to use an LDAP database to route incomingmail addressed to the specified domain The LDAPDEFAULTSPEC option identifiesthe LDAP server and the LDAP basename for searches LDAP uses port unlessyou specify a different port by adding p ldapport to the definesendmail uses the values of two tags in the LDAP database mailLocalAddress for the addressee on incoming mail mailRoutingAddress for the destination to which email should be sentsendmail also supports the tag mailHost which if present routes mail to theMXdesignated mail handler for the specified host The recipient address remainsthe value of the mailRoutingAddress tagLDAP database entries support a wild card entry domain that reroutes mailaddressed to anyone at the specified domain as was done in the virtusertableBy default mail addressed to userhostmydomain would first trigger a lookupon userhostmydomain If that failed sendmail would try hostmydomainbut not usermydomain Including the lineLDAPROUTEEQUIVALENThostmydomainwould also try the keys usermydomain and mydomain This feature enables asingle database to route mail at a complex site You can also take the entries for theLDAPROUTEEQUIVALENT clauses from a file which makes the feature quite usableThe syntax for that form isLDAPROUTEEQUIVALENTFILEfilenameAdditional arguments to the ldaprouting feature let you specify more details aboutthe LDAP schema and control the handling of addressee names that have a detailpart As always see the cfREADME file for exact detailsMasquerading featuresAn email address is usually made up of a username a host and a domain but manysites do not want the names of their internal hosts exposed on the Internet TheMASQUERADEAS macro lets you specify a single identity for other machines tohide behind All mail appears to emanate from the designated machine or domainThis is fine for regular users but for debugging purposes system users such as rootshould be excluded from the masqueradeFor example the sequenceMASQUERADEASatrustcomEXPOSEDUSERrootEXPOSEDUSERMailerDaemonwould stamp mail as coming from useratrustcom unless it was sent by root or themail system in these cases the mail would carry the name of the originating hostMASQUERADEAS is actually just the tip of a vast masquerading iceberg that extendsdownward through a dozen variations and exceptions The allmasquerade andmasqueradeenvelope features in combination with MASQUERADEAS hide justthe right amount of local info See the cfREADME for detailsMAILHUB and SMARTHOST macrosMasquerading makes all mail appear to come from a single host or domain by rewriting the headers and optionally the envelope But most sites want all mail toactually come from or go to a single machine so that they can control the flowof viruses spam and company secrets You can achieve this control with a combination of MX records in DNS the MAILHUB macro for incoming mail and theSMARTHOST macro for outgoing mailFor example in a structured email implementation MX records would direct incoming email from the Internet to an MTA in the networks demilitarized zoneAfter verification that the received email was free of viruses and spam and was directed to valid local users the mail could be relayed with the following define tothe internal routing MTA for deliverydefineMAILHUB smtproutingMTAmydomainLikewise client machines would relay their mail to the SMARTHOST designated inthe nullclient feature in their configuration The SMARTHOST could then filter forviruses and spam so that mail from your site did not pollute the InternetThe syntax of SMARTHOST parallels that of MAILHUB and the default deliveryagent is again relay For exampledefineSMARTHOST smtpoutgoingMTAmydomainYou can use the same machine as the server for both incoming and outgoing mailBoth the SMARTHOST and the MAILHUB must allow relaying the first from clientsinside your domain and the second from the MTA in the DMZClient configurationMost of your sites machines should be configured as clients who just submit outgoingmail generated by users and dont receive mail at all One of sendmails FEATUREsnullclient is just right for this situation It creates a config file that forwards allSee page for moreinformation aboutDNS MX recordsSee the next sectionfor more aboutnullclientmail to a central hub over SMTP The entire config file after the VERSIONID andOSTYPE lines would be simplyFEATUREnocanonifyFEATUREnullclient mailserverEXPOSEDUSERrootwhere mailserver is the name of your central hub The nocanonify feature tells sendmailnot to do DNS lookups or rewrite addresses with fully qualified domain names Allthat work will be done by the mailserver host This feature is similar to SMARTHOSTand assumes that the client will MASQUERADEAS mailserver The EXPOSEDUSERclause exempts root from the masquerading and so facilitates debuggingThe mailserver machine must allow relaying from its null clients That permissionis granted in the accessdb described on page The null client must have anassociated MX record that points to mailserver and must also be included in themailservers cw file usually etcmaillocalhostnames These settings allow themailserver to accept mail for the clientsendmail should run as an MSA without the bd flag if the user agents on theclient machine can be taught to use port for submitting mail If not you canrun sendmail in daemon mode bd but set the DAEMONOPTIONS configurationoption to listen for connections only on the loopback interfacem configuration optionsYou set config file options with the m define command A complete list of optionsthat are accessible as m variables along with their default values is given in thecfREADME fileThe defaults are OK for a typical site that is not too paranoid about security andnot too concerned with performance The defaults try to protect you from spamby turning off relaying by requiring addresses to be fully qualified and by requiring that senders domains resolve to an IP address If your mail hub machine isbusy and services a lot of mailing lists you might need to tweak some of the performance valuesTable on the next page lists some options that you might need to adjust about of over configuration options Their default values are shown in parentheses To save space the option names are shown without their conf prefix forexample the FASTSPLIT option is actually named confFASTSPLIT We divided thetable into subsections that identify the kind of issue the variable addresses resourcemanagement performance security and spam abatement and miscellaneous options Some options fit in more than one category but we have listed them only onceTable Basic sendmail configuration optionsOption name Description default valueResourcesMAXDAEMONCHILDREN Max number of child processes a no limitMAXMESSAGESIZE Max size in bytes of a single message infiniteMINFREEBLOCKS Min filesystem space to accept mail TOlotsofstuff Timeouts for all kinds of things variousPerformanceDELAYLA Load avg to slow deliveries no limitFASTSPLIT Suppresses MX lookups as recipients are sortedand split across queues trueMCICACHESIZE of open outgoing TCP connections cached MCICACHETIMEOUT Time to keep cached connections open mMINQUEUEAGE Minimum time jobs must stay in queue QUEUELA Load average at which mail should be queuedinstead of delivered immediately CPUsREFUSELA Load avg at which to refuse mail CPUsSecurity and spamAUTHMECHANISMS SMTP auth mechanisms bCONNECTIONRATETHROTTLE Limits connection acceptance rate no limitDONTBLAMESENDMAIL Overrides security and file checking safe cMAXMIMEHEADERLENGTH Sets max size of MIME headers no limit dMAXRCPTSPERMESSAGE Slows spam delivery defers extra recipientsand sends a temporary error msg infinitePRIVACYFLAGS Limits info given out by SMTP authwarningsMiscDOUBLEBOUNCEADDRESS Catches lots of spam some sites use devnullwhich can hide serious problems postmasterLDAPDEFAULTSPEC Map spec for LDAP database including thehostport the server is running on undefineda More specifically the maximum number of child processes that can run at once When the limit isreached sendmail refuses connections This option can prevent or create denial of service attacksb The default value is EXTERNAL GSSAPI KERBEROSV DIGESTMD CRAMMD dont add PLAIN LOGINbecause the password is transmitted as cleartext That might be OK internally but not on the Internetunless the connection is also secured through the use of SSLc Dont change this setting casuallyd This option can prevent user agent buffer overflows is a good value to useit means bytes per header and bytes per parameter to that headerSpamrelated features in sendmailsendmail has a variety of features and configuration options that can help youcontrol spam and viruses Rules that control third party aka promiscuous aka open relaying thatis the use of your mail server by one offsite user to send mail to anotheroffsite user Spammers often use relaying to mask the true source of theirmail and thereby avoid detection by ISPs Relaying also lets spammers useyour cycles and save their own The access database for filtering recipient addresses This feature is ratherlike a firewall for email Blacklists that catalog open relays and known spamfriendly sites thatsendmail can check against Throttles that can slow down mail acceptance when certain types of badbehavior are detected Header checking and input mail filtering by means of a generic mail filtering interface called libmilter It allows arbitrary scanning of messageheaders and content and lets you reject messages that match a particularprofile Milters are plentiful and powerful see milterorgRelay controlsendmail accepts incoming mail looks at the envelope addresses decides wherethe mail should go and then passes the message along to an appropriate destinationThat destination can be local or it can be another transport agent farther along inthe delivery chain When an incoming message has no local recipients the transport agent that handles it is said to be acting as a relayOnly hosts that are tagged with RELAY in the access database see page orthat are listed in etcmailrelaydomains are allowed to submit mail for relayingSome types of relaying are useful and legitimate How can you tell which messagesto relay and which to reject Relaying is actually necessary in only three situations When the transport agent acts as a gateway for hosts that are not reachablein any other way for example hosts that are not always turned on laptops Windows PCs and virtual hosts In this situation all the recipientsfor which you want to relay lie within the same domain When the transport agent is the outgoing mail server for other notsosmart hosts In this case all the senders hostnames or IP addresses arelocal or at least enumerable When you have agreed to be a backup MX destination for another siteAny other situation that appears to require relaying is probably just an indication ofbad design with the possible exception of support for mobile users You can obviatethe first use of relaying above by designating a centralized server to receive mailwith POP or IMAP being used for client access The second case should always beallowed but only for your own hosts You can check IP addresses or hostnames Inthe third case you can list the other site in your access database and allow relayingjust for that sites IP address blocksAlthough sendmail comes with relaying turned off by default several features canturn relaying back on either fully or in a limited and controlled way These featuresare listed below for completeness but our recommendation is that you be carefulabout opening things up too much The accessdb feature is the safest way to allow limited relaying FEATURErelayentiredomain allows relaying for just your domain RELAYDOMAINdomain adds more domains to be relayed RELAYDOMAINFILEfilename same takes domain list from a file FEATURErelayhostsonly affects RELAYDOMAIN accessdbYou need to make an exception if you use the SMARTHOST or MAILHUB designations to route mail through a particular mail server machine That server must beset up to relay mail from local hosts Configure it withFEATURErelayentiredomainIf you consider turning on relaying in some form consult the sendmail documentation in cfREADME to be sure you dont inadvertently become a friend of spammers When you are done have one of the relaychecking sites verify that you didnot inadvertently create an open relaytry spamhelporgUser or site blacklistingIf you have local users or hosts to which you want to block mail useFEATUREblacklistrecipientsIt supports the following types of entries in your access fileTonobody ERROR Mailbox disabled for this userToprintermydomain ERROR This host does not accept mailTouserhostmydomain ERROR Mailbox disabled for this userThese lines block incoming mail to user nobody on any host to host printer and toa particular users address on one machine The use of the To tag lets these userssend messages just not receive them some printers have that capabilityTo include a DNSstyle blacklist for incoming email use the dnsbl featureFEATUREdnsbl zenspamhausorgThis feature makes sendmail reject mail from any site whose IP address is in anyof the three blacklists of known spammers SBL XBL and PBL maintained atspamhausorg Other lists catalog sites that run open relays and blocks of addressesthat are known to be havens for spammers These blacklists are distributed througha clever tweak of the DNS system hence the name dnsblYou can pass a third argument to the dnsbl feature to specify the error messageyou would like returned If you omit this argument sendmail returns a fixed errormessage from the DNS database that contains the recordsYou can include the dnsbl feature several times to check multiple lists of abusersThrottles rates and connection limitsTable lists several sendmail controls that can slow down mail processingwhen clients behavior appears suspiciousTable sendmails slow down configuration primitivesPrimitive DescriptionBADRCPTTHROTTLE Slows down spammers collecting addressesMAXRCPTSPERMESSAGE Defers delivery if a message has too many recipientsratecontrol feature Limits the rate of incoming connectionsconncontrol feature Limits the number of simultaneous connectionsgreetpause feature Delays HELO response forces strict SMTP complianceAfter the nosuchlogin count reaches the limit set in the BADRCPTTHROTTLE option sendmail sleeps for one second after each rejected RCPT command slowing aspammers address harvesting to a crawl To set that threshold to usedefineconfBADRCPTTHROTTLE Setting the MAXRCPTSPERMESSAGE option causes the sender to queue extrarecipients for later This is a cheap form of greylisting for messages that have a suspiciously large number of recipientsThe ratecontrol and conncontrol features allow perhost or pernet limits on therate at which incoming connections are accepted and the number of simultaneousconnections respectively Both use the etcmailaccess file to specify the limitsand the domains to which they should apply the first with the tag ClientRate inthe key field and the second with tag ClientConn To enable rate controls insertlines like these in your mc fileFEATUREratecontrol nodelayterminateFEATUREconncontrol nodelayterminateThen add to your etcmailaccess file the list of hosts or nets to be controlled andtheir restriction thresholds For example the linesClientRate ClientRate limit the hosts and to two new connections per minute andten new connections per minute respectively The linesClientConn ClientConn ClientConn FEATUREaccessdb must be there tooset limits of two simultaneous connections for seven for and ten simultaneous connections for all other hostsAnother nifty feature is greetpause When a remote transport agent connects toyour sendmail server the SMTP protocol mandates that it wait for your serverswelcome greeting before speaking However its common for spam mailers to blurtout an EHLOHELO command immediately This behavior is partially explainableas poor implementation of the SMTP protocol in spamsending tools but it mayalso be a feature that aims to save time on the spammers behalf Whatever the causethis behavior is suspicious and is known as slammingThe greetpause feature makes sendmail wait for a specified period of time at thebeginning of the connection before greeting its newfound friend If the remoteMTA does not wait to be properly greeted and proceeds with an EHLO or HELOcommand during the planned awkward moment sendmail logs an error and refuses subsequent commands from the remote MTAYou can enable greeting pauses with this entry in the mc fileFEATUREgreetpause This line causes a millisecond delay at the beginning of every new connectionYou can set perhost or pernet delays with a GreetPause prefix in the access database but most sites use a blanket value for this featureSecurity and sendmailWith the explosive growth of the Internet programs such as sendmail that acceptarbitrary usersupplied input and deliver it to local users files or shells have frequently provided an avenue of attack for hackers sendmail along with DNS andeven IP is flirting with authentication and encryption as a builtin solution to someof these fundamental security issuessendmail supports both SMTP authentication and encryption with TLS TransportLayer Security formerly known as SSL the Secure Sockets Layer TLS broughtwith it six new configuration options for certificate files and key files New actionsfor access database matches can require that authentication must have succeededsendmail carefully inspects file permissions before it believes the contents of saya forward or an aliases file Although this tightening of security is generally welcome its sometimes necessary to relax the tough policies To this end sendmailintroduced the DontBlameSendmail option so named in hopes that the namemight suggest to sysadmins that what they are doing is unsafeThis option has many possible values at last count The default is safe thestrictest possible For a complete list of values see docopopps in the sendmaildistribution or the OReilly sendmail book Or just leave the option set to safeOwnershipsThree user accounts are important in the sendmail universe the DefaultUser theRunAsUser and the TrustedUserBy default all of sendmails mailers run as the DefaultUser unless the mailers flagsspecify otherwise If a user mailnull sendmail or daemon exists in the passwd fileDefaultUser will be that Otherwise it defaults to UID and GID We recommend the use of the mailnull account and a mailnull group Add it to etcpasswdwith a star as the password no valid shell no home directory and a default groupof mailnull Youll have to add the mailnull entry to the group file too The mailnullaccount should not own any files If sendmail is not running as root the mailersmust be setuidIf RunAsUser is set sendmail ignores the value of DefaultUser and does everythingas RunAsUser If you are running sendmail setgid then the submission sendmailjust passes messages to the real sendmail through SMTP The real sendmail doesnot have its setuid bit set but it runs as root from the startup filesThe RunAsUser is the UID that sendmail runs under after opening its socket connection to port Ports numbered less than can be opened only by the superuser therefore sendmail must initially run as root However after performingthis operation sendmail can switch to a different UID Such a switch reduces therisk of damage or access if sendmail is tricked into doing something bad Dont usethe RunAsUser feature on machines that support user accounts or other services itis meant for use only on firewalls or bastion hostsBy default sendmail does not switch identities and continues to run as root Ifyou change the RunAsUser to something other than root you must change severalother things as well The RunAsUser must own the mail queue be able to read allmaps and include files be able to run programs etc Expect to spend a few hoursdiscovering all the file and directory ownerships that must be changedsendmails TrustedUser can own maps and alias files The TrustedUser is allowedto start the daemon or rebuild the aliases file This facility exists mostly to supportGUI interfaces to sendmail that need to provide limited administrative control tocertain users If you set TrustedUser be sure to guard the account that it points tobecause this account can easily be exploited to gain root access The TrustedUseris different from the TRUSTEDUSERS class which determines who can rewrite theFrom line of messages Bastion hosts are specially hardened hosts intended to withstand attack when placed in a DMZ oroutside a firewall The TRUSTEDUSERS feature is typically used to support mailing list softwarePermissionsFile and directory permissions are important to sendmail security Use the settingslisted in Table to be safeTable Owner and permissions for sendmailrelated directoriesPath Owner Mode What it containsvarspoolclientmqueue smmspsmmsp Queue for initial submissionsvarspoolmqueue RunAsUser Mail queue directory var varspool root Path to mqueueetcmail TrustedUser Maps the config file aliasesetcmail TrustedUser Parent directory for mapsetc root Path to mail directorysendmail no longer reads forward files that have link counts greater than if thedirectory paths that lead to them have lax permissions This rule bit Evi when one ofher forward files which she usually hardlinked to either forwardtoboulder orforwardtosandiego silently failed to forward her mail from a small site at whichshe did not receive much mail It was months before she realized that I never gotyour mail was her own fault and not a valid excuseYou can turn off many of the restrictive file access policies mentioned above withthe DontBlameSendmail option But dont do thatSafer mail to files and programsWe recommend that you use smrsh instead of binsh as your program mailer andthat you use maillocal instead of binmail as your local mailer Both programsare included in the sendmail distribution To incorporate them into your configuration add the linesFEATUREsmrsh pathtosmrshFEATURElocallmtp pathtomaillocalto your mc file If you omit the explicit paths the commands are assumed to livein usrlibexec You can use sendmails confEBINDIR option to change the defaultlocation of the binaries to whatever you want Table helps you find where ourfriendly vendors have stashed thingssmrsh is a restricted shell that executes only the programs contained in one directory usradmsmbin by default smrsh ignores userspecified paths and tries tofind any requested commands in its own knownsafe directory smrsh also blocksthe use of certain shell metacharacters such as the input redirection symbolSymbolic links are allowed in smbin so you need not make duplicate copies of theTable Location of sendmails restricted delivery agentsOS smrsh maillocal smbinUbuntu usrlibsmbin usrlibsmbin usradmDebian usrlibsmbin usrlibsmbin usradmRed Hat usrsbin etcsmrshCentOS usrsbin etcsmrshFreeBSD usrlibexec usrlibexec usradmprograms you allow The vacation program is a good candidate for smbin Dontput procmail there its insecureHere are some example shell commands and their possible smrsh interpretationsvacation eric Executes usradmsmbinvacation ericcat etcpasswd Rejected cat not in smbinvacation eric etcpasswd Rejected no allowedsendmails SafeFileEnvironment option controls where files can be written whenemail is redirected to a file by aliases or a forward file It causes sendmail to execute a chroot system call making the root of the filesystem no longer but rathersafe or whatever path you specified in the SafeFileEnvironment option An aliasthat directed mail to the etcpasswd file for example would actually be writtento safeetcpasswdThe SafeFileEnvironment option also protects device files directories and otherspecial files by allowing writes only to regular files Besides increasing security thisoption ameliorates the effects of user mistakes Some sites set the option to hometo allow access to home directories while keeping system files offlimitsMailers can also be run in a chrooted directoryPrivacy optionssendmail privacy options also control What external folks can determine about your site through SMTP What you require of the host on the other end of an SMTP connection Whether your users can see or run the mail queueTable on the next page lists the possible values for the privacy options as ofthis writing see the file docopopps in the distribution for current informationWe recommend conservatism in your mc file usedefineconfPRIVACYOPTIONS goaway authwarnings restrictmailqrestrictqrunTable Values of the PrivacyOption variableValue Meaningauthwarnings Adds warning header if an outgoing message seems forgedgoaway Disables all SMTP status queries EXPN VRFY etcneedexpnhelo Does not expand addresses EXPN without a HELOneedmailhelo Requires SMTP HELO identifies remote hostneedvrfyhelo Does not verify addresses VRFY without a HELOnobodyreturn Does not return the message body in a DSNnoetrna Disallows asynchronous queue runsnoexpn Disallows the SMTP EXPN commandnoreceipts Turns off delivery status notification for success return receiptsnoverbb Disallows verbose mode for EXPNnovrfy Disallows the SMTP VRFY commandpublic Does no privacysecurity checkingrestrictexpand Restricts info displayed by the bv and v flags crestrictmailq Allows only mqueue directorys group to see the queuerestrictqrun Allows only mqueue directorys owner to run the queuea ETRN is an ESMTP command for use by dialup hosts It requests that the queue be run just formessages to that hostb Verbose mode follows forward files when an EXPN command is given and reports more information on the whereabouts of a users mail Use noverb or better yet noexpn on any machine exposed to the outside worldc Unless executed by root or the TrustedUsersendmails default value for the privacy options is authwarnings the above linewould reset that value Notice the double sets of quotes some versions of m require them to protect the commas in the list of privacy option valuesRunning a chrooted sendmail for the truly paranoidIf you are worried about the access that sendmail has to your filesystem you canstart it in a chrooted jail Create a minimal filesystem in your jail including thingslike devnull etc essentials passwd group resolvconf sendmailcf any mapfiles mail the shared libraries that sendmail needs the sendmail binary themail queue directory and any log files You will probably have to fiddle with the listto get it just right Use the chroot command to start a jailed sendmail For example sudo chroot jail usrsbinsendmail bd qmDenial of service attacksDenial of service attacks are difficult to prevent because no a priori method can determine that a message is an attack rather than a valid piece of email Attackers cantry various nasty things including flooding the SMTP port with bogus connectionsfilling disk partitions with giant messages clogging outgoing connections and mailbombing sendmail has some configuration parameters that can help slow downor limit the impact of a denial of service attack but these parameters can also interfere with the delivery of legitimate mailThe MaxDaemonChildren option limits the number of sendmail processes It prevents the system from being overwhelmed with sendmail work However it alsoallows an attacker to easily shut down SMTP serviceThe MaxMessageSize option can help prevent the mail queue directory from fillingBut if you set it too low legitimate mail will bounce You might mention your limitto users so that they arent surprised when their mail bounces We recommend afairly high limit such as MB anyway since some legitimate mail is hugeThe ConnectionRateThrottle option which limits the number of permitted connections per second can slow things down a bit Finally setting MaxRcptsPerMessagewhich controls the maximum number of recipients allowed on a single messagemay also helpsendmail has always been able to refuse connections option REFUSELA or queueemail QUEUELA according to the system load average A variation DELAYLA keepsthe mail flowing but at a reduced rateIn spite of all these protections for your mail system someone mail bombing youwill still interfere with legitimate mail Mail bombing can be quite nastyTLS Transport Layer SecurityTLS a encryptionauthentication system is specified in RFC It is implemented in sendmail as an extension to SMTP called STARTTLSStrong authentication can replace a hostname or IP address as the authorizationtoken for relaying mail or for accepting a connection from a host in the first placeAn entry such asTLSSrvsecureexamplecom ENCRTLSCltlaptopexamplecom PERMVERIFYin the accessdb indicates that STARTTLS is in use and that email to the domainsecureexamplecom must be encrypted with at least bit encryption keys Emailfrom a host in the laptopexamplecom domain should be accepted only if the clienthas authenticated itselfAlthough STARTTLS provides strong encryption note that its protection coversonly the journey to the next hop MTA Once the message arrives at the next hopit might be forwarded to another MTA that does not use a secure transport methodIf you have control of all possible MTAs in the path you can create a secure mailtransport network If not you will need to rely on a UAbased encryption package such as PGPGPG or a centralized email encryption service see page See page forgeneral information about TLSGreg Shapiro and Claus Assmann of Sendmail Inc have stashed some slightlydated extra documentation about security and sendmail on the web Its availablefrom sendmailorggshapiro and sendmailorgca The index link in ca is especially usefulsendmail testing and debuggingmbased configurations are to some extent pretested You probably wont need todo lowlevel debugging if you use them But one thing the debugging flags cannottest is your designWhile researching this chapter we found errors in several of the configuration filesand designs that we examined The errors ranged from invoking a feature withoutthe prerequisite macro eg enabling masqueradeenvelope without having turnedon masquerading with MASQUERADEAS to total conflict between the design of thesendmail configuration and the firewall that controlled whether and under whatconditions mail was allowed inYou cannot design a mail system in a vacuum You must synchronize it with or atleast not be in conflict with your DNS MX records and your firewall policyQueue monitoringYou can use the mailq command which is equivalent to sendmail bp to view thestatus of queued messages Messages are queued while they are being delivered orwhen delivery has been attempted but has failedmailq prints a humanreadable summary of the files in varspoolmqueue at anygiven moment The output is useful for determining why a message may have beendelayed If it appears that a mail backlog is developing you can monitor the statusof sendmails attempts to clear the jamThere are two default queues one for messages received on port and anotherfor messages received on port the client submission queue You can invokemailq Ac to see the client queueBelow some typical output from mailq shows three messages waiting to be delivered sudo mailqvarspoolmqueue requestsQID Size QTime SenderRecipientkgYYk Sat Jul MAILERDAEMON BITMIME Deferred Connection refused by agribusinessonlinecom NimtzagribusinessonlinecomkULkAHB Fri Jun randyatrustcom Deferred Name server kwirelesscom host name lookup fa relderkwirelesscomkUJDm Fri Jun MAILERDAEMON reply read error from mxlevelcom lfinistbbnplanetcomIf you think you understand the situation better than sendmail or you just wantsendmail to try to redeliver the queued messages immediately you can force aqueue run with sendmail q If you use sendmail q v sendmail shows the playbyplay results of each delivery attempt information that is often useful for debugging Left to its own devices sendmail retries delivery every queue run intervaltypically every minutesLoggingsendmail uses syslog to log error and status messages with the syslog facility mailand levels debug through crit messages are tagged with the string sendmailYou can override the logging string sendmail with the L commandline optionthis capability is handy if you are debugging one copy of sendmail while othercopies are doing regular email choresThe confLOGLEVEL option specified on the command line or in the config filedetermines the severity level that sendmail uses as a threshold for logging Highvalues of the log level imply low severity levels and cause more info to be loggedTable gives an approximate mapping between sendmail log levels and syslogseverity levelsTable sendmail log levels L vs syslog levelsL Syslog levels L Syslog levels No logging notice alert or crit info crit debug err or warningRecall that a message logged to syslog at a particular level is reported to that leveland all those above it The etcsyslogconf or etcrsyslogconf file determinesthe eventual destination of each message Table shows their default locationsTable Default sendmail log locationsSystem Log file locationDebian varlogmaillogUbuntu varlogmaillogRed Hat varlogmaillogCentOS varlogmaillogFreeBSD varlogmaillogSee Chapter for more information about syslogSeveral programs can summarize sendmail log files with the end products rangingfrom simple counts and text tables mreport to fancy web pages Yasma You mightneed to limit access to this data or at least inform your users that you are collecting it EximThe Exim mail transport and submission agent was written in by Philip Hazelof the University of Cambridge and is distributed under the GNU General PublicLicense The current release Exim version came out in spring Tons ofExim documentation are available online as are a couple of books by the authorof the softwareGoogling for Exim questions often seems to lead to old undated and sometimesinappropriate materials so check the official documentation first A pagespecification and configuration document docspectxt is included in the distribution This document is also available from eximorg as a PDF file Its the definitive reference work for Exim and is updated religiously with each new releaseThere are two cultures with respect to Exim configuration Debians and the restof the worlds Debian runs its own set of mailing lists to support users we do notcover the Debianspecific configuration extensions hereExim is like sendmail in that it is implemented as a single process that performsessentially all the ongoing chores associated with email However Exim does notcarry all sendmails historical baggage support for ancient address formats needing to get mail to hosts not on the Internet etc Many aspects of Exims behaviorare specified at compile time the chief examples being Exims database and message store formatsThe workhorses in the Exim system are called routers and transports Both are included in the general category of drivers Routers decide how messages shouldbe delivered and transports decide on the mechanics of making deliveries Routers are an ordered list of things to try whereas transports are an unordered set ofdelivery methodsExim installationYou can download the latest distribution from eximorg or from your favoritepackage repository Refer to the toplevel README file and the file srcEDITMEin which you must set installation locations user IDs and other compiletime parameters EDITME is over lines long but its mostly comments that lead youthrough the compilation process required changes are well labeled After your editssave the file as LocalMakefile or LocalMakefileosname if you are buildingconfigurations for several different operating systems from the same distributiondirectory before you run makeHere are a few of the important variables our opinion and suggested values Eximdevelopers opinion from the EDITME file The first five are required and the restare recommendedBINDIRECTORYusreximbin Where the exim binary should liveSPOOLDIRECTORYvarspoolexim Mail spool directoryCONFIGUREFILEusreximconfigure Exims configuration fileSYSTEMALIASESFILEetcaliases Location of aliases fileEXIMUSERrefexim User to run as after rootly choresROUTERACCEPTyes Router drivers to includeROUTERDNSLOOKUPyesROUTERIPLITERALyesROUTERMANUALROUTEyesROUTERQUERYPROGRAMyesROUTERREDIRECTyesTRANSPORTAPPENDFILEyes Transport drivers to includeTRANSPORTAUTOREPLYyesTRANSPORTPIPEyesTRANSPORTSMTPyesSUPPORTMAILDIRyes Mailbox formats to understandSUPPORTMAILSTOREyesSUPPORTMBXyesLOOKUPDBMyes DB lookup methods to includeLOOKUPLSEARCHyes Linear search lookupLOOKUPDNSDByes Allow neararbitrary DNS lookupsUSEDByes Use Berkeley DB from READMEDBMLIBldb from READMEWITHCONTENTSCANyes Include content scanning via ACLsEXPERIMENTALSPFyes Include SPF support needs libspfCFLAGS Iusrlocalinclude From wwwlibspforgLDFLAGS lspfLOGFILEPATHvarlogeximslog Log files file syslog or bothLOGFILEPATHsyslogLOGFILEPATHsyslogvarlogeximslogEXICYCLOGMAX Compresscycle log files keep Routers and transports must be compiled into the code if you intend to use themIn these days of large memories you might as well leave them all in Some defaultpaths are certainly nonstandard for example the binary in usreximbin and thePID file in varspoolexim You might want to tweak these values to match yourother installed softwareAbout ten database lookup methods are available including MySQL Oracle andLDAP If you include LDAP you must specify the LDAPLIBTYPE variable to tellExim which LDAP library you are using You may also need to specify the path toLDAP include files and librariesThe EDITME file does a good job of telling you about any dependencies your database choices might entail Any entries above that have from README in theircomment line were not listed in srcEDITME but rather in the READMEEDITME has many additional security options that you might want to include suchas support for SMTP AUTH TLS PAM and options for controlling file ownershipsand permissions You can disable certain Exim options at compile time to limit thedamage a hacker might cause if the software is compromisedIts advisable to read the entire EDITME file before you complete the installationIt gives you a good feel for what you can control at run time through the configuration file The toplevel README file has lots of detail about OSspecific quirksthat you migh need to add to the EDITME file as wellOnce you have modified EDITME and installed it as LocalMakefile run makeat the top of the distribution tree followed by sudo make install The next stepis to test your shiny new exim binary and see if it delivers mail as expected Thedocspectxt file contains good testing documentationOnce you are satisfied that Exim is working properly link usrsbinsendmail toexim so that Exim can emulate the traditional commandline interface to the mailsystem used by many user agents You must also arrange for exim to be started atboot timeExim startupOn a mail hub machine exim typically starts at boot time in daemon mode andruns continuously listening on port and accepting messages through SMTPSee Chapter Booting and System Management Daemons for startup detailsfor your operating systemLike sendmail Exim can wear several hats and if started with specific flags or alternative command names it performs different functions Exims mode flags aresimilar to those understood by sendmail because exim works hard to maintaincompatibility when called by user agents and other tools Table lists a fewcommon flagsAny errors in the config file that can be detected at parse time are caught by eximbV but some errors can only be caught at run time Misplaced braces are a common mistakeThe exim man page gives lots of detail on all the nooks and crannies of exims commandline flags and options including extensive debugging informationExim utilitiesThe Exim distribution includes a bunch of utilities to help you monitor debug andsanitycheck your installation Below is the current list along with a brief descriptionof each See the documentation from the distribution for more detailTable Common exim commandline flagsFlag Meaningbd Runs in daemon mode and listens for connections on port bf or bF Runs in user or system filter test modebi Rebuilds hashed aliases same as newaliasesbp Prints the mail queue same as mailqbt Enters address test modebV Checks for syntax errors in the configuration filedcategory Runs in debug mode flexible categorybased configurationq Starts a queue runner same as runq exicyclog rotates log files exigrep searches the main log exilog visualizes log files across multiple servers eximcheckaccess checks address acceptance from a given IP address eximdbmbuild builds a DBM file eximdumpdb dumps a hints database eximfixdb patches a hints database eximlock locks a mailbox file eximtidydb cleans up a hints database eximstats extracts statistics from the log exinext extracts retry information exipick selects messages according to various criteria exiqgrep searches the queue exiqsumm summarizes the queue exiwhat lists what Exim processes are doingAnother utility that is part of the Exim suite is eximon an X Windows applicationthat displays Exims state the state of Exims queue and the tail of the log file Aswith the main distribution you build it by editing a wellcommented EDITME filein the eximmonitor directory and running make However in the case of eximonthe defaults are usually fine so you should not have to do much configuration tobuild the application Some configuration and queue management can be donefrom the eximon GUI as wellExim configuration languageThe Exim configuration language or more accurately languages one for filters onefor regular expressions etc feels a bit like the ancient s language ForthWhen first reading an Exim configuration you might find it hard to distinguishbetween keywords and option names which are fixed by Exim and variable nameswhich are defined by sysadmins through configuration statements For CS wizards its Turingcomplete mere mortals can substitute powerful and complicatedAlthough Exim is advertised as being easy to configure and is extensively documented there can be quite a learning curve for new users The section How Eximreceives and delivers mail in the specification document is essential reading fornewcomers It gives a good feel for the underlying concepts of the systemWhen assigned a value the Exim languages predefined options sometimes cause anaction The values of about predefined variables may also change in responseto an action These variables can be included in conditional statementsThe language for evaluating if statements and the like may remind you of the reverse Polish notation used during the heyday of HewlettPackard calculators Letslook at a simple example In the lineaclsmtprcpt if interfaceport aclcheckrcpt aclcheckrcptsubmit the aclsmtprcpt option when set causes an ACL to be implemented for eachrecipient SMTP RCPT command in the SMTP exchange The value assigned tothis option is either aclcheckrcpt or aclcheckrcptsubmit depending onwhether or not the Exim variable interfaceport has value We do not detail the Exim configuration language in this chapter but refer youinstead to the extensive documentation In particular pay close attention to thestring expansion section of the Exim specificationExim configuration fileExims runtime behavior is controlled by a single configuration file usually calledusreximconfigure Its name is one of the required variables specified in theEDITME file and compiled into the binaryThe supplied default configuration file srcconfiguredefault is well commentedand is a good starting place for sites just getting set up with Exim In fact we recommend that you dont stray too far from it until you thoroughly understand theExim paradigm and need to elaborate on the default configuration for a specificpurpose Exim works hard to support common situations and has sensible defaultsIts also helpful to stick with the variable names used in the default config file Thesenaming conventions are assumed by folks on the eximusers mailing list Thosepeople are also a good resource to consult regarding your configuration questionsexim prints a message to stderr and exits if you have a syntax error in your configuration file It doesnt catch all syntax errors immediately however because it doesnot expand variables until it needs toThe order of entries in the configuration file is not quite arbitrary the global configuration options section must be first and must exist All other sections are optionaland can appear in any orderPossible sections include Global configuration options mandatory acl access control lists that filter addresses and messages authenticators for SMTP AUTH or TLS authentication routers ordered sequence to determine where a message should go transports definitions of the drivers that do the actual delivery retry policy settings for dealing with problem messages rewrite global address rewriting rules localscan a hook for fancy flexibilityEach section except the first starts with a begin sectionname statementfor example begin acl There is no end sectionname statement the end is signaled bythe next sections begin statement Indentation to show subordination makes theconfig file easier to read for humans but it is not meaningful to EximSome configuration statements name objects that will later be used to control theflow of messages Those names must begin with a letter and contain only lettersnumbers and the underscore character If the first nonwhitespace character ona line is the rest of the line is treated as a comment Note that this means youcannot put a comment on the same line as a statement it will not be recognized asa comment because the first character is not Exim lets you include files anywhere in the configuration file Two forms of include are usedinclude absolutepathincludeifexists absolutepathThe first form generates an error if the file does not exist Although include fileskeep your config file tidy they are read several times during the life of a messageso it might be best just to include their contents directly into your configurationGlobal optionsLots of stuff is specified in the global options section including operating parameters limits sizes timeouts properties of the mail server on this host list definitions local hosts local hosts to relay for remote domains to relay for and macroshostname contact location error messages SMTP bannerOptionsOptions are set with the basic syntaxoptionname valueswhere the values can be Booleans strings integers decimal numbers or time intervals Multivalued options are allowed in which case the various values are separated by colonsUse of the colon as a value separator presents a problem when you express IPvaddresses which use colons as part of the address You can escape the colons bydoubling them but the easiest and most readable fix is to redefine the separatorcharacter with the character as you assign values to the option For example bothof the following two lines set the value of the localhostinterfaces option whichcontains the IPv and IPv localhost addresseslocalinterfaces localinterfaces The second form in which the semicolon has been defined as the separator is morereadable and less fragileThere are a zillion optionsmore than in the options index of the documentation And we said sendmail was complicated Most options have sensible defaultsand all have descriptive names Its handy to have a copy of the docspectxt filefrom the distribution in your favorite text editor when you are researching a newoption We dont cover all the options below just the ones that occur in our example configuration bitsListsExim has four kinds of lists introduced by the keywords hostlist domainlistaddresslist and localpartslist Here are two examples that use hostlisthostlist myrelaylist myfriendexamplecomhostlist myrelaylist usrlocaleximrelayhoststxtMembers can be listed inline or taken from a file If inline they are separatedby colons There can be up to named lists of each type In the inline exampleabove we included all machines on a local network and a specific hostnameThe symbol can be a member of a list it means the name of the local host andhelps you write a single generic configuration file that works for most nonhub machines at your site The notation is also useful and means all IP addresses onwhich Exim is listening that is all the IP addresses of the local hostLists can include references to other lists and the character to indicate negationLists that include references to variables eg variablename make processingslower because Exim cannot cache the results of evaluating the list which it otherwise does by defaultTo reference a list just put in front of its name to match members of the list or to match nonmembers for example myrelaylist Omit space between the sign and the name of the listMacrosYou can use macros to define parameters error messages etc The parsing is primitive so you cannot define a macro whose name is a subset of another macro without unpredictable resultsThe syntax isMACRONAME rest of the lineFor example the first of the following lines defines a macro named ALIASQUERYthat looks up a users alias entry in a MySQL database The second line shows theuse of the macro to perform an actual lookup with the result being stored in thevariable called dataALIASQUERY select mailbox from user where login quotemysqllocalpartdata lookup mysqlALIASQUERYMacro names are not required to be all caps but they must begin with a capitalletter However the allcaps convention aids clarity The configuration file can include ifdefs that evaluate a macro and use it to determine whether or not to includea portion of the config file Every imaginable form of ifdef is supported they allbegin with a dotAccess control lists ACLsAccess control lists filter the addresses of incoming messages and either accept ordeny them Exim divides incoming addresses into a local part that represents theuser and a domain part that is the recipients domainACLs can be applied at any of the various stages of an SMTP conversation HELOMAIL RCPT DATA etc Typically an ACL enforces strict adherence to the SMTPprotocol at the HELO stage checks the sender and the senders domain at the MAILstage checks the recipients at the RCPT stage and scans the message content atthe DATA stageA slew of options named aclsmtpcommand specify which ACL should be appliedafter each command in the SMTP protocol For example the aclsmtprcpt optiondirects the ACL to run on each address that is a recipient of the message Anothercommonly used checkpoint is aclsmtpdata which checks the ACL against themessage after it has been received for example to scan contentYou can define ACLs in the acl section of the config file in a file that is referencedby the aclsmtpcommand option or inline when the option is definedA sample ACL called myaclcheckrcpt is defined below We would invoke it byassigning its name to the aclsmtprcpt option in the global options section ofthe config file If this ACL denies an address at the level of the RCPT commandthe sending server should give up and not try the address againThis is a long ACL specification so we break it up into digestible pieces that we candecode individuallyThe first portionbegin aclmyaclcheckrcpt accept hosts control dkimdisableverifyThe default name for this access control list is aclcheckrcpt you probably shouldnot change its name as we did here We used a nonstandard name simply to emphasize that the name is something you specify not a keyword thats special to EximThe first accept line containing just a colon is an empty list The empty list of remote hosts matches cases in which a local MUA submitted a message on the MTAsstandard input If the address being tested meets this condition the ACL acceptsthe address and disables DKIM signature validation which is turned on by defaultIf the address does not match this address clause control drops through to the nextclause in the ACL definition deny message Restricted characters in address domains localdomains localparts deny message Restricted characters in address domains localdomains localparts The first deny stanza is intended for messages coming into your local domains Itrejects any address whose local part the username starts with a dot or containsthe special characters or The second deny applies to messages beingsent out by your users It too disallows certain special characters and sequences inthe local parts of addresses in case your users machines have been infected witha virus or other malware In the past such addresses have been used by spammersto confuse ACLs or have been associated with other security problemsIn general if you are intending to use localparts supposedly the recipientsusername in a directory path to store mail or look for a vacation file for example be careful that your ACLs have filtered out any special characters that couldcause unwanted behavior The example looks for the sequence which couldbe problematic if the username were inserted into a path accept localparts postmaster domains localdomainsThis accept stanza guarantees that mail to postmaster always gets through if itssent to a local domain and that can help with debugging require verify senderThe require line checks to see if a bounce message can be returned however itchecks only the senders domain If the senders username has been forged abounce message could still fail that is the bounce message itself could bounceYou can add more extensive checking here by calling another program but somesites consider such callouts abusive and might add your mail server to a blacklistor badreputation list accept hosts relayfromhosts control submission control dkimdisableverifyThe above accept stanza checks for hosts that are allowed to relay through thishost namely local hosts that are submitting mail into the system The control linespecifies that Exim should act as a mail submission agent and fix up any headerdeficiencies as the message arrives from the user agent The recipients address isnot checked because many user agents are confused by error returns This part ofthe configuration is appropriate only for local machines that relay to a smart hostnot for any external domains you might be willing to relay for DKIM verificationis disabled because these messages are outbound from your users or relay friends accept authenticated control submission control dkimdisableverifyThe last accept stanza deals with local hosts that authenticate through SMTP AUTHOnce again these messages are treated as submissions from user agents require message Relay not permitted domains localdomains relaytodomains require verify recipientHere we check the destination domain to which the message is headed and requirethat it be either in our list of localdomains or in our list of domains to whichwe allow relaying relaytodomains These domain lists are defined outside thecontext of the ACL Any destinations not in one of those lists are refused with acustomized error message acceptFinally given that all previous requirements have been met but that no morespecificaccept or deny rule has been triggered we verify the recipient and accept the message Most Internet messages to local users fall into this category require means deny if not matchedWe havent included any blacklist scanning in the example above To access a blacklist use one of the examples in the default config file or something like this deny condition if isipsenderhostaddress authenticated hosts mywhitelistips dnslists listdnswlorg domains localdomains verify recipient message You are on RBL dnslistdomain dnslisttext dnslists zenspamhausorg logwrite Blacklisted sender senderhostaddress dnslistdomain dnslisttextTranslated to English the code specifies that if a message matches all of the following criteria it is rejected with a custom error message and logged also with acustom message Its from an IPv address some lists dont handle IPv correctly Its not associated with an authenticated SMTP session Its from a sender not in the local whitelist Its from a sender not in the global Internet whitelist Its addressed to a valid local recipient The sending host is on the zenspamhausorg blacklistThe variables dnslisttext and dnslistdomain are set by the assignment todnslists which triggers the blacklist lookup This deny clause could be placed rightafter your checks for unusual characters in addressesHeres another example ACL that rejects mail if the remote side does not say HELOproperlyaclcheckmaildeny message Bad command must send HELOEHLO first condition if defsenderheloname acceptExim solves the early talker problem a more specific case of not saying HELOproperly with the smtpenforcesync option which is turned on by defaultContent scanning at ACL timeExim supports powerful content scanning at several points in a messages traversal of the mail system at ACL time after the SMTP DATA command at deliverytime through the transportfilter option or with a localscan function after allACL checks have been completed You must compile support for content scanninginto Exim by setting the WITHCONTENTSCAN variable in the EDITME file it iscommented out by default This option endows ACLs with extra power and flexibility and adds two new configuration options spamdaddress and avscannerScanning at ACL time allows a message to be rejected inline with the MTAs conversation with the sending host The message is never accepted for delivery so itneed not be bounced This way of rejecting the message is nice because it avoidsbackscatter spam caused by bounce messages to forged sender addressesAuthenticatorsAuthenticators are drivers that interact with the SMTP AUTH commands challengeandresponse sequence and identify an authentication mechanism acceptableto both client and server Exim supports the following mechanisms AUTHCRAMMD RFC AUTHPLAINTEXT which includes both PLAIN and LOGIN AUTHSPA which supports Microsofts Secure Password AuthenticationIf Exim is receiving email it is acting as an SMTP AUTH server If it is sendingmail it is a client Options that appear in the definitions of authenticator instancesare tagged with a prefix of either server or client to allow for different configurations depending on the role Exim is playingAuthenticators are used in access control lists as in the following clause in the ACLexample on page deny authenticated Below is an example that shows both the clientside and serverside LOGIN mechanisms This simple example uses a fixed username and password which is OK forsmall sites but probably inadvisable for larger installationsbegin authenticatorsmyclientfixedlogin driver plaintext publicname LOGIN clientsend myusername mypasswdmyserverfixedlogin driver plaintext publicname LOGIN serveradvertisecondition if deftiscipher serverprompts User Name Password servercondition if and eqauthusername eqauthmypasswd serversetid authAuthentication data can come from many sources LDAP PAM etcpasswd etcThe serveradvertisecondition clause above prevents mail clients from sendingpasswords in the clear by requiring TLS security through STARTTLS or SSL onconnection If you want the same behavior when Exim acts as the client systemuse the clientcondition option in the client clause too again with tiscipherRefer to the Exim documentation for details of all possible authentication optionsand for examplesRoutersRouters work on recipient email addresses either by rewriting them or by assigningthem to a transport and sending them on their way A particular router can havemultiple instances each with different optionsYou specify a sequence of routers A message starts with the first router and progressesthrough the list until the message is either accepted or rejected The accepting routertypically hands the message to a transport driver Routers handle both incomingand outgoing messages They feel a bit like subroutines in a programming languageA router can return any of the dispositions shown in Table for a messageTable Exim router statusesStatus Meaningaccept The router accepts the address and hands it to a transport driverpass The router cant handle the address go on to the next routerdecline The router chooses not to handle the address next router pleasefail The address is invalid the router queues it for a bounce messagedefer The message is left in the queue to be dealt with latererror There is an error in the router specification the message is deferredIf a message receives a pass or decline from all the routers in the sequence it isunroutable Exim bounces or rejects such messages depending on the contextIf a message meets the preconditions for a router and the router ends with a nomorestatement then that message will not be presented to any additional routers regardless of its disposition by the current router For example if your remote SMTProuter has the precondition domains localdomains and has nomore setthen only messages to local users that is those that would fail the domains precondition will continue to the next router in the sequenceRouters have many possible options some common examples are preconditions acceptance or failure conditions error messages to return and transport drivers to useThe next few sections detail the routers called accept dnslookup manualroute andredirect The example configuration snippets assume that Exim is running on alocal machine in the examplecom domain Theyre all pretty straightforward referto the documentation if you want to use some of the fancier routersThe accept routerThe accept router labels an address as OK and passes the associated message to atransport driver Below are examples of accept router instances called localusersfor delivering local mail and savetofile for appending to an archivelocalusersdriver acceptdomains examplecomchecklocalusertransport mylocaldeliverysavetofiledriver acceptdomains dialupexamplecomtransport batchsmtpappendfileThe localusers router instance checks that the domain part of the destination address is examplecom and that the local part of the address is the login name of alocal user If both conditions are met the router hands the message to the transportdriver instance called mylocaldelivery which is defined in the transports sectionThe savetofile instance is designed for dialup users it appends the message toa file specified in the batchsmtpappendfile transport definitionThe dnslookup routerThe dnslookup router typically handles outgoing messages It looks up the MX record of the recipients domain and hands the message to an SMTP transport driverfor delivery Here is an instance called remoteusersremoteusersdriver dnslookupdomains examplecomtransport myremotedeliveryThe dnslookup code looks up the MX records for the addressee If no MX recordsexist it tries the A record A common extension to this router instance prohibitsdelivery to certain IP addresses a prime example is the RFC private addresses that cannot be routed on the Internet See the ignoretargethosts option formore informationThe manualroute routerThe flexible manualroute driver can pretty much route email in whatever way youwant The routing information can be a table of rules that match by recipient domainroutelist or a single rule that applies to all domains routedataSee page formore informationabout RFC private address spacesBelow are two examples of manualroute instances The first example implementsthe smart host concept in which all outgoing nonlocal mail is sent to a centralsmart host for processing This instance is called smarthost and applies to allrecipients domains that are not the character in the localdomains listsmarthostdriver manualroutedomains localdomainstransport remotesmtproutedata smarthostexamplecomThe router instance below firewall speaks SMTP to send incoming messages tohosts inside the firewall perhaps after scanning them for spam and viruses Itlooks up the routing data for each recipient domain in a DBM database that contains the names of local hostsfirewalldriver manualroutetransport remotesmtproutedata lookupdomain dbm internalhostroutesThe redirect routerThe redirect driver does address rewriting such as that called for in the systemwidealiases file or in a users forward file It usually does not assign the rewrittenaddress to a transport that task is left to other routers in the chainThe first instance shown below systemaliases looks up aliases with a linearsearch lsearch of the etcaliases file Thats fine for a small aliases file but ifyours is huge replace that linear search with a database lookup The second instanceuserforward first verifies that mail is addressed to a local user then checks thatusers forward filesystemaliasesdriver redirectdata lookuplocalpart lsearch etcaliasesuserforwarddriver redirectchecklocaluserfile homeforwardnoverifyThe checklocaluser option ensures that the recipient is a valid local user Thenoverify says not to verify the validity of the address to which the forward fileredirects the message just ship itPeruser filtering through forward filesExim not only allows forwarding through forward files but also allows filteringIt supports its own filtering system as well as the Sieve filtering that is being standardized by the IETF If the first line of a users forward file isExim filterorSieve filterthen the subsequent filtering commands there are about of them can determine where the message should be delivered Filtering does not actually delivermessagesit just meddles with the destination For exampleExim filterif headersubject contains sysadminthensave homemailsysadminendifLots of options are available to control what users can and cannot do in theirforward files The option names begin with forbid or allow Theyre importantbecause they can prevent users from running shells loading libraries into binariesor accessing the embedded Perl interpreter when they shouldnt Check for newforbid options when you upgrade to be sure your users cant get too fancy intheir forward filesTransportsRouters decide where messages should go and transports actually take them thereLocal transports typically append to a file pipe to a local program or speak theLMTP protocol to an IMAP server Remote transports speak SMTP to their counterparts across the InternetThere are five Exim transports appendfile lmtp smtp autoreply and pipe wedetail appendfile and smtp The autoreply transport is typically used to send vacation messages and the pipe transport hands messages as input to a commandthrough a UNIX pipe As with routers you must define instances of transports andits OK to have multiple instances of the same type of transport Order is significantfor routers but not for transportsThe appendfile transportThe appendfile driver stores messages in mbox mbx Maildir or mailstore format ina specified file or directory You must have included the appropriate mailbox formatswhen you compiled Exim they are commented out of the EDITME file by defaultThe following example defines the mylocaldelivery transport an instance ofthe appendfile transport referred to in the localusers router instance definitionon page mylocaldeliverydriver appendfilefile varmaillocalpartdeliverydateaddenvelopetoaddreturnpathaddgroup mailmode The various add lines add headers to the message The group and mode clausesensure that the transport agent can write to the fileThe smtp transportThe smtp transport is the workhorse of any mail system Here we define two instances one for the standard SMTP port and one for the mail submission port myremotedeliverydriver smtpmyremotedeliveryportdriver smtpport headersadd Xprocessedby MACROHEADER port The second instance myremotedeliveryport specifies the port and also aheader to be added to the message that includes an indication of the outgoing portMACROHEADER would be defined elsewhere in the configuration fileRetry configurationThe retry section of the configuration file must exist or Exim will never attempt redelivery of messages that could not be delivered on the first attempt You can specifythree time intervals each less frequent than the previous one After the last intervalhas expired messages bounce back to the sender as undeliverable retry statementsunderstand the suffixes m h d and w to indicate minutes hours days and weeksYou can specify different intervals for different hosts or domainsHeres what a retry section looks likebegin retry F h m F h h F d hThis example means For any domain an address that fails temporarily should beretried every minutes for hours then every hour for the next hours thenevery hours for days and finally bounced as undeliverableRewriting configurationThe rewriting section of the configuration file starts with begin rewrite Its usedto fix up addresses not to reroute messages For example you could use it on youroutgoing addresses To make mail appear to be from your domain not from individual hosts To map usernames to a standard format such as FirstLastDo not apply rewriting to addresses in incoming mailLocal scan functionTo further customize Exim for example to filter for the latest and greatest virus youcould write a C function that does your scanning and install it in the localscansection of the config file Refer to the Exim documentation for details and examples of how to do thisLoggingExim by default writes three different log files a main log a reject log and a panic log Each log entry includes the time the message was written You specify thelocation of the log files in the EDITME file before building Exim or in the runtime config file in the value of the logfilepath option By default logs are keptin the varspooleximlog directoryThe logfilepath option accepts up to two colonseparated values Each valuemust be either the keyword syslog or an absolute path with a s embedded wherethe names main reject and panic can be substituted For examplelogfilepath syslog varlogeximswould log both to syslog with facility mail and to the separate files eximmaineximreject and eximpanic in the varlog directory Exim submits the mainlog entries to syslog at priority info the reject entries at priority notice and thepanic entries at priority alertThe main log contains one line for the arrival and delivery of each message It can besummarized by the Perl script eximstats which is included in the Exim distributionThe reject log records information about messages that have been rejected for policy reasons malware spam etc It includes the summary line for the message fromthe main log and also the original headers of the message that was rejected If youchange your policies check the reject log to make sure that all is still wellThe panic log is for serious errors in the software exim writes here just beforeit gives up The panic log should not exist in the absence of problems Ask cronto check it for you and if it exists fix the problem that caused the panic and thendelete the file exim will recreate it when the next panicworthy situation arisesWhen debugging you can increase the amount and type of data logged Invoke thelogselector option For examplelogselector smtpconnection smtpincompletetransaction The logging categories that can be included or excluded by the logselector mechanism are listed in the Exim specification in the section called Log files toward theend About categories are defined including all which will really fill your disksexim also keeps a temporary log for each message it handles It is named with themessage ID and lives in varspooleximmsglog If you are having trouble with aparticular destination check thereDebuggingExim has powerful debugging aids You can configure the amount of informationyou want to see about each potential debugging topic exim d tells exim to go intodebugging mode in which it stays in the foreground and does not detach from theterminal You can add specific debugging categories to d with a or in front ofthem to verbosify or eliminate a category For example dexpandacl requestsregular debugging output plus extra details regarding string expansions and ACLinterpretation These two categories are common problem spots You can tunemore than categories of debugging information see the man page for a listA common technique when debugging mail systems is to start the MTA on a nonstandard port and then talk to it through telnet For example to start exim in daemon mode listening on port with debugging info turned on run sudo exim d oX bdYou can then telnet to port and type SMTP commands in an attempt to reproduce the problem you are debuggingAlternatively you can have swaks do your SMTP talking for you Its a Perl scriptthat makes SMTP debugging faster and easier swaks help gets you some documentation and jetmoreorgjohncodeswaks supplies complete detailsIf your log files show timeouts of around seconds thats suggestive of a DNS issue PostfixPostfix is another popular alternative to sendmail Wietse Venema started the Postfix project when he spent a sabbatical year at IBMs T J Watson Research Center in and he is still actively developing it Postfixs design goals included not onlysecurity first and foremost but also an open source distribution policy speedyperformance robustness and flexibility All major Linux distributions includePostfix and since version macOS has shipped Postfix instead of sendmail asits default mail systemThe most important things to know about Postfix are first that it works almost outof the box the simplest config files are only a line or two long and second that itleverages regular expression maps to filter email effectively especially in conjunctionwith the PCRE PerlCompatible Regular Expression library Postfix is compatiblewith sendmail in the sense that Postfixs aliases and forward files have the sameformat and semantics as those of sendmailPostfix speaks ESMTP Virtual domains and spam filtering are both supported Foraddress rewriting Postfix relies on table lookups from flat files Berkeley DB DBMLDAP NetInfo or SQL databasesPostfix architecturePostfix comprises several small cooperating programs that send network messagesreceive messages deliver email locally etc Communication among them is performedthrough local domain sockets or FIFOs This architecture is quite different fromthat of sendmail and Exim wherein a single large program does most of the workThe master program starts and monitors all Postfix processes Its configuration filemastercf lists the subsidiary programs along with information about how theyshould be started The default values set in that file cover most needs in generalno tweaking is necessary One common change is to comment out a program forexample smtpd when a client should not listen on the SMTP portThe most important server programs involved in the delivery of email are shownin Exhibit BExhibit B Postfix server programssmtplmtplocalvirtualpipeqmgrbouncetrivialrewritecleanupsmtpdpickupReceiving mailsmtpd receives mail entering the system through SMTP It also verifies that theconnecting clients are authorized to send the mail they are trying to deliver Whenemail is sent locally through the usrlibsendmail compatibility program a file isSee page for moreinformation aboutregular expressionswritten to the varspoolpostfixmaildrop directory That directory is periodicallyscanned by the pickup program which processes any new files it findsAll incoming email passes through cleanup which adds missing headers and rewrites addresses according to the canonical and virtual maps Before insertingmail into the incoming queue cleanup passes it through trivialrewrite whichdoes minor fixing of the addresses such as appending a mail domain to addressesthat are not fully qualifiedManaging mailwaiting queuesqmgr manages five queues that contain mail waiting to be delivered incoming mail that is arriving active mail that is being delivered deferred mail for which delivery has failed in the past hold mail blocked in the queue by the administrator corrupt mail that cant be read or parsedThe queue manager generally follows a simple FIFO strategy to select the next message to process but it also supports a complex preemption algorithm that prefersmessages with few recipients over bulk mailTo avoid overwhelming a receiving host especially one that has been down Postfixuses a slowstart algorithm to control how fast it tries to deliver email Deferredmessages are given a tryagain time stamp that exponentially backs off so as not towaste resources on undeliverable messages A status cache of unreachable destinations avoids unnecessary delivery attemptsSending mailqmgr aided by trivialrewrite decides where a message should be sent The routing decision made by trivialrewrite can be overridden through lookup tablestransportmapsDelivery to remote hosts via the SMTP protocol is performed by the smtp programlmtp delivers mail with LMTP the Local Mail Transfer Protocol defined in RFCLMTP is derived from SMTP but the protocol has been modified so that the mailserver is not required to manage a mail queue This mailer is particularly useful fordelivering email to mailbox servers such as the Cyrus IMAP suitelocals job is to deliver email locally It resolves addresses in the aliases table andfollows instructions found in recipients forward files Messages are forwarded toanother address passed to an external program for processing or stored in usersmail foldersThe virtual program delivers email to virtual mailboxes that is mailboxes thatare not related to a local UNIX account but that still represent valid email destinations Finally pipe implements delivery through external programsSecurityPostfix implements security at several levels Most of the Postfix server programs canrun in a chrooted environment They are separate programs with no parentchildrelationship None of them are setuid The mail drop directory is groupwritableby the postdrop group to which the postdrop program is setgidPostfix commands and documentationSeveral commandline utilities permit user interaction with the mail system postalias builds modifies and queries alias tables postcat prints the contents of queue files postconf displays and edits the main configuration file maincf postfix starts and stops the mail system must be run as root postmap builds modifies or queries lookup tables postsuper manages mail queues sendmail mailq newaliases are sendmailcompatible replacementsThe Postfix distribution includes a set of man pages that describe all the programsand their options Online documents at postfixorg explain how to configure andmanage various aspects of Postfix These documents are also included in the Postfixdistribution in the READMEFILES directoryPostfix configurationThe maincf file is Postfixs principal configuration file The mastercf file configuresthe server programs It also defines various lookup tables that are referenced frommaincf and that provide different types of service mappingsThe postconf man page describes every parameter you can set in the maincf fileThere is also a postconf program so if you just type man postconf youll get the manpage for that instead of postconf Use man s postconf to get the right versionThe Postfix configuration language looks a bit like a series of sh comments andassignment statements Variables can be referenced in the definition of other variables by being prefixed with a Variable definitions are stored just as they appearin the config file they are not expanded until they are used and any substitutionsoccur at that timeYou can create new variables by assigning values to them Be careful to choosenames that do not conflict with existing configuration variablesAll Postfix configuration files including the lookup tables consider lines startingwith whitespace to be continuation lines This convention results in readable configuration files but you must start new lines in column oneWhat to put in maincfMore than parameters can be specified in the maincf file However just a fewof them need to be set at an average site The author of Postfix strongly recommendsthat only parameters with nondefault values be included in your configuration Thatway if the default value of a parameter changes in the future your configurationwill automatically adopt the new valueThe sample maincf file that comes with the distribution includes many commentedoutexample parameters along with some brief documentation The original versionis best left alone as a reference Start with an empty file for your own configurationso that your settings do not become lost in a sea of commentsBasic settingsThe simplest possible Postfix configuration is an empty file Surprisingly this is aperfectly reasonable setup It results in a mail server that delivers email locally within the same domain as the local hostname and that sends any messages directed tononlocal addresses directly to the appropriate remote serversNull clientAnother simple configuration is a null client that is a system that doesnt deliveremail locally but rather forwards outbound mail to a designated central server Toimplement this configuration you define several parameters starting with mydomainwhich defines the domain part of the hostname and myorigin which is the maildomain appended to unqualified email addresses If these two parameters are thesame you can write something like thismydomain cscoloradoedumyorigin mydomainAnother parameter you should set is mydestination which specifies the mail domains that are local If the recipient address of a message has mydestination as itsmail domain the message is delivered through the local program to the corresponding user assuming that no relevant alias or forward file is found If more thanone mail domain is included in mydestination these domains are all consideredaliases for the same domainFor a null client you want no local delivery so leave this parameter emptymydestination Finally the relayhost parameter tells Postfix to send all nonlocal messages to aspecified host instead of sending them directly to their apparent destinationsrelayhost mailcscoloradoeduThe square brackets tell Postfix to treat the specified string as a hostname DNS Arecord instead of a mail domain name DNS MX recordSince null clients should not receive mail from other systems the last thing to doin a null client configuration is to comment out the smtpd line in the mastercf fileThis change prevents Postfix from running smtpd at all With just these few linesyouve defined a fully functional null clientFor a real mail server youll need a few more configuration options as well assome mapping tables We cover these in the next few sectionsUse of postconfpostconf is a handy tool that helps you configure Postfix When run without arguments it prints all the parameters as they are currently configured If you namea specific parameter as an argument postconf prints the value of that parameterThe d option makes postconf print the defaults instead of the currently configured values For example postconf mydestinationmydestination postconf d mydestinationmydestination myhostname localhostmydomain localhostAnother useful option is n which tells postconf to print only the parameters thatdiffer from the default If you ask for help on the Postfix mailing list thats the configuration information you should put in your emailLookup tablesMany aspects of Postfixs behavior are shaped through the use of lookup tableswhich can map keys to values or implement simple lists For example the defaultsetting for the aliasmaps table isaliasmaps dbmetcmailaliasesData sources are specified with the notation typepath Multiple values can be separated by commas spaces or both Table on the next page lists the availabledata sources postconf m shows this information as wellThe dbm and sdbm types are only for compatibility with the traditional sendmailalias table Berkeley DB hash is a more modern implementation its safer andfaster If compatibility is not a problem then go withaliasdatabase hashetcmailaliasesaliasmaps hashetcmailaliasesThe aliasdatabase specifies the table that is rebuilt by newaliases and shouldcorrespond to the table that you specify in aliasmaps The two parameters areseparate because aliasmaps might include nonDB sources such as mysql thatnever need to be rebuiltAll DBclass tables dbm sdbm hash and btree compile a text file to an efficientlysearchable binary format The syntax for these text files is similar to that of theTable Information sources for Postfix lookup tablesType Descriptiondbmsdbm Legacy dbm or gdbm database filecidr Network addresses in CIDR formhashbtree Berkeley DB hash table or Btree file replaces dbmldap LDAP directory servicemysql MySQL databasepcre Perlcompatible regular expressionspgsql PostgreSQL databaseproxy Access through proxymap eg to escape a chrootregexp POSIX regular expressionsstatic Return of value specified as path regardless of the keyunix The etcpasswd and etcgroup files aa unixpasswdbyname is the passwd file and unixgroupbyname is the group fileconfiguration files with respect to comments and continuation lines Entries arespecified as simple keyvalue pairs separated by whitespace except for alias tableswhich use a colon after the key to retain sendmail compatibility For example thefollowing lines are appropriate for an alias tablepostmaster david tobiaswebmaster eviAs another example heres an access table for relaying mail from any client with ahostname ending in cscoloradoeducscoloradoedu OKText files are compiled to their binary formats with the postmap command fornormal tables and the postalias command for alias tables The table specificationincluding the type must be given as the first argument For example sudo postmap hashetcpostfixaccesspostmap can also query values in a lookup table no match no output postmap q blabla hashetcpostfixaccess postmap q cscoloradoedu hashetcpostfixaccessOKLocal deliveryThe local program delivers mail to local recipients It also handles local aliasingFor example if mydestination is set to cscoloradoedu and email arrives for therecipient evicscoloradoedu local first consults the aliasmaps tables and thensubstitutes any matching entries recursivelyIf no aliases match local looks for a forward file in user evis home directory andfollows the instructions in this file if it exists The syntax is the same as for theright side of an alias map Finally if no forward file is found the email is delivered to evis local mailboxBy default local writes to standard mboxformat files under varmail You canchange that behavior with the parameters shown in Table Table Parameters for local mailbox delivery set in maincfParameter Descriptionhomemailbox Delivers mail to user under the specified relative pathmailspooldirectory Delivers mail to a central directory that serves all usersmailboxcommand Delivers mail with an external program typically procmailmailboxtransport Delivers mail through a service as defined in mastercf arecipientdelimiter Allows extended usernames see description belowa This option interfaces with mailbox servers such as the Cyrus imapdThe mailspooldirectory and homemailbox options normally generate mboxformat mailboxes but they can also produce Maildir mailboxes To request this behavior add a slash to the end of the pathnameIf recipientdelimiter is mail addressed to eviwhatevercscoloradoedu isaccepted for delivery to the evi account With this facility users can create specialpurpose addresses and sort their mail by destination address Postfix first attempts lookups on the full address and only if that fails does it strip the extendedcomponents and fall back to the base address Postfix also looks for a correspondingforwarding file forwardwhatever for further aliasingVirtual domainsTo host a mail domain on your Postfix mail server you have three choices List the domain in mydestination Delivery is performed as described abovealiases are expanded and mail is delivered to the corresponding accounts List the domain in the virtualaliasdomains parameter This optiongives the domain its own addressing namespace that is independent ofthe systems user accounts All addresses within the domain must be resolvable through mapping to real addresses outside of it List the domain in the virtualmailboxdomains parameter As with thevirtualaliasdomains option the domain has its own namespace Allmailboxes must live beneath a specified directoryList the domain in only one of these three places Choose carefully because manyconfiguration elements depend on that choice We have already reviewed the handling of the mydestination method The other options are discussed belowVirtual alias domainsIf a domain is listed as a value of the virtualaliasdomains parameter mail tothat domain is accepted by Postfix and must be forwarded to an actual recipienteither on the local machine or elsewhereThe forwarding for addresses in the virtual domain must be defined in a lookuptable included in the virtualaliasmaps parameter Entries in the table have theaddress in the virtual domain on the left side and the actual destination addresson the right An unqualified name on the right is interpreted as a local usernameConsider the following example from maincfmyorigin cscoloradoedumydestination cscoloradoeduvirtualaliasdomains admincomvirtualaliasmaps hashetcmailadmincomvirtualIn etcmailadmincomvirtual we could then have the linespostmasteradmincom evi davidadmincomdavidadmincom davidschweikertcheviadmincom eviMail for eviadmincom would be redirected to evicscoloradoedu myoriginis appended and would ultimately be delivered to the mailbox of user evi becausecscoloradoedu is included in mydestinationDefinitions can be recursive the right hand side can contain addresses that are further defined on the left hand side Note that the right hand side can only be a listof addresses To execute an external program or to use include files redirect theemail to an alias which can then be expanded according to your needsTo keep everything in one file set virtualaliasdomains to the same lookuptable as virtualaliasmaps and put a special entry in the table to mark it as avirtual alias domain In maincfvirtualaliasdomains virtualaliasmapsvirtualaliasmaps hashetcmailadmincomvirtualIn etcmailadmincomvirtualadmincom notusedpostmasteradmincom evi davidadmincomThe right hand side of the entry for the mail domain admincom is never actuallyused admincoms existence in the table as an independent entry is enough to makePostfix consider it a virtual alias domainVirtual mailbox domainsDomains listed under virtualmailboxdomains are similar to local domains butthe list of users and their corresponding mailboxes must be managed independentlyof the systems user accountsThe parameter virtualmailboxmaps points to a table that lists all valid users inthe domain The map format isuserdomain pathtomailboxIf the path ends with a slash the mailboxes are stored in Maildir format The valueof virtualmailboxbase is always prefixed to the specified pathsYou often want to alias some of the addresses in the virtual mailbox domain Avirtualaliasmap will do that for you Here is a complete example In maincfvirtualmailboxdomains admincomvirtualmailboxbase varmailvirtualvirtualmailboxmaps hashetcmailadmincomvmailboxesvirtualaliasmaps hashetcmailadmincomvaliasesetcmailadmincomvmailboxes might contain entries like theseeviadmincom nemethevietcmailadmincomvaliases might containpostmasteradmincom eviadmincomYou can use virtual alias maps even on addresses that are not within virtual aliasdomains Virtual alias maps let you redirect any address from any domain independently of the type of the domain canonical virtual alias or virtual mailboxSince mailbox paths can only be put on the right hand side of the virtual mailboxmap this mechanism is the only way to set up aliases in that domainAccess controlMail servers should relay mail for third parties only on behalf of trusted clients Ifa mail server forwards mail from unknown clients to other servers it is a socalledopen relay which is bad See page for more detailsFortunately Postfix doesnt act as an open relay by default In fact its defaults arequite restrictive you are more likely to need to liberalize the permissions than totighten them Access control for SMTP transactions is configured in Postfix throughaccess restriction lists The parameters shown in Table control what shouldbe checked during the different phases of an SMTP sessionTable Postfix parameters for SMTP access restrictionParameter When appliedsmtpdclientrestrictions On connection requestsmtpddatarestrictions On DATA command mail bodysmtpdetrnrestrictions On ETRN commandasmtpdhelorestrictions On HELOEHLO command start of the sessionsmtpdrecipientrestrictions On RCPT TO command recipient specificationsmtpdrelayrestrictions On relay attempt to a third party domainsmtpdsenderrestrictions On MAIL FROM command sender specificationa This is a special command used for resending messages in the queueThe most important parameter is smtpdrecipientrestrictions Thats becauseaccess control is most easily performed when the recipient address is known andcan be identified as being local or not All other parameters in Table are emptyin the default configuration The default value issmtpdrecipientrestrictions permitmynetworksrejectunauthdestinationEach of the specified restrictions is tested in turn until a definitive decision aboutwhat to do with the mail is reached Table shows the common restrictionsTable Common Postfix access restrictionsRestriction Functioncheckclientaccess Checks client host address through a lookup tablecheckrecipientaccess Checks recipient mail address through a lookup tablepermitmynetworks Grants access to addresses listed in mynetworksrejectunauthdestination Rejects mail for nonlocal recipients no relayingEverything can be tested in these restrictions not just specific information likethe sender address in the smtpdsenderrestrictions Therefore for simplicity you might want to put all the restrictions under a single parameter Make thatsmtpdrecipientrestrictions because it is the only one that can test everythingexcept the DATA partsmtpdrecipientrestrictions and smtpdrelayrestrictions are where mailrelaying is tested Keep the rejectunauthdestination restriction and carefullychoose the permit restrictions before itAccess tablesEach restriction returns one of the actions shown in Table Access tables areused in restrictions such as checkclientaccess and checkrecipientaccess toselect an action according to the client host address or recipient address respectivelyTable Actions for access tablesAction Meaningnn text Returns temporary error code nn and message textnn text Returns permanent error code nn and message textDEFERIFPERMIT If restrictions result in PERMIT changes it to a temp errorDEFERIFREJECT If restrictions result in REJECT changes it to a temp errorDISCARD Accepts the message but silently discards itDUNNO Pretends the key was not found tests further restrictionsFILTER transportdest Passes the mail through the filter transportdestHOLD Blocks the mail in the queueOK Accepts the mailPREPEND header Adds a header to the messageREDIRECT addr Forwards the mail to a specified addressREJECT Rejects the mailWARN message Enters the given warning message in the logsFor example suppose you wanted to allow relaying for all machines within thecscoloradoedu domain and that you wanted to allow only trusted clients to postto the internal mailing list newslettercscoloradoedu You could implementthese policies with the following lines in maincfsmtpdrecipientrestrictions permitmynetworkscheckclientaccess hashetcpostfixrelayingaccessrejectunauthdestinationcheckrecipientaccess hashetcpostfixrestrictedrecipientsNote that commas are optional when the list of values for a parameter is specifiedIn etcpostfixrelayingaccesscscoloradoedu OKIn etcpostfixrestrictedrecipientsnewslettercscoloradoedu REJECT Internal listThe text after REJECT is an optional string that is sent to the client along with theerror code It tells the sender why the mail was rejectedAuthentication of clients and encryptionFor users sending mail from home it is usually easiest to route outgoing mail throughthe home ISPs mail server regardless of the sender address that appears on thatmail Most ISPs trust their direct clients and allow relaying If this configuration isntpossible or if you are using a system such as Sender ID or SPF ensure that mobileusers outside your network can be authorized to submit messages to your smtpdThe solution to this problem is to have the SMTP AUTH mechanism authenticatedirectly at the SMTP level Postfix must be compiled with support for the SASL library to make this work You can then configure the feature like thissmtpdsaslauthenable yessmtpdrecipientrestrictions permitmynetworkspermitsaslauthenticatedYou also need to support encrypted connections to avoid sending passwords inclear text Add lines like the following to maincfsmtpdtlssecuritylevel maysmtpdtlsauthonly yessmtpdtlsloglevel smtpdtlsreceivedheader yessmtpdtlscertfile etccertssmtppemsmtpdtlskeyfile smtpdtlscertfilesmtpdtlsprotocols SSLvYou need to put a properly signed certificate in etccertssmtppem Its also a goodidea to turn on encryption on outgoing SMTP connectionssmtptlssecuritylevel maysmtptlsloglevel DebuggingWhen you have a problem with Postfix first check the log files The answers to yourquestions are most likely there its just a question of finding them Every Postfixprogram normally issues a log entry for every message it processes For examplethe trail of an outbound message might look like thisAug nova postfixpickup EA uidfromdwseeethzchAug nova postfixcleanup EAmessageid GAeeethzchAug nova postfixqmgr EAfromdwseeethzch sizenrcpt queue activeAug nova postfixsmtp EAtoevieeethzchrelaytardiseeethzchdelay statussent Ok queued as DDBAug nova postfixqmgr EA removedAs you can see the interesting information is spread over many lines Note that theidentifier EA is common to every line Postfix assigns a queue ID as soon asa message enters the mail system and never changes it Therefore when searchingthe logs for the history of a message first concentrate on determining the messagesqueue ID Once you know that its easy to grep the logs for all the relevant entriesPostfix is good at logging helpful messages about problems that it notices Howeverits sometimes difficult to spot the important lines among the thousands of normalstatus messages This is a good place to consider using some of the tools discussedin the section Management of logs at scale which starts on page Looking at the queueAnother place to look for problems is the mail queue As in the sendmail systema mailq command prints the contents of a queue You can use it to see if and whya message has become stuckAnother helpful tool is the qshape script thats shipped with recent versions of PostfixIt shows summary statistics about the contents of a queue The output looks like this sudo qshape deferred T TOTAL expncom chinabankph probhelperbiz qshape summarizes the given queue here the deferred queue sorted by recipientdomain The columns report the number of minutes the relevant messages have beenin the queue For example you can see that messages bound for expncom havebeen in the queue longer than minutes All the destinations in this example aresuggestive of messages having been sent from vacation scripts in response to spamqshape can also summarize by sender domain with the s flagSoftbouncingIf softbounce is set to yes Postfix sends temporary error messages whenever itwould normally send permanent error messages such as user unknown or relaying denied This is a great testing feature it lets you monitor the disposition ofmessages after a configuration change without the risk of permanently losing legitimate email Anything you reject will eventually come back for another try Dontforget to turn off this feature when you are done testing or you will have to dealwith every rejected message over and over again Recommended readingRather than jumble together the references listed here weve sorted them by MTAand topicsendmail referencesCostales Bryan Claus Assmann George Jansen and Gregory Neil Shapiro sendmail th Edition Sebastopol CA OReilly Media This book is the definitive tome for sendmail configuration pages worthIt includes a sysadmin guide as well as a complete reference section An electronic edition is available too The author mix includes two key sendmail developersClaus and Greg who enforce technical correctness and add insight to the mixInstallation instructions and a good description of the configuration file are covered in the Sendmail Installation and Operation Guide which can be found in thedocop subdirectory of the sendmail distribution This document is quite completeand in conjunction with the README file in the cf directory gives a good nutsandbolts view of the sendmail systemsendmailorg sendmailorgca and sendmailorggshapiro all contain documentsHOWTOs and tutorials related to sendmailExim referencesHazel Philip The Exim SMTP Mail Server Official Guide for Release nd Edition Cambridge UK User Interface Technologies Ltd Hazel Philip Exim The Mail Transfer Agent Sebastopol CA OReilly Media The Exim specification is the defining document for Exim configuration It is quitecomplete and is updated with each new distribution A text version is included in thefile docspectxt in the distribution and a PDF version is available from eximorgThe web site also includes several howto documentsPostfix referencesDent Kyle D Postfix The Definitive Guide Sebastopol CA OReilly Media Hildebrandt Ralf and Patrick Koetter The Book of Postfix State of the ArtMessage Transport San Francisco CA No Starch Press This book is the best it guides you through all the details of Postfix configurationeven for complex environments The authors are active in the Postfix communityand participate regularly on the postfixusers mailing list The book is unfortunatelyout of print but used copies are readily availableRFCsRFCs updated by and updated by are the current versionsof RFCs and They define the SMTP protocol and the formats of messagesand addresses for Internet email RFCs and cover extensions for internationalized email addresses There are currently almost emailrelated RFCstoo many to list here See the general RFC search engine at rfceditororg for moreUNIX and Linux are the predominant platforms for serving web applications According to data from wtechscom of the top one million web sites are servedby either Linux or FreeBSD Above the OS level open source web server softwarecommands more than of the marketAt scale web applications do not run on a single system Instead a collection of software components distributed through a meshwork of systems cooperate to answerrequests as quickly and as flexibly as possible Each piece of this architecture mustbe resilient to server failures load spikes network partitions and targeted attacksCloud infrastructure helps address these needs Its ability to provision capacity quicklyin response to demand is an ideal match for the sudden and sometimes unexpectedtidal waves of users that materialize on the web In addition cloud providers addon services include a variety of convenient recipes that meet common requirementsgreatly simplifying the design deployment and operation of web systems HTTP the Hypertext Transfer ProtocolHTTP is the core network protocol for communication on the web Lurking beneatha deceptively simple facade of stateless requests and responses lie layers of refine Web Hostingments that bring both flexibility and complexity A wellrounded understandingof HTTP is a core competency for all system administratorsIn its simplest form HTTP is a clientserver onerequestoneresponse protocolClients also called user agents submit requests for resources to an HTTP serverServers receive incoming requests and process them by retrieving files from localdisks resubmitting them to other servers querying databases or performing anynumber of other possible computations A typical page view on the web entailsdozens or hundreds of such exchangesAs with most Internet protocols HTTP has adapted over time albeit slowly Thecentrality of the protocol to the modern Internet makes updates a highstakesproposition Official revisions are a slog of committee meetings mailing list negotiations public review periods and maneuvering by stakeholders with vested andconflicting interests During the long gaps between official revisions documentedin RFCs unofficial protocol extensions are born from necessity become ubiquitousand are eventually included as features in the next specificationHTTP versions and are sent over the wire in plain text Adventurous administrators can interact with servers directly by running telnet or netcat Theycan also observe and collect HTTP exchanges by using protocolagnostic packetcapture software such as tcpdumpThe web is in the process of adopting HTTP a major protocol revision that preserves compatibility with previous versions but introduces a variety of performanceimprovements In an effort to promote the universal use of HTTPS secure encrypted HTTP for the next generation of the web major browsers such as Firefox andChrome have elected to support HTTP only over TLSencrypted connectionsHTTP moves from plain text to binary format in an effort to simplify parsingand improve network efficiency HTTPs semantics remain the same but becausethe transmitted data is no longer directly legible to humans generic tools such astelnet are no longer useful The handy hi commandline utility part of the Golanguage networking repository at githubcomgolangnet helps restore some interactivity and debuggability to HTTP connections Many HTTPspecific toolssuch as curl also support HTTP nativelyUniform Resource Locators URLsA URL is an identifier that specifies how and where to access a resource URLs arenot HTTPspecific they are used for other protocols as well For example mobileoperating systems use URLs to facilitate communication among appsYou may sometimes see the acronyms URI Uniform Resource Identifier and URNUniform Resource Name used as well The exact distinctions and taxonomic relationships among URLs URIs and URNs are vague and unimportant Stick with URLThe general pattern for URLs is schemeaddress where scheme identifies the protocolor system being targeted and address is some string thats meaningful within thatSee page forgeneral information about TLSscheme For example the URL mailtoulsahadmincom encapsulates an emailaddress If its invoked as a link target on the web most browsers will bring up apreaddressed window for sending mailFor the web the relevant schemes are http and https In the wild you might also seethe schemes ws WebSockets wss WebSockets over TLS ftp ldap and many othersThe address portion of a web URL allows quite a bit of interior structure Heresthe overall patternschemeusernamepasswordhostnameportpathqueryanchorAll the elements are optional except scheme and hostnameThe use of a username and password in the URL enables HTTP basic authentication which is supported by most user agents and servers In general its a badidea to embed passwords into URLs because URLs are apt to be logged sharedbookmarked visible in ps output etc User agents can get their credentials from asource other than the URL and that is typically a better option In a web browserjust leave the credentials out and let the browser prompt you for them separatelyHTTP basic authentication is not selfsecuring which means that the password isaccessible to anyone who listens in on the transaction Therefore basic authentication should really only be used over secure HTTPS connectionsThe hostname can be a domain name or IP address as well as an actual hostnameThe port is the TCP port number to connect to The http and https schemes defaultto ports and respectivelyThe query section can include multiple parameters separated by ampersands Eachparameter is a keyvalue pair For example Adobe InDesign users may find thefollowing URL eerily familiarhttpadobecomsearchindexcfmtermindesigncrashlocenusAs with passwords sensitive data should never appear as a URL query parameterbecause URL paths are often logged as plain text The alternative is to transmit parameters as part of the request body You cant really control this in other peoplesweb software but you can make sure your own site behaves properlyThe anchor component identifies a subtarget of a specific URL For example Wikipedia uses named anchors extensively as section headings allowing specific partsof an entry to be linked to directlyStructure of an HTTP transactionHTTP requests and responses are similar in structure After an initial line bothinclude a sequence of headers a blank line and finally the body of the messagecalled the payloadSee page for moredetails about HTTPbasic authenticationHTTP requestsThe first line of a request specifies an action for the server to perform It consists ofa request method also known as the verb a path on which to perform the actionand the HTTP version to use For example a request to retrieve a toplevel HTMLpage might look like thisGET indexhtml HTTPTable shows the common HTTP request methods Verbs marked as safeshould not change the servers state However this is more a convention than amandate Its ultimately up to the software that handles the request to decide howto interpret the verbTable HTTP request methodsVerb Safe PurposeGET Yes Retrieves the specified resourceHEAD Yes Like GET but requests no payload retrieves metadata onlyDELETE No Deletes the specified resourcePOST No Applies request data to the given resourcePUT No Similar to POST but implies replacement of existing contentsOPTIONS Yes Shows what methods the server supports for the specified pathGET is by far the most commonly used HTTP verb followed by POST REST APIsdiscussed in Application programming interfaces APIs on page are more likelyto employ the more exotic verbs such as PUT and DELETEHTTP responsesThe initial line in a response called the status line indicates the disposition of therequest It looks like thisHTTP OKThe important part is the threedigit numeric status code The phrase that followsit is a helpful English translation that software ignoresThe first digit in the code determines its class that is the general nature of the result Table on the next page shows the five defined classes Within a class additional detail is provided by the remaining two digits More than status codesare defined but only a few of these are commonly seen in the wild The distinction between POST and PUT is subtle and largely of concern to web API developers PUTsshould be idempotent meaning that a PUT can be repeated without causing ill effects For examplea transaction that causes the server to send email should not be represented as a PUT The rules forHTTP caching also differ significantly between PUT and POST See RFC for more detailsTable HTTP response classesCode General indication Examplesxx Request received processing continues Switching Protocolsxx Success OK Createdxx Further action needed Moved Permanently Foundaxx Unsatisfiable request Forbidden Not Foundxx Server or environment failure Service Unavailablea Most often used inappropriately according to the spec for temporary redirectsHeaders and the message bodyHeaders specify metadata about a request or response such as whether to allowcompression what types of content are accepted expected or provided and howintermediate caches should handle the data For requests the only required header is Host which is used by web server software to determine which site is beingcontactedTable shows some common headersTable Commonly encountered HTTP headersName example value Dir a ContentHost wwwadmincom Domain name and port being requestedContentType applicationjson Data format wanted or containedAuthorization Basic QWxFtZ Credentials for HTTP basic authenticationLastModified Wed Sep Objects last known modification dateCookie flavoroatmeal Cookie returned from a user agentContentLength Length of the body in bytesSetCookie flavoroatmeal Cookie to be stored by the user agentUserAgent curl User agent submitting the requestServer nginx Server software responding to the requestUpgrade HTTP Request to change to another protocolExpires Sat Oct Length of time the response can be cachedCacheControl maxage Like Expires but allows more controla Direction requestonly responseonly or bothTable is by no means a definitive list In fact both sides of the transaction caninclude any headers they wish Both sides must ignore headers they dont understandHeaders are separated from the message body by a blank line For requests thebody can include parameters for POST or PUT requests or the contents of a fileto upload For responses the message body is the payload of the resource beingrequested eg HTML image data or query results The message body is not necessarily humanreadable since it can contain images or other binary data The bodycan also be empty as for GET requests or most error responsescurl HTTP from the command linecurl cURL is a handy commandline HTTP client thats available for most platforms Here we use curl to explore an HTTP exchangeBelow is an invocation of curl that requests the root of the web site admincom onTCP port which is the default for unencrypted nonHTTPS requests The response payload ie the admincom homepage and some informative messagesfrom curl itself have been hidden by the o devnull and s flags We also includethe v flag to request that curl display verbose output which includes headers curl s v o devnull httpadmincom Rebuilt URL to httpadmincom Hostname was NOT found in DNS cache Trying Connected to admincom port GET HTTP UserAgent curl Host admincom Accept HTTP OK Date Mon Apr GMT Server Apache Ubuntu is not blacklisted Server Apache Ubuntu LastModified Sat Feb GMT ETag ddbcc AcceptRanges bytes ContentLength Vary AcceptEncoding ContentType texthtml bytes data Connection to host admincom left intact By convention custom and experimental headers were originally prefixed with X But some Xheaders such as XForwardedFor became de facto standards and it was then infeasible to removethe prefix because that would break compatibility The use of X is now deprecated by RFC Administrators will also encounter libcurl a client library that developers can use to give their ownsoftware curllike superpowersLines starting with and denote the request and response respectively In therequest the client tells the server that the user agent is curl that its looking forhost admincom and that it will accept any type of content as a response The server identifies itself as Apache and replies with contents of type HTML alongwith a variety of other metadataWe can set headers explicitly with curls H argument This feature is especiallyhandy for making requests directly against IP addresses bypassing DNS For example we could check that the server for wwwadmincom responds identically torequests targeted at admincom by setting the Host header which informs the remote server of the domain the user agent is attempting to contact curl H Host wwwadmincom s v o devnull same output as the previous example but with a different Host headerWe use the O argument to download a file This example downloads a tarball ofthe curl source code to the current directory curl O httpcurlhaxxsesnapshotscurltargzWeve only scratched the surface of curls capabilities It can handle other requestmethods such as POST and DELETE store and submit cookies download filesand assist with many different debugging scenariosGoogles Chrome browser offers a feature called Copy as cURL that creates a curlcommand to simulate the browsers own behavior including headers cookies andother details You can easily retry requests with various adjustments and see theresults exactly as the browser would Rightclick a resource name in the Networktab of the developer tools panel to uncover this optionTCP connection reuseTCP connections are expensive In addition to the memory needed to maintainthem the threeway handshake used to establish each new connection adds latencyequivalent to a full round trip before an HTTP request can even beginThe HTTP Archive a project that tracks web statistics estimates that the averagesite incurs requests for resources per page load If each resource required a newTCP connection the performance of the web would be atrocious indeed This wasin fact the case early in the life of the webThe original HTTP specification did not include any provisions for connectionreuse but some adventurous developers added experimental support as an extensionThe Connection KeepAlive header was added informally to clients and serversthen improved and made the default in HTTP With keepalive also knownas persistent connections HTTP clients and servers send multiple requests over a TCP Fast Open TFO is a proposal that aims to improve this situation by allowing the SYN andSYNACK packets of TCPs threeway handshake to also carry data See RFCsingle connection thus saving some of the cost and latency of initiating and tearingdown multiple connectionsTCP overhead turns out to be nontrivial even when HTTP persistent connections are enabled Most browsers open as many as six parallel connections to theserver to improve performance Busy servers in turn must maintain many thousands of TCP connections in various states resulting in network congestion andwasted resourcesHTTP introduces multiplexing as a solution allowing several transactions to beinterleaved on a single connection HTTP servers can therefore support moreclients per system since each client imposes lower overheadHTTP over TLSOn its own HTTP provides no networklevel security The URL headers andpayload are open to inspection and modification at any point between the clientand server A malevolent infiltrator can intercept messages alter their contents orredirect requests to servers of its choiceEnter Transport Layer Security TLS which runs as a separate layer between TCPand HTTP TLS supplies only the security and encryption for the connection itdoes not involve itself at the HTTP layerThe user agent verifies the servers identity as part of the TLS connection processeliminating the possibility of spoofing by counterfeit servers Once the connectionis established its contents are protected against snooping and modification for theduration of the exchange Attackers can still see the host and port used at the TCPlayer but they cannot access HTTP details such as the URL of a request or theheaders that accompany itSee Transport Layer Security on page for more details on TLS cryptographyVirtual hostsIn the early days of the web a server typically hosted only a single web site Whenadmincom was requested for example clients performed a DNS lookup to findthe IP address associated with that name and then sent an HTTP request to port at that address The server at that address knew that it was dedicated to hostingadmincom and served results accordinglyAs web use increased administrators realized that they could achieve economiesof scale if a single server could host more than one site at once But how do youdistinguish requests bound for admincom from those bound for examplecom ifboth kinds of traffic end up at the same network port The precursor of TLS was known as SSL the Secure Sockets Layer All versions of SSL are obsoleteand formally deprecated but the name SSL remains in wide colloquial use Outside of cryptographiccontexts assume that references to SSL really mean TLSOne possibility is to define virtual network interfaces effectively permitting severaldifferent IP addresses to be bound to a single physical connection Most systemsallow this and it works fine but the scheme is fiddly and requires management atseveral different layersA better solution virtual hosts was provided by HTTP in RFC This schemedefines a Host HTTP header that user agents set explicitly to indicate what sitetheyre attempting to contact Servers examine the header and behave accordinglyThis convention conserves IP addresses and simplifies management especially forsites that have hundreds or thousands of web sites on a single serverHTTP requires user agents to provide a Host header so virtual hosts are nowthe standard way that web servers and administrators handle server consolidationThe use of namebased virtual hosts in combination with TLS is a bit tricky underthe hood TLS certificates are issued to specific hostnames which are chosen whenthe certificate is generated A TLS connection must be established before the server can read the Host header from the HTTP request but without that header theserver does not know which virtual host it should be impersonating and hencewhich certificate to selectThe solution is SNI Server Name Indication in which the client submits the hostname that its requesting as part of the initial TLS connection message Modernservers and clients all handle SNI automatically Web software basicsA rich library of open source software facilitates the construction of flexible resilient web applications Table lists a few general categories of services that speakHTTP and perform specific functions within the web application stackTable Partial list of HTTP server typesType Purpose ExamplesApplication server Runs web app code interfaces to web servers Unicorn TomcatCache Speeds access to frequently requested content Varnish SquidLoad balancer Relays requests to downstream systems Pound HAProxyWeb app firewall a Inspects HTTP traffic for common attacks ModSecurityWeb server Serves static content couples to other servers Apache NGINXa Often abbreviated WAFA web proxy is an intermediary that receives HTTP requests from clients optionally performs some processing and relays the requests to their ultimate destination Proxies are normally transparent to clients Load balancers web applicationfirewalls and cache servers are all specialized types of proxy servers A web serveralso acts as a sort of proxy if it relays requests to application serversExhibit A illustrates the role that each service plays in an HTTP exchange Requestscan be fulfilled higher in the stack if the requested resource can be satisfied or rejected with a xx or xx code if a problem occurs Requests that require a query tothe database traverse every layerExhibit A Components of a web application stackWeb application rewallApplication serverCaching layerWeb serverLoad balancerSearches for malicious requestsForwards to a healthy cache serverServes from cache if possibleResponds with static content if possibleProcesses request and formulates a responseInbound requests Outbound responsesTo maximize availability each layer should run on more than one node simultaneously Ideally redundancy should span geographical regions so that the overalldesign is not dependent on any single physical data center This goal is a lot easierto achieve when you build on a cloud platform that offers welldefined geographicregions as a fundamental building block as most doRealworld architectures arent usually as straightforward as Exhibit A suggests Inaddition most web software components perform functions in more than one areaNGINX is best known as a web server for example but its also a highly capablecache and load balancer An NGINX web server with caching features enabled ismore efficient than a stack of separate servers running on individual virtual machinesWeb servers and HTTP proxy softwareMost sites use web servers either to proxy HTTP connections to application servers orto serve static content directly A few of the features provided by web servers include Virtual hosts allowing many sites to coexist peacefully within a single server Handling of TLS connections Configurable logging that tracks requests and responses HTTP basic authentication Routing to different downstream systems according to requested URLs Execution of dynamic content through application serversThe leading open source web servers are the Apache HTTP Server known colloquially as httpd and NGINX which is pronounced engine XNetcraft an English Internet research and security company publishes monthlymarket share statistics for web servers As of June Netcraft shows that around of active web sites run Apache NGINX accounts for and has been steadily rising since Apache httpd is the original project from the Apache Software Foundation nowknown for supporting a variety of excellent open source projects httpd has beenunder active development since and is widely regarded as the reference HTTPserver implementationNGINX is a versatile server designed for speed and efficiency Like httpd NGINXsupports service of static web content load balancing monitoring of downstreamservers proxying caching and other related functionsSome development systems notably Nodejs and the language Go implement webservers internally and can handle many HTTP workflows without the need for aseparate web server These systems incorporate sophisticated connection management features and are robust enough for production workloadsThe HO server hoexampenet note the numeral one in exampe is a newerweb server project that takes full advantage of HTTP features and achieves evenbetter performance than NGINX Because it was first released in it cant claimthe track record of Apache or NGINX On the other hand neither is it constrainedby historical implementation decisions as is httpd Its certainly worth a look fornew deploymentsIts hard to make strong recommendations among these options because theyre allquite good That said for mainstream production use we suggest NGINX It offersexceptional performance and a relatively simple and modern configuration systemLoad balancersYou cant run a highly available web site on a single server Not only does this configuration expose your users to every potential hiccup experienced by the serverbut it also gives you no way to update the software operating system or configuration without downtimeSingle servers are also exquisitely vulnerable to load spikes and intentional attacksThe more overloaded a server becomes the more time it spends thrashing insteadof getting useful work done Past a certain load threshold one that you will haveto discover through bitter experience performance craters abruptly rather thandegrading gracefullyTo avoid these problems you can use a load balancer which is a type of proxy serverthat distributes incoming requests among a set of downstream web servers Loadbalancers also monitor the status of those servers to ensure that they are providingtimely and correct responsesExhibit B shows the placement of load balancers in an architecture diagramExhibit B The role of a load balancerInternet DMZClientsLoad balancerWeb serversSecure netLoad balancers solve many of the problems inherent in a singlesystem design Load balancers do not process requests but merely route them to othersystems As a result they can handle many more concurrent requests thandoes a typical web server When a web server needs a software upgrade or has to be taken offlinefor any other reason it can easily be removed from the rotation If one of the servers experiences a problem the healthcheck mechanismon the load balancer detects the problem and removes the errant systemfrom the server pool until it becomes healthy againTo avoid becoming single points of failure themselves load balancers usually runin pairs Depending on the configuration one balancer might act as a passivebackup while the other serves live traffic or both balancers might serve requestssimultaneouslyThe way that requests are distributed is usually configurable Here are a few common algorithms Round robin in which requests are distributed among the active serversin a fixed rotation order Load equalization in which new requests go to the downstream serverthats currently handling the smallest number of connections or requests Partitioning in which the load balancer selects a server according to ahash of the clients IP address This method ensures that requests fromthe same client always reach the same web serverLoad balancers normally operate at layer four of the OSI model in which theyroute requests based just on the IP address and port of the request However theycan also operate at layer seven by inspecting requests and routing them accordingto their target URL cookie values or other HTTP headers For example requestsfor examplecomlinux might route to a separate set of servers than do requests forexamplecombsdAs an added bonus load balancers can improve security They usually reside in theDMZ portion of a network and proxy requests to web servers behind an internalfirewall If HTTPS is in use they also perform TLS termination the connectionfrom the client to the load balancer uses TLS but the connection from the loadbalancer to the web server can be vanilla HTTP This arrangement offloads someprocessing overhead from the web serversLoad balancers can distribute other kinds of traffic in addition to or instead ofHTTP A common use is to add a load balancer that distributes requests to databases such as MySQL or RedisThe most common open source load balancers for UNIX and Linux are NGINXalready introduced as a web server and HAProxy a highperformance TCP andHTTP proxy beloved by veteran administrators for its flexible configuration stabilityand robust performance Both are excellent and well documented and both havelarge user communities Apache httpd also has a loadbalancing module thoughwe havent seen it used as widelyCommercial load balancers such as those from F and Citrix are available both ashardware devices to be installed in a data center and as software solutions They typically offer a graphical configuration interface more features than open source toolsextra functions in addition to straightforward load balancing and hefty price tagsAmazon offers a dedicated loadbalancing service the Elastic Load Balancer ELBfor use with EC virtual machines ELB is a completely managed service no virtual machine is required for the load balancer itself ELB handles an extremelylarge number of concurrent connections and can balance traffic among multipleavailability zonesIn ELB terminology a listener accepts connections from clients and proxiesthem to backend EC instances that do the actual work Listeners can proxy TCPHTTP or HTTPS traffic The load is distributed according to the least connections algorithmELB is not the most fully featured load balancer but it is our recommended solutionfor AWShosted systems because it requires virtually no administrative attentionCachesWeb caches were born from the observation that clients often repeatedly access thesame content within a short time Caches live between clients and web servers andSee page formore informationabout network DMZsdemilitarized zonesstore the results of the most frequent requests sometimes in memory They canthen intervene to answer requests for which they know the correct response reducing load on the authoritative web servers and improving response times for usersIn caching jargon an origin is the original content provider the source of truthabout the content Caches get their content directly from the origin or from another upstream cacheSeveral factors determine caching behavior The values of HTTP headers including CacheControl ETag and Expires Whether the request is served by HTTPS for which caching is more nuanced The response status code some are not cacheable see RFC The contents of HTML meta tagsStatic blobs such as images videos CSS stylesheets and JavaScript files are wellsuited to caching because they rarely change Dynamic content loaded from a database or another system in real time is more difficultbut not necessarily impossibleto cache For highly variable pages that should never be cached developersset the following HTTP headerCacheControl nocache nostoreExhibit C shows the placement of several potential cache layers in an HTTP requestExhibit C Caching players involved in handling an HTTP requestLocal network InternetBrowsercacheProxycacheReverse proxycacheOriginserverServer networkBrowser cachesAll modern web browsers save recently used resources images stylesheets JavaScript files and some HTML pages locally to speed up backtracking and return Because HTTPS payloads are encrypted responses cannot be cached unless the cache server terminates the TLS connection decrypting the payload The connection from the cache server to the origin may then require a separately encrypted TLS connection or not depending on whether the connection between the two is trusted These are not respected by all caches so they tend to be less effectivevisits In theory browser caches should follow exactly the same caching rules asdoes any other HTTP cacheProxy cacheYou can install a proxy cache at the edge of an organizations network to speed upaccess for all users When a user loads a web site the requests are first received bythe proxy cache If a requested resource is cached that resource is immediately returned to the user without the remote site being consultedYou can configure a proxy cache in two ways actively by changing users browsersettings to point to the proxy or passively by having a network router send all webtraffic through the cache server The latter configuration is known as an intercepting proxy There are also methods by which user agents can automatically discoverthe relevant proxiesReverse proxy cacheWeb site operators configure a reverse proxy cache to offload traffic from theirweb and application servers Incoming requests are first routed to the reverse proxycache from which they may be served immediately if the requested resources areavailable The reverse proxy passes requests for uncached resources along to theappropriate web serverServer sites use reverse proxy caches primarily because they reduce load on theorigin servers They may also have the beneficial side effect of speeding responsetimes for clientsCache problemsWeb caches are tremendously important to the performance of the web but they alsointroduce complexity A problem at any caching layer can introduce stale contentthat is out of date with respect to the origin server Cache problems can befuddleboth users and administrators and are sometimes difficult to debugStale cache entries are best detected by a direct query at each hop along the path Ifyou are the site operator try using curl to request a problematic page directly fromthe origin then from the reverse proxy cache and if applicable from the proxycache and from any other caches in the request path This is why your browsers Back button usually evinces a slight lag instead of zipping you instantly tothe previous page Even though most of the resources needed to render the page are cached locallythe pages toplevel HTML wrapper is typically dynamic and uncacheable so a round trip to the server is still required The browser could simply rerender the materials on hand from the previous visitand one or two used to do thatbut this shortcut breaks the correctness of caching and leads to a variety of subtle problemsYou can use curl H CacheControl nocache to politely request a cache refreshConformant caches will obey but if youre still seeing old data dont assume thatyour reload request has been honored unless you can prove it on the serverCache softwareTable lists a few of the open source caching software implementations Of thesewe find ourselves using NGINX most frequently Its caching is easy to configureand NGINX is often already in use as a proxy or web serverTable Open source caching softwareServer NotesSquid One of the first open source cache implementationsNormally used as a proxy cacheIncludes important features like antivirus and TLSVarnish Exceptional configuration languageMultithreadedModular and extensibleApache modcache Good choice for sites already running httpdNGINX Good choice for sites already running NGINXHas a reputation for good performanceApache Traffic Server Runs at extremely hightraffic sitesSupports HTTPDonated to the Apache Foundation by YahooContent delivery networksA content delivery network CDN is a globally distributed system that improvesweb performance by moving content closer to users Nodes in a CDN are dispersedgeographically to hundreds or thousands of locations When clients request content from a site that uses a CDN they are routed to the closest node called an edgeserver thereby decreasing latency and reducing congestion for the originEdge servers are similar to proxy caches They store copies of content locally Ifthey dont have a local copy of a requested resource or if their version of the content has expired they retrieve the resource from the origin respond to the clientand update their cache This is the same as invoking ShiftReload in a web browserCDNs use DNS to redirect clients to the geographically nearest host Exhibit Dexplains how it worksExhibit D The role of DNS in a content delivery networkUser requests cdnexamplecomimagepngClient sends DNS request for cdnexamplecomCDNs DNS server determinesthe physical location of theclient from its IP addressThe DNS reply is the address of thegeographically closest edge serverClient requests imagepngfrom the resolved IP addressEdge server returns theimage from its cacheMobile deviceCDN DNS serverCDN edge serverCDNs can now host dynamic content but traditionally they have been best suitedto static content such as images stylesheets JavaScript libraries HTML files anddownloadable objects Streaming services like Netflix and YouTube use CDNs toserve large media filesCDNs also offer value beyond performance improvements Most CDNs providesecurity services such as denialofservice attack prevention and web applicationfirewalls Some specialty CDNs offer other innovations that optimize page rendering and reduce the load on origin serversA substantial portion of content on the web today is served by CDNs If youre ata large site expect to pull out your pocketbook to pay for the privilege of fast performance If you run a smaller service optimize your local caching layers beforeturning to a CDNOne of the oldest and most prestigious read expensive CDNs is Akamai headquartered in Massachusetts Akamai counts some of the worlds largest governments andbusinesses among its customers It has the largest global network as well as someof the most advanced CDN features Nobody was ever fired for choosing AkamaiCloudFlare is another popular CDN Unlike Akamai CloudFlare has a history ofselling to smaller customers although their target market has more recently shiftedto enterprise Pricing is listed clearly on their web site and they offer some of thebest security features available CloudFlare was one of the first largescale providersto deploy HTTP for all its customersAmazons CDN service is called CloudFront It integrates with other AWS servicessuch as S EC and ELB but can also work for sites hosted outside of Amazonscloud As with other AWS products pricing is competitive and metered by usageLanguages of the webThe web has evolved from being mostly static to a rich interactive experience Theweb apps that enable this bounty are coded in a variety of programming languageseach with associated tooling and unique quirks Administrators need to managesoftware libraries install application servers and configure web applications according to the standards established for each languages ecosystemAll the languages mentioned in the following sections are in common use on theweb today They all feature engaged communities of developers extensive supportlibraries and welldocumented best practices Sites typically choose whicheverlanguages and frameworks their teams are most comfortable withRubyRuby is well known in DevOps and system administration circles for its use in Chefand Puppet Its also the language of Ruby on Rails a widely used web frameworkRails is a good choice for rapid development processes and is often used for prototyping new ideasRails has a reputation for mediocre performance and for fostering monolithic applications Over time many Rails applications tend to accumulate baggage that makesthem harder to modify the end result is often a gradual performance sag over timeRuby features a large collection of libraries called gems that developers can use tosimplify their projects Most are hosted at rubygemsorg They are curated by thecommunity but many are of marginal quality Managing a systems installed versionsof Ruby and its various gem dependencies can be both tedious and troublesomePythonPython is a generalpurpose language used not only in web development but also ina wide swath of scientific disciplines Its easy to read and to learn The most widelydeployed web framework for Python is Django which has many of the same benefits and drawbacks as Ruby on RailsJavaJava now controlled by Oracle is used most often in enterprise environmentswith slower development workflows Java offers fast performance at the expense ofcomplex clunky tooling and many layers of abstraction Javas challenging licenserequirements and obtuse conventions can frustrate neophytesNodejsJavaScript is known first and foremost as a clientside scripting language that runswithin web browsers As a language it has been ridiculed as hastily designed difficult to read and frequently counterintuitive Now Nodejsan engine for executingJavaScript on serversbrings JavaScript to the data center as wellSee the sectionstarting on page for a shortsummary of RubySee the sectionstarting on page for a shortsummary of PythonTo be fair Nodejs boasts high concurrency and native realtime messaging capabilities As a newer language it has so far avoided much of the cruft built up overthe years in other systemsPHPPHP is simple to get started with and for that reason it tends to attract new andinexperienced programmers PHP applications are notorious for being difficult tomaintain Past versions of PHP made it far too easy for developers to create largesecurity holes in their applications but recent versions have made improvementsin this area PHP is the language used by WordPress Drupal and several othercontent management systemsGoGo is a lowerlevel language from Google It has gained popularity in recent yearsthrough its use in major open source projects such as Docker Its excellent for systems programming but is also well suited for web applications because of its powerfulnative concurrency primitives One benefit for administrators is that Go softwareusually compiles to a standalone binary making it simple to deployApplication programming interfaces APIsWeb APIs are application interfaces intended for use not by humans but by softwareagents An API defines a set of methods through which a remote system can makeuse of an applications data and services APIs have become ubiquitous on the webbecause they promote cooperation among many diverse clientsAPIs are nothing new Operating systems define APIs to allow userspace applications to interact with the kernel Nearly all software packages use defined interfacesto facilitate modularity and separation of functions within the code base Howeverweb APIs are a bit special because they are exposed to the world on the public webwith the intention of promoting use by outside developersWeb API calls are normal HTTP requests Theyre only APIs because the clientand server have agreed by convention that certain URLs and verbs have specificmeanings and effects within the domain of their interactionWeb APIs commonly use some kind of textbased serialization format to encodedata for exchange These formats are relatively simple and can be parsed by applications written in any programming language Many formats exist but by far themost common are JavaScript Object Notation JSON and Extensible MarkupLanguage XMLHTTP APIs are perhaps easiest to explain by example The Spotify music serviceexposes an API that represents its music library A client of the API can request Douglas Crockford who named and promoted the JSON format says its pronounced like the nameJason But somehow JAYsawn seems to have become more common in the technical communityinformation about albums artists and tracks execute searches and perform other related actions This API is used both by Spotifys own client applications itsbrowser desktop and mobile clients and by third parties that want to incorporateSpotifys servicesBecause web APIs consist of HTTP requests you can interact with them with allthe normal HTTP tools including web browsers and curl For example we canobtain Spotifys JSON record for The Beatles curl httpsapispotifycomvartistsWrFJztbogyGnTHbHJFl jq externalurls spotify httpsopenspotifycomartistWrFJztbogyGnTHbHJFl followers href null total genres british invasion href httpsapispotifycomvartistsWrFJztbogyGnTHbHJFl id WrFJztbogyGnTHbHJFl images removed for concision name The Beatles popularity type artist uri spotifyartistWrFJztbogyGnTHbHJFlHere weve piped the JSON output through jq to clean up the formatting a bitOn a terminal jq also colorizes the outputSpotifys API is also an example of a RESTful API which is the predominantapproach today REST Representational State Transfer is an architectural styleof API design introduced by Roy Fielding in his doctoral dissertation The termwas originally quite specific but is now more loosely applied to web services that explicitly use HTTP verbs to communicate intent and use a directorylikepath structure to locate resources Most REST APIs use JSON as their underlyingrepresentation for dataREST contrasts starkly with SOAP Simple Object Access Protocol an earliersystem for implementing HTTP APIs that defines strict and elaborate multilevelguidelines for interactions among systems SOAP APIs use a complex XMLbasedformat that funnels all calls through a few specific URLs resulting in large HTTP How did we know that WrFJztbogyGnTHbHJFl is the Spotify ID for The Beatles Try searchingthrough the API httpsapispotifycomvsearchtypeartistqbeatles jq does far more than just formatting and is highly recommended for parsing and filtering JSONfrom the command line Find it at stedolangithubiojq Fielding is also a primary author of the HTTP specificationpayloads poor performance and endless difficulties in development debuggingand deployment Web hosting in the cloudCloud providers offer dozens of services for hosting web applications and the landscape changes weekly We cant possibly cover everything but a few points standout as being of particular interest to web administratorsSmall sites with few users and a tolerance for occasional outages can get away witha single virtual cloud instance as a web server or possibly two instances behinda load balancer for improved reliability But the cloud offers many opportunitiesto improve these simple configurations without significant increases in cost andcomplexity of administrationBuild versus buyAdministrators working on a cloud platform can build custom selfmanaged webapplications out of raw virtual machines Alternatively they can farm out partsof the design to offtheshelf cloud services thus reducing the labor involved indesigning configuring and maintaining everything by hand For the sake of efficiency we prefer to rely on vendor services when possibleLoad balancers are a good example of this tradeoff On AWS for example you caneither run an EC instance with open source loadbalancing software or sign upfor an AWSprovided Elastic Load Balancer The former offers greater customization but requires you to manage the load balancers operating system configure theloadbalancing software tune performance and promptly install security patchesas they are released in the future In addition the glue needed to gracefully handlefailures of the software or the instance will be somewhat more complexAn ELB on the other hand can be created in a matter of seconds and requires nofurther administrative action AWS handles everything behind the scenes Unlessthe ELB lacks a specific feature that you need it is clearly the expedient choiceUltimately this is a decision between building a service or outsourcing it to thevendor For the sake of your own sanity avoid the building option unless the function in question is a core competence for your business The development of the SOAP ecosystem is an interesting case study of the ways that technical initiatives can go awry In particular it illustrates the risks of attempting to design systems for a hazyand uncertain future SOAP put a lot of effort into remaining platform language data andtransportneutral and indeed it largely achieved these goalseven basic data types such as integerswere left open to definition Unfortunately the resulting system was complex and didnt fit well withrealworld needsPlatformasaServiceThe PaaS concept simplifies web hosting for developers by eliminating infrastructureas a concern Developers package their code in a specific format and upload it tothe PaaS provider which provisions appropriate systems and runs it automaticallyThe provider issues a DNS endpoint connected to the clients running applicationwhich the client can then customize by using a DNS CNAME recordAlthough PaaS offerings greatly simplify infrastructure management they sacrificeflexibility and customization Most offerings either do not allow administrativeaccess to a shell or they actively discourage it Users of a PaaS must accept certaindesign decisions made by the vendor Users ability to implement some featuresmay be constrainedGoogle App Engine pioneered the PaaS concept and remains one of the most prominent products App Engine supports Python Java PHP and Go and has manysupporting features such as cronlike scheduled job execution programmatic access to logs realtime messaging and access to various databases It is consideredthe Cadillac of PaaS offeringsThe competing product from AWS is called Elastic Beanstalk In addition to all thelanguages supported by App Engine it supports Ruby Nodejs Microsoft NETand Docker containers It integrates with Elastic Load Balancers and AWSs AutoScaling feature leveraging the power of the AWS ecosystemIn practice weve found Elastic Beanstalk to be a mixed bag Customization is possible through an extension framework that is proprietary and tedious Users are stillresponsible for running the EC instances that host the application So althoughElastic Beanstalk might be a fine fit for prototyping we believe that the system isnot a good choice for production workloads consisting of many servicesHeroku is another respected vendor in this space An application on Heroku is deployed to a dyno Herokus word for a lightweight Linux container Users controlthe dyno deployments Heroku has a strong network of partnerships that offer databases load balancing and other integrations Herokus pricing is higher than someother offerings in part because its own infrastructure runs on AWS under the hoodStatic content hostingIt seems like overkill to run an operating system just for the sake of hosting staticweb sites Fortunately the cloud providers can host them for you In AWS S youcreate a bucket for your content then configure a CNAME record from your domainto an endpoint within the provider In Google Firebase you use a commandlinetool to copy your local content to Google which provisions an SSL certificate andhosts your files In both cases you can serve your content from a CDN for betterperformanceSee page formore informationabout content delivery networksServerless web applicationsAWS Lambda is an eventbased computing service Developers who use Lambdawrite code that runs in response to an event such as the arrival of a message in aqueue a new object in a bucket or even an HTTP request Lambda feeds the eventpayload and metadata as inputs to a userdefined function which performs processing and returns a response There are no instances or operating systems to manageTo process HTTP requests Lambda is used in conjunction with another AWS service called API Gateway a proxy that can scale to hundreds of thousands of simultaneous requests API Gateway is interposed in front of an origin to add featuressuch as access control rate limiting and caching HTTP requests are received by theAPI Gateway and when a request arrives the gateway triggers a Lambda functionIn combination with static hosting on S Lambda and API Gateway can lead to afully serverless platform for running web applications as illustrated in Exhibit EExhibit E Serverless web hosting with AWS Lambda API Gateway and Sapiexamplecom resolves to anAPI gateway for requests thatrequire realtime processingexamplecom resolvesto the CloudFront CDNStatic content ishosted in an S bucketHTTP requests triggerLambda functionsLambda processes requestspossibly by querying a databaseCloudFrontedge nodesUsersAPI Gateway Lambda RDSThis technology is still in its youth but its already altering the mechanics of hostingweb applications We expect enhancements frameworks competing services andbest practices to mature rapidly in the coming years Apache httpdThe httpd web server is ubiquitous among the many flavors of UNIX and Linux Itis portable across many architectures and prebuilt packages exist for all major systems Unfortunately OS vendors have varied and highly opinionated approachesto httpd configurationA modular architecture has been fundamental to Apaches adoption Dynamic modules can be turned on through configuration offering alternative authenticationoptions improved security support for running code written in most languagesURLrewriting superpowers and many other featuresFor largely historical reasons Apache has a pluggable connection handling systemcalled multiprocessing modules MPMs that determines how HTTP connectionsare managed at the network IO layer The event MPM is the modern choice andis recommended over the worker and prefork alternativesTo bind to privileged ports those below such as HTTP port and HTTPSport the initial httpd process must run as root That process then forks additional workers under a local account with lower privileges to handle actual requestsSites that do not need to listen on port or can be run entirely without rootprivilegeshttpd is configured through directives Apachespeak for configuration optionsin plain text files that use a distinctive Apachestyle syntax Though hundreds ofdirectives exist administrators usually need to tweak only a few The directives andtheir values are documented directly in the default files that ship with the OS aswell as on Apaches web sitehttpd in useSystem V init BSD init and systemd can all manage httpd Whichever optionis standard for your system is the one you should default to For debugging andconfiguration however you can interact with the daemon independently of thestartup scriptsAdministrators can either run httpd directly or use apachectl Invoking httpdoffers direct control over the server daemon but remembering and typing allthe options is a challenge apachectl is a shell script wrapper around httpd Eachoperating system vendor customizes apachectl to conform to the conventions ofits init process It can start stop reload and show the status of ApacheFor example heres how to start the server with the default configuration apachectl startPerforming sanity check on apache configurationSyntax OKStarting apache apachectl statusapache is running as pid In this output from a FreeBSD system apachectl first performs a lintlike configuration check by running httpd t then starts the daemon Some legacy software that is not considered threadsafe such as modphp should use the preforkMPM It uses processes rather than threads for each connection httpd is both the name given to the daemons binary and to the project Ubuntu has taken the libertyof renaming httpd to apache which matches the name of the apt package but otherwise does littlemore than confuse everyone lint is a UNIX program that evaluates C code for potential bugs The term is now applied morebroadly to any tool that inspects software and configuration files for errors bugs or other problemsapachectl graceful waits for any currently open connections to conclude and thenrestarts the server This feature is handy for updating without interrupting activeconnections Its available through the system start and stop scripts as wellUse apachectls f flag to start Apache with a custom configuration eg apachectl f etchttpdconfcustomconfigconf k startSome vendors deprecate this use of apachectl in favor of running httpd directlyRefer to Chapter Booting and System Management Daemons to learn how toconfigure httpd to start automatically at boot timehttpd configuration logisticsAlthough an entire httpd configuration can be contained in a single file OS maintainers typically use the Include directive to split the default configuration intomultiple files and directories This architecture simplifies site management and isbetter suited to automation Predictably the specifics of the configuration hierarchy differ by system Table lists the Apache configuration defaults for each ofour example platformsTable Apache configuration details by platformRHELCentOS DebianUbuntu FreeBSDPackage name httpd apache apacheConfig root etchttpd etcapache usrlocaletcapachePrimary config file confhttpdconf apacheconf httpdconfModule config confmodulesd modsavailablemodsenabledmodulesdVirtual host config confd sitesavailablesitesenabledIncludesLog location varloghttpd varlogapache varloghttpdlogUser apache wwwdata wwwWhen httpd starts it consults a primary configuration file usually httpdconf andincorporates any additional files as referenced by Include directives The defaulthttpdconf is heavily commented and serves as a quick reference Configurationoptions in this file can be grouped into three categories Global settings such as the path to httpds configuration root the userand group as which to run the modules to activate and the network interfaces and ports to listen on VirtualHost sections that define how to provide service for a given domainusually delegated to subdirectories and Included in the main configuration Instructions for answering requests that dont match any VirtualHostdefinitionMany admins will be satisfied with the global settings and need only manage individual VirtualHostsModules exist independently of the httpd core and often have their own configuration options Most OS vendors choose to separate out module configurationinto subdirectoriesDebian and Ubuntu approach Apache configuration idiosyncratically A structureof subdirectories configuration files and symlinks creates a more flexible systemfor managing the server at least in theoryExhibit F attempts to clarify this puzzle The master apacheconf file includes allfiles from the enabled subdirectories in etcapache These files are in fact symbolic links to files in the available subdirectories A pair of configuration commands that create and remove symlinks is provided for each set of subdirectoriesExhibit F Subdirectories of etcapache on Debianbased systemsSymlinks managed withaenmod adismodmodsenabledApache modules and their congurations The load les invokeLoadModule and the conf les congure the corresponding modulesmodsavailableSymlinks managed withaenconf adisconfconfenabledGlobal optional conguration snippets that are not included in themain Apache conguration leconfavailableSymlinks managed withaensite adissitesitesenabledVirtual host denitions This is where administrators should putsitespecic conguration informationsitesavailableIn our experience the Debian system is unnecessary and overly complex A simple siteconfiguration subdirectory usually provides sufficient structure If yourerunning Debian or Ubuntu though it makes sense to stick with their defaultsVirtual host configurationThe lions share of httpd configuration lies in virtual host definitions Its generallya good idea to create a file for each siteWhen an HTTP request arrives httpd identifies which virtual host to select by consulting the HTTP Host header and network port It then matches the path portionof the requested URL to a Files Directory or Location directive to determine howto serve the requested content This mapping process is known as request routingThe following sample shows the HTTP and HTTPS configuration for admincomVirtualHost ServerName admincomServerAlias wwwadmincomServerAlias ulsahadmincomRedirect httpsadmincomVirtualHostVirtualHost ServerName admincomServerAlias wwwadmincomServerAlias ulsahadmincomDocumentRoot varwwwadmincomCustomLog varlogapacheadmincomaccess combinedErrorLog varlogapacheadmincomerrorSSLEngine onSSLCertificateFile etcsslcertsadmincomcrtSSLCertificateKeyFile etcsslprivateadmincomkeyDirectory varwwwadmincom Require all grantedDirectoryDirectory varwwwadmincomphotos Options IndexesDirectoryIfModule modrewritec RewriteEngine on RewriteRule usahlsah ulsahIfModuleExtendedStatus OnLocation serverstatus SetHandler serverstatus Require ip LocationVirtualHostMuch of this is self explanatory but a few details are worth noting The first VirtualHost answers on port and redirects all HTTP requestsfor admincom wwwadmincom and ulsahadmincom to use HTTPS Requests for admincomphotos receive an index of all files in that directory Requests for usah or lsah are rewritten to ulsahServer status accessible in this configuration at wwwadmincomserverstatus isa module that shows useful runtime performance information including statisticsabout the daemons CPU and memory usage request status the average numberof requests per second and more Monitoring systems can use this feature to collect data about the web server for alerting reporting and visualization of HTTPtraffic Here access to server status is restricted to a single IP address HTTP basic authenticationIn the HTTP basic authentication scheme clients pass a baseencoded username and password in the Authorization HTTP header If a user includes a nameand password in a URL eg httpsuserpasswwwadmincomserverstatusthe browser performs the encoding and transfers the value to the Authorizationheader automaticallyThe username and password are not encrypted so basic authentication does notprovide any confidentiality Thus it is safe to use only in combination with HTTPSBasic authentication in Apache is configured in Location or Directory blocks Forexample the following snippet requires authentication to access serverstatus abest practice and limits access to a subnetLocation serverstatus SetHandler serverstatus Require ip AuthType Basic AuthName Restricted AuthUserFile varwwwhtpasswd Require validuserLocationNote that the account information is stored externally to the configuration file Usehtpasswd to create the account entries htpasswd c varwwwhtpasswd benNew password passwordRetype new password passwordAdding password for user ben cat varwwwhtpasswdbenaprmPhxCjhfqMavkdHfVRVscESp chown wwwdata varwwwhtpasswd Set ownership chmod varwwwhtpasswd Restrictive permissionsPassword files are conventionally hidden files called htpasswd but they can benamed anything Even though the passwords are encrypted set the permissions onhtpasswd files to be readable only by the webserver user This precaution limitsattackers ability to see usernames and to run passwords through cracking softwareConfiguring TLSSSL might have changed its name to TLS but in the interest of backward compatibility Apache retains the SSL name for its configuration options as do many othersoftware packages Just a few lines are needed to set up TLS SSLEngine onSSLProtocol all SSLv SSLvSSLCertificateFile etcsslcertsadmincomcrtSSLCertificateKeyFile etcsslprivateadmincomkeyHere the TLS certificate and key are located in Linuxs central system locationetcssl The public certificates can be readable by anyone but the key should beaccessible only to the Apache masterprocess user typically root We prefer to setpermissions to for the certificate and for the keyAll versions of the actual SSL protocol precursor to TLS are known to be insecureand should be disabled with the SSLProtocol directive shown aboveA few ciphers have known weaknesses You can configure the web servers supportedciphers with the SSLCipherSuite directive The best practices for precisely whichsettings to use are constantly in flux The Mozilla Server Side TLS guide is the bestresource that we are aware of for staying current on best practices for TLS It alsohas a handy configuration syntax reference for Apache NGINX and HAProxyRunning web applications within Apachehttpd can be extended to run programs written in Python Ruby Perl PHP andother languages from within the module system Modules run inside Apache processes and have access to the full HTTP requestresponse life cycleModules provide additional configuration directives that let administrators controlthe runtime characteristics of applications Table lists some common application server modulesTable Application server modules for httpdModule Langmodphp PHP Deprecated use only with the prefork MPMmodwsgi Python The Web Server Gateway Interface a Python standardmodpassenger Multiple Flexible commercially supported application server formultiple languages including Ruby Python and Nodejsmodproxyfcgi Any A standard server interface usable from any languagemodperl Perl A Perl interpreter that lives within httpdSee page fora complete citation for the ServerSide TLS guideThe following example to configure a Python Django application for apiadmincomuses modwsgiLoadModule wsgimodule modwsgisoVirtualHost ServerName apiadmincomCustomLog varlogapacheapiadmincomaccess combinedErrorLog varlogapacheapiadmincomerrorSSLEngine onSSLCertificateFile etcsslcertsapiadmincomcrtSSLCertificateKeyFile etcsslprivateapiadmincomkeyWSGIDaemonProcess adminapi useruser groupgroup threadsWSGIScriptAlias varwwwapiadmincomadminapiwsgiDirectory varwwwapiadmincom WSGIProcessGroup adminapi WSGIApplicationGroup GLOBAL Require all grantedDirectoryVirtualHostOnce modwsgiso has been loaded by Apache several WSGI configuration directives become available The WSGIScriptAlias file in the configuration aboveadminapiwsgi contains Python code that is needed by the WSGI moduleLogginghttpd offers bestinclass logging capabilities with finegrained control over thedata that is logged and the ability to separate log data by virtual host Administratorsuse these logs to debug configuration problems detect potential security threatsand analyze usage informationA sample log message from admincomaccesslog looks like this Jun GET search HTTP curlThe message shows The source of the request in this case the local host A time stamp The path of the requested resource search and the HTTP method GET The response status code The size of the response The user agent the curl commandline toolThe documentation for modlogconfig has all the details on how to customizethe log formatA busy web site generates a large number of request logs that can quickly fill up thedisk Administrators are responsible for ensuring that this never happens Keepweb server logs on a dedicated partition to prevent a large log file from affectingthe rest of the systemOn most Linux distributions the default package installation of Apache includesan appropriate logrotate configuration FreeBSD comes with no such default andadministrators should instead add an entry in etcnewsyslogconf for Apaches logsThe log directory and the files within should be writable only by the user of themaster httpd process which is normally root If nonroot users have write accessthey can create a symlink to another file causing it to be overwritten with bogusdata The system defaults are set safely so avoid customizing the owner and group NGINXA busy web server must respond to many thousands of concurrent requests Most ofthe time needed to handle each request is spent waiting for data to arrive from thenetwork or disk The time spent actively processing the request is short by comparisonTo handle this workload efficiently NGINX uses an eventbased system in whichjust a few worker processes handle many requests simultaneously When a requestor response an event is ready for servicing a worker process quickly completesprocessing before returning to handle the next event Above all NGINX aims toavoid blocking on network or disk IOThe event MPM included in newer releases of Apache uses a similar architecturebut for highvolume and performancesensitive sites NGINX remains the software of choiceAdministrators running NGINX will notice at least two processes a master and aworker The master performs housekeeping duties such as opening sockets readingthe configuration and keeping the other NGINX processes running Workers domost of the heavy lifting by handling and processing requests Some configurationsuse additional processes dedicated to caching As in Apache the master processruns as root so that it can open sockets for any ports below The other processes run as a less privileged userThe number of worker processes is configurable A good rule of thumb is to run asmany worker processes as the system has CPU cores Debian and Ubuntu configure NGINX this way by default if its installed from a package FreeBSD and RHELdefault to a single worker processInstalling and running NGINXAlthough NGINX continues to grow in popularity and is a staple among some of theworlds busiest web sites OS distributions still lag on NGINX support The versionsavailable in the official repositories for Debian and RHEL are usually out of dateSee page formore informationabout logrotatethough FreeBSD is typically more current NGINX is open source so it can be builtand installed manually The projects web page nginxorg offers packages for aptand yum than are generally more current than those supplied by the distributionsThe systems normal service management is appropriate for daytoday wranglingof nginx You can also run the nginx daemon during development and debuggingUse the c argument to specify a custom configuration file The t option performsa check of the configuration file syntaxnginx uses signals to trigger various maintenance actions Table lists theseMake sure you target the master nginx process usually the one with the lowest PIDTable Signals understood by the nginx daemonSignal FunctionTERM or INT Shuts down immediatelyQUIT Completes and closes all current connections then shuts downUSR Reopens log files used to facilitate log rotationsHUP Reloads the configurationaUSR Gracefully replaces the server binary without interrupting service ba This option tests the syntax of the new configuration and if the syntax is valid starts new workerswith the new configuration It then gracefully shuts down the old workersb See the nginx commandline documentation for details on how this worksConfiguring NGINXThe NGINX configuration style is generally Clike it uses curly braces to distinguishblocks of configuration lines and semicolons to separate lines The main configuration file is called nginxconf by default Table summarizes the most importantsystemspecific aspects of NGINX configurationTable NGINX configuration details by platformRHELCentOS DebianUbuntu FreeBSDPackage name nginx a nginx nginxDaemon path sbinnginx usrsbinnginx usrlocalsbinnginxConfiguration root etcnginx etcnginx usrlocaletcnginxVirtual host config b confd sitesavailablesitesenabledNo prescribed locationDefault user nginx wwwdata nobodya You must enable the EPEL software repository see fedoraprojectorgwikiEPELb Relative to the configuration root directorySee page formore informationabout signalsWithin the nginxconf file blocks of configuration directives surrounded by curlybraces are called contexts A context contains directives specific to that block ofconfiguration For example heres a minimal but complete NGINX configurationthat shows three contextsevents http server servername wwwadmincom root varwwwadmincomThe outermost context called main is implicit and configures the core functionality The events and http contexts live within main events is a required context thatconfigures connection handling Since its blank in this example default values areimplied Fortunately the defaults are sensible Run one worker process use the unprivileged user account Listen on port if started as root or port otherwise Write logs to varlognginx chosen at compile timeThe http context contains all directives relating to web and HTTP proxy servicesserver contexts which define virtual hosts are nested within http Multiple servercontexts within http would configure multiple virtual hostsAliases can be included in servername to match the Host header against a groupof subdomainshttp server servername admincom wwwadmincom root varwwwadmincomserver servername examplecom wwwexamplecom root varwwwexamplecomThe value for servername can also be a regular expression and the match can evenbe captured and named as a variable for use later in configuration By using thisfeature you can refactor the previous configuration tohttp server servername wwwdomainexampleadmincom root varwwwdomainSee page foran overview of regular expressionsThe regular expression which starts with a tilde matches either examplecom oradmincom optionally preceded by www The value of the matched domain is storedin the domain variable which is then used to determine which server root to selectNamebased virtual hosts can be distinguished from IPbased hosts by using thelisten and servername directives togetherserver listen servername admincom wwwadmincom root varwwwadmincomsiteserver listen servername admincom wwwadmincom root varwwwadmincomsiteThis configuration shows two versions of admincom being served from differentweb roots The IP address of the interface on which the request was received determines which version of the site the client seesThe root is the base directory where HTML images stylesheets scripts and otherfiles for the virtual host are stored By default NGINX just serves files out of theroot but you can use the location directive to do more sophisticated request routing If a given path isnt matched by a location directive NGINX automaticallyfalls back to the rootThe following example uses location in combination with the proxypass directiveIt instructs NGINX to serve most requests from the web root but forward requestsfor httpwwwadmincomnginx to nginxorgserver servername admincom wwwadmincom root varwwwadmincom location nginx proxypass httpnginxorg proxypass instructs NGINX to act as a proxy and replay requests from clients toanother downstream server We revisit the proxypass directive when we describehow to use NGINX as a load balancer on page location can use regular expressions to perform powerful pathbased routing todifferent sources based on the requested content The official NGINX documentation Be aware that use of this syntax commits NGINX to performing a regular expression match on everyHTTP request We use it here to demonstrate NGINXs flexibility but in practice you would probablywant to just list all possible hostnames in plain text Its perfectly reasonable to use regular expressions in nginxconf but make sure theyre delivering actual value and try to keep them low in theconfiguration hierarchy so that they activate only in specific situationsanalyzes how NGINX evaluates the servername listen and location directivesto route requestsA common pattern among distributions is to set sensible defaults for many directives in the global http context then use the include directive to add sitespecificvirtual hosts to the final configuration For example the default nginxconf file forUbuntu includes the lineinclude etcnginxconfdconfThis architecture helps eliminate redundancy since all children inherit the global settings Administrators in straightforward environments may not need to doanything more than write virtual host configurations expressed as server contextsConfiguring TLS for NGINXAlthough NGINX didnt borrow much from Apaches configuration style its TLSconfiguration is one area in which its strikingly similar As in Apache the configuration keywords all refer to SSL TLSs earlier nameEnable TLS and point to the certificate and private key file like soserver listen ssl onsslcertificate etcsslcertsadmincomcrtsslcertificatekey etcsslprivateadmincomcrt sslprotocols TLSv TLSv TLSv sslciphers ECDHERSAAESGCMSHAECDHE truncated sslpreferserverciphers on servername admincom wwwadmincom root varwwwadmincomsiteOnly the actual TLS protocols not the older SSL versions should be enabled allSSL protocols have been deprecated Permissions on the certificate and key shouldfollow the recommendations outlined in the Apache TLS section on page Use the sslciphers directive to require cryptographically strong cipher suites andto disable weaker ciphers The sslpreferserverciphers option in conjunctionwith sslciphers instructs NGINX to choose from the servers list rather than fromthe clients otherwise the client could suggest any cipher it pleased The previousexample does not show a full list of ciphers because the appropriate list is quite longrefer to the Mozilla Server Side TLS guide cited on page for recommendedvalues If you prefer a shorter cipher list try the one at cipherlistLoad balancing with NGINXIn addition to being a web and cache server NGINX is also a capable load balancerIts configuration style is flexible but somewhat nonobviousUse the upstream module to create named groups of servers For example the following clause defines adminservers as a collection of two serversupstream adminservers server webadmincom maxfailsserver webadmincom maxfailsupstream groups can be referenced from virtual host definitions In particular theycan be used as proxying destinations just like hostnameshttp server servername admincom wwwadmincom location proxypass httpadminservers healthcheck interval fails passes urihealthcheck matchadminhealth line not split in original file match adminhealth status header ContentType texthtml body Red Leader Standing ByHere traffic for admincom and wwwadmincom is farmed out to the web andweb servers in round robin order the defaultThis configuration also sets up health checks for the backend servers Checks areperformed every seconds interval against each server at the healthcheckendpoint urihealthcheck NGINX will mark the server down if the healthcheck fails on three consecutive attempts fails but will add the server backto the rotation if it succeeds just once passesThe match keyword is peculiar to NGINX It dictates the conditions under whichthe health check is considered successful In this case NGINX must receive a response code the ContentType header must be set to texthtml and the body ofthe response must contain the phrase Red Leader Standing ByWeve added an additional condition within the upstream context that sets themaximum number of connection attempt failures to two That is if NGINX cannot connect to the server at all within two attempts it gives up and removes thatserver from the pool This is an additive connectivity check that complements themore structured checks from the healthcheck clause HAProxyHAProxy is the most widely used open source loadbalancing software It proxiesHTTP and TCP supports sticky sessions to pin a given client to a specific web server and offers advanced healthchecking capabilities Recent versions also supportTLS IPv and HTTP compression Support for HTTP is a work in progress andis expected to mature quickly beginning with HAProxy version HAProxys configuration is usually contained in a single file haproxycfg Its sosimple that OS vendors generally dont overcomplicate things and instead embracethe default directory structure recommended by the projectOn Debian and RHEL systems the configuration is in etchaproxyhaproxycfgFreeBSD doesnt provide a default as there really is no sensible one for load balancing its entirely dependent on your setup You can find an example configurationon FreeBSD in usrlocalshareexampleshaproxy after the HAProxy packagehas been installedThe following simple example configuration sets HAProxy to listen on port anddistribute requests in a round robin fashion between two web servers web andweb on port globaldaemonmaxconn defaultsmode httptimeout connect Millisecondstimeout client timeout server frontend httpinbind defaultbackend webserversbackend webserversbalance roundrobinserver web server web This example introduces HAProxys frontend and backend keywords illustrated inExhibit Gfrontend dictates how HAProxy will receive requests from clients which addressesand ports to use what types of traffic to serve and other clientfacing considerations backend configures the set of servers that actually process requests Multiple frontendbackend pairs can exist in a single configuration allowing a singleHAProxy to service multiple sitesThe timeout settings allow finegrained control over how long a system should waitwhen trying to open a new connection to a server and how long to keep connectionsExhibit G HAProxy frontend and backend specificationsClients HAProxyWeb serversrequests proxied to webservers on port backendport frontendopen once they have been established Finetuning these values is important onbusy web servers On local networks the timeout connect value can be quite lowms or less because new connections should be established quicklyHealth checksAlthough the previous configuration provides basic functionality it doesnt checkthe status of downstream web servers If web or web goes offline half of incoming requests would begin to failHAProxys statuscheck feature performs regular HTTP requests to determine thehealth of each server As long as servers respond with an HTTP response codethey remain in service and continue to receive requests from the load balancerIf a server fails a status check by returning anything other than status thenHAProxy removes the errant server from the pool However HAProxy continues toperform health checks on the server If it starts to respond successfully once againHAProxy will return it to the poolThe specifics of the health check such as what request method to use the interval between checks and the path to request can all be adjusted In this exampleHAProxy performs a GET request for on each server every secondsbackend webserversbalance roundrobinoption httpchk GET server web check inter server web check inter Its reassuring to know that you can contact a machines web server but thats hardlythe last word on server health Wellconstructed web applications commonly expose a healthcheck endpoint that performs a thorough probe of the applicationto determine its true health These checks may include verification of database orcache connectivity as well as performance monitoring Use these more sophisticated checks if they are availableServer statisticsHAProxy offers a convenient web interface that displays server stats much likemodstatus in Apache HAProxys version shows the state of each server in thepool and lets you manually enable and disable servers as neededThe syntax is straightforwardlisten stats mode httpstats enablestats hideversionstats realm HAProxy Statisticsstats uri stats auth myusermypass stats admin if TRUEServer stats can be configured either within a specific listener or within a backendor frontend block to limit the feature to that configuration aloneSticky sessionsHTTP is a stateless protocol so each transaction is an independent session Fromthe perspective of the protocol requests from the same client are unrelatedAt the same time most web applications need state to track user behavior over timeThe classic example of state is a shopping cart Users browse a store add items to thecart and when ready to check out submit their payment information The web application needs some way to track the contents of the cart across multiple page viewsMost web applications use cookies to track state The web application generates asession for a user and puts the session ID in a cookie that is sent back to the user inthe response header Each time a client submits a request to the server the cookieis sent with the request The server uses the cookie to recover the clients contextIdeally web applications should store their state information in a persistent andshared medium such as a database However some poorly behaved web applicationskeep their session data locally in the servers memory or on its local disk Whenplaced behind a load balancer these applications break because a single clients requests might be routed to multiple servers depending on the vagaries of the loadbalancers scheduling algorithmTo address this issue HAProxy can insert a cookie of its own into responses afeature known as sticky sessions Any future requests from the same client willinclude the cookie HAProxy can use the value of the cookie to route the requestback to the same serverA version of the previous configuration modified to support sticky sessions lookslike the following Note the addition of the cookie directivebackend webserversbalance roundrobinoption httpchk GET cookie SERVERNAME insert httponly secureserver web cookie web check inter server web cookie web check inter In this configuration HAProxy maintains a SERVERNAME cookie to track the server that a client is dealing with The secure keyword specifies that the cookie shouldonly be sent over TLS connections and httponly informs browsers to use the cookie only over HTTP Refer to RFC for further information on these attributesTLS terminationHAProxy versions and later include TLS support A common configuration is toterminate TLS connections at the HAProxy server and communicate with backendservers over plain HTTP This approach offloads the cryptographic overhead fromthe backend servers and reduces the number of systems that need a private keyFor particularly securityconscious sites its also possible to use HTTPS fromHAProxy to the backend servers You can use the same TLS certificate or a different one either way you will still need to terminate and reinitiate TLS at the proxySince HAProxy terminates the TLS connection from clients youll need to add thepertinent configuration to the frontend configuration blockfrontend httpsinbind ssl crt etcsslprivateadmincompemdefaultbackend webserversApache and NGINX require the private key and certificate to be in separate files inPEM format but HAProxy expects both components to be present in the same fileYou can simply concatenate the separate files to create a composite file cat etcsslprivateadmincomkeycertsadmincomcrt etcsslprivateadmincompem chmod etcsslprivateadmincompem ls l etcsslprivateadmincompemr root root Jun etcsslprivateadmincompemSince the private key is part of the composite file ensure that the file is owned byroot and is not readable by any other user If you do not run HAProxy as root because you are not accessing any privileged ports make sure the ownership of thekey file matches the identity under which HAProxy runsAll usual best practices for TLS apply to HAProxy disable SSLera protocols andexplicitly configure the acceptable cipher suites Recommended readingAdrian David et al Weak DiffieHellman and the Logjam Attack weakdhorgThis page describes the Logjam attack on the DiffieHellman key exchange protocoland suggests ways to secure systems properlyCloudFlare Inc blogcloudflarecom This is the corporate blog of content delivery network CloudFlare Some posts are just marketing information but manyinclude insights on the latest web trends and technologiesGoogle Inc Web Fundamentals developersgooglecomwebfundamentals Thisis a useful guide to various best practices for web development including sectionson site design user interfaces security performance and other topics of interestto both developers and administrators The caching discussion is particularly goodGrigorik Ilya High Performance Browser Networking OReilly Media Anexceptional guide to the protocols strengths limitations and performance aspectsof the web Useful for developers and system administrators alikeIANA Index of HTTP Status Codes wwwianaorgassignmentshttpstatuscodesInternational Engineering Task Force Hypertext Transfer Protocol Version httpgithubiohttpspec The working draft of the HTTP specificationMozilla SecurityServer Side TLS wikimozillaorgSecurityServerSideTLSAn excellent resource that documents best practices for TLS configuration acrossmany platformsStenberg Daniel danielhaxxseblog This is the blog of Daniel Stenberg theauthor of curl and a prolific HTTP expertvan Elst Remy Strong Ciphers for Apache nginx and Lighthttpd cipherlist Correct and secure cipher configuration for Apache httpd NGINX and the lighttpdweb servers as well as a TLS configuration testerSECTION THREESTORAGEData storage systems are looking more and more like a giant set of Lego blocks thatyou can assemble in an infinite variety of configurations You can build anythingfrom a lightningfast storage space for a missioncritical database to a vast archivalvault that stores three copies of all data and can be rewound to any point in the pastMechanical hard drives remain a popular storage medium when capacity is themost important consideration but solid state drives SSDs are preferred for performancesensitive applications Caching systems both software and hardwarehelp combine the best features these storage typesOn cloud servers you usually have a choice of storage hardware but youll pay morefor SSDbacked virtual disks You can also choose from a variety of purposespecific storage types such as object stores infinitely expandable network drives andrelational databasesasaserviceRunning on top of this real and virtual hardware are a variety of software components that mediate between the raw storage devices and the filesystem hierarchyseen by users These components include device drivers partitioning conventionsRAID implementations logical volume managers systems for virtualizing disksover a network and the filesystem implementations themselves StorageIn this chapter we discuss the administrative tasks and decisions that occur ateach of these layers We begin with fast path instructions for adding a basic diskto Linux or FreeBSD We then review storagerelated hardware technologies andlook at the general architecture of storage software We then work our way up thestorage stack from lowlevel formatting to the filesystem level Along the way wecover disk partitioning RAID systems and logical volume managersAbove the level of individual machines lie a variety of schemes for sharing data on anetwork Chapters and describe two common file sharing systems NFSfor native sharing among UNIX and Linux systems and SMB for interoperabilitywith Windows and macOS systems I just want to add a diskBefore we launch into many pages of storage architecture and theory we first address the most common scenario you want to install a hard disk and make it accessible through the filesystem Nothing fancy no RAID all the drives space in asingle volume and the default filesystem typeStep one is to attach the drive If the machine in question is a cloud server you generally provision a virtual drive of the desired size within the providers administrative GUI or through their API and then attach it to an existing virtual server asa separate step Its normally unnecessary to reboot the server because cloud andvirtual kernels recognize such hardware changes on the flyIn the case of physical hardware drives that communicate through a USB port cansimply be powered on and plugged in SATA and SAS drives need to be mountedin a bay enclosure or cradle Although some hardware and drivers are designed topermit hotaddition of SATA drives that feature requires hardware support and isuncommon in massmarket hardware Reboot the system to make sure the OS isin a configuration thats reasonably reproducible at boot timeIf youre running a desktop machine with a window system and all the stars alignthe system might offer to format a new disk for you when you plug it in Thats particularly likely if youre plugging in an external USB disk or thumb drive The autoformat option usually works fine use it if its offered However check the mountdetails afterwards by running the mount command in a terminal window to makesure the drive hasnt been mounted with restrictions you dont want eg with execution or normal ownerships disabledIf you set up the disk by hand its critically important to identify and format the rightdisk device A newly added drive is not necessarily represented by the highestnumbereddevice file and on some systems the addition of a new drive can change the devicenames of existing drives after a reboot usually Doublecheck the identity of the newdrive by reviewing its manufacturer size and model number before you do anythingthats potentially destructive Use the commands mentioned in the next two sectionsLinux recipeFirst run lsblk to list the systems disks and identify the new drive If that outputdoesnt give you enough information to conclusively identify the new drive youcan list model and serial numbers with lsblk o MODELSERIALOnce you know which device file refers to the new disk assume its devsdb installa partition table on the disk Several commands and utilities can do this includingparted gparted fdisk cfdisk and sfdisk it doesnt matter which one you use aslong as it understands GPTstyle partition tables gparted is probably the easiestoption on a system with a graphical user interface Below we show the fdisk recipe which works on all Linux systems Some systems still ship a version of partedthat doesnt understand GPT sudo fdisk devsdbWelcome to fdisk utillinux Changes will remain in memory only until you decide to write themBe careful before using the write commandCommand m for help gBuilding a new GPT disklabel GUIDABDAADEECCCommand m for help nPartition number default ReturnFirst sector default ReturnLast sector sectors or sizeKMGTP default ReturnCreated partition Command m for help wThe partition table has been alteredCalling ioctl to reread partition tableSyncing disksThe g subcommand creates a GPT partition table The n subcommand creates anew partition pressing Return in response to all fdisks questions allocates allfree space to the new partition partition Finally the w subcommand writes thenew partition table to the diskThe device file for the newly created partition is the same as the device file for the diskas a whole with a appended to it In the example above the partition is devsdbYou can now create a filesystem on devsdb with the mkfs command The L option gives the filesystem a shorthand label here spare The label stays the sameeven if the disk that contains the filesystem is assigned a different device nameduring a subsequent bootSee page for anexplanation of GPTpartition tables sudo mkfs t ext L spare devsdbmkefs DecDiscarding device blocks doneFilesystem labelspareOS type LinuxBlock size logNext create a mount point and mount the new filesystem sudo mkdir spare sudo mount LABELspare spareYou could equivalently specify devsdb instead of LABELspare as a way of identifying the partition but that name wont necessarily work in the futureTo have the filesystem automatically mounted at boot time edit the etcfstab fileand duplicate one of the existing entries Change the device name and mount pointto match those shown in the mount command above For exampleLABELspare spare ext errorsremountro You can also use a UUID to identify the filesystem see page See page for more details on Linux device files for disks See page forpartitioning information See page for information about the ext filesystemFreeBSD recipeRun geom disk list to list the disk devices that the kernel is aware of UnfortunatelyFreeBSD doesnt divulge much information beyond device names and sizes Youcan resolve any ambiguity as to which disk is which by running geom part list tosee which devices have existing partitions An unformatted disk should have nopartitionsOnce you know the disk name you can install a partition table and create a filesystem In this example we assume that the disk name is ada and that you want tomount the new filesystem as spare sudo gpart create s GPT ada Create GPT partition tableada created sudo gpart add l spare t freebsdufs a M ada Create partitionadap added sudo newfs L spare devadap Create filesystemdevadap MB sectors block size fragmentsize using cylinder groups of MB blks inodessuperblock backups for fsckffs b at The l option to gpart add applies a text label to the new partition The label makesthe partition accessible through the path devgptspare regardless of what devicename the kernel assigns to the underlying disk device The L option to newfs applies a similar but distinct label to the new filesystem to make the partition accessible as devufsspareMount the filesystems with the following commands sudo mkdir spare sudo mount devufsspare spareTo have the filesystem automatically mounted at boot time add it to the etcfstabfile see page Storage hardwareEven in todays postInternet world computer data can be stored in only a fewbasic ways hard disks flash memory magnetic tapes and optical media The lasttwo technologies have significant limitations that disqualify them from use as asystems primary filesystem However theyre still sometimes used for backupsand for near line storagecases in which instant access and rewritability are notof primary concernAfter years of traditional magnetic disk technology performanceminded system builders finally received a practical alternative in the form of solid state disksSSDs These flashmemorybased devices offer a different set of tradeoffs fromthose of a standard disk and they will be influencing the architectures of databasesfilesystems and operating systems for years to comeAt the same time traditional hard disks are continuing their exponential increasesin capacity Thirty years ago at the dawn of the form factor that remains inuse today a MB hard disk cost Today a gardenvariety TB drive runs or so Thats more than times more storage for the money or double the TB every years During that same period the sequential throughputof massmarket drives has increased from kBs to MBs a comparativelypaltry factor of And randomaccess seek times have hardly budged The morethings change the more they stay the sameDisk sizes are specified in gigabytes that are billions of bytes as opposed to memory which is specified in gigabytes gibibytes really of bytesThe difference is about Be sure to check your units when estimating and comparing capacitiesHard disks and SSDs are enough alike that they can act as dropin replacementsfor each other at least at the hardware level They use the same hardware interfacesand interface protocols And yet they have very different strengths as Table on the next page summarizesSee page for moreinformation on IECunits gibibytes etcTable Comparison of HDD and SSD technology aCharacteristic HDD SSDTypical size TB TBRandom access time ms msSequential read MBs MBsRandom read MBs MBsIOPS b opss opssCost GB GBReliability Poor Poor cLimited writes No In theorya Performance and cost values are as of mid b IO operations per secondc Fewer wholedevice failures than HDD but more data lossIn the next sections we take a closer look at each of these technologies along witha more recent category of storage devices hybrid drivesHard disksA typical hard drive contains several rotating platters coated with magnetic filmThey are read and written by tiny skating heads mounted on a metal arm that swingsback and forth to position them The heads float close to the surface of the plattersbut dont actually touch themReading from a platter is quick its the mechanical maneuvering needed to addressa particular sector that drives down randomaccess throughput Delays come fromtwo main sourcesFirst the head armature must swing into position over the appropriate track Thispart is called seek delay Second the system must wait for the right sector to passunderneath the head as the platter rotates That part is rotational latency Diskscan stream data at hundreds of MBs if reads are optimally sequenced but randomreads are unlikely to achieve more than a few MBsA set of tracks on different platters that are all the same distance from the spindleis called a cylinder The cylinders data can be read without any additional movement of the arm Although heads move amazingly fast they still move much moreslowly than the disks spin around Therefore any disk access that does not requirethe heads to seek to a new position will be fasterSpindle speeds vary RPM remains the massmarket standard for enterpriseand performanceoriented drives A few RPM and RPM drives remain available at the high end but the advent of inexpensive SSDs now limits thesedrives to a small and shrinking niche market Higher rotational speeds decreaselatency and increase the bandwidth of data transfers but the drives tend to run hotHard disk reliabilityHard disks fail frequently A Google Labs study of drives surprised thetech world with the news that hard disks more than two years old had an averageannual failure rate AFR of more than much higher than the failure rates manufacturers predicted from extrapolating their shortterm testing The overall patternwas a few months of infant mortality a twoyear honeymoon of annual failure ratesof a few percent and then a jump up to the AFR range Overall hard disksin the Google study had less than a chance of surviving a fiveyear tour of dutyInterestingly Google found no correlation between failure rate and two environmental factors that were formerly thought to be important operating temperatureand drive activity The complete paper can be found at googlYSenkMore recently Backblaze a cloud storage provider has posted regular updates aboutits experience with various hard disk models at backblazecomblog This data is years more recent than the original Google study but suggests the same basic pattern high infant mortality followed by a two or threeyear honeymoon and thena precipitous rise in annual failure rate The absolute numbers are pretty close tooFailure modes and metricsHard disk failures typically stem from either platter surface defects bad blocks ormechanical failures Drives attempt to transparently correct errors in the formercategory and remap the recovered data to a different portion of the disk Whenblock errors become visible at the operating system level ie in the logs thatmeans data has already been lost Its a bad prognostic sign pull the drive fromservice and replace itA disks firmware and hardware interface usually remain operable after a failure andit can be entertaining to attempt to query the disk for details about whats going onsee page However disks are so cheap that its rarely worth your time to dothis except perhaps as a learning exerciseDrive reliability is often quoted by manufacturers in terms of mean time betweenfailures MTBF denominated in hours A typical value for an enterprise drive isaround million hours However MTBF is a statistical measure and should notbe read to imply that an individual drive will run for years before failingMTBF is defined as the inverse of AFR in the drives steadystate periodthat isafter breakin but before wearout A manufacturers MTBF of million hourscorresponds to an AFR of per year This value is almost but not quite concordant with the AFR range observed by Google and Backblaze duringthe honeymoon years of their sample drives livesManufacturers MTBF values are probably accurate but they are cherrypickedfrom the most reliable phase of each drives life MTBF values should therefore beregarded as an upper bound on reliability they do not predict your actual expectedfailure rate over the long term Based on the limited data quoted above you mightconsider dividing manufacturers MTBFs by a factor of or so to arrive at a morerealistic estimate of fiveyear failure ratesDrive typesOnly two manufacturers of hard drives remain Seagate and Western Digital Youmay see a few other brands for sale but theyre all ultimately made by these sametwo companies both of which have been on decadelong acquisition bingesBrands segment their hard disk offerings into a few general categories Value drives These products offer lots of storage at the lowest possibleprice point Performance isnt a priority but its usually decent Todayslowend drives are often faster than the highperformance drives of fiveor ten years ago Massmarket performance drives These stepup products targeted atend users often gamers have higher spindle speeds and larger cachesthan those of their value equivalents They perform notably better thanvalue drives on most benchmarks As with value drives firmware tuningemphasizes singleuser access patterns such as large sequential reads andwrites The drives often run hot NAS drives NAS stands for networkattached storage but these drivesare intended for use in all sorts of servers RAID systems and arraysanywhere that multiple drives are housed and accessed together Theyredesigned to be constantly on and working and to balance performancereliability and low heatemissionBenchmarks that replicate standalone access patterns may not revealmuch performance difference from value drives but NAS drives typicallyhandle multiple streams of independent operations more intelligently because of firmware tuning NAS drives often have a longer warranty thanvalue drives their pricing is somewhere between that for value and performance drives Enterprise drives Enterprise can mean a lot of things in the context ofhard disks but most commonly it means expensive Heres where youllfind drives with nonSATA interfaces and uncommon features such as RPM spindle speeds These are generally premium drives withlong often fiveyear warrantiesThe differences among these drive categories are about half real and half marketingAll classes of drives work fine in all applications but performance and reliabilitymay vary NAS drives are probably the best allaround choice for drives to keep onhand to fill a variety of potential needs Our technical reviewer Jon Corbet refers to these as reliability guaranteed not to exceed valuesHard disks are commodity products and one brands model of a given size classand spindle speed is much like anothers These days you need a dedicated qualification laboratory to make fine distinctions among competing drives at least interms of performanceReliability is another matter The Google and Backblaze data demonstrate significant differences among models The least reliable are an order of magnitude morelikely to fail than the best Unfortunately theres really no way to identify the turkeys until theyve been sold for a year or two and have established a reputation inthe real worldNo matter even the best drives are relatively failure prone Theres no escaping theneed for backups and redundant storage when important data is at stake Designyour infrastructure with the assumption that drives will fail then figure out howmuch an incrementally more reliable drive is worth within this contextWarranties and retirementBecause hard drives are more likely to require warranty service than are other typesof hardware warranty length is an important purchasing consideration The industry standard has shrunk to a paltry two years suspiciously close to the length of theaverage hard drives honeymoon period The threeyear warranty offered on manyNAS drives is a significant advantageHard disk exchanges under warranty are straightforward if you can demonstratethat drives fail a diagnostic test supplied by the manufacturer Test programs typically run only under Windows and are intolerant of virtualization environmentsand of intervening connection hardware such as USB cradles If your operationsentail frequent drive exchanges you may find it worthwhile to maintain a dedicated Windows machine as a drive testing stationIt usually pays to be aggressive in taking drives out of service even if you cant quitedocument that they are broken enough to be eligible for exchange under warrantyEven seemingly insignificant signs eg funny noises or block errors within temporary files are likely indications that a drive is nearing the end of its lifeSolid state disksSSDs spread reads and writes across banks of flash memory cells which are individually rather slow in comparison to modern hard disks But because of parallelismthe SSD as a whole meets or exceeds the bandwidth of a traditional disk The greatstrength of SSDs is that they continue to perform well when data is read or writtenat random an access pattern thats predominant in realworld use That said Hitachi HGST now part of Western Digital deserves recognition as a particularlyhighreliability brand Over the last decade its drives have consistently led the reliability charts However HGSTbranded drives command a significant price premium over their competitors offeringsStorage device manufacturers like to quote sequential transfer rates for their products because the numbers are impressively high But for traditional hard disks thesesequential numbers have almost no relationship to the throughput observed withrandom reads and writesSSDs performance comes at a cost however Not only are they more expensive pergigabyte of storage than are hard disks but they also introduce several new wrinklesand uncertainties into the storage equation Anand Shimpis March article onSSD technology is a superb introduction to the promise and perils of the SSD Itcan be found at tinyurlcomdexnbtRewritability limitsEach page of flash memory in an SSD typically KiB on current products can berewritten only a limited number of times usually about depending on theunderlying technology To limit the wear on any given page the SSD firmwaremaintains a mapping table and distributes writes across all the drives pages Thisremapping is invisible to the operating system which sees the drive as a linear series of blocks Think of it as virtual memory for storageThe theoretical limits on the rewritability of flash memory are probably less an issue than they might initially seem Just as a matter of arithmetic you would haveto stream MBs of data to a GB SSD for more than continuous years tostart running up against the rewrite limit The more general question of longtermSSD reliability is as yet unanswered however We have a pretty good idea of howSSDs manufactured five years ago held up over time but todays products will nodoubt behave differentlyFlash memory and controller typesSSDs are constructed from several types of flash memory The main differenceamong the types has to do with how many bits of information are stored in eachindividual flash memory location Singlelevel cells SLC memories store a single bit theyre the fastest but most expensive option Also common in the mix aremultilevel cells MLC and triplelevel cells TLCSSD reviews lovingly describe these implementation details as a matter of coursebut its not clear why buyers should care Some SSDs are faster than others butno particular hardwarerelated insight is needed to appreciate this fact Standardbenchmarks capture the performance differences quite wellIn theory SLC flash memory has a reliability advantage over other types In practicereliability seems to have more to do with how well a drives firmware manages thememory and with how much memory the manufacturer has set aside for replacingcells that develop problems It pays to know your workloads For access patterns that are in fact heavily sequential hard disks canstill be competitive with SSDs especially when hardware costs are taken into considerationThe controllers that coordinate SSD components are still evolving Some are better than others but these days all mainstream offerings tend to be respectable Ifyou want to invest time in scrutinizing SSD hardware its usually more efficient toresearch the reputations of the flash memory controllers used to implement SSDsthan to investigate the individual brands and models of SSD SSD manufacturersare usually pretty open about the controllers theyre using If they wont tell youreviewers certainly willPage clusters and preerasingA further complication is that flash memory pages must be erased before they can berewritten SSDs handle this detail for you However erasing is a separate operationthat is slower than writing Its also impossible to erase individual pagesclusters ofadjacent pages typically pages or KiB must be erased together The writeperformance of an SSD can drop substantially when the pool of preerased pagesis exhausted and the drive must recover pages onthefly to service ongoing writesRebuilding a buffer of erased pages is harder than it might seem because filesystems designed for traditional hard disks do not actually erase data blocks they areno longer using A storage device doesnt know that the filesystem now considersa given block to be free it knows only that long ago someone gave it data to storethere For an SSD to maintain its cache of preerased pages and thus its write performance the filesystem must be capable of informing the SSD that certain pagesare no longer needed Support for this operation known as TRIM has finally become widespread among filesystems On our example systems the only filesystemthat does not yet support TRIM is ZFS on LinuxSSD reliabilityA paper by Bianca Schroeder et al googllzuXc summarized a vast set ofSSDrelated data from Googles data centers The main conclusions Memory technology has no relationship to reliability Reliability varieswidely among models but as with hard disks it can be assessed onlyretrospectively Most read errors occur at the bit level and are corrected through redundant storage coding These raw but correctable read errors are commonand expected They occur on most SSD drives on most days of operation The most common failure mode is to discover more bad bits in a blockthan can be fixed by the coding system These errors are detectable butuncorrectable they necessarily entail data loss Even among the most reliable SSD models of drives experienced atleast one uncorrectable read error Among the least reliable models Although both drive age and workload correlate with uncorrectableerror rates the correspondence is weak In particular the study foundno evidence for the notion that older SSDs are ticking time bombs thatasymptotically approach certain failure Because uncorrectable errors are only marginally correlated to workloadthe standard reliability figure quoted by manufacturersthe uncorrectable bit error rate or UBERis meaningless Workload has little effecton the number of errors observed so reliability should not be characterized as a rateThe most notable of these findings is that unreadable blocks are common and thatthey typically occur in isolation The usual scenario is for an SSD to report a blockerror but then continue to function normallyOf course unreliable storage devices are nothing new backups and redundancyremain essential no matter what hardware youre using However SSD failures aresneakier than those you might be accustomed to from dealing with hard disksUnlike a hard disk an SSD will rarely demand your attention by failing in someobvious and unambiguous way SSDs need structured and systematic monitoringErrors develop over time regardless of a drives duty cycle so SSDs are probablynot a good choice for archival storage And conversely an isolated bad block isnot an indication that an SSD has gone bad or is nearing the end of its useful lifeIn the absence of a larger pattern of failures its fine to reformat such a drive andreturn it to serviceHybrid drivesAfter spending many years in the vaporware category SSHDshard disks withbuiltin flash memory cacheshave become increasingly available Current products are pitched at consumersThe initialism SSHD stands for solid state hybrid drive and is something of a triumph of marketing designed as it is to encourage confusion with SSDs SSHDs arejust traditional hard disks with some extras on the logic board in reality theyreabout as solid state as the average dishwasherBenchmarks of current SSHD products have generally been unimpressive evenwhen the benchmarks attempt to emulate realworld access patterns In large partthats because the current products often include only a token amount of flashmemory cacheDespite current SSHDs lackluster performance the basic idea of multilevel caching is sound and has been well exploited in systems such as ZFS and Apples FusionDrive As the price of flash memory continues to fall we anticipate that platterbaseddrives will continue to include more and more cache Those products may or maynot be sold explicitly as SSHDsSee Chapter formore informationabout setting up acomprehensive surveillance programAdvanced Format and KiB blocksFor decades the standard size of a disk block was fixed at bytes Thats too smallto be practical from the perspective of most filesystems so the filesystems themselves have long aggregated byte sectors into page clusters of KiB to KiB thatare read and written togetherSince no software that communicates with storage hardware actually has an interest in reading and writing data at byte granularity its inefficient and wastefulfor the hardware to maintain such tiny sectors Over the last decade the storageindustry has migrated to a new standard block size of KiB known as AdvancedFormat All modern storage devices use KiB sectors internally although most ofthem continue to emulate byte blocks from the perspective of clientsThere are currently three different worlds that a storage device can live in n or native devices are the old ones that actually have bytesectors These devices are no longer manufactured but of course there arestill plenty of them out there in the real world These drives know nothingabout Advanced Format Kn or Knative devices are Advanced Format devices that have KiBsectors or pages in the case of SSDs and that report their block sizeas KiB to the host computer All interfacing hardware and all softwarethat deals directly with the device must be aware of and prepared to dealwith KiB blocksKn is the wave of the future but because it demands both hardware andsoftware support its adoption will be gradual Enterprise drives with Kninterfaces started becoming available in but at this point youre inno danger of encountering a Kn drive unless you explicitly order one e or emulated devices use KiB blocks internally but they report their sector size as bytes to the host computer Firmware in thedevice aggregates byte block operations into operations on the actualKiB storage blocksThe transition from n to e was completed in These two systems lookessentially identical from the perspective of the host computer so e deviceswork fine with old computers and old operating systemsThe one thing to know about e is that its sensitive to misalignment betweenfilesystem page clusters and hardware disk blocks Because the disk can only reador write KiB pages despite its emulation of traditional byte blocks filesystemcluster boundaries and hard disk block boundaries should coincide You wouldntwant a KiB logical cluster to correspond to half of one KiB disk block and halfof anotherwith that layout the disk might have to read or write twice as manyphysical pages as it should to service a given number of logical clustersSince filesystems usually count off their clusters starting at the beginning of whatever storage is allocated to them you can finesse the alignment issue by aligning diskpartitions to a powerof boundary that is large in comparison with the likely sizeof disk and filesystem pages eg KiB Partitioning tools on modern versions ofWindows Linux and BSD automatically enforce such alignment However edisks that were mispartitioned on legacy systems cant be transparently correctedyoull need to run an alignment utility to adjust the partition boundaries and physically move the data Or you can simply erase the device entirely and start over Storage hardware interfacesThese days only a few interface standards are in common use If a system supportsseveral different interfaces use the one that best meets your requirements for speedredundancy mobility and priceThe SATA interfaceSerial ATA SATA is the predominant hardware interface for storage In additionto supporting high transfer rates currently Gbs SATA has native support forhotswapping and optional command queueing two features that finally makeATA a viable alternative to SAS in server environmentsSATA cables slide easily onto their mating connectors but they can just as easilyslide off Cables with locking catches are available but theyre a mixed blessing Onmotherboards with six or eight SATA connectors packed together it can be hard todisengage the locking connectors without a pair of needlenosed pliersSATA also introduces an external cabling standard called eSATA The cables areelectrically identical to standard SATA but the connectors are slightly differentYou can add an eSATA port to a system that has only internal SATA connectors byinstalling an inexpensive converter bracketBe leery of external multidrive enclosures that have only a single eSATA portsome of these are smart RAID enclosures that require a proprietary driver andthe drivers rarely support UNIX or Linux Others are dumb enclosures that have aSATA port multiplier built in These are potentially usable on UNIX systems butsince not all SATA host adapters support port expanders pay close attention to thecompatibility information Enclosures with multiple eSATA portsone per drivebayare always safeThe PCI Express interfaceThe PCI Express Peripheral Component Interconnect Express abbreviated PCIebackplane bus has been used on PC motherboards for more than a decade Itsnow the predominant standard for connecting all kinds of addon circuit boardseven video cardsAs the SSD market developed it became clear that even at Gbs the speed of SATAinterfaces would soon become inadequate to handle the fastest storage devices Ratherthan assuming the traditional shape of a laptop hard disk highend SSDs beganto take the form of circuit boards that plugged directly into the systems PCIe busPCIe was attractive because of its flexible architecture and fast signaling rate Theversion that is now mainstream PCIe has a signaling rate of gigatransfersper second GTs The actual throughput depends on how many signaling channels a device has there can be as few as or as many as The widest devices canachieve more than GBs of throughput The soontodebut PCIe standarddoubles the basic signaling rate to GTsWhen comparing PCIe to SATA keep in mind that SATAs speed of Gbs is quoted in gigabits per second Fullwidth PCIe is actually more than times fasterthan SATAThe SATA standard is feeling the pressure Unfortunately the SATA ecosystem isconstrained by past design choices and by the need to support existing cabling andconnectors Its unlikely that the speed of SATA interfaces can be meaningfully improved over the next few yearsInstead recent work has focused on attempting to unify SATA and PCIe at the levelof interconnections The M standard for plugin cards routes SATA PCIe withup to four data lanes and USB connectivity over a standard connector Oneor two of these slots are now standard on laptop computers and they can also befound on desktop systemsM cards are about an inch wide and can be up to about four inches long They arethin with only a few millimeters allowed on both sides for componentsU is more recent tweak to the M approach its just starting to become availableInstead of USB U feature SAS connectivity in addition to SATA and PCIeThe SAS interfaceSAS stands for Serial Attached SCSI the SCSI portion of which denotes the SmallComputer System Interface a generic data pipe that once connected many differenttypes of peripherals These days USB has captured the market for peripheral connections and SCSI is found only in the form of SAS an enterpriselevel interfaceused to connect large numbers of storage devicesNow that SAS and SCSI are largely synonymous the vast history of different SCSItechnologies dating back to serves mostly to create confusion Operating systems further muddy the waters by filtering all disk access through a SCSI subsystem regardless of whether an actual SCSI device is involved or not Our advice isto ignore all this history and consider SAS as its own system Its not quite GBs because some of the bandwidth is consumed by signaling overhead Howeverthe amount of overhead is so small about that it can safely be ignoredLike SATA SAS is a pointtopoint system you plug a drive into a SAS port througha cable or directmount backplane However SAS allows expanders to connectmultiple devices to a single host port Theyre analogous to SATA port multipliersbut whereas support for port multipliers is hit or miss SAS expanders are alwayssupportedSAS currently operates at Gbs twice the speed of SATAIn past editions of this book SCSI was the obvious interface choice for server applications It offered the highest available bandwidth outoforder command executionaka tagged command queueing lower CPU utilization easier handling of largenumbers of storage devices and access to the markets most advanced hard drivesThe advent of SATA has removed or minimized most of these advantages so SASsimply does not deliver the clear advantages that SCSI used to SATA drives compete with and in some cases outperform equivalent SAS disks in nearly everycategory At the same time both SATA devices and the interfaces and cabling usedto connect them are cheaper and far more widely availableSAS still holds a few trump cards Manufacturers continue to use the SATASAS divide to stratify the storage market To help justify premium pricing the fastest and most reliabledrives are still available only with SAS interfaces SATA is limited to a queue depth of pending operations SAS can handle thousands SAS can handle many storage devices hundreds or thousands on a single host interface But keep in mind that all those devices share a singlepipe to the host you are still limited to Gbs of aggregate bandwidthThe SAS vs SATA debate may ultimately be moot because the SAS standard includes support for SATA drives SAS and SATA connectors are similar enough thata single SAS backplane can accommodate drives of either type At the logical layerSATA commands are simply tunneled over the SAS busThis convergence is an amazing technical feat but the economic argument for it isless clear The expense of a SAS installation is mostly in the host adapter backplaneand infrastructure the SAS drives themselves arent outrageously priced Onceyouve invested in a SAS setup you might as well stick with SAS from end to endOn the other hand perhaps the modest price premiums for SAS drives are a resultof the fact that SATA drives can easily be substituted for themUSBThe Universal Serial Bus USB is a popular option for connecting external harddisks Current speeds are Gbs for USB and up to GBs for USB Both The speed of USB is often cited as Gbs but because of mandatory encoding overhead the actual transfer rate is more like Gbssystems are fast enough to accommodate all but the fastest SSDs streaming data atfull speed Watch out for USB however it tops out at Mbs which is tooslow to keep up with even a mechanical hard driveStorage devices themselves never come with native USB interfaces External drivessold with these interfaces are invariably SATA drives with a protocol converter builtinto the enclosure You can also buy these enclosures separately and install yourchoice of hard disksUSB adapters are also available in the form of cradles and cable dongles Cradlesare particularly helpful when disks must be swapped out frequently just yank outthe old disk and pop in a new oneUSB thumb drives are perfectly legitimate storage devices They present a blockinterface similar to that of any other disk although throughput is typically mediocre The underlying technology is similar to that of an SSD but without some ofthe flourishes that give SSDs their superior speed and robustness Attachment and lowlevel management of drivesThe way a disk is attached to the system depends on the interface The rest is allmounting brackets and cabling Fortunately modern connection schemes are allpretty much idiotproofSAS is a hotpluggable interface so its fine to plug in new drives without poweringoff the system or restarting it The kernel should automatically recognize new devices and create device files for them SATA interfaces can also theoretically supporthotplugging However the SATA specification does not require support for thisfeature and most massmarket hardware does not implement itIts fine to attempt hotplugging a SATA drive to find out if hotplugging works ona particular system You wont hurt anything The worst that can happen is that thesystem ignores the driveInstallation verification at the hardware levelAfter you install a new disk check to make sure that the system acknowledges itsexistence at the lowest possible level On a physical PC this is easy the BIOS showsyou a list of SATA and USB disks connected to the system SAS disks may be included here as well if the motherboard supports them directly If the system has aseparate SAS interface card you might need to invoke the BIOS setup for that cardto see the disk inventory Hotplugging might seem like a neat trick that creates all sorts of options such as the ability to swapout a bad drive with little or no softwareside wrangling However its tricky to get the higher layersof the storage stack tuned to achieve these feats safely and reliably We dont describe the managementof hotplugging in this bookSee page formore informationabout dynamic handling of devicesOn cloud servers and systems that support hotpluggable drives you might haveto do some sleuthing Check the diagnostic output from the kernel as it probes fordevices For example one of our test systems showed the following messages foran older SCSI disk attached to a BusLogic SCSI host adapterscsi BusLogic BTscsi host Vendor SEAGATE Model STW Rev Type DirectAccess ANSI SCSI revision Detected scsi disk sda at scsi channel id lun scsi Target Queue Depth AsynchronousSCSI device sda hdwr sector bytes Sectors MB GBYou may be able to review this information after the system has finished bootinglook in your system log files See the material starting on page for more information about the handling of boottime messages from the kernelSeveral commands can print out a list of the disks that the system is aware of OnLinux systems the best option is usually lsblk which is standard on all distributions For more information ask for model and serial numberslsblk o MODELSERIALOn FreeBSD use geom disk listDisk device filesA newly added disk is represented by device files in dev See page for generalinformation about device filesAll our example systems automatically create these files for you but you still need toknow where to look for the device files and how to identify the ones that correspondto your new device Formatting the wrong device file is a rapid route to disasterTable summarizes the device naming conventions for disks on our example systems Instead of showing the abstract pattern according to which devices are namedTable simply shows a typical example for the name of the systems first diskTable Device naming standards for disksSystem Whole disk PartitionLinux devsda devsdaFreeBSD devada devadapDevice names for whole disks comprise a basename that depends on the devicedriver and a sequence number or letter that differentiates disks from each other Forexample devsda on Linux is the first drive managed by the sd driver The nextdrive would be devsdb and so on FreeBSD has different driver names and usesnumbers instead of letters but the pattern is the sameDont ascribe too much significance to the driver names that show up in disk devicefiles Modern kernels funnel both SATA and SAS management through a genericSCSI layer so dont be surprised to see SATA disks masquerading as SCSI devicesDriver names also vary on cloud and virtualized systems a virtual SATA disk mayor may not have the same driver name as an actual SATA diskDevice files for partitions add an additional decoration to the device file to indicate the partition number Partition numbering normally starts at rather than Ephemeral device namesDisk names are assigned in sequence as the kernel enumerates the various interfaces and devices on the system Adding a disk can cause existing disks to changetheir names In fact even rebooting the system can sometimes cause name changesThese facts suggest a couple of good rules for system administrators to follow Never make changes to disks partitions or filesystems without verifyingthe identity of the disk youre working on even on a stable system Never mention a disk device in any sort of configuration file lest it changeout from under you at some point in the futureThe latter issue is most notable when you are setting up the etcfstab file whichlists filesystems for the system to mount at boot time It was once common to identify disk partitions by their device files in etcfstab but this is no longer safe Seepage for some alternative approachesLinux has a couple of general ways around the ephemeral names issue Subdirectories under devdisk list disks by various stable characteristics such as theirmanufacturer ID or connection information These device names which are reallyjust links back to the toplevel files in dev are stable but theyre long and awkwardAt the level of filesystems and disk arrays Linux uses both unique ID strings andtext labels to persistently identify objects In many cases the existence of these longIDs is cleverly concealed so that you dont have to deal with them directlyparted l lists the sizes partition tables model numbers and manufacturers ofevery disk on the systemFormatting and bad block managementPeople sometimes use the word formatting to mean writing a partition table ona disk and setting up filesystems in the partitions But in this section we use theword formatting to mean the more fundamental operation of setting up a disksmedia at the hardware level Wed prefer to call the former operation initializingbut in the real world the terms are used more or less interchangeably so you haveto decode the meaning through contextThe formatting process writes address information and timing marks on the plattersto delineate each sector It also identifies bad blocks imperfections in the mediathat result in areas that cannot be reliably read or written All modern disks havebad block management built in so neither you nor the driver need worry aboutmanaging defects The drive firmware substitutes knowngood blocks from an areaof backup storage on the disk that is reserved for this purposeAll hard disks come preformatted and the factory formatting is at least as good asany formatting you can do in the field It is best to avoid doing a lowlevel formatif its not required Dont reformat new drives as a matter of courseIf you encounter read or write errors on a disk first check for cabling terminationand address problems all of which can cause symptoms similar to those of a badblock If after this procedure you are still convinced that the disk has defects youmight be better off replacing it with a new one rather than waiting long hours fora format to complete and hoping the problem doesnt come backBad blocks that manifest themselves after a disk has been formatted may or may notbe automatically handled If the drive is sure that the affected data can be reliablyreconstructed the newly discovered defect might be mapped out on the fly andthe data rewritten to a new location For more serious or less clearly recoverableerrors the drive aborts the read or write operation and reports the error back tothe host operating systemSATA disks are usually not designed to be formatted outside the factory Howeveryou might be able to obtain formatting software from the manufacturer usually forWindows Make sure the software matches the drive you plan to format and followthe manufacturers directions carefullySAS disks format themselves in response to a standard command that you send fromthe host computer The procedure for sending this command varies from system tosystem On PCs you can often send the command from the SAS controllers BIOSTo issue the format command from within the operating system use the sgformatcommand on Linux and the camcontrol command on FreeBSDVarious utilities let you verify the integrity of a disk by writing random patterns toit and then reading them back Thorough tests take a long time hours and unfortunately seem to be of little prognostic value Unless you suspect that a disk isbad and are unable to simply replace it or you bill by the hour you can skip thesetests Barring that let the tests run overnight Dont be concerned about wearingout a disk with overuse or aggressive testing Enterpriseclass disks are designedfor constant activity On the other hand at for a TB drive why botherATA secure eraseSince PATA and SATA disks have implemented a secure erase commandthat overwrites the data on the disk according to a method the manufacturer hasdetermined to be secure against recovery efforts Secure erase is NISTcertified formost needs Under the US Department of Defense categorization its approvedfor use at security levels less than secretWhy is this feature even needed First filesystems generally do no erasing of theirown so an rm rf of a disks data leaves everything intact and recoverable withsoftware tools Its critically important to remember this fact when disposing ofdisks whether their destination is eBay or the trashSecond even a manual rewrite of every sector on a traditional hard disk can leavemagnetic traces that are recoverable by a determined attacker with access to a laboratory Secure erase performs as many overwrites as are needed to eliminate theseshadow signals Magnetic remnants wont be a serious concern for most sites butits always nice to know that youre not exporting your organizations confidentialdata to the world at large Some sites may have regulatory or business requirementsthat dictate how data is to be erasedFinally secure erase has the effect of resetting SSDs to their fully erased state Thisreset can improve performance in cases in which the ATA TRIM command thecommand to erase a block cannot be issued either because the filesystem used onthe SSD does not know to issue it or because the SSD is connected through a hostadapter or RAID interface that does not propagate TRIMThe ATA secure erase command is passwordprotected at the drive level to reducethe risk of inadvertent activation Therefore you must set the password on a drivebefore invoking the command Dont bother to record the password however youcan reset it at will There is no danger of locking the driveUnder Linux you can use the hdparm command to activate secure erase sudo hdparm usermaster u securitysetpass password devdisk sudo hdparm usermaster u securityerase password devdiskThe analogous FreeBSD command is camcontrol sudo camcontrol security disk U user s password e passwordThe SAS world has no analog to ATAs secure erase command but the SCSI formatunit command described under Formatting and bad block management startingon page is a reasonable alternativeMany systems have a shred utility that attempts to securely erase the contents ofindividual files Unfortunately it relies on the assumption that a files blocks canbe overwritten in place This assumption is invalid in so many circumstances any Now that most filesystems support the TRIM command to inform SSDs of blocks that are no longerneeded by the system this statement is not quite as true as it used to be However TRIM is advisoryan SSD is not required to erase anything in responsefilesystem on any SSD any logical volume that has snapshots anything on ZFS orBtrfs that shreds general utility is questionableFor sanitizing an entire PC system at once another option is Dariks Boot and Nukedbanorg This tool runs from its own boot disk so its not a tool youll use everyday It is quite handy for decommissioning old hardware howeverhdparm and camcontrol set disk and interface parametersThe hdparm Linux and camcontrol FreeBSD commands can do more thanjust send secure erase commands They give you a general way to interact with thefirmware of SATA and SAS hard disksAs tools that operate close to the hardware layer these commands work properlyonly on nonvirtualized systems On a traditional physical server they are actuallythe best way to get information about the systems disk devices hdparm I andcamcontrol devlist we dont mention them elsewhere eg in the adding a diskrecipes at the start of this chapter only because they dont work on virtual systemshdparm comes from the prehistoric world of IDE and has gradually grown to include coverage of SATA and SCSI features camcontrol started as a SCSI wranglingtool and has been extended to cover some SATA features The syntaxes are differentbut the tools cover approximately the same territory these daysAmong other things these tools can set drive power options enable or disablenoise reduction options set the readonly flag and print detailed drive informationHard disk monitoring with SMARTHard disks are faulttolerant systems that use errorcorrection coding and intelligentfirmware to hide their imperfections from the host operating system In some cases an uncorrectable error that the drive is forced to report to the OS is merely thelatest event in a long crescendo of correctable but inauspicious problems It wouldbe nice to know about those omens before the crisis occursSATA devices implement a detailed form of status reporting that is sometimes predictive of drive failures This standard called SMART for selfmonitoring analysis and reporting technology exposes more than operational parameters forinvestigation by the host computerThe Google disk drive study mentioned on page has been widely summarizedin media reports as concluding that SMART data is not predictive of drive failureThat summary is not accurate In fact Google found that four SMART parameterswere highly predictive of failure but that failure was not consistently preceded bychanges in SMART values Of failed drives in the study showed no change inthe four most predictive parameters On the other hand predicting nearly half offailures sounds pretty good to usThose four sensitive SMART parameters are Scan error count Reallocation count Offline reallocation count Number of sectors on probationThose values should all be zero According to the Google Labs study a nonzerovalue in these fields raises the likelihood of failure within days by a factor of or respectivelyTo take advantage of SMART data you need software that queries your drives toobtain it and then judges whether the current readings are sufficiently ominousto warrant administrator notification Unfortunately reporting standards vary bydrive manufacturer so decoding isnt necessarily straightforward Most SMARTmonitors collect baseline data and then look for sudden changes in the bad direction rather than interpreting absolute values According to the Google studytaking account of these soft SMART indicators in addition to the Big Four predicts of all failuresThe standard software for SMART wrangling is the smartmontools package fromsmartmontoolsorg Its installed by default on Red Hat CentOS and FreeBSD systems and is usually in the default package repository on other systemsThe smartmontools package consists of a smartd daemon that monitors drivescontinuously and a smartctl command you can use for interactive queries or forscripting The daemon has a single configuration file normally etcsmartdconfwhich is extensively commented and includes plenty of examplesSCSI has its own system for outofband status reporting but unfortunately thestandard is much less granular in this respect than is SMART The smartmontoolsattempt to include SCSI devices in their schema but the predictive value of theSCSI data is less clear The software side of storage peeling the onionIf youre accustomed to plugging in a disk and having your Windows system askif you want to format it you may be a bit taken aback by the apparent complexityof storage management on UNIX and Linux systems Why is it all so complicatedTo begin with much of the complexity is optional On UNIX and Linux systemswith a window manager you can log in to your systems desktop connect that sameUSB drive and have much the same experience as on Windows Youll get a simplesetup for personal data storage If thats all you need youre good to goAs usual in this book were primarily interested in enterpriseclass storage systemsfilesystems that are accessed by many users or processes both local and remoteand that are reliable highperformance easy to back up and easy to adapt to futureneeds These systems require a bit more thought and UNIX and Linux give youplenty to think aboutElements of a storage systemExhibit A shows a typical set of software components that can mediate betweena raw storage device and its end users The architecture shown in Exhibit A is forLinux but other systems include similar features although not necessarily in thesame packagesExhibit A Storage management layersFilesystems swap areas database storageLogical volumesPartitions RAID arrays Volume groupsStorage devicesThe arrows in Exhibit A mean can be built on For example a Linux filesystemcan be built on top of a partition a RAID array or a logical volume Its up to theadministrator to construct a stack of modules that connect each storage device toits final applicationSharpeyed readers will note that the graph has a cycle but realworld configurations should not loop Linux allows RAID and logical volumes to be stacked ineither order but neither component should be used more than once though it istechnically possible to do thisHeres what the pieces in Exhibit A represent A storage device is anything that looks like a disk It can be a hard diska flash drive an SSD an external RAID array implemented in hardwareor even a network service that gives blocklevel access to a remote deviceThe exact hardware doesnt matter as long as the device allows randomaccess handles block IO and is represented by a device file A partition is a fixedsize subsection of a storage device Each partitionhas its own device file and acts much like an independent storage deviceFor efficiency the same driver that handles the underlying device usually implements partitioning Partitioning schemes consume a few blocksat the start of the device to record the ranges of blocks in each partition Volume groups and logical volumes are associated with logical volumemanagers LVMs These systems aggregate physical devices to form poolsof storage called volume groups An administrator can then subdivide thispool into logical volumes in much the same way that disks can be dividedinto partitions For example a TB disk and a TB disk could be aggregated into an TB volume group and then split into two TB logical volumes At least one volume would include data blocks from both hard disksSince the LVM adds a layer of indirection between logical and physicalblocks it can freeze the logical state of a volume simply by making a copyof the mapping table Therefore logical volume managers often have somekind of a snapshot feature Writes to the volume are then directed tonew blocks and the LVM keeps both the old and new mapping tablesOf course the LVM has to store both the original image and all modifiedblocks so it can eventually run out of space if a snapshot is never deleted A RAID array a redundant array of inexpensiveindependent diskscombines multiple storage devices into one virtualized device Dependingon how you set up the array this configuration can increase performanceby reading or writing disks in parallel increase reliability by duplicating or paritychecking data across multiple disks or both RAID can beimplemented by the operating system or by various types of hardwareAs the name suggests RAID is typically conceived of as an aggregationof bare drives but modern implementations let you use as a componentof a RAID array anything that acts like a disk A filesystem mediates between the raw bag of blocks presented by a partition RAID array or logical volume and the standard filesystem interfaceexpected by programs paths such as varspoolmail UNIX file typesUNIX permissions etc The filesystem determines where and how thecontents of files are stored how the filesystem namespace is representedand searched on disk and how the system is made resistant to or recoverable from corruptionMost storage space ends up as part of a filesystem but on some systemsnot current versions of Linux swap space and database storage can potentially be slightly more efficient without help from a filesystem Thekernel or database imposes its own structure on the storage renderingthe filesystem unnecessaryIf it seems to you that this taxonomy has a few too many little components that simply implement one block storage device in terms of another youre in good companyThe trend over the last few years has been toward consolidating these componentsto increase efficiency and remove duplication Although logical volume managersdid not originally function as RAID controllers most have absorbed some RAIDlike features notably striping and mirroringOn the cutting edge today are systems that combine a filesystem a RAID controller and an LVM system all in one tightly integrated package ZFS was the earliestexample but the Btrfs filesystem for Linux has similar design goals We have lotsmore to say about ZFS and Btrfs starting on page Spoiler alert if you canuse one of these systems you probably shouldThe Linux device mapperFor simplicity we omitted a central component of the Linux storage stack fromExhibit A on page the device mapper This is a protean little beastie that hasfingers inserted in multiple contexts prime examples being the implementation ofLVM the implementation of filesystem layers for containerization see Chapter and the implementation of wholedisk encryption search the web for LUKSThe device mapper abstracts the idea of one block device being built on a collectionof other block devices Given a mapping table of devices it implements the ongoingtranslation among them and routes each block to its appropriate homeFor the most part the device mapper is part of the implementation of Linux storageand not something youll deal with directly However youll see its traces wheneveryou access devices under devmapper You can also set up your own mapping tables with the dmsetup command although cases in which you might need to dothat are relatively rareIn the next sections we look in more detail at the layers involved in storage configuration partitioning RAID logical volume management and filesystems Disk partitioningPartitioning and logical volume management are both ways of dividing up a diskor pool of disks in the case of LVM into separate chunks of known size Linuxand FreeBSD support both of these methodsTraditionally partitioning was the lowest possible level of disk management andonly disks could be partitioned You could put individual disk partitions underthe control of a RAID controller or logical volume manager for example but youcouldnt then partition the resulting logical volumes or RAID volumesThe rule that only disks can be partitioned is increasingly being waived in favor ofa more general model in which disks partitions LVM pools and RAID arrays canbe derived from one another in any order or combination From the standpointof software architecture this is beautiful and elegant But from the standpoint ofpracticality it has the unfortunate side effect of implying that theres some validreason to partition entities other than disksIn fact partitioning is less desirable than logical volume management in mostrespects Its coarse and brittle and lacks features such as snapshot managementPartitioning decisions are difficult to revise later The only notable advantages ofpartitioning over logical volume management are its simplicity and the fact thatWindows and PC BIOSs understand and expect it A few versions of UNIX that runon proprietary hardware have done away with partitioning altogether and nobodyon those systems seems to miss itBoth partitions and logical volumes make backups easier prevent users from poaching each others disk space and confine potential damage from runaway programsAll systems have a root partition that includes and most of the local hosts configuration data In theory everything needed to bring the system up to singleusermode is part of the root partition Various subdirectories most commonly varusr tmp share and home can be broken out into their own partitions or volumes Most systems also have at least one swap areaOpinions differ on the best way to divide up disks as do the defaults used by various systems Most setups are relatively simple Exhibit B illustrates a traditionalpartitionsandfilesystems schema as it might be found on a couple of data diskson a Linux system The boot disk is not shownExhibit B Traditional data disk partitioning scheme Linux device namesPhysicallayerPartitionlayerFilesystemlayerdevsdahomedevsdaoptdevsdalabeldevsdbspareHard disk devsdbHard disk labelHere are some general points to guide you In the distant past it was sometimes useful to have a backup root devicethat you could boot to if something went wrong with the normal rootpartition These days a bootable USB thumb drive or an OS installationDVD is a better recovery option for most systems Backup root partitionsare more trouble than theyre worth Putting tmp on a separate filesystem limits temporary files to a finite sizeand saves you from having to back them up Some systems use a memorybased filesystem to hold tmp for performance reasons The memorybased filesystems are still backed by swap space so they work well ina broad range of situations Since log files are kept in varlog its a good idea for either var or varlogto be a separate disk partition Leaving var as part of a small root partitionmakes it easy to fill the root and bring the machine to a halt Its useful to put users home directories on a separate partition or volumeEven if the root partition is corrupted or destroyed user data has a goodchance of remaining intact Conversely the system can continue to operate even after a users misguided shell script fills up home Splitting swap space among several physical disks can potentially increaseperformance although with todays cheap RAM its usually better not toswap at all This technique works for filesystems too put the busy oneson different disks See page for notes on this subject As you add memory to your machine also add swap space See page for more information about virtual memory Try to cluster quickly changing information on a few partitions that arebacked up frequently The Center for Internet Security publishes configuration guidelines for avariety of operating systems at wwwcisecurityorgcisbenchmarks Theyare benchmarks in the sense of being best practices The documentsinclude helpful recommendations for partitioning and filesystem layoutTraditional partitioningSystems that support partitions implement them by writing a label at the beginning of the disk to define the range of blocks included in each partition The exactdetails vary the label must often coexist with other startup information such as aboot block and it often contains extra information such as a name or unique IDthat identifies the disk as a wholeThe device driver responsible for representing the disk reads the label and uses thepartition table to calculate the physical location of each partition Typically onedevice file represents each partition and an additional device file represents thedisk as a wholeDespite the universal availability of logical volume managers some situations stillrequire or benefit from traditional partitioning Only two partitioning schemes are used these days MBR and GPT Wediscuss the details of both schemes in the next sections On PC hardware the boot disk must have a partition table Systems manufactured before usually require MBR and some new systems requireGPT Most new systems support both Installing an MBR or GPT partition table makes a disk comprehensible toWindows even if the contents of the individual partitions are not Thoughyou may have no particular plans to interoperate with Windows consider the ubiquity of Windows the prevalence of virtual machines and theportability of hard disks Partitions have a defined location on the disk so they guarantee localityof reference Logical volumes do not at least not by default In most cases this fact isnt terribly important However short seeks are faster thanlong seeks on mechanical hard disks and the throughput of a disks outercylinders those containing the lowestnumbered blocks can exceed thethroughput of its inner cylinders by or more RAID systems see page use disks or partitions of matched size Agiven RAID implementation might accept entities of different sizes but itwill probably use only the block ranges that all devices have in commonRather than letting extra space go to waste you can isolate it in a separatepartition If you do this however use the spare partition for data that isinfrequently accessed otherwise traffic on the partition will degrade theperformance of the RAID arrayMBR partitioningMBR Master Boot Record partitioning is an old Microsoft standard that datesback to the s Its a cramped and illconceived format that cant support diskslarger than TB Who knew disks could ever get that bigMBR offers no advantages over GPT except that its the only format from whichold PC hardware can boot Windows Unless youre forced by circumstances to useMBR partitions you typically dont want them Unfortunately MBR is still a common default setup for many distributions installersThe MBR label occupies a single byte disk block most of which is consumed byboot code Only enough space remains to define four partitions These are termedprimary partitions because they are defined directly in the MBRYou can in theory define one of the primary partitions to be an extended partition which means that it contains its own subsidiary partition table Unfortunatelyextended partitions have been known to cause a variety of subtle problems Its bestto avoid them in these twilight years of MBRThe Windows partitioning system lets one partition be marked active Boot loaderslook for the active partition and try to load the operating system from itEach partition also has a onebyte type attribute that is supposed to signal the partitions contents Generally the codes represent either filesystem types or operatingsystems These codes are not centrally assigned but some common conventionshave evolved They are summarized by Andries E Brouwer at googlATiThe MSDOS command that partitioned hard disks was called fdisk Most operating systems that support MBRstyle partitions have adopted this name for theirown partitioning commands but there are many variations among fdisks Windowsitself has moved on the commandline tool in recent versions is called diskpartWindows also has a partitioning GUI thats available through the Disk Management plugin of mmcIt does not matter whether you partition a disk with Windows or some other operating system The end result is the sameGPT GUID partition tablesIntels extensible firmware interface EFI project replaced the rickety conventionsof PC BIOSs with a more modern and functional architecture EFI firmware is nowstandard for new PC hardware and EFIs partitioning scheme has gained universalsupport among operating systemsThe EFI partitioning scheme known as a GUID partition table or GPT removesthe obvious weaknesses of MBR It defines only one kind of partition no morelogical partitions in the extended partition and you can create arbitrarily manyof them Each partition has a type specified by a byte ID code a globally uniqueID or GUID that requires no central arbitrationSignificantly GPT retains primitive compatibility with MBRbased systems by dragging along an MBR as the first block of the partition table This fakie MBR makesthe disk look like its occupied by one large MBR partition at least up to the TBlimit of MBR It isnt useful per se but the hope is that the decoy MBR will at leastprevent nave systems from attempting to reformat the diskVersions of Windows from the Vista era forward support GPT disks for data butonly systems with EFI firmware can boot Windows from them Linux and its GRUBboot loader have fared better GPT disks are supported by the OS and bootable onany system Intelbased macOS systems use both EFI and GPT partitioningAlthough GPT has already been well accepted by operating system kernels manydisk management utilities are unmaintained and lack support for it Make sure thatany utility you run on a GPT disk actually supports GPTLinux partitioningLinux systems give you several options for partitioning which makes for treacherous terrain given that some of the offerings are not GPTaware Default to parteda commandline tool that understands several label formats including Solarissnative one and can move and resize partitions in addition to simply creating anddeleting them A GUI version gparted runs under GNOMEIn general we recommend gparted over parted Both are simple but with gpartedyou can specify the size of the partitions you want instead of specifying the startingand ending block ranges For partitioning the boot disk most distributions graphical installers are the best option since they typically suggest a partitioning plan thatworks well with that particular distributions layout EFI has more recently become UEFI a unified EFI effort supported by multiple vendors HoweverEFI remains the more common term in general use UEFI and EFI are essentially interchangeableFreeBSD partitioningLike Linux FreeBSD has several partitioning tools Ignore all except gpart Theothers exist only to lure you into making some kind of terrible mistakeThe mysterious geoms youll see referred to in the gpart man page and in otherstoragerelated contexts on FreeBSD are FreeBSDs abstraction of storage devicesNot all geoms are disk drives but all disk drives are geoms so you can use a genericdisk name such as ada wherever a geom is called forThe FreeBSD adding a disk recipe on page uses gpart to configure the partition table on a new disk Logical volume managementImagine a world in which you dont know exactly how large a partition needs to beSix months after creating the partition you discover that it is much too large butthat a neighboring partition doesnt have enough space Sound familiar A logicalvolume manager lets you reallocate space dynamically from the greedy partitionto the needy partitionLogical volume management is essentially a supercharged and abstracted versionof disk partitioning It groups individual storage devices into volume groups Theblocks in a volume group can then be allocated to logical volumes which are represented by block device files and act like disk partitionsHowever logical volumes are more flexible and powerful than disk partitions Hereare some of the magical operations a volume manager lets you carry out Move logical volumes among different physical devices Grow and shrink logical volumes on the fly Take copyonwrite snapshots of logical volumes Replace online drives without interrupting service Incorporate mirroring or striping in your logical volumesThe components of a logical volume can be put together in various ways Concatenation keeps each devices physical blocks together and lines the devices up oneafter another Striping interleaves the components so that adjacent virtual blocksare actually spread over multiple physical disks By reducing singledisk bottlenecksstriping can often result in higher bandwidth and lower latencyIf youve had some prior exposure to RAID see the section starting on page you might find striping reminiscent of RAID LVM implementations of stripingtend to be more flexible than RAID though For example they may automaticallyoptimize striping or allow devices of different sizes to be striped even if stripingwont actually happen of the time The line between LVM and RAID has become blurry indeed and even parity schemes like RAID and RAID are makingregular appearances in volume managersLinux logical volume managementLinuxs volume manager called LVM is essentially a clone of HPUXs volumemanager which is itself based on software by Veritas The commands for the twosystems are essentially identical Table summarizes the LVM command setTable LVM commands in LinuxEntity Operation CommandPhysical volume Create pvcreateInspect pvdisplayModify pvchangeCheck pvckVolume group Create vgcreateModify vgchangeExtend vgextendInspect vgdisplayCheck vgckEnable vgscanLogical volume Create lvcreateModify lvchangeResize lvresizeInspect lvdisplayThe toplevel architecture of LVM is that individual disks and partitions physicalvolumes are gathered into storage pools called volume groups Volume groups arethen subdivided into logical volumes which are the block devices that hold filesystemsA physical volume needs to have an LVM label applied with pvcreate Applyingsuch a label is the first step to accessing the device through the LVM In additionto bookkeeping information the label includes a unique ID to identify the devicePhysical volume is a somewhat misleading term because physical volumes neednot have a direct correspondence to physical devices They can be disks but theycan also be disk partitions or RAID arrays LVM doesnt careYou can control LVM with either a large group of simple commands the ones listed in Table or with the single lvm command and its various subcommandsThese options are essentially identical in fact the individual commands are justlinks to lvm which looks to see how its been called to know how to behave manlvm is a good introduction to the system and its toolsA Linux LVM configuration proceeds in a few distinct phases Creating defining really and initializing physical volumes Adding the physical volumes to a volume group Creating logical volumes on the volume groupLVM commands start with letters that make it clear at which level of abstractionthey operate pv commands manipulate physical volumes vg commands manipulatevolume groups and lv commands manipulate logical volumes A few commandswith the prefix lvm eg lvmchange operate on the system as a wholeIn the following example we set up a TB hard disk devsdb for use with LVMand create a logical volume We assume that the disk has been partitioned as described on page with all space being assigned to a single partition devsdbWe could omit the partitioning step entirely and just use the raw disk as our physical device but there is no performance benefit to doing so Partitioning makesthe disk comprehensible to the broadest variety of software and operating systemsThe first step is to label the sdb partition as an LVM physical volume sudo pvcreate devsdbPhysical volume devsdb successfully createdOur physical device is now ready to be added to a volume group sudo vgcreate DEMO devsdbVolume group DEMO successfully createdAlthough were using only a single physical device in this example we could ofcourse add additional devices To step back and examine our handiwork we usethe vgdisplay command sudo vgdisplay DEMO Volume group VG Name DEMOSystem IDFormat lvmMetadata Areas Metadata Sequence No VG Access readwriteVG Status resizableOpen LV Max PV Cur PV Act PV VG Size GiBPE Size MiBTotal PE Alloc PE Size Free PE Size GiBVG UUID nrxjXHNxnvrdnMAWeOQEdDwEOA PE is a physical extent the allocation unit according to which the volume groupis subdividedThe final steps are to create the logical volume within DEMO and then to create afilesystem within that volume We make the logical volume GB in size sudo lvcreate L G n web DEMOLogical volume web createdMost of LVMs interesting options live at the logical volume level Thats wherestriping mirroring and contiguous allocation would be requested if we were using those featuresWe can now access the volume through the device devDEMOweb We discussfilesystems in general starting on page but here is a quick overview of creating an ext filesystem so that we can demonstrate a few additional LVM tricks sudo mkfs devDEMOweb sudo mkdir mntweb sudo mount devDEMOweb mntwebVolume snapshotsYou can create copyonwrite duplicates of any LVM logical volume whether ornot it contains a filesystem This feature is handy for creating a quiescent image ofa filesystem to be backed up elsewhere but unlike ZFS and Btrfs snapshots LVMsnapshots are unfortunately not very useful as a general method of version controlThe problem is that logical volumes are of fixed size When you create one storagespace is allocated for it up front from the volume group A copyonwrite duplicateinitially consumes no space but as blocks are modified the volume manager mustfind space in which to store both the old and new versions This space for modifiedblocks must be set aside when you create the snapshot and like any LVM volumethe allocated storage is of fixed sizeNote that it does not matter whether you modify the original volume or the snapshot which by default is writable Either way the cost of duplicating the blocks ischarged to the snapshot Snapshots allocations can be pared away by activity onthe source volume even when the snapshots themselves are idleIf you do not allocate as much space for a snapshot as is consumed by the volumeof which it is an image you can potentially run out of space in the snapshot Thatsmore catastrophic than it sounds because the volume manager then has no way tomaintain a coherent image of the snapshot additional storage space is required justto keep the snapshot the same The result of running out of space is that LVM stopsmaintaining the snapshot and the snapshot becomes corruptedSo as a matter of practice LVM snapshots should be either shortlived or as largeas their source volumes So much for lots of cheap virtual copiesTo create devDEMOwebsnap as a snapshot of devDEMOweb we woulduse the following command sudo lvcreate L G s n websnap DEMOwebLogical volume websnap createdNote that the snapshot has its own name and that the source of the snapshot mustbe specified as volumegroupvolumeIn theory mntweb should really be unmounted first to ensure the consistency ofthe filesystem In practice ext protects us against filesystem corruption althoughwe might lose a few of the most recent data block updates This is a perfectly reasonable compromise for a snapshot used as a backup sourceTo check on the status of your snapshots run lvdisplay If lvdisplay tells you thata snapshot is inactive that means it has run out of space and should be deletedTheres little you can do with a snapshot once it reaches this pointFilesystem resizingFilesystem overflows are more common than disk crashes and one advantage oflogical volumes is that theyre much easier to juggle and resize than are hard partitions We have experienced everything from servers used for personal video storageto departments full of email pack ratsThe logical volume manager doesnt know anything about the contents of its volumes so you must do your resizing at both the volume and filesystem levels Theorder depends on the specific operation Reductions must be filesystemfirst andenlargements must be volumefirst Dont memorize these rules just think aboutwhats actually happening and use common senseSuppose that in our example mntweb has grown more than we predicted andneeds another GB of space We first check the volume group to be sure additional space is available sudo vgdisplay DEMO Volume group VG Name DEMOSystem IDFormat lvmMetadata Areas Metadata Sequence No VG Access readwriteVG Status resizableOpen LV Max PV Cur PV Act PV VG Size GiBPE Size MiBTotal PE Alloc PE Size GiBFree PE Size GiBVG UUID nrxjXHNxnvrdnMAWeOQEdDwEONote that GB of space has been consumed GB for the original filesystemand GB for the snapshot However plenty of space is still available We unmountthe filesystem and use lvresize to add space to the logical volume sudo umount mntweb sudo lvchange an DEMOweb sudo lvresize L G DEMOwebSize of logical volume DEMOweb changed from GiB extents to GiB extentsLogical volume DEMOweb successfully resized sudo lvchange ay DEMOwebThe lvchange commands are needed to deactivate the volume for resizing and toreactivate it afterwards This part is needed only because an existing snapshot ofweb remains from our previous example After the resize operation the snapshotwill see the additional GB of allocated space but since the filesystem it contains is only GB in size the snapshot will still be usableWe can now resize the filesystem with resizefs The comes from the originalext filesystem but the command supports all versions of ext Since resizefs candetermine the size of the new filesystem from the volume we dont need to specify the new size explicitly We would have to do so when shrinking the filesystem sudo resizefs devDEMOwebresizefs SepResizing the filesystem on devDEMOweb to k blocksThe filesystem on devDEMOweb is now k blocks longThats it Examining the output of df again shows the changes sudo mount devDEMOweb mntweb df h mntwebFilesystem Size Used Avail Use Mounted ondevmapperDEMOweb G M G mntwebCommands for resizing other filesystems work similarly For XFS filesystems thedefault on Red Hat and CentOS systems use xfsgrowfs for UFS filesystems thedefault on FreeBSD use growfs XFS filesystems must be mounted to be expanded As the names of these commands suggest XFS and UFS filesystems can be expanded but not made smaller If you need to remove space youll need to copy thefilesystems contents to a new smaller filesystemIts worth noting that disks you allocate and attach to virtual machines in the cloudare essentially logical volumes although the volume manager itself lives elsewherein the cloud These volumes are usually resizable through the cloud providers management console or commandline utilityThe procedure for resizing cloud filesystems is much the same as the one outlinedabove but keep in mind that because these virtual devices impersonate disk drivesthey probably have partition tables Youll need to resize on three separate layers atthe cloud provider level at the partition level and at the filesystem levelFreeBSD logical volume managementFreeBSD has a fullfledged logical volume manager of its own Previous versions wereknown by the name Vinum but now that the system has been rewritten to conformto FreeBSDs generalized geom architecture for storage devices the name has beenchanged to GVinum Like LVM GVinum implements a variety of RAID typesFreeBSD has recently put a lot of effort into ZFS support and although GVinum hasnot been officially deprecated developers public comments suggest that ZFS is therecommended approach for logical volume management and RAID going forwardAccordingly we do not discuss GVinum here ZFS is covered starting on page RAID redundant arrays of inexpensive disksEven with backups the consequences of a disk failure on a server can be disastrous RAID redundant arrays of inexpensive disks is a system that distributesor replicates data across multiple disks RAID not only helps avoid data loss butalso minimizes the downtime associated with hardware failures often to zero andpotentially increases performanceRAID can be implemented by dedicated hardware that presents a group of harddisks to the operating system as a single composite drive It can also be implemented simply by the operating systems reading or writing multiple disks according tothe rules of RAIDSoftware vs hardware RAIDBecause the disks themselves are always the most significant bottleneck in a RAIDimplementation there is no reason to assume that a hardwarebased implementation of RAID will necessarily be faster than a software or OSbased implementation Hardware RAID has been predominant in the past for two main reasons lackof software alternatives no direct OS support for RAID and hardwares ability tobuffer writes in some form of nonvolatile memoryThe latter feature does improve performance because it makes writes appear tocomplete instantaneously It also protects against a potential corruption issue calledthe RAID write hole which we describe in more detail starting on page But beware many of the common RAID cards sold for PCs have no nonvolatilememory at all they are just glorified SATA interfaces with some RAID softwareonboard RAID implementations on PC motherboards fall into this category as RAID is sometimes glossed as redundant arrays of independent disks too Both versions are historically accuratewell Youre better off using the RAID features in Linux or FreeBSD on these systems Or better yet use ZFS or BtrfsWe have experienced a disk controller failure on an important production serverAlthough the data was replicated across several physical drives a faulty hardwareRAID controller destroyed the data on all disks A lengthy and ugly restore processensued The rebuilt server now relies on the kernels software to manage its RAIDenvironment removing the possibility of another RAID controller failureRAID levelsRAID can do two basic things First it can improve performance by striping dataacross multiple drives thus allowing several drives to work simultaneously to supplyor absorb a single data stream Second it can replicate data across multiple drivesdecreasing the risk associated with a single failed diskReplication assumes two basic forms mirroring in which data blocks are reproduced bitforbit on several different drives and parity schemes in which one ormore drives contain an errorcorrecting checksum of the blocks on the remainingdata drives Mirroring is faster but consumes more disk space Parity schemes aremore diskspaceefficient but have lower performanceRAID is traditionally described in terms of levels that specify the exact detailsof the parallelism and redundancy implemented by an array The term is perhapsmisleading because higher levels are not necessarily better The levels are simplydifferent configurations use whichever versions suit your needsIn the following illustrations numbers identify stripes and the letters a b and cidentify data blocks within a stripe Blocks marked p and q are parity blocksLinear mode also known as JBOD for just a bunch of disks is noteven a real RAID level And yet every RAID controller seems to implement it JBOD concatenates the block addresses of multiple drives to create a single larger virtual drive It has no data redundancy or performancebenefit These days JBOD functionality is best achieved through a logicalvolume manager rather than a RAID system RAID level increases performance Itcombines two or more drives of equal sizebut instead of stacking them endtoendit stripes data alternately among the disksin the pool Sequential reads and writesare therefore spread among several disksdecreasing write and access timesNote that RAID has reliability characteristics that are significantly inferior to separate disks A twodrive array has roughly double the annualfailure rate of a single drive and so onaaaabbbbRAID RAID level is colloquially known asmirroring Writes are duplicated to twoor more drives simultaneously This arrangement makes writes slightly slowerthan they would be on a single drive However it offers read speeds comparable toRAID because reads can be farmed outamong the several duplicate disk drives RAID levels and are stripes of mirrors or mirrors of stripes Logically they are concatenations of RAID and RAID but many controllers and software implementations support them directly The goal of bothmodes is to simultaneously obtain the performance of RAID and theredundancy of RAID These configurations need at least four devices RAID level stripes both data and parity information adding redundancywhile simultaneously improving read performance In addition RAID is more efficient in its use of disk space than is RAID If an array has Ndrives at least three are required N of them can store data Thespaceefficiency of RAID is therefore at least whereas that of mirroring cannot be higher than aaaaaaaaRAID bbbbbbbbRAID RAID aaaabbbbRAID aaaabbbbRAID RAID RAID Mirror ofstripesRAID Stripe ofmirrorsaapabpabpbbccccpRAID RAID RAID level is similar to RAID with two parity disks A RAID arraycan withstand the complete failure of two drives without losing dataRAID requires at least four devicesRAID levels and are defined but rarely deployed Logical volume managersusually include both striping RAID and mirroring RAID featuresAs RAID systems logical volume managers and filesystem all rolled into one ZFSand Btrfs support striping mirroring and configurations similar to RAID andRAID See page for more details on these optionsLinux supports both ZFS and Btrfs though you might have to install ZFS separately Btrfss RAID and RAID support is not officially ready for production useFor simple striped and mirrored configurations outside the context of one of thesefilesystems Linux gives you a choice between a dedicated RAID system md seepage and the logical volume manager LVM The LVM approach is perhapsmore flexible but the md approach may be a bit more rigorously predictable If youopt for md you can still use LVM to manage the space on the RAID volume ForRAID and RAID you must use md to implement software RAIDZFS is the preferred RAID implementation for FreeBSD However two additionalimplementations are availableAt the disk driver level FreeBSDs geom system can combine disks into RAID arrayswith support for RAID RAID and RAID RAID is similar to RAID butuses a dedicated parity disk instead of distributing parity among all disks in a poolYou can stack geoms so RAID and RAID are possible as wellFreeBSD also includes support for RAID RAID and RAID in its logical volumemanager GVinum However with the advent of full support for ZFS on FreeBSDthe future of GVinum appears to be in question It is not yet officially deprecatedbut no longer seems to be actively maintainedDisk failure recoveryThe Google disk failure study cited on page should be pretty convincing evidence of the need for some form of storage redundancy in most production environments At an annual failure rate your organization needs only harddisks in service to expect an average of one disk failure per monthaapabpqbpqacqbbpcccqRAID JBOD and RAID modes are of no help when hardware problems occur you mustrecover your data manually from backups Other forms of RAID enter a degradedmode in which the offending devices are marked as faulty The RAID arrays continue to function normally from the perspective of storage clients although perhapsat reduced performanceBad disks must be swapped out for new ones as soon as possible to restore redundancy to the array A RAID array or twodisk RAID array can tolerate the failure of only a single device Once that failure has occurred the array is vulnerableto a second failureThe specifics of the process are usually pretty simple You replace the failed diskwith another of similar or greater size then tell the RAID implementation to replace the old disk with the new one What follows is an extended period duringwhich the parity or mirror information is rewritten to the new blank disk This istypically an overnight operation The array remains available to clients during thisphase but performance is likely to be poorTo limit downtime and the vulnerability of the array to a second failure most RAIDimplementations let you designate one or more disks as hot spares When a failure occurs the faulted disk is automatically swapped for a spare and the process ofresynchronizing the array begins immediately Where supported hot spares shouldbe used as a matter of courseDrawbacks of RAID RAID is a popular configuration but it has some weaknesses too The followingissues apply to RAID also but for simplicity we frame the discussion in termsof RAID First its critically important to note that RAID does not replace regular offlinebackups It protects the system against the failure of one diskthats it It does notprotect against the accidental deletion of files It does not protect against controllerfailures fires hackers or any number of other hazardsSecond RAID isnt known for its great write performance RAID writes datablocks to N disks and parity blocks to the Nth disk Whenever a random block iswritten at least one data block and the parity block for that stripe must be updatedFurthermore the RAID system doesnt know what the new parity block ought tocontain until it has read the old parity block and the old data Each random writetherefore expands into four operations two reads and two writes Sequential writesmay fare better if the implementation is smartFinally RAID is vulnerable to corruption in certain circumstances Its incrementalupdating of parity data is more efficient than reading the entire stripe and recalcu Parity data is distributed among all the drives in the array each stripe has its parity stored on adifferent drive Since theres no dedicated parity disk its unlikely that any single disk will act as abottlenecklating the stripes parity from the original data On the other hand it means that atno point is parity data ever validated or recalculated If any block in a stripe shouldfall out of sync with the parity block that fact will never become evident in normaluse reads of the data blocks will still return the correct dataOnly when a disk fails does the problem become apparent The parity block willlikely have been rewritten many times since the occurrence of the original desynchronization Therefore the reconstructed data block on the replacement disk willconsist of essentially random dataThis kind of desynchronization between data and parity blocks isnt all that unlikely either Disk drives are not transactional devices Without an additional layer ofsafeguards there is no simple way to guarantee that either two blocks or zero blockson two different disks will be properly updated Its quite possible for a crash powerfailure or communication problem at the wrong moment to create dataparity skewThis problem is known as the RAID write hole and it has received increasingattention over the last ten years or so The implementors of the ZFS filesystem claimthat because ZFS uses variablewidth stripes it is immune to the RAID writehole Thats also why ZFS calls its RAID implementation RAIDZ instead of RAID though in practice the concept is similarAnother potential solution is scrubbing validating parity blocks one by one whilethe array is relatively idle Most RAID implementations include some form of scrubbing function You just have to remember to activate it regularly by initiating itfrom cron or a systemd timermdadm Linux software RAIDThe standard software RAID implementation for Linux is called md the multiple disks driver Its frontended by the mdadm command md supports all theRAID configurations listed above as well as RAID An earlier system known asraidtools is no longer usedYou can also implement RAID on Linux through the logical volume managerLVM or through Btrfs or another filesystem with builtin volume managementand RAID features We address LVM starting on page and nextgenerationfilesystems starting on page Generally these multiple implementations represent different epochs of software development with mdadm being the earliestand ZFSBtrfs the most recentAll these systems are actively maintained so choose whichever you prefer Sites without an installed base are best off jumping directly to an allinone system like BtrfsCreating an arrayThe following scenario configures a RAID array composed of three identical TBhard disks Although md can use raw disks as components we prefer to give everydisk a partition table for consistency so we start by running gparted creating aGPT partition table on each disk and assigning all the disks space to a single partition of type Linux RAID Its not strictly necessary to set the partition type butits a useful reminder to anyone who might inspect the table laterThe following command builds a RAID array from three wholedisk partitions sudo mdadm create devmdextra level raiddevicesdevsdf devsdg devsdhmdadm Defaulting to version metadatamdadm array devmdextra startedThe virtual file procmdstat always contains a summary of mds status and thestatus of all the systems RAID arrays It is especially useful to keep an eye on theprocmdstat file after adding a new disk or replacing a faulty drive watch catprocmdstat is a handy idiom cat procmdstatPersonalities linear multipath raid raid raid raidraid raidmd active raid sdh sdg sdf blocks super level k chunk algo UU recovery finishmin speedKsecbitmap pages KB KB chunkunused devices noneThe md system does not keep track of which blocks in an array have been used soit must manually synchronize all the parity blocks with their corresponding datablocks md calls the operation a recovery since its essentially the same procedureused when you swap out a bad hard disk It can take many hours on a large arraySome helpful notifications appear in the system logs too usually varlogmessagesor varlogsyslogkernel md bindsdfkernel md bindsdgkernel md bindsdhkernel mdraidmd device sdg operational as raid disk kernel mdraidmd device sdf operational as raid disk kernel mdraidmd allocated kBkernel mdraidmd raid level active with out of devicesalgorithm kernel RAID conf printoutkernel level rd wdkernel disk o devsdfkernel disk o devsdgkernel created bitmap pages for device mdmdadm NewArray event detected on md device devmdmdadm DegradedArray event detected on md device devmdkernel md bitmap initialized from disk read pages set of bitskernel md detected capacity change from to kernel RAID conf printoutkernel level rd wdkernel disk o devsdfkernel disk o devsdgkernel disk o devsdhkernel md recovery of RAID array mdkernel md minimum guaranteed speed KBsecdiskkernel md using maximum available idle IO bandwidth but not more than KBsec for recoverykernel md using k window over a total of kmdadm RebuildStarted event detected on md device devmdThe initial creation command also serves to activate the array make it availablefor use On subsequent reboots most distributions including all our examplesautomatically discover and activate any existing arraysNote that you specify a device pathname for the composite array when you runmdadm create Oldstyle md device paths looked like devmd but when youspecify a path under the devmd directory as was done in this example mdadmwrites your chosen name into the arrays superblock This measure ensures that youcan always locate the array by its logical path even when the array is autostartedand might be assigned a different array number As you can see from the log entriesabove the array also has a traditional name here devmd devmdextra isjust a symbolic link to the actual array devicemdadmconf document array configurationmdadm does not technically require a configuration file but it will use a configuration file if you supply one typically etcmdadmmdadmconf or etcmdadmconfWe recommend that you add ARRAY entries to the configuration file as you createnew arrays Doing so documents the RAID configuration in a standard place andgives administrators an obvious place to look for information when problems occurmdadm detail scan dumps the current RAID setup in the format required forinclusion in mdadmconf For example sudo mdadm detail scanARRAY devmdextra metadata nameubuntuextra UUIDbdefbbafcdcbcbWith the addition of this line mdadm can now read mdadmconf at startup orshutdown to easily manage the array For example to take down the array createdabove we could run sudo mdadm S devmdextraAnd to start it up again run sudo mdadm As devmdextraThe first of these commands would work even without the mdadmconf file butthe second would notWe formerly recommended that you add DEVICE entries for the components of eacharray to mdadmconf too We take that back Device names are more ephemeralthese days and mdadm is better at finding and identifying array components thanit used to be We dont think DEVICE entries are a best practice anymoremdadm has a monitor mode in which it runs continuously as a daemon process and raises an alarm when problems are detected on a RAID array Use thisfeature To set it up add a MAILADDR or PROGRAM line to your mdadmconf file AMAILADDR notifies you of issues by email and a PROGRAM configuration runs anexternal reporting tool that you supply as is useful for integrating with monitoringsystems see Chapter You also need to arrange for the monitor daemon to run at boot time All our example distributions have an init script that does this for you but the names andprocedures for enabling it are slightly differentdebian sudo updatercd mdadm enableubuntu sudo updatercd mdadm enableredhat sudo systemctl enable mdmonitorcentos sudo systemctl enable mdmonitorSimulating a failureWhat happens when a disk actually fails Lets find out mdadm offers the handyoption to simulate a failed disk sudo mdadm devmdextra f devsdgmdadm set devsdg faulty in devmdextra sudo tail varlogmessagesApr ubuntu kernel mdraidmd Disk failure on sdgdisabling devicemdraidmd Operation continuing on devices cat procmdstatPersonalities linear multipath raid raid raid raidraid raidmd active raid sdh sdf sdgF blocks super level k chunk algo UUunused devices noneBecause RAID is a redundant configuration the array continues to function indegraded mode so users will not necessarily be aware of the problemTo remove the drive from the RAID configuration use mdadm r sudo mdadm devmdextra r devsdgmdadm hot removed devsdg from devmdextraOnce the disk has been logically removed you can shut down the system and replace the drive Hotswappable drive hardware lets you make the change withoutturning off the system or rebootingIf your RAID components are raw disks replace them only with an identicaldrive You can replace partitionbased components with any partition of similarsize which is a good reason to build your arrays on top of partitions rather thanraw disks Still for bandwidth matching its best if the underlying drive hardwareis similar Of course if your RAID configuration is built on top of partitions youmust run a partitioning utility to define the partitions appropriately before addingthe replacement disk to the arrayIn our example the failure is just simulated so we can add the drive back to thearray without replacing any hardware sudo mdadm devmdextra a devsdgmdadm hot added devsdcmd immediately starts to rebuild the array As always you can see its progress inprocmdstat A rebuild can take hours so consider this fact in your disaster recovery and testing plans FilesystemsEven after a hard disk has been conceptually divided into partitions or logical volumes it is still not ready to hold files All the abstractions and goodies describedin Chapter The Filesystem must be implemented in terms of raw disk blocksThe filesystem is the code that implements these and it needs to add a bit of itsown overhead and dataEarly systems bundled the filesystem implementation into the kernel but it soonbecame apparent that support for multiple filesystem types was an important designgoal UNIX systems developed a welldefined kernel interface that allowed multipletypes of filesystems to be active at once The filesystem interface also abstracted theunderlying hardware so filesystems see approximately the same interface to storagedevices as do other UNIX programs that access the disks through device files in devSupport for multiple filesystem types was initially motivated by the need to supportNFS and filesystems for removable media But once the floodgates were openedthe what if era began many different groups started to work on improved filesystems Some were systemspecific and others such as ReiserFS were not tied toany particular OSMost systems have settled on one or two filesystems as mainstream defaults Thesefilesystems are rigorously tested along with the rest of the system before stable releases are issuedThe predominant pattern is for systems to officially support one traditionalstylefilesystem UFS ext or XFS and one nextgeneration filesystem that includes volume management and RAID features ZFS or Btrfs Support for the latter optionsis usually best on physical hardware cloud systems can use them for data partitionsbut sometimes not for the boot diskAlthough other filesystem implementations are often just a package installationaway addon filesystems do bring risk and potential instability Filesystems arefoundational so they need to be stable and reliable under all use scenariosFilesystem developers work hard to achieve this level of robustness but the riskcant be entirely eliminatedUnless youre setting up a storage pool or data disk for a specific application werecommend against straying from your systems supported filesystems Thats whatthe documentation and administrative tools most likely assumeThe upcoming sections describe the most common filesystems and their management in a bit more detail We first describe the traditional filesystems UFS extand XFS then move on to the nextgeneration systems ZFS page and Btrfspage Traditional filesystems UFS ext and XFSUFS ext and XFS have separate code bases and histories but over time theyvebecome eerily similar to one another from an administrative perspectiveThese filesystems exemplify the old school approach in which volume managementand RAID are implemented separately from the filesystem itself The filesystemslimit themselves to plainvanilla file storage on block devices of defined size Theirfeatures are more or less limited to those outlined in Chapter Older filesystems in this category were subject to subtle corruption if power wasinterrupted in the middle of a write operation because then disk blocks couldcontain inconsistent data structures The fsck command was used at boot timeto check filesystems for this kind of problem and to automatically patch the mostcommon issuesModern filesystems include a feature called journaling that averts the possibility ofthis type of corruption When a filesystem operation occurs the required modifications are first written to the journal Once the journal update is complete a commitrecord is written to mark the end of the entry Only then is the normal filesystem Apple recently converted the worlds iOS devices of which there are more than a billion to a completely new writtenfromscratch filesystem called APFS That this transition was executed invisiblyand without notable disasters was truly a historic feat of engineeringmodified If a crash occurs during the update the filesystem can later replay thejournal log to reconstruct a perfectly consistent filesystemJournaling reduces the time needed to perform filesystem consistency checks seethe fsck section on page to approximately one second per filesystem Barring some type of hardware failure the state of a filesystem can almost instantly beassessed and restoredThe Berkeley Fast File System implemented by McKusick et al in the s was anearly standard that spread to many UNIX systems With some small adjustmentsit eventually became known as the UNIX File System UFS and formed the basisof several other filesystem implementations including Linuxs ext series UFS remains the default filesystem used by FreeBSDThe second extended filesystem ext was for a long time the mainstream Linuxstandard It was designed and implemented primarily by Rmy Card TheodoreTso and Stephen Tweedie Although the code for ext was written specifically forLinux it is functionally similar to the Berkeley Fast File SystemExt added journaling and ext is a comparatively modest update that raises a fewsize limits increases the performance of certain operations and allows the use ofextents disk block ranges for storage allocation rather than just individual diskblocks Ext is the default filesystem on Debian and UbuntuXFS was developed by Silicon Graphics Inc later known as SGI It was the defaultfilesystem for IRIX SGIs version of UNIX and was one of the first extentbasedfilesystems That made it particularly suitable for sites that processed large media filesas many SGI customers did XFS is the default filesystem on Red Hat and CentOSFilesystem terminologyLargely because of their common history many filesystems share some descriptiveterminology The implementations of the underlying objects have often changed butthe terms are still widely used by administrators as labels for fundamental conceptsInodes are fixedlength table entries each of which holds information about onefile The term is probably short for index nodes although its exact etymology isunclear Inodes were originally preallocated at the time a filesystem was createdbut some filesystems now create them dynamically as they are needed Either wayan inode usually has an identifying number which you can see with ls iInodes are the things pointed to by directory entries When you create a hard linkto an existing file you create a new directory entry but you do not create a new inodeA superblock is a record that describes the characteristics of the filesystem It contains information about the length of a disk block the size and location of the inode tables the disk block map and usage information the size of the block groups In most cases only metadata changes are journaled The actual data to be stored is written directly tothe filesystem Some filesystems can use the journal for data too but at a significant performance costand a few other important parameters of the filesystem Because damage to thesuperblock could erase crucial information several copies of it are maintained inscattered locationsThe kernel caches disk blocks to increase efficiency All types of blocks can becached including superblocks inode blocks and directory information Caches arenormally not write through so there might be some delay between the point atwhich an application thinks it has written a block and the point at which the blockis actually saved to disk Applications can request more predictable behavior for afile but this option lowers throughputThe sync system call flushes modified blocks to their permanent homes on diskpossibly making the ondisk filesystem fully consistent for a split second Thisperiodic save minimizes the amount of data loss that might occur if the machinewere to crash with many unsaved blocks Filesystems can do syncs on their ownschedule or leave this up to the OS Modern filesystems have journaling mechanisms that minimize or eliminate the possibility of structural corruption causedby a crash so sync frequency now mostly has to do with how many data blocksmight be lost in a crashA filesystems disk block map is a table of the free blocks it contains When new filesare written this map is examined to devise an efficient layout scheme The blockusage summary records basic information about the blocks that are already in useFilesystem polymorphismFilesystems are software packages with multiple components One part lives in thekernel or even potentially in user space under Linux search for FUSE and implements the nuts and bolts of translating the standard filesystem API into readsand writes of disk blocks Other parts are userlevel commands that initialize newvolumes to the standard format check filesystems for corruption and performother formatspecific tasksLong ago the standard userlevel commands knew about the filesystem that thesystem used and they simply implemented the appropriate functionality mkfs ornewfs created new filesystems fsck fixed problems and mount mostly just invokedthe appropriate underlying system callsThese days many more filesystems exist so systems have had to decide how to address this cornucopia of options For a long time Linux tried to fit all filesystemsinto the standard mold of mkfs and fsck by making those commands be wrappersThe wrappers called discrete commands named eg mkfsfsname or fsckfsnamedepending on the type of filesystem being manipulated These days the pretenseof homogeneity among filesystems has been stretched past the breaking point andmost systems now advise you to call the filesystemspecific commands directlyFilesystem formattingThe general recipe for creating a new Linux filesystem ismkfsfstype L label otheroptions deviceOn FreeBSD the process for creating a UFS filesystem is similar but with newfsnewfs L label otheroptions deviceThe L option to both mkfs and newfs sets a volume label for the filesystem suchas spare home or extra This is just one option among many but its an optionthat we recommend you use on every filesystem Labeling the filesystem frees youfrom having to track the device on which its been installed Its particularly handygiven that disk device names can change whenever hardware is adjustedThe available otheroptions are filesystemspecific but their use is uncommonfsck check and repair filesystemsBecause of block buffering and the fact that disk drives are not really transactionaldevices filesystem data structures can potentially become selfinconsistent If theseproblems are not corrected quickly they propagate and snowballThe original fix for corruption was a command called fsck filesystem consistencycheck spelled aloud or pronounced FS check or fisk that carefully inspectedall data structures and walked the allocation tree for every file It relied on a set ofheuristic rules about what the filesystem state might look like after failures at various points during an updateThe original fsck scheme worked surprisingly well but because it involved readingall the data on a disk it could take hours on a large drive An early optimizationwas a filesystem clean bit that could be set in the superblock when the filesystemwas properly unmounted When the system restarted it would see the clean bit andknow to skip the fsck checkNow filesystem journals let fsck pinpoint the activity that was occurring at the timeof a failure fsck can simply rewind the filesystem to the last known consistent stateDisks are normally fscked automatically at boot time if they are listed in the systems etcfstab file The fstab file has legacy fsck sequence fields that orderedand parallelized filesystem checks But now that fscks are fast the only thing thatmatters is that the root filesystem be checked firstYou can run fsck by hand to perform an indepth examination more akin to theoriginal fsck procedure but be aware of the time requiredLinux extfamily filesystems can be set to force a recheck after they have been remounted a certain number of times or after a certain period of time even if all theunmounts were clean This precaution is probably good hygiene and in mostcases the default value usually around mounts is acceptable However on systems that mount filesystems frequently such as desktop workstations even thatfrequency of fscks can become tiresome To increase the interval to mountsuse the tunefs command sudo tunefs c devsdatunefs SepSetting maximal mount count to If a filesystem appears damaged and fsck cannot automatically repair it do not experiment with it before making an ironclad backup The best insurance policy is todd the entire disk to a backup file or backup diskMost filesystems create a lostfound directory at the root of each filesystem inwhich fsck can deposit files whose parent directory cannot be determined Thelostfound directory has some extra space preallocated so that fsck can store orphaned files there without having to allocate additional directory entries on anunstable filesystem Dont delete this directorySince the name given to a file is recorded only in the files parent directory namesfor orphan files are not available and so the files placed in lostfound are namedwith their inode numbers The inode table does record the UID of the files ownerhowever so getting a file back to its original owner is relatively easyFilesystem mountingA filesystem must be mounted before it becomes visible to processes The mountpoint for a filesystem can be any directory but the files and subdirectories beneathit are not accessible while a filesystem is mounted there See Filesystem mountingand unmounting on page for more informationAfter installing a new disk mount new filesystems by hand to be sure that everything is working correctly For example the command sudo mount devsda mnttempmounts the filesystem in the partition represented by the device file devsda device names will vary among systems on a subdirectory of mnt which is a traditional path used to contain temporary mountsYou can verify the size of a filesystem with the df command The example below usesthe Linux h flag to request human readable output Unfortunately most systemsdf defaults to an unhelpful unit such as disk blocks but there is usually a flag tomake df report something specific such as kibibytes or gibibytes df h mntwebFilesystem Size Used Avail Use Mounted ondevmapperDEMOweb G M G mntweb Some systems have a mklostfound command you can use to recreate this directory if it is deletedSetup for automatic mountingYou will generally want to configure the system to mount local filesystems at boottime The etcfstab file lists the device names and mount points of all the systemsdisks among other thingsmount umount swapon and fsck all read the fstab file so its helpful if the datapresented there is correct and complete mount and umount use the catalog tofigure out what you want done if you specify only a partition name or mount pointon the command line For example with the Linux fstab configuration shown onpage the command sudo mount mediacdromwould have the same effect as typing sudo mount t udf o usernoautoexecutf devscd mediacdromThe command mount a mounts all regular filesystems listed in the filesystemcatalog it is usually executed from the startup scripts at boot time The t fstypeargument constrains the operation to filesystems of a certain type For example sudo mount at extmounts all local ext filesystems The mount command reads fstab sequentiallyTherefore filesystems that are mounted beneath other filesystems must follow theirparent partitions in the fstab file For example the line for varlog must follow theline for var if var is a separate filesystemThe umount command for unmounting filesystems accepts a similar syntax Youcannot unmount a filesystem that a process is using as its current directory or onwhich files are open Several commands can identify the processes that are interfering with your umount attempt see page The FreeBSD fstab file is the most traditional of our example systems Heres a sample from a system with only one real filesystem beyond the root spare Device Mountpoint FStype Options Dump Passdevgptrootfs ufs rw devgptswapa none swap sw devgptswapb none swap sw fdesc devfd fdescfs rw proc proc procfs rw devgptspare spare ufs rw Each line holds six fields separated by whitespace Each line describes a single filesystem The fields are traditionally aligned for readability but alignment is not requiredThe first field gives the device name The fstab file can include mounts from remotesystems in which case the first field contains an NFS path The notation server exportdenotes the export directory on the machine named server The noauto mount option excludes a given filesystem from automatic mounting by mount aSee Chapter for more information about NFSThe second field specifies the mount point and the third field names the type offilesystem The exact type name used to identify local filesystems varies amongmachinesThe fourth field specifies mount options to be applied by default There are manypossibilities see the man page for mount for the ones that are common to all filesystem types Individual filesystems usually introduce options of their ownThe fifth and sixth fields are vestigial They are supposedly a dump frequencycolumn and a column that controls fsck parallelism Neither is important on contemporary systemsThe devices listed for devfd and proc are dummy entries These virtual filesystems are taskspecific and dont require any additional information to be mounted The other devices are identified by their GPT partition labels which is a morerobust option than using actual device names To find out the label of an existingpartition rungpart show l diskto print the partition table of the appropriate disk To set the label on a partition usegpart modify i index l label disk UFS filesystems also have labels of their own and these show up beneath thedevufs directory The UFS labels and partition labels are separate but they canbe and probably should be set to the same value In this example devufssparewould work just as well as devgptspare To find a filesystems current label runtunefs p deviceTo set the label runtunefs L label deviceUnmount the filesystem before setting the labelBelow are some additional examples culled from an Ubuntu systems fstab Thegeneral format is the same but Linux systems use a different way to avoid namingdisk devices file system mount point type options d pproc proc proc defaults UUIDaefa ext errorsremountro UUIDebd none swap sw devscd mediacdrom udfiso usernoautoexecutf A cautionary note partition tables are sometimes referred to as disk labels Make sure when readingdocumentation that you distinguish between the label of an individual partition and the label of thedisk itself Overwriting a disks partition table is potentially disastrousThe first line addresses the proc filesystem which in fact is presented by a kerneldriver and has no actual backing store As in the FreeBSD example above the procdevice listed in the first column is just a placeholderThe second and third lines use filesystem IDs UUIDs which weve truncated tomake the excerpt more readable instead of device names to identify volumes Thissystem is similar to the UFS label system used by FreeBSD except that the identifiers are long random numbers instead of text strings Use the blkid command todiscover the UUID of a particular filesystemFilesystems can also have administratively assigned labels use elabel or xfsadminto read or set them If you want to use labels in fstab which is tidier just substitute LABELlabel for UUIDlongrandomnumberGPT disk partitions can have UUIDs and labels of their own that are independentof the UUIDs and labels of the filesystems they contain For use of these options toidentify partitions in the fstab file the incantations are PARTUUID and PARTLABELHowever common practice seems to have converged on the use of filesystem UUIDsYou can also identify devices with pathnames beneath the devdisk directory Subdirectories such as devdiskbyuuid and devdiskbypartuuid are automaticallymaintained by udevUSB drive mountingUSB storage devices come in many flavors personal thumb drives digital cameras and large external disks to name a few Most of these are supported by UNIXsystems as data storage devicesIn the past special tricks were necessary to manage USB devices But now thatoperating systems have embraced dynamic device management as a fundamentalrequirement USB drives are just one more type of device that shows up or disappears without warningFrom the perspective of storage management the issues are twofold Getting the kernel to recognize a device and to assign a device file to it Finding out what assignment has been madeThe first step usually happens automatically Once a device file has been assignedyou can use the normal procedures described in Disk device files on page tofind out what it is For additional information about dynamic device managementsee Chapter Drivers and the KernelSwapping recommendationsRaw partitions or logical volumes rather than structured filesystems are normallyused for swap space Instead of using a filesystem to keep track of the swap areascontents the kernel maintains its own simplified mapping from memory blocksto swap space blocksOn some systems its also possible to swap to a file in a filesystem partition Witholder kernels this configuration can be slower than using a dedicated partition butits still handy in a pinch In any event logical volume managers eliminate most ofthe reasons you might want to use a swap file rather than a swap volumeThe more swap space you have the more virtual memory your processes can allocate The best virtual memory performance is achieved when the swap area is splitamong several drives Of course the best option of all is to not swap consider adding RAM if you find yourself needing to optimize swap performanceThe proper amount of swap space to allocate depends on how a machine is usedThere is no penalty to overprovisioning except that you lose the extra disk spaceWe suggest half the amount of RAM as a rule of thumb but never less than GBon a physical serverIf a system will hibernate personal machines usually it needs to be able to savethe entire contents of memory to swap in addition to saving all the pages that wouldbe swapped in normal operation On these machines increase the swap space recommended above by the amount of RAMCloud and virtualized instances have their own peculiarities with respect to swapspace Paging is always a performance killer so some sources recommend runningwithout swap space entirely if you need more memory you need a larger instanceOn the other hand small instances usually have such meager RAM allotmentsthat they can barely boot without a swap area The general rule is that its fine forinstances to have swap space as long as you dont use it at steady state or pay extrafor it Whatever approach you decide to take check your base images to see howtheyre set up Some come with swap preconfigured and some dontSome Amazon EC instances come with a local instance store This is essentiallya slice of a local hard disk on the machine that runs the hypervisor The contentsof the instance store dont persist across starts and stops The store is included inthe price of the instance so you may as well use it for swap space if nothing elseOn Linux systems you initialize swap areas with mkswap which takes the devicename of the swap volume as an argument mkswap writes some header informationto the swap area That data includes a UUID which is why swap partitions count asfilesystems from the perspective of etcfstab and can be identified there by UUIDYou can manually enable swapping to a particular device with swapon device Howeveryou will generally want to have this function automatically performed at boot timeJust list swap areas in the regular fstab file and give them a filesystem type of swapTo review the systems active swapping configuration run swapon s on Linux systems or swapctl l on FreeBSD Nextgeneration filesystems ZFS and BtrfsAlthough ZFS and Btrfs are usually referred to as filesystems they represent vertically integrated approaches to storage management that include the functions of alogical volume manager and a RAID controller Although the current versions ofboth systems have a few limitations most fall into the not yet implemented category rather than the cant do for architectural reasons categoryCopyonwriteBoth ZFS and Btrfs avoid overwriting data in place and instead use a scheme knownas copy on write To update a block of metadata for example the filesystem modifies the inmemory copy and then writes it to a previously vacant disk block Ofcourse that data block probably has a parent block that points to it so the parentis rewritten as well as is the parents parent and so on back to the topmost levelof the filesystem In practice caching and careful design of data structures optimizeout most of these writes at least in the short termThe advantage of this architecture is that the ondisk copy of the filesystem remainsperpetually consistent Before the root block is updated the filesystem looks exactly as it did the last time the root was updated A few empty blocks have beenmodified but nothing points to them so it makes no difference The filesystem asa whole moves directly from one consistent state to anotherError detectionZFS and Btrfs also take data integrity far more seriously than do traditional filesystems These systems store checksums for every disk block and they verify all blocksread to ensure that misreads are detected On storage pools that include mirroringor parity bad data is automatically reconstructed from a knowngood copyDisk drives implement their own layers of error detection and error correction andalthough they fail frequently theyre not supposed to do so without reporting anerror back to the host computer Nevertheless they sometimes do return bad datawithout an error indicationOne commonly cited rule of thumb is to expect an instance of silent data corruption for every TB of data read A study by Bairavasundaram et al examinedservice records of more than million disk drives in NetApp servers and foundthat of drives showed evidence of silent read errors in each year of serviceThese error rates are small but by all indications theyre staying about the same evenas disk capacities and the volumes of data stored on disks expand exponentiallySoon well have hard disks so large that you cant read the entire contents without Interestingly one key finding of this study was that enterprisegrade hard disks were an order of magnitude less likely to experience these types of errorsa betterthaneven chance of encountering a silent error The extra validation doneby ZFS and Btrfs is starting to look really importantParity RAID does not address this issue at least in normal use Parity cant bechecked without a reading of the contents of an entire stripe and its inefficient toexpand every disk access into a fullstripe read Scrubbing can help find latent errors but only if theyre reproduciblePerformanceAll the traditional filesystems that remain in common use have similar performanceIts possible to contrive workloads for which one filesystem or another has an edgebut generalpurpose benchmarks rarely show much differenceCopyonwrite filesystems access storage media somewhat differently from traditional filesystems and they lack the decades of iterative refinement that have broughtthe oldguard filesystems to their current state of polish Usually the traditionalfilesystems set the upper bound on filesystem performanceIn many benchmarks ZFS and Btrfs show performance comparable to traditional filesystems But at their worst these filesystems can be about half as fast as thetraditional optionsJudging from Linux benchmarks the only platform on which direct comparison ispossible since Btrfs is Linuxonly Btrfs currently has a slight performance edgeover ZFS However the results vary widely by access pattern It is not uncommonfor one of these filesystems to perform well on a particular benchmark while theother lags far behindThe performance picture is complicated by the fact that each of these filesystemshas some potential tricks up its sleeve to increase performance Benchmarks usually dont take account of these endarounds ZFS lets you add caching SSDs to astorage pool it automatically copies frequently read data to the cache and avoidshitting the hard disks entirely On Btrfs you can use chattr C to disable copyonwrite semantics for the data in specific files usually large or frequently modifiedones thereby skirting some common lowperformance scenariosFor general use as root filesystems and home directory storage ZFS and Btrfsperform well and offer many useful advantages They can also work well as datastorage for specific server workloads However in these latter scenarios its worthtaking some time to doublecheck their behavior in your particular environment ZFS all your storage problems solvedZFS was introduced in as a component of OpenSolaris and it quickly madeits way to Solaris and to various BSDbased distributions In it became A relatedand also underappreciatedissue is the risk of random bit errors in RAM They are infrequent but they do happen All production servers should use and monitor ECC memoryusable as a root filesystem and it has been the frontline filesystem of choice forSolaris ever since UFS remains the default root filesystem on FreeBSD but ZFShas been an officially supported option since FreeBSD ZFS is more than just a filesystem RAID controller and volume manager wrappedinto one As originally conceived for OpenSolaris it was a comprehensive rethinkingof storagerelated administration that addressed everything from the way filesystemswere mounted to the way they were exported to other systems over NFS and SMBModern BSD and Linux systems need to accommodate a variety of filesystems sotheyve been forced to back off a bit from ZFSs original comprehensive approachNevertheless ZFS remains a thoughtfully designed system that solves quite a fewadministrative problems through its architecture rather than through the additionof featuresZFS on LinuxAlthough ZFS is free software its use on Linux has been hampered by the factthat the source code is covered by Sun Microsystems Common Development andDistribution License CDDL The Free Software Foundation maintains that theCDDL is incompatible with the GNU Public License which covers the Linux kernel Although addon versions of ZFS for Linux have long been available throughthe OpenZFS project openzfsorg the FSFs position has discouraged Linux distributions from bundling ZFS into their base systemsAfter nearly a decade of impasse over this issue the FSFs position is at last beingchallenged by Canonical Ltd developers of Ubuntu After a legal review Canonicalformally disputed the FSFs interpretation of the GPL and included ZFS in Ubuntu in the form of a loadable kernel module So far mid no lawsuit hasresulted If Canonical remains unpunished its possible that ZFS might become afully supported root filesystem on Ubuntu and that other distributions might follow suit in supporting itZFS architectureExhibit C shows a schematic of the major objects in the ZFS system and their relationship to each otherA ZFS storage pool is analogous to a volume group in other logical volumemanagement systems Each pool is composed of virtual devices which can beraw storage devices disks partitions SAN devices etc mirror groups or RAIDarrays ZFS RAID is similar in spirit to RAID in that it uses one or more paritydevices to implement redundancy for the array However ZFS calls the schemeRAIDZ and uses variablesized stripes to eliminate the RAID write hole All If nothing else the story of ZFS is an interesting case in which the GPL has actively impeded the development of an open source software package and blocked its adoption by users and distributors Ifyoure interested in the legal details Richard Fontanas wrapup of open source legal news for atgooglPCit includes a helpful summarywrites to the storage pool are striped across the pools virtual devices so a pool thatcontains only individual storage devices is effectively an implementation of RAID although the devices in this configuration are not required to be of the same sizeUnfortunately the current ZFS RAID is a bit brittle you cannot add new devicesto an array once it has been defined nor can you permanently remove a device Asin most RAID implementations devices in a RAID set must be the same size Youcan force ZFS to accept mixed sizes but the size of the smallest volume then dictates the overall size of the array To use disks of different sizes efficiently in combination with ZFS RAID you must partition the disks ahead of time and define theleftover regions as separate devicesMost configuration and management of ZFS is done through two commands zpooland zfs Use zpool to build and manage storage pools Use zfs to create and managethe entities created from pools chiefly filesystems and raw volumes used as swapspace database storage or backing for SAN volumesExample disk additionBefore we descend into the details of ZFS heres a highlevel example Supposeyouve added a new disk to your FreeBSD system and the disk has shown up asdevada An easy way to determine the correct device is to run geom disk listThe first step is to add the disk to a new storage pool sudo zpool create demo adaStep two is well there is no step two ZFS creates the pool demo creates a filesystem root inside that pool and mounts that filesystem as demo The filesystem isautomatically remounted when the system bootsExhibit C ZFS architectureZFSStorage devicesPartitionsRAIDZ arrays RAID arrays mirrorsVirtual devicesFilesystems swap areas database storageStorage pools ls a demo It would be even more impressive if we could simply add our new disk to the existing storage pool of the root disk which on FreeBSD is called zroot by defaultThe command would be sudo zpool add rpool ada Unfortunately the root poolcan contain only a single virtual device Other pools can be painlessly extended inthis manner howeverFilesystems and propertiesIts fine for ZFS to automatically create a filesystem on a new storage poolby default ZFS filesystems consume no particular amount of space All filesystems thatlive in a pool can draw from the pools available spaceUnlike traditional filesystems which are independent of one another ZFS filesystems are hierarchical and interact with their parent and child filesystems in severalways You create new filesystems with zfs create sudo zfs create demonewfs zfs list r demoNAME USED AVAIL REFER MOUNTPOINTdemo K G K demodemonewfs K G K demonewfsThe r flag to zfs list makes it recurse through child filesystems Most other zfs subcommands understand r too Ever helpful ZFS automounts the new filesystemas soon as you create itTo simulate traditional filesystems of fixed size you can adjust the filesystems properties to add a reservation an amount of space reserved in the storage pool forthe filesystems use and a quota This adjustment of filesystem properties is oneof the keys to ZFS management and its something of a paradigm shift for administrators who are accustomed to other systems Here we set both values to GB sudo zfs set reservationg demonewfs sudo zfs set quotag demonewfs zfs list r demoNAME USED AVAIL REFER MOUNTPOINTdemo G G K demodemonewfs K M K demonewfsThe new quota is reflected in the AVAIL column for demonewfs Similarly thereservation shows up immediately in the USED column for demo Thats becausethe reservations of demos descendant filesystems are included in its size tally The REFER column shows the amount of data referenced by the active copy of each filesystemdemo and demonewfs have similar REFER values because theyre both empty filesystems not because theres any inherent relationship between the numbersBoth property changes are purely bookkeeping entries The only change to theactual storage pool is the update of a block or two to record the new settings Noprocess goes out to format the GB of space reserved for demonewfs Most ZFSoperations including the creation of new storage pools and new filesystems aresimilarly lightweightUsing this hierarchical system of space management you can easily group severalfilesystems to guarantee that their collective size does not exceed a certain threshold you need not specify limits on individual filesystemsYou must set both the quota and reservation properties to properly emulate atraditional fixedsize filesystem The reservation alone simply ensures that thefilesystem has enough room available to grow at least that large The quota limitsthe filesystems maximum size without guaranteeing that space is available for thisgrowth another object could snatch up all the pools free space leaving no roomfor demonewfs to expandOn the other hand there are few reasons to set up a filesystem this way in real lifeWe show the use of these properties simply to demonstrate ZFSs space accountingsystem and to emphasize that ZFS is compatible with the traditional model shouldyou wish to enforce itProperty inheritanceMany properties are naturally inherited by child filesystems For example if wewanted to mount the root of the demo pool in mntdemo instead of demo wecould simply set the roots mountpoint parameter sudo zfs set mountpointmntdemo demo zfs list r demoNAME USED AVAIL REFER MOUNTPOINTdemo G G K mntdemodemonewfs K M K mntdemonewfs ls mntdemonewfsSetting the mountpoint parameter automatically remounts the filesystems and themount point change affects child filesystems in a predictable and straightforwardway The usual rules regarding filesystem activity still apply however see page Use zfs get to see the effective value of a particular property zfs get all dumps themall The SOURCE column tells you why each property has its particular value localmeans that the property was set explicitly and a dash means that the property is The reservation and quota properties take into account all storage costs of the filesystem including space consumed by snapshots To limit only the size of the active copy of the filesystem use therefreservation and refquota properties instead The ref prefix indicates amount of data referredto by the active filesystem the same total shown in the REFER column in zfs list outputreadonly If the property value is inherited from an ancestor filesystem SOURCEshows the details of that inheritance as well zfs get all demonewfsNAME PROPERTY VALUE SOURCEdemonewfs type filesystem demonewfs creation Mon Apr demonewfs used K demonewfs available M demonewfs referenced K demonewfs compressratio x demonewfs mounted yes demonewfs quota G localdemonewfs reservation G localdemonewfs mountpoint mntnewfs inherited from demodemonewfs checksum on defaultdemonewfs compression off default many more about in allVigilant readers might notice that the available and referenced properties looksuspiciously similar to the AVAIL and REFER columns shown by zfs list In factzfs list is just a different way of displaying filesystem properties If we had included the full output of our zfs get command above there would be a used propertyin there too Use the o option to specify the properties you want zfs list to showIt wouldnt make sense to assign values to used and to the other size properties sothese properties are readonly If the specific rules for calculating used dont meetyour needs other properties such as usedbychildren and usedbysnapshots maygive you better insight into how your disk space is being consumedYou can set additional nonstandard properties on filesystems for your own useand for the use of your local scripts The process is the same as that for standardproperties For example many backup and snapshot utilities for ZFS read theirconfiguration information from filesystem propertiesThe names of custom properties must include a colon to distinguish them fromstandard propertiesOne filesystem per userSince filesystems consume no space and take no time to create the optimal number of them is closer to a lot than a few If you keep users home directories ona ZFS storage pool you may find it helpful to make each home directory a separate filesystemThere are several benefits If you need to set disk usage quotas home directories are a natural granularity at which to do this You can set quotas on both individual usersfilesystems and on the filesystem that contains all users Snapshots are per filesystem If each users home directory is a separatefilesystem the user can access old snapshots through zfs This featurealone is a huge time saver for administrators because it means that userscan service most of their own file restore needs ZFS lets you delegate permission to perform various operations such astaking snapshots or rolling back the filesystem to an earlier state If youprefer you can give users control over these operations for their own homedirectories We do not describe the details of ZFS permission managementin this book however see the man page entry for zfs allowSnapshots and clonesJust like a logical volume manager ZFS brings copyonwrite to the user level byallowing you to create instantaneous snapshots However theres an important difference ZFS snapshots are implemented per filesystem rather than per volume sothey have arbitrary granularityOn the command line you create snapshots with zfs snapshot For example thefollowing command sequence illustrates creation of a snapshot use of the snapshotthrough the filesystems zfssnapshot directory and reversion of the filesystem toits previous state sudo touch mntdemonewfsnowyouseeme ls mntdemonewfsnowyouseeme sudo zfs snapshot demonewfssnap sudo rm mntdemonewfsnowyouseeme ls mntdemonewfs ls mntdemonewfszfssnapshotsnapnowyouseeme sudo zfs rollback demonewfssnap ls optdemonewfsnowyouseemeYou assign a name to each snapshot at the time its created The complete specifierfor a snapshot is usually written in the form filesystemsnapshot This directory is hidden by default it does not appear in ls a output You can make it visible with zfsset snapdirvisible filesystemUse zfs snapshot r to create snapshots recursively The effect is the same as executing zfs snapshot on each contained object individually each subcomponentreceives its own snapshot All the snapshots have the same name but theyre logically distinct because the filesystem portion is differentZFS snapshots are readonly and although they can bear properties they are nottrue filesystems However you can instantiate a snapshot as a fullfledged writablefilesystem by cloning it sudo zfs clone demonewfssnap demosubclone ls mntdemosubclonenowyouseeme sudo touch mntdemosubcloneandmetoo ls mntdemosubcloneandmetoo nowyouseemeThe snapshot that is the basis of the clone remains undisturbed and readonly However the new filesystem demosubclone in this example retains a link to both thesnapshot and the filesystem on which its based and neither of those entities canbe deleted as long as the clone existsCloning isnt a common operation but its the only way to create a branch in afilesystems evolution The zfs rollback operation demonstrated above can onlyreturn a filesystem to its most recent snapshot so to use it you must permanentlydelete zfs destroy any snapshots made since the snapshot that is your reversiontarget Cloning lets you go back in time without losing access to recent changesFor example suppose that youve discovered a security breach that occurred sometime within the last week For safety you want to return a filesystem to its state ofa week ago to be sure today that it contains no hackerinstalled back doors At thesame time you dont want to lose recent work or the data for forensic analysis Thesolution is to clone the weekago snapshot to a new filesystem zfs rename the oldfilesystem and then zfs rename the clone in place of the original filesystemFor good measure also zfs promote the clone this operation inverts the relationship between the clone and the filesystem of origin After promotion the mainlinefilesystem has access to all the old filesystems snapshots and the old movedasidefilesystem becomes the cloned branchRaw volumesYou create swap areas and raw storage areas with zfs create just as you create filesystems The V size argument makes zfs treat the new object as a raw volume insteadof a filesystem The size can use any common unit for example mSince the volume does not contain a filesystem it is not mounted instead it showsup in the devzvol directory and can be referenced as if it were a hard disk or partition ZFS mirrors the hierarchical structure of the storage pool in these directoriesso sudo zfs create V m demoswap creates a MB swap volume located atdevzvoldemoswapYou can create snapshots of raw volumes just as you can with filesystems but because theres no filesystem hierarchy in which to put a zfssnapshot directory thesnapshots show up in the same directory as their source volumes Clones work toojust as youd expectBy default raw volumes receive a space reservation equal to their specified sizeYoure free to reduce the reservation or to do away with it entirely but note that thisconfiguration can make writes to the volume return an out of space error Clientsof raw volumes might not be designed to deal with such an errorStorage pool managementNow that weve waded into some of the features that ZFS offers at the filesystem andblockclient level we can go for a longer swim in ZFSs storage poolsUp to now weve used a pool called demo that we created from a single disk backon page Here it is in the output of zpool list zpool listNAME SIZE ALLOC FREE EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOTdemo M K G x ONLINE zroot G G G x ONLINE The pool named zroot contains the bootable root filesystem Bootable pools arecurrently restricted in several ways they can contain only a single virtual deviceand that device must be either a mirror array or a single disk drive it cannot be astriping set or a RAIDZ array This is either an implementation limit or a strongpush in the direction of robustness for the root filesystem were not sure whichzpool status adds more detail about the virtual devices that make up a storage pooland reports their current status zpool status demo pool demo state ONLINE scan none requestedconfigNAME STATE READ WRITE CKSUMdemo ONLINE ada ONLINE errors No known data errorsTime to get rid of this demo pool and set up something a bit more sophisticatedWeve attached five TB drives to our example system We first create a pool calledmonster that includes three of those drives in a RAIDZ singleparity configuration sudo zpool destroy demo sudo zpool create monster raidz ada ada ada zfs list monsterNAME USED AVAIL REFER MOUNTPOINTmonster K T K monsterZFS also understands raidz and raidz for double and triple parity configurationsThe minimum number of disks is always one more than the number of parity devices Here one drive out of three is used for parity so roughly TB is available foruse by filesystemsFor illustration we then add the remaining two drives configured as a mirror sudo zpool add monster mirror ada adainvalid vdev specificationuse f to override the following errorsmismatched replication level pool uses raidz and new vdev is mirror sudo zpool add f monster mirror ada adazpool initially balks at this configuration because the two virtual devices have different redundancy schemes This particular configuration is OK since both vdevshave some redundancy In actual use do not mix redundant and nonredundantvdevs since theres no way to predict which blocks might be stored on which devices partial redundancy is useless zpool status monster pool monster state ONLINE scan none requestedconfigNAME STATE READ WRITE CKSUMmonster ONLINE raidz ONLINE ada ONLINE ada ONLINE ada ONLINE mirror ONLINE ada ONLINE ada ONLINE errors No known data errorsZFS distributes writes among all a storage pools virtual devices As demonstratedin the preceding example it is not necessary for all virtual devices to be the samesize However the components within a redundancy group should be of similarsize If they are not only the smallest size is used on each component Multiplesimple disks used together in a storage pool is essentially a RAID configuration In this example the disks are all the same size but the virtual devices are not TB vs TBYou can add additional vdevs to a pool at any time However existing data wont beredistributed to take advantage of parallelism Unfortunately you cannot currentlyadd additional devices to an existing RAID array or mirror This is an area in whichBtrfs has a distinct advantage since it accommodates all sorts of reorganizations ina relatively clean and automatic mannerZFS has an especially nice implementation of read caching that makes good use ofSSDs To set up this configuration just add the SSDs to the storage pool as vdevs oftype cache The caching system uses an adaptive replacement algorithm developedat IBM that is smarter than a normal LRU least recently used cache It knows aboutthe frequency at which blocks are referenced as well as their recentness of use soreads of large files are not supposed to wipe out the cacheHot spares are handled as vdevs of type spare You can add the same disk to multiple storage pools whichever pool experiences a disk failure first gets to claim thespare disk Btrfs ZFS lite for LinuxOracles Btrfs filesystem project Btree file system officially pronounced butterFS or better FS though its hard not to think butter face aimed to repeat manyof ZFSs advances on the Linux platform during the long interregnum when ZFSseemed like it might be lost to Linux because of licensing issuesAlthough Btrfs remains under active development its been a standard part of theLinux kernel trunk since Its available and ready to use on nearly all Linuxsystems and SUSE Enterprise Linux has even made it a supported option for theroot filesystem Because the code base evolves quickly its probably best to avoidBtrfs on stabilityoriented distributions such as Red Hat for now old versions haveknown issuesBtrfs vs ZFSBecause they share some technical underpinnings comparisons between Btrfs andZFS are probably inevitable However Btrfs is not a ZFS clone and it doesnt seekto reproduce ZFSs architecture For example you mount Btrfs volumes just likethose of other filesystems by running the mount command or by listing them inthe etcfstab fileAlthough Btrfs volumes and their subvolumes exist in a unified namespace theresno hierarchical relationship among them To make a change to a group of Btrfssubvolumes you must modify each of them individually Btrfs commands do notoperate recursively and volume properties are not inheritable This isnt an omission so much as a design choice why load up the filesystem the developers askwith features that you can emulate in a shell scriptBtrfs reflects this preference for simplicity in a variety of ways For example Btrfsstorage pools can include only one group of disks in one particular configurationeg RAID whereas ZFS pools can include multiple disk groups as well as caching disks intent logs and hot sparesAs is common in the software arena debates over the relative merits of ZFS andBtrfs tend to become heated and to focus on stylistic distinctions However severalimportant differences between the two systems rise above the level of nitpickingand personal preference Btrfs is the clear winner when it comes to changing your hardware configuration ZFS didnt even show up for this fight You can add or removedisks at any time or even change RAID type and Btrfs redistributes existing data accordingly while remaining online In ZFS such changes areusually impossible without your dumping your data to external mediaand starting over Even without memoryintensive features such as deduplication enabledZFS functions best with a generous amount of RAM GB is the recommended minimum Thats a lot of memory for a virtual server ZFSs ability to cache frequently read data on separate cache SSDs is a killerfeature for many use cases and one for which Btrfs currently has no answer As of the Btrfs implementations of parity raid RAID and arenot yet ready for production use Thats not our opinion its the officialword from the developers This is a significant missing featureSetup and storage conversionIn this section we demonstrate a few common Btrfs procedures analogous to thoseshown for ZFS in previous sections We first set up Btrfs for use on a set of two TBhard disks configured for RAID mirroring sudo mkfsbtrfs L demo d raid devsdb devsdcLabel demoUUIDNode size Sector size Filesystem size TiBBlock group profiles Data RAID GiB Metadata RAID GiB System RAID MiBSSD detected noIncompat features extref skinnymetadataNumber of devices Devices ID SIZE PATH GiB devsdb GiB devsdc sudo mkdir mntdemo sudo mount LABELdemo mntdemoWe could name any of the component devices in the mount command line but itssimplest to just use the label we assigned to the group demoThe btrfs filesystem usage command shows how the space on these disks is currently being used sudo btrfs filesystem usage mntdemoOverall Device size TiB Device allocated GiB Device unallocated TiB Device missing B Used MiB Free estimated GiB min GiB Data ratio Metadata ratio Global reserve MiB used BDataRAID SizeGiB UsedKiB devsdb GiB devsdc GiBMetadataRAID SizeGiB UsedKiB devsdb GiB devsdc GiBSystemRAID SizeMiB UsedKiB devsdb MiB devsdc MiBUnallocated devsdb GiB devsdc GiBThe interesting thing to note here is the small initial allocations into the RAID groups for data metadata and system blocks Most disk space remains in an unallocated pool that has no intrinsic structure The mirroring we requested isnt imposed on the disks as a whole just on the blocks that are actually in use Its not arigid structure so much as a policy to be implemented at the level of block groupsThis distinction is key to understanding how Btrfs can adapt to changing requirements and hardware provisioning Heres what happens when we store some filesinto the new filesystem and then add a third disk mkdir mntdemousr cd usr tar cf cd mntdemousr sudo tar xfp sudo btrfs device add devsdd mntdemo sudo btrfs filesystem usage mntdemo Overall omitted from this outputDataRAID SizeGiB UsedGiB devsdb GiB devsdc GiBMetadataRAID SizeGiB UsedMiB devsdb GiB devsdc GiBSystemRAID SizeMiB UsedKiB devsdb MiB devsdc MiBUnallocated devsdb GiB devsdc GiB devsdd GiBThe new disk devsdd has become available to the pool but the existing blockgroups are fine as they were so none of them reference the new disk Future allocations would automatically take advantage of the new disk If we like we can forceBtrfs to level the data among all disks sudo btrfs balance start fullbalance mntdemoStarting balance without any filtersDone had to relocate out of chunksConversion among RAID levels is also a form of balancing Now that we have threedisks available we can convert to RAID sudo btrfs balance start dconvertraid mconvertraid mntdemoDone had to relocate out of chunksIf we had glanced at the usage data during the conversion wed have seen blockgroups for both RAID and RAID active simultaneously Disk removals worksimilarly Btrfs incrementally copies all blocks to groups that dont include the leaving disk and eventually no data remains thereVolumes and subvolumesSnapshots and quotas are filesystemlevel entities in Btrfs so its helpful to be ableto define portions of the file tree as distinct entities Btrfs calls these subvolumesA subvolume looks a lot like a regular filesystem directory and in fact it remainsaccessible as a subdirectory of its parent volume as shown below sudo btrfs subvolume create mntdemosubCreate subvolume mntdemosub btrfs subcommands can be abbreviated to any unique prefix For example btrfs filesystem usage isalso accessible as btrfs f u We spell out commands for clarity and propriety sudo touch mntdemosubfileinasubvolume ls mntdemosubfileinasubvolumeThe subvolume is not automatically mounted its visible here as part of the parentvolume However you can mount a subvolume independently of its parent withthe subvol mount option For example mkdir sub sudo mount LABELdemo o subvolsub sub ls subfileinasubvolumeThere is no way to prevent a subvolume from showing up within its parent volumewhen the parent is mounted To create the illusion of multiple independent noninteracting volumes just make them subvolumes of the root and mount each of themseparately with the subvol option The root itself is not required to be mountedanywhere In fact Btrfs lets you specify a volume other than the root to be the default mount target when no subvol is requested see btrfs subvolume setdefaultTo see or manipulate the full Btrfs hierarchy under this configuration just mountthe root on a scratch directory with subvol Its fine for volumes to be mountedseveral times and accessible through multiple pathsVolume snapshotsBtrfss version of volume snapshots works a lot like cp except that copies are shallow and initially share all their storage with the parent volume sudo btrfs subvolume snapshot mntdemosub mntdemosubsnapCreate a snapshot of mntdemosub in mntdemosubsnapUnlike ZFS snapshots Btrfs snapshots are writable by default In fact there is nosuch thing as a snapshot per se in Btrfs a snapshot is just a volume that happensto share some storage with another volume sudo touch mntdemosubanotherfile ls mntdemosubanotherfile fileinasubvolume ls mntdemosubsnapfileinasubvolumeFor an immutable snapshot just pass the r option to btrfs subvolume snapshotBtrfs does not make a fundamental distinction between readonly snapshots andwritable copies in the way that ZFS does In ZFS writable copies are clones To create one first make a readonly snapshot then create a clone based on that snapshotBtrfs does not enforce any particular naming or location conventions when it comesto defining subvolumes and snapshots so its up to you to decide how these entitiesshould be organized and named The Btrfs documentation at btrfswikikernelorgsuggests a couple of conventions for your considerationBtrfs also has no rollback operation that resets a volume to its state as of a particular snapshot Instead you can just move the original volume aside and mv orcopy a snapshot in its place ls mntdemosubanotherfile fileinasubvolume sudo mv mntdemosub mntdemosubold sudo btrfs subvolume snapshot mntdemosubsnap mntdemosubCreate a snapshot of mntdemosubsnap in mntdemosub ls mntdemosubfileinasubvolumeNote that this change confuses direct mounts of the subvolume Theyll need to beremounted afterwardShallow copiesThe analogy between Btrfs snapshots and cp is more than just coincidence Youcannot create snapshotsas suchof files or of directories that are not subvolumeroots But interestingly you can create shallow copies of arbitrary files and directories with cp reflink even across subvolume boundariesThis option activates Btrfsspecific magic inside cp that negotiates directly with thefilesystem to arrange for copyonwrite duplication The semantics are identical tothose of a normal cp and also perilously close to those of a snapshotBtrfs doesnt track shallow copies for you as it would with snapshots and it alsodoesnt necessarily guarantee perfect pointintime consistency for actively modified directory hierarchies But in other respects the two operations are markedlysimilar One nice feature of shallow copies is that they require no special permissions any user can take advantage of themIf you specify the cp option in the form reflinkauto cp shallowcopies when it canand behaves normally otherwise That makes it a tempting target for a bashrc aliasalias cpcp reflinkauto Data backup strategyOn a good day your main focus within the storage environment is to ensure thatperformance remains good and that sufficient free space is available Unfortunatelynot every day is a good day With the Google Labs study finding that a disk drivehas less than a chance of surviving for five years the deck is stacked againstus Always have systems in place to protect valuable data against catastrophic lossand be prepared to activate your recovery procedure at short noticeRAID and other data replication schemes protect against the failure of a single facility or piece of hardware However there are many other ways to lose data thatthese technologies do not address For example if you experience a security breachor an infection by ransomware your data can be altered or corrupted even thoughthe physical layer remains perfectly intact Automated replication of compromiseddata to multiple disks or sites only increases the misery You need immutable pointintime backups of critical data that you can revert to as a fallback optionIn past decades media such as magnetic tapes were a popular storage methodfor offline backups However the capacity of these media proved unable to keepup with the exponentially growing sizes of hard drives and SSDs Along with thephysical challenges of transporting and storing tapes and of maintaining finickymechanical tape drives the capacity issues ultimately relegated tape media to thestatus of mm camera film its still technically on the market but you have towonder whos actually buying the stuffToday most cloud platforms let you capture pointintime backups in the form ofsnapshots usually on an automated schedule You pay a monthly fee for the storageconsumed by each snapshot and can set your own retention policiesRegardless of the exact technology you use to implement backups you need a written plan that answers at least the following questionsOverall strategy What data is to be backed up What system or technology will perform the backups Where will backup data be stored Will backups be encrypted If so where will encryption keys be stored How much will it cost to store backups over timeTimelines How often will backups be performed How often will backups be validated and restoretested How long will backups be retainedPeople Who will have access to backup data Who will have access to the encryption keys that protect backup data Who will be in charge of verifying the execution of backups Who will be in charge of validating and restoretesting backupsUse and protection How will backup data be accessed or restored in an emergency How will you ensure that neither a hacker nor a bogus process can corruptmodify or delete backups That is how will you achieve immutability How will backup data be protected against being taken hostage by an adversarial cloud provider vendor or governmentThe best answers to these questions vary by organization type of data regulatoryenvironment technology platform and budget just to name a few potential factorsTake time today to map out a backup plan for your environment or to review yourexisting backup plan Recommended readingLucas Michael W and Allan Jude FreeBSD Mastery ZFS Tilted WindmillPress Jude Allan and Michael W Lucas FreeBSD Mastery Advanced ZFS TiltedWindmill Press The two titles above are the goto references for modern ZFS Although they purport to be FreeBSDspecific most of the material applies to ZFS on Linux as wellThe Advanced ZFS book is particularly useful in its coverage of topics as varied asjails permission delegation caching strategies and performance analysisLucas Michael W and Allan Jude FreeBSD Mastery Storage Essentials TiltedWindmill Press McKusick Marshall Kirk George V NevilleNeil and Robert N M Watson The Design and Implementation of the FreeBSD Operating System nd EditionUpper Saddle River NJ AddisonWesley Professional This book addressesa variety of kernelrelated subjects but it includes complete chapters on UFS ZFSand the VFS layerThe Network File System protocol commonly known as NFS lets you share filesystems among computers NFS is nearly transparent to users and no information islost when an NFS server crashes Clients can simply wait until the server returnsand then continue as if nothing had happenedNFS was introduced by Sun Microsystems in It was originally implementedas a surrogate filesystem for diskless clients but the protocol proved to be well designed and useful as a general file sharing solution These days all UNIX vendorsand Linux distributions offer some version of NFS The NFS protocol is an openstandard that is documented in RFCs see RFCs and in particular Meet network file servicesThe goal of a network file service is to grant shared access to files and directoriesthat are stored on the disks of remote systems User applications must be able toread and write to these files with the same system calls they use for local files thatfiles are stored elsewhere on the network should be transparent to applications Ifmore than one network client or application attempts to modify a file simultaneously the file sharing service must resolve any conflicts that ariseThe Network File SystemThe competitionNFS is not the only file sharing system around The Server Message Block SMBprotocol underlies the file sharing capabilities built into Windows and macOS However UNIX and Linux can also speak SMB by running the Samba addon packageIf you run a hybrid network that includes a variety of different operating systemsyou may find that SMB is the path that presents the fewest compatibility hurdlesNFS is most commonly used in shops where UNIX and Linux are predominantIn those contexts it offers a somewhat more natural fit and a higher degree of integration But even in these environments SMB remains a plausible option Its uncommonbut not unheard offor sites consisting exclusively of UNIX and Linuxsystems to rely on SMB as their primary file sharing protocolSharing files over a network seems like a simple task but in fact its a confoundinglycomplex problem that abounds with edge cases and subtleties Many protocol issueshave come to light only through bugs encountered in unusual situations Both NFSand SMB show the scars of battles fought to maintain security performance andreliability over decades of development and widespread use Todays administratorscan be confident that these protocols will not regularly corrupt data or otherwiseincur the wrath of irate users but it has taken a lot of work and experience to getto this pointStorage area network SAN systems are another option for highperformancestorage management on a network SAN servers need not understand filesystemsbecause they serve only disk blocks unlike NFS and SMB which operate at thelevel of filesystems and files rather than raw storage devices A SAN affords fastreadwrite access but its unable to manage concurrent access by multiple clientswithout the help of a clustered filesystemFor big data projects several open source distributed filesystems have come intocommon use GlusterFS and Ceph implement both POSIXcompliant filesystemsand RESTful object storage distributed among a cluster of nodes for fault toleranceCommercial versions of both systems are sold by Red Hat which acquired both developers Both systems are productionready highly capable filesystems worthy ofconsideration for use cases like big data processing and high performance computingCloudbased systems have additional options Refer to page Issues of stateOne decision made when designing a network filesystem is to determine what partof the system will track the files that each client has open information referred togenerically as state A server that records the status of files and clients is said to bestateful one that does is stateless Both approaches have been used over the yearsand both have benefits and drawbacksStateful servers keep track of all open files across the network This mode of operation introduces many layers of complexity more than you might expect and makesSee Chapter for more details onSMB and Sambarecovery in the event of a crash far more difficult When the server returns froma hiatus a negotiation between the client and server must occur to reconcile thelast known state of the connection Statefulness lets clients maintain more controlover files and facilitates the robust management of files opened in readwrite modeOn a stateless server each request is independent of the requests that have precededit If either the server or the client crashes nothing is lost in the process Under thisdesign it is painless for servers to crash or reboot since they do not maintain anycontext However its impossible for the server to know which clients have openeda file for writing so it cannot manage concurrencyPerformance concernsNetwork filesystems should present a seamless experience to users Accessing a fileover the network should be no different from accessing a file on a local filesystemUnfortunately wide area networks have high latencies which make operations behave erratically and low bandwidth which yields slow performance on large filesMost file service protocols including NFS incorporate techniques to minimizeperformance problems on both local and wide area networksMost protocols try to minimize the number of network requests For example readahead caching preloads portions of a file into a local memory buffer to avoid a delaywhen a new section of the file is read A little extra network bandwidth is consumedin an effort to avoid the latency of a full roundtrip exchange with the serverSimilarly some systems cache writes in memory and send updates in batches reducing the delay incurred when communicating write operations to the serverThese types of batch operations are referred to generically as request coalescingSecurityAny service that grants convenient access to files on a network has great potentialto cause security problems Local filesystems implement complex access controlalgorithms and safeguard files with granular permissions On a network these tasksare greatly complicated by differences in configurations among machines and byvagaries such as race conditions bugs in file service software and unresolved edgecases in file sharing protocolsThe rise of directory and centralized authentication services has improved the security of network filesystems The bottom line is that no client can be trusted toauthenticate itself sanely so a trusted central system must verify identities andapprove access to files Most file sharing services can be integrated with a varietyof different authentication providersFile sharing protocols do not typically address the issues of privacy and integrityor at least they do not do so directly As with authentication this responsibilityis generally outsourced to another layer such as a Kerberos SSH or a VPN tunnel However recent versions of SMB have added strong encryption and integritychecking Many sites that run NFS on a trusted LAN choose to forgo cryptographybecause an easy and highperformance solution is unavailable The NFS approachThe newest version of the NFS protocol has been refined to increase platform independence to improve performance over wide area networks such as the Internetand to add strong modular security features Most implementations also includediagnostic utilities to help debug configuration and performance problemsNFS is a network protocol so in theory it could be implemented in user space justlike most other network services However the traditional approach has been forparts of the NFS implementation on both server and client sides to live insidethe kernel mostly to improve performance This general pattern continues evenon Linux where locking functions and certain system calls have proved difficultto export to user space Fortunately the kernelresident parts of NFS need no configuration and are largely transparent to administratorsNFS is not an offtheshelf solution for all file sharing problems High availabilitycan only be achieved with warm standbys but NFS has no builtin provisions forsynchronizing with backup servers The sudden disappearance of an NFS serverfrom the network can result in clients holding stale file handles that can be cleanedup only with a reboot Strong security is possible but is overly complex Despitethese drawbacks NFS remains the most common choice for UNIX and Linux filesharing on a LANProtocol versions and historyThe first public release of the NFS protocol was version in The original protocol made some expensive tradeoffs to favor consistency over performance and wasquickly replaced Its highly unlikely that youll encounter this version in use todayNFS version which dates from the early s eliminates this bottleneck with acoherency scheme that permits asynchronous writes It also updates several otheraspects of the protocol that were found to have caused performance problems andit improves the handling of large files The net result is that NFS version is quitea bit faster than version NFS version dating from but not used widely until later in that decade isa major overhaul that includes many new fixes and features Highlighted enhancements include Compatibility and cooperation with firewalls and NAT devices Integration of the lock and mount protocols into the core NFS protocol Stateful operation Strong modular security Support for replication and migration Support for both UNIX and Windows clients Access control lists ACLs Support for Unicode filenames Good performance even on lowbandwidth connectionsThe various NFS protocol versions cannot talk to one another but NFS servers including those on all our example systems typically implement all three versions Inpractice all combinations of NFS clients and servers can interoperate with someversion of the protocol Always use the V protocol if both sides support itNFS remains actively developed and in widespread use Version written by someof the original stakeholders from Suns heyday reached RFC draft status in early The Elastic File System service from AWS which became generally availablein mid adds NFSv filesystems for use by EC instancesAlthough V is a significant step forward in many ways it hasnt much altered theprocess of configuring and administering NFS In some ways this is a feature forexample you still use the same configuration files and commands to administerall versions of NFS In other ways its a problem some aspects of the configurationprocess feel juryrigged particularly on FreeBSD and some options have becomeambiguous or overloaded with different meanings or configuration formats depending on which version of NFS you are usingRemote procedure callsWhen Sun developed the first versions of NFS in the s they realized that manyof the networkrelated problems that needed solving for NFS would apply to othernetworkbased services too They developed a more general framework for remoteprocedure calls known as RPC or SunRPC and built NFS on top of that This workopened the door for applications of all kinds to run procedures on remote systemsas if they were being run locallySuns RPC system was primitive and somewhat hackish far better systems existtoday to fill this need Nevertheless NFS still relies on Sunstyle RPCs for muchof its functionality Operations that read and write files mount filesystems accessfile metadata and check file permissions are all implemented as RPCs The NFSprotocol specification is written generically so a distinct RPC layer is not technically required However we are aware of no NFS implementations that stray fromthe original architecture in this regardTransport protocolsNFS version originally used UDP because that was what performed best on theLANs and computers of the s Although NFS does its own packet sequencereassembly and error checking UDP and NFS both lack the congestion control algorithms that are essential for good performance on a large IP network As do infinitely more horrifying monstrosities than SunRPC check out SOAPTo remedy these problems and others NFS migrated to a choice of UDP or TCPin version and to TCP only in version The TCP option was first explored asa way to help NFS work through routers and over the Internet Over time mostof the original reasons for preferring UDP over TCP have evaporated in the warmlight of fast CPUs cheap memory and highspeed networksStateA client must explicitly mount an NFS filesystem before using it just as a clientmust mount a filesystem stored on a local disk However NFS versions and arestateless and the server does not keep track of which clients have mounted eachfilesystem Instead the server simply discloses a secret cookie at the conclusionof a successful mount negotiation The cookie identifies the mounted directory tothe NFS server and so opens a way for the client to access its contents Cookiespersist between reboots of the server so a crash does not leave the client in an unrecoverable muddle The client can simply wait until the server is available againand resubmit the requestNFSv on the other hand is a stateful protocol both client and server maintaininformation about open files and locks When the server fails the clients assist inthe recovery process by sending the server their precrash state information A returning server waits for a predefined grace period for former clients to report theirstate information before it permits new operations and locks The cookie management of V and V no longer exists in NFSvFilesystem exportsNFS servers maintain a list of directories called exports or shares that theymake available to clients over the network By definition all servers export at leastone directory Clients can then mount these exports and add them to their fstab filesIn V and V each export is treated as an independent entity that is exported separately In the V specification a server exports a single hierarchical pseudofilesystem that incorporates all its exported directories Essentially the pseudofilesystemis the servers own filesystem namespace skeletonized to remove anything that isnot exportedFor example consider the following list of directories with the directories to beexported in boldfacewwwdomainwwwdomainwwwdomainvarlogshttpdvarspool Technically any transport protocol that implements congestion control can be used but TCP is theonly reasonable choice todaySee page formore informationabout the fstab fileIn NFS version each exported directory must be separately configured Clientsystems must execute three different mount requests to obtain access to all theservers exportsIn NFS version however the pseudofilesystem bridges the disconnected portions of the directory structure to create a single view for NFS clients Rather thanrequesting a separate mount for each of wwwdomain wwwdomain andvarlogshttpd the client can simply mount the servers entire pseudoroot directory and browse the hierarchyThe directories that are not exported wwwdomain and varspool do not appear during browsing In addition individual files contained in var www andvarlogs are not visible to the client because the pseudofilesystem portion of thehierarchy includes only directories Thus the client view of the NFSvexportedfilesystem is var logs httpd www domain domainThe server specifies the root of the exported filesystems in a configuration file knownas the exports file usually kept in etc Pure NFSv clients cannot peruse the list ofmounts on a remote server Instead they simply mount the pseudoroot and thenall available exports become accessible through that mount pointThats the story according to the RFC specification In practice the situation is somewhat fuzzy The Solaris implementation conformed to this specification Linux madea halfhearted attempt at support for the pseudofilesystem in the early NFSv codebut later revised it to support the scheme more fully todays version appears to respect the intent of the RFC FreeBSD does not implement the pseudofilesystem asdescribed by the RFC The FreeBSD export semantics are essentially the same as inversion all subdirectories within an export are available to clientsFile lockingFile locking as implemented by the flock lockf or fcntl systems calls has beena sore point on UNIX systems for a long time On local filesystems it has beenknown to work less than perfectly In the context of NFS the ground is shakierstill By design early versions of NFS servers are stateless they have no idea whichmachines are using any given file However to implement locking state information is needed What to doThe traditional answer was to implement file locking separately from NFS Inmost systems two distinct daemons lockd and statd attempted to make a go ofit Unfortunately the task was difficult for a variety of subtle reasons and NFS filelocking under lockd and statd is generally unreliableNFSv removed the need for lockd and statd by folding locking and hence statefulness and all that it implies into the core protocol This change introduces significant complexity but obviates many of the related problems of earlier NFS versionsUnfortunately separate lockds and statds are still needed to support V and Vclients if your site has them Our example systems all ship with the earlier versionsof NFS enabled so the separate daemons still run by defaultSecurity concernsIn many ways NFS V and V are poster children for everything that is or ever hasbeen wrong with UNIX and Linux security The protocol was originally designedwith essentially no concern for security and convenience has its price NFSv hasaddressed the security concerns of earlier versions by mandating support for strongsecurity services and establishing better means of user identificationAll versions of the NFS protocol are intended to be securitymechanism independent and most servers support multiple flavors of security A few of the commonflavors include AUTHNONE no authentication AUTHSYS UNIXstyle user and group access control RPCSECGSS a stronger flavor that enables flexible security schemesHistorically most sites used AUTHSYS authentication which depends on UNIXuser and group identifiers In this scheme the client simply sends the local UID andGID of the user requesting access to the server The server compares the values tothose from its own etcpasswd file and determines whether the user should haveaccess Thus if users mary and bob share the same UID on two different clientsthey will have access to each others files Furthermore users that have root accesson a system can su to whatever UID they wish the server will then give them access to the corresponding users filesEnforcing passwd file consistency among systems is essential in environments thatuse AUTHSYS But even this is only a security fig leaf any rogue host or heavenforbid Windows machine can authenticate its users however it likes and thereby subvert NFS securityTo prevent such problems most sites can use a more robust security mechanismsuch as Kerberos in combination with the NFS RPCSECGSS layer This configuration requires both the client and server to participate in a Kerberos realm TheKerberos realm authenticates clients centrally avoiding the problems of selfidentification described above Kerberos can also provide strong encryption and guaranteed integrity for files transferred over the network All protocolconformantNFS version systems must implement RPCSECGSS but its optional in version Or its network database equivalent such as NIS or LDAPSee page formore informationabout KerberosNFS version requires TCP as a transport protocol and communicates over port Since V does not rely on any other ports opening access through a firewallis as simple as opening TCP port As with all access list configurations besure to specify source and destination addresses in addition to the port If your sitedoesnt need to provide NFS services to hosts on the Internet block access throughthe firewall or use a local packet filterFile service over wide area networks with NFSv and V is not recommended because of the long history of bugs in the RPC protocols and the lack of strong security mechanisms Administrators of NFS version servers should block access toTCP and UDP ports and also the portmap port Given the myriad and obvious shortcomings of AUTHSYS security we stronglyrecommend discontinuing all use of NFSv If you have ancient operating systemsthat cant be updated to NFSv compatibility at least implement packet filters torestrict network connectivityIdentity mapping in version Before launching into the following discussion we should warn you that we consider all implementations of AUTHSYS security to be more or less broken for security purposes We strongly suggest Kerberos and RPCSECGSS authenticationits the only reasonable choiceAs discussed in Chapter User Management UNIX operating systems identifyusers through a collection of UIDs and GIDs in the local passwd file or an LDAPdirectory NFS version on the other hand represents users and groups as stringidentifiers of the form usernfsdomain and groupnfsdomain NFSv clients andservers run an identity mapping daemon that translates UNIX identifier values tostrings that match this formatWhen a V client performs operations that return identities such as listing theowners of a set of files with ls l the underlying operation is a series of stat callsthe servers identity mapping daemon uses its local passwd file to convert the UIDand GID of each file object to a string such as benadmincom The clients identitymapper then reverses the process converting benadmincom into local UID andGID values which may or may not be the same as the servers If a string value doesnot match any local identity the nobody user account is assigned as a placeholderAt this point the remote filesystem call stat has completed and returned UID andGID values to its caller here the ls command Since ls was called with the l option it needs to display text names instead of numbers So ls in turn retranslatesthe IDs back to textual names using the getpwuid and getgrgid library routinesThese routines once again consult the passwd file or its network database equivalent What a long strange trip its beenConfusingly the identity mapper is used only when retrieving and setting file attributes typically ownerships Identity mapping plays no role in authentication orSee page formore informationabout firewallsaccess control all of which is handled in the traditional form by RPC The identitymapper may do a better job of mapping than the underlying NFS protocol causing the apparent file permissions to conflict with the permissions the NFS serverwill actually enforceConsider for example the following commands on an NFSv clientbennfsclient id benuidben gidben groupsbenbennfsclient id johnuidjohn gidjohn groupsjohnbennfsclient ls ld bendrwxrxrx john root May benbennfsclient touch benfilebennfsclient ls l benfilerwrwr john nfsnobody May benfileFirst ben is shown to have UID and john to have UID An NFSexportedhome directory called ben appears to have permissions and is owned by johnHowever ben is able to create a file in the directory even though the ls l outputindicates that he lacks write permissionOn the server john has UID Since john has UID on the client the identity mapper performs UID conversion as described above with the result that johnappears to be the owner of the directory However the identity mapping daemonplays no role in access control For the file creation operation bens UID of is sent directly to the server where it is interpreted as johns UID and permissionis grantedHow do you know which operations are identity mapped and which are not Itssimple whenever a UID or GID appears in the filesystem API as with stat or chownit is mapped Whenever the users own UIDs or GIDs are used implicitly for accesscontrol they are routed through the designated authentication systemFor this reason enforcing consistent passwd files or relying on LDAP is essentialfor users of AUTHSYS securityUnfortunately for administrators identity mapping daemons are not standardizedacross systems so their configuration processes may be different Specifics for ourexample systems are covered on page Root access and the nobody accountAlthough users should generally be given identical privileges wherever they go itstraditional to prevent root from running rampant on NFSmounted filesystems Bydefault the NFS server intercepts incoming requests made on behalf of UID andchanges them to look as if they came from some other user This modification iscalled squashing root The root account is not entirely shut out but it is limitedto the abilities of a normal userA placeholder account named nobody is defined specifically to be the pseudouser as whom a remote root masquerades on an NFS server The traditional UID fornobody is the bit twoscomplement equivalent of UID You canchange the default UID and GID mappings for root in the exports file Some systems have an allsquash option to map all client UIDs to the same pseudouserUID on the server This configuration eliminates all distinctions among users andcreates a sort of publicaccess filesystemThe intent behind these precautions is nice but their ultimate value is not as greatas it might seem Remember that root on an NFS client can su to whatever UID itwants so user files are never really protected The only real effect of root squashing is to prevent access to files that are owned by root and not readable or writableby the worldPerformance considerations in version NFSv was designed to achieve good performance over wide area networks MostWANs have higher latency and lower bandwidth than those of LANs NFS takesaim at these problems with the following refinements An RPC called COMPOUND clumps multiple file operations into onerequest reducing the overhead and latency incurred from multiple remote procedure calls A delegation mechanism allows clientside caching of files Clients canmaintain local control over files including those open for writingThese features are part of the core NFS protocol and do not require much attentionfrom system administrators Serverside NFSAn NFS server is said to export a directory when it makes the directory available for use by other machines Exports are presented to NFSv clients as a singlefilesystem hierarchy through the pseudofilesystemIn NFS version the process used by clients to mount a filesystem is separate fromthe process used to access files The operations use separate protocols and the requests are served by different daemons mountd for mount discovery and requestsand nfsd for actual file service On some systems these daemons are called rpcnfsdand rpcmountd as a reminder that they rely on RPC as an underlying mechanism Although the Red Hat NFS server defaults to UID the nobody account in the passwd file usesUID You can leave things as they are add a passwd entry for UID or change anonuid andanongid to if you wish It really doesnt matter Some systems also have an nfsnobody accountand hence require the portmap daemon to be running In this chapter we omitthe rpc prefix for readabilityNFSv does not use mountd at all If you absolutely must run old clients that onlysupport NFSv mountd must remain activeBoth mountd and nfsd should start when the system boots and both should remain running as long as the system is up Both Linux and FreeBSD automaticallyrun the daemons when you enable NFS serviceNFS uses a single accesscontrol database that tells which filesystems should beexported and which clients can mount them The operative copy of this databaseis usually kept in a file called xtab and also in tables internal to the kernel xtab isa binary file maintained for use by the server daemonMaintaining a binary file by hand is not much fun so most systems assume that youwould rather maintain a text file usually etcexports that enumerates the systemsexported directories and their access settings The system can then consult this textfile at boot time to automatically construct the xtab fileetcexports is the canonical humanreadable list of exported directories Its contents are read by exportfs a on Linux and at a simple restart of the NFS serveron FreeBSD Hence when you edit etcexports run exportfs a to activate yourchanges on Linux or run service nfsd restart on FreeBSD If you serve V clientsfrom FreeBSD restart mountd as well service mountd reloadNFS deals with the logical layer of the filesystem Any directory can be exported itdoesnt have to be a mount point or the root of a physical filesystem However forsecurity NFS does pay attention to the boundaries between filesystems and doesrequire each device to be exported separately For example on a machine that hasset up chimchimusers as a separate partition you could export chimchim without implicitly exporting chimchimusersClients are usually allowed to mount subdirectories of an exported directory if theywish although the protocol does not require this feature For example if a serverexports chimchimusers a client could mount only chimchimusersjoe andignore the rest of the users directoryLinux exportsOn Linux the exports file consists of a list of exported directories in the leftmostcolumn followed by the hosts that are allowed to access them and associated optionson the right Whitespace separates the filesystem from the list of clients and eachclient is followed immediately by a parenthesized list of commaseparated optionsLines can be continued with a backslash For example the linehome usersadmincomrw rolets home be mounted readwrite by all machines in the usersadmincom domainand readonly by all machines on the class C network If a system inthe usersadmincom domain resides on the network that client willbe granted readonly access The least privileged rule winsFilesystems listed in the exports file without a specific set of hosts are usuallymountable by all machines Thats a sizable security holeThe same sizable security hole can be created accidentally by a misplaced spaceFor example the linehome usersadmincom rwpermits any host readwrite access except for usersadmincom which has readonly permission the default OopsThere is unfortunately no way to list multiple client specifications for a single setof options You must repeat the options for all desired clients Table lists thetypes of client specifications that can appear in the exports fileTable Client specifications in the Linux etcexports fileType Syntax MeaningHostname hostname Individual hostsNetgroup groupname NIS netgroups infrequently usedWild cards and FQDNs a with wild cards will not match a dotIPv networks ipaddrmask CIDRstyle specifications eg IPv networks ipaddrmask IPv addresses with CIDR notation dba Fully qualified domain namesTable on the next page shows the most commonly used export options understood by LinuxThe subtreecheck option the default verifies that every file accessed by a clientlies within an exported subdirectory If you turn off this option only the fact thatthe file is within an exported filesystem is verified Subtree checking can causeoccasional problems when a requested file is renamed while the client has the fileopen If you anticipate many such situations consider setting nosubtreecheckasync tells the NFS server to ignore the protocol spec and reply to requests before they are written to disk This might result in a slight performance boost butmight also result in corrupted data if the server restarts unexpectedly The defaultbehavior is syncThe replicas option is merely a convenience that helps clients discover mirrors ifthe server goes offline The actual replication of the filesystem must be handledout of band through some other mechanism such as rsync or DRBD replicationsoftware for Linux The replica referral feature was added in NFSvTable Common export options in LinuxOption Descriptionro Exports readonlyrw Exports for reading and writing the defaultrwlist Exports readmostly The list enumerates the hosts allowed tomount for writing all others must mount readonlyrootsquash Maps squashes UID and GID to the values specified byanonuid and anongid This is the defaultnorootsquash Allows normal access by root Dangerousallsquash Maps all UIDs and GIDs to their anonymous versions aanonuidxxx Specifies the UID to which remote roots should be squashedanongidxxx Specifies the GID to which remote roots should be squashednoaccess Blocks access to this dir and subdirs use with nested exportswdelay Delays writes in hopes of coalescing multiple updatesnowdelay Writes data to disk as soon as possibleasync Makes server reply to write requests before actual disk writenohide Reveals filesystems mounted within exported file treeshide Is the opposite of nohidesubtreecheck Verifies that each requested file is within an exported subtreenosubtreecheck Verifies only that file requests refer to an exported filesystemsecurelocks Requires authorization for all lock requestsinsecurelocks Specifies less stringent locking criteria supports older clientssecflavor Lists security methods for the exported directory bpnfs Enables V parallel NFS extensions for direct client accessreplicaspathhost Sends clients a list of alternative locations for this exporta This option is useful for supporting PCs and other untrusted singleuser hostsb Values include sys UNIX authentication the default dh DES not recommended krb Kerberos authentication krbi Kerberos authentication and integrity krbp Kerberos authentication integrityand privacy and none anonymous access not recommendedEarly versions of the Linux NFSv implementation required administrators to designate a pseudofilesystem root with the fsid flag in etcexports This is no longer required To create a pseudofileystem as described by the RFC just list exportsas normal and from an NFSv client mount on the server The subdirectoriesunder the mount point will be the exported filesystems If you do designate an export as fsid that filesystem and all its subdirectories are exported for V clientsFreeBSD exportsIn keeping with longstanding UNIX tradition the exports format used on FreeBSDis entirely different from that of Linux Each line in the file except for lines that startwith which are comments is composed of three components a list of directoriesto export the options to apply to those exports and the set of hosts to which theexport applies As on Linux a backslash denotes a line continuationvarwww roalldirs wwwadmincomThe line above exports varwww and all of its subdirectories readonly to all hostsmatching the pattern wwwadmincom To implement different mount options fordifferent clients simply repeat the line and specify different values For examplevarwww alldirsseckrbp network dballows readwrite access for all hosts in the named IPv network Kerberos is usedfor authentication integrity and privacyOn FreeBSD exports are per serverfilesystem Multiple exports to the same set ofclient hosts from the same filesystem must be named on the same line For examplevarwww varwww roalldirs wwwadmincomIt would be an error for www and www to be on separate lines with the same hostdesignations assuming that www and www reside within the same filesystemTo enable NFSv you must designate a root by prefixing a line with V for exampleV exports seckrbpkrbikrbsys network admincomOnly one effective V root path is allowed However it can be specified more thanonce with different options for different clients The root can appear anywhere inthe exports fileThe V line does not actually export any filesystems It simply chooses a base directory for NFSv clients to mount To activate it list an export within the rootexportswww network admincomDespite the V root designation the FreeBSD NFS server does not implement thepseudofilesystem as described by the RFC When a V root is designated and atleast one export is present under that root a V client can mount the root and access all of the files and directories within it regardless of their export status Thisinformation is not clear in the exports documentation and the ambiguity canbe quite dangerous Do not designate the servers own filesystem root as the Vroot otherwise the servers entire root filesystem will be available to clientsBecause of the V root V and V clients have a different path to mount than Vclients have For example given the following exportsexportswww network mask V exports network mask a V or V client in the network mounts exportswww but becauseof the pseudofilesystem designation on exports a V client must mount the export as www Alternatively a V client can mount and access the www directoryunder that mount pointUse network ranges for best performance when exporting to a large number ofclients For IPv you can use CIDR notation or a subnet mask For IPv you mustuse CIDR the mask option is not permitted For examplevarwww network mask varwww network varwww network dbFreeBSD has fewer export options than Linux affords Table summarizes themTable Common export options in FreeBSDOption Descriptionalldirs Allows mounts at any point in the filesystemro Exports readonly readwrite is the defaulto Synonym for ro exports readonlymaprootxxx The username or UID to map for access from a remote root usermapallxxx Maps all client users to the specified user like maprootsecflavor Specifies allowable security methods aa Specify multiple flavors in a commaseparated list in order of preference Possible values aresys UNIX authentication the default krb Kerberos authentication krbi Kerberos authentication and integrity krbp Kerberos authentication integrity and privacy and noneanonymous access not recommendednfsd serve filesOnce a clients mount request has been validated the client can request variousfilesystem operations These requests are handled on the server side by nfsd theNFS operations daemon nfsd does not need to run on an NFS client machineunless the client exports filesystems of its ownnfsd has no configuration file options are passed as commandline arguments Youstart and stop nfsd with your systems standard service mechanisms ie systemctlon Linux systems running systemd and the service command on FreeBSD Table shows which file and option to adjust in order to change the arguments passedto nfsdOn Linux systems run systemctl restart nfsconfigservice nfsserverserviceto enable nfsd configuration changes In FreeBSD use service nfsd restart andservice mountd restartThe N option to nfsd disables the specified version of NFS For example to disable versions and add N N to the appropriate file and option specified In reality nfsd simply makes a nonreturning system call to NFS server code embedded in the kernelin Table and restart the service This is a good idea if you are sure you dontneed to support older clientsnfsd takes a numeric argument that specifies how many server threads to fork Selecting the appropriate number of nfsds is important and is unfortunately somethingof a black art If the number is too low or too high NFS performance can sufferThe optimal number of nfsd threads depends on the operating system and thehardware in use If you notice that ps usually shows the nfsds in state D uninterruptible sleep and that some idle CPU is available consider increasing the numberof threads If you find the load average as reported by uptime rising as you addnfsds youve gone too far back off a bit from that thresholdRun nfsstat regularly to check for performance problems that might be associatedwith the number of nfsd threads See page for more details on nfsstatOn FreeBSD the minthreads and maxthreads options to nfsd enable automaticmanagement of the number of threads within the specified bounds On FreeBSD seeman rcconf and refer to the options prefixed with nfs for more NFS server settings Clientside NFSNFS filesystems are mounted in much the same way as local disk filesystems Themount command understands the notation hostnamedirectory to mean the pathdirectory on the host hostname As with local filesystems mount maps the remotedirectory on the remote host into a directory within the local file tree After themount completes you access an NFSmounted filesystem just like a local filesystem The mount command and its associated NFS extensions represent the mostsignificant concerns to the system administrator of an NFS clientBefore an NFS filesystem can be mounted it must be properly exported see page On an NFSv client you can verify that a server has properly exported itsfilesystems by running the showmount command showmount e monkExport list for monkhomeben harpatrustcomTable Where to set startup options for nfsdSystem Config file Option to setUbuntu etcdefaultnfskernelserver RPCNFSDOPTSaRed Hat etcsysconfignfs RPCNFSDARGSFreeBSD etcrcconf nfsserverflagsa Some versions of the nfskernelserver package incorrectly suggest that youedit RPCMOUNTDOPTS to set some nfsd options Do not be fooledThis example reports that the directory homeben on the server monk has beenexported to the client system harpatrustcomIf an NFS mount is not working first verify that the filesystems have been properly exported on the server Make sure that after updating the servers exports fileyou ran exportfs a Linux or service nfsd restart and service mountd reloadFreeBSD Next recheck the showmount outputIf the directory is properly exported on the server but showmount returns an erroror an empty list doublecheck that all the necessary processes are running on theserver portmap and nfsd add mountd statd and lockd for V Make sure thehostsallow and hostsdeny files allow access to those daemons and that you areon the right client systemThe path information displayed by showmount eg homeben above is validonly for NFS version and servers NFS version servers export a single unifiedpseudofilesystem and do not use the mount protocol The traditional NFS concept of separate mount points doesnt jibe with version s model so showmountsimply doesnt apply to the V worldUnfortunately NFSv has no good replacement for showmount On the serverthe command exportfs v shows the existing exports but of course this works onlylocally If you dont have direct access to the server you can try to mount the servers V root and traverse the directory structure manually You can also mount anysubdirectory of the exported root filesystemTo actually mount the filesystem in versions and youd use a command such as sudo mount o rwhardintrbg serverhomeben nfsbenTo accomplish the same under version on a Linux system youd type sudo mount o rwhardintrbg server nfsbenIn this case the options after o specify that the filesystem be mounted readwriterw that operations be interruptible intr and that retries be done in the background bg Table introduces the most common Linux mount optionsThe client side of NFS usually tries to autonegotiate a suitable version of the protocol You can specify a specific version by passing o nfsversnOn FreeBSD mount is a wrapper that calls sbinmountnfs for NFS mounts Thiswrapper sets NFS options and invokes the nmount system call To mount a version server on FreeBSD type sudo mount t nfs o nfsv server mntIf you dont specify a version explicitly mount negotiates one automatically in descending order In fact a simple mount server mnt does the trick in this case because mount can infer from the format that the filesystem youre referring to is NFSTable NFS mount flags and options for LinuxFlag Descriptionrw Mounts the filesystem readwrite must be exported that wayro Mounts the filesystem readonlybg If the mount fails server doesnt respond keeps trying it in thebackground and continues with other mount requestshard If a server goes down makes operations that access it block until theserver comes back upsoft If a server goes down makes operations that access it fail and returnan error to avoid processes hanging on inessential mountsintr Allows users to interrupt blocked operations they return an errornointr Does not allow user interruptsretransn Specifies the number of times to repeat a request before returningan error on a softmounted filesystemtimeon Sets the timeout period in tenths of a second for requestsrsizen Sets the read buffer size to n byteswsizen Sets the write buffer size to n bytessecflavor Specifies the security flavornfsversn Sets the NFS protocol versionprotoproto Selects a transport protocol must be tcp for NFS version Filesystems mounted hard the default cause processes to hang when their serversgo down This behavior is particularly bothersome when the processes in questionare standard daemons so we do not recommend serving critical system binariesover NFS In general the intr option reduces the number of NFSrelated headaches Automount solutions such as autofs discussed starting on page alsoprescribe some remedies for mounting ailmentsThe read and write buffer sizes are negotiated to the highest value supported by bothclient and server You can set them to any value between KiB and MiBYou can see the available space on an NFS mount with df just as you would on alocal filesystem df nfsbenFilesystem kblocks Used Available Use Mounted onleopardhomeben nfsbenumount works on NFS filesystems just like it does on local filesystems If the NFSfilesystem is in use when you try to unmount it you get an error such asumount nfsben device is busy Jeff Forys one of our technical reviewers remarked Most mounts should use hard intr and bg because these options best preserve NFSs original design goals soft is an abomination an ugly Satanichack If the user wants to interrupt cool Otherwise wait for the server and all will eventually be wellagain with no data lostUse fuser or lsof to find processes with open files on the filesystem Kill them orin the case of shells change directories If all else fails or your server is down tryrunning umount f to force the filesystem to be unmountedMounting remote filesystems at boot timeYou can use the mount command to establish temporary network mounts but youshould list mounts that are part of a systems permanent configuration in etcfstabso that they are mounted automatically at boot time Alternatively mounts can behandled by an automatic mounting service such as autofsThe following fstab entries mount the home filesystem from the server monk filesystem mountpoint fstype flags dump fsckmonkhome nfshome nfs rwbgintrhardnodevnosuid You can make your changes take effect immediately without rebooting by running mount a t nfsThe flags field of etcfstab specifies options for NFS mounts these options arethe same ones you would specify on the mount command lineRestricting exports to privileged portsNFS clients are free to use any TCP or UDP source port they like when connecting to an NFS server However some servers may insist that requests come from aprivileged port a port numbered lower than Others allow this behavior tobe set as an option The use of privileged ports delivers little actual securityNevertheless most NFS clients adopt the traditional and still recommended approach of defaulting to a privileged port to avert the potential for conflict UnderLinux you can accept mounts from unprivileged ports with the insecure exportoption Identity mapping for NFS version We introduced the general ideas behind NFSvs identity mapping system startingon page In this section we discuss the administrative aspects of the identitymapping daemonAll systems that participate in an NFSv network should have the same NFS domain In most cases its reasonable to use your DNS domain as the NFS domainFor example admincom is a straightforward choice of NFS domain for the serverulsahadmincom Clients in subdomains eg booksadmincom may or may notwant to use the same domain name eg admincom to facilitate NFS communicationUnfortunately for administrators NFSv UID mapping has no standard implementation so the details of administration differ slightly among systems TableSee page formore informationabout the fstab file names the mapping daemons on Linux and FreeBSD and notes the locationof their configuration filesTable NFSv identity mapping daemons and their configuration filesSystem Daemon Configuration file man pageLinux usrsbinrpcidmapd etcidmapdconf nfsidmapFreeBSD usrsbinnfsuserd nfsuserdflags in etcrcconf idmapOther than having their NFS domains set identity mapping services require littleassistance from administrators The daemons are started at boot time by the samescripts that manage other NFS daemons After making configuration changes youllneed to restart the identity mapper daemon Options such as verbose logging andalternative management of the nobody account are usually available nfsstat dump NFS statisticsnfsstat displays various statistics maintained by the NFS system nfsstat s showsserverside statistics and nfsstat c shows information for clientside operationsBy default nfsstat shows statistics for all protocol versions For example nfsstat cClient rpc calls badcalls retrans badxid timeout wait newcred timers Client nfs calls badcalls nclget nclsleep null getattr setattr readlink lookup root read write wrcache create remove rename link symlink mkdir readdir rmdir fsstat This example is from a relatively healthy NFS client If more than of RPC callstime out its likely that your NFS server or network has a problem You can usuallydiscover the cause by checking the badxid field If badxid is near with timeoutsgreater than packets to and from the server are getting lost on the networkYou might be able to solve this problem by lowering the rsize and wsize mountparameters read and write block sizesIf badxid is nearly as high as timeout then the server is responding but too slowlyEither replace the server or increase the timeo mount parameterRunning nfsstat and netstat occasionally and becoming familiar with their outputhelps you discover NFS problems before your users do We suggest including thisdata as part of your sites monitoring and alerting system Dedicated NFS file serversFast reliable file service is an essential element of a production computing environment Although you can certainly roll your own file servers from workstations andofftheshelf hard disks doing so is often not the bestperforming or easiesttoadminister solution though it is usually the cheapestDedicated NFS file server products have been on the market for many years Theyoffer a host of potential advantages over the homebrew approach They are optimized for file service and typically deliver the best possibleNFS performance As storage requirements grow they can scale smoothly to support terabytes of storage and hundreds of users They are more reliable than standalone boxes thanks to their simplifiedsoftware redundant hardware and use of disk mirroring They usually handle file service for both UNIX and Windows clients Mosteven contain integrated HTTPS FTP and SFTP servers They often include backup and checkpoint facilities that are superior tothose found on vanilla UNIX systemsSome of our favorite dedicated NFS servers are made by NetApp Their products runthe gamut from very small to very large and their pricing is OK EMC is anotherplayer in the highend server market They make good products but be preparedfor sticker shock and build up your tolerance for marketing buzzwordsIn an AWS environment the Elastic File System service is a scalable NFSv serverasaservice that exports filesystems to EC instances Each filesystem can support multiple GiBs throughput depending upon the size of the filesystem Seeawsamazoncomefs for more information Automatic mountingMounting filesystems at boot time by listing them in etcfstab can cause administrative headaches on large networks First its tedious to maintain the fstab fileon hundreds of machines even with help from scripts and configuration management systems Each host may have slightly different needs and so require individual attention Second if shared filesystems are mounted from many different hostsclients become dependent on many different downstream servers Chaos ensueswhen one of those servers crashes Every command that accesses that serversmount points will hangYou can moderate these problems with an automounter a type of daemon thatmounts filesystems when they are referenced and unmounts them when they areno longer being used In addition to deferring mounts until they are actually needed most automounters can also accept a list of replicas identical backup copiesfor a filesystem These backups let the network continue to function even when aprimary server becomes unavailableAs described by Edward Tomasz Napieraa author of the FreeBSD automounterthis magic requires the cooperation of several related pieces of software autofs a kernelresident filesystem driver that watches a filesystem formount requests pauses the calling program and invokes the automounter to mount the target filesystem before returning control to the caller automountd and autounmountd which read the administrative configuration and actually mount or unmount filesystems automount an administrative utilityFor the most part automounters are transparent to users Instead of mirroring anactual filesystem the automounter makes up a virtual filesystem hierarchy according to the specifications given in its configuration files When a user references adirectory within the automounters virtual filesystem the automountd interceptsthe reference and mounts the actual filesystem the user is trying to reach The NFSfilesystem is mounted beneath the autofs filesystem in normal UNIX fashionThe idea of an automounter originally comes from Sun The Linux version functionally mimics that of Sun although it is in fact an independent implementationFreeBSD maintains yet another implementation having sacrificed a once widelyused automounter amd in the FreeBSD releaseThe various automount implementations understand three different kinds of configuration files referred to as maps direct maps indirect maps and master mapsDirect and indirect maps contain information about the filesystems to be automounted A master map lists the direct and indirect maps that automount shouldpay attention to Only one master map can be active at once the default mastermap is kept in etcautomaster on FreeBSD and in etcautomaster on LinuxOn most systems automount is a standalone command that reads its configuration files sets up any necessary autofs mounts and exits Actual references to automounted filesystems are handled through autofs by a separate daemon processautomountd This daemon does its work silently and does not need additionalconfigurationOn Linux systems the daemon is called automount instead of automountd andthe setup function is performed by a system startup script systemd for modern A direct map can also be managed as an NIS database or in an LDAP directory but doing so is trickydistributions Linux details are given on page In the following discussionwe refer to the setup command as automount and the daemon as automountdIf you change the master map or one of the direct maps that it references you mustrerun automount to pick up the changes With the v option automount showsyou the adjustments its making to its configuration You can add L to achieve adry run effect that lets you examine your configuration and debug problemsautomount autounmountd on FreeBSD accepts a t argument that tells howlong in seconds an automounted filesystem can remain unused before beingunmounted The default is seconds minutes Since an NFS mount whoseserver has crashed can cause programs that touch it to hang its good hygiene toclean up automounts that are no longer in use dont raise the timeout too muchIndirect mapsIndirect maps automount several filesystems under a common directory However the path of the directory is specified in the master map not in the indirect mapitself For example an indirect map might look like thisusers harpharpusersdevel soft harpharpdevelinfo ro harpharpinfoThe first column names the subdirectory in which each automount should be installed and subsequent items list the mount options and the NFS path of the filesystem This example perhaps stored in etcautoharp tells automount that it canmount the directories harpusers harpdevel and harpinfo from the serverharp with info being mounted readonly and devel being mounted softIn this configuration the paths on harp and the local host are the same Howeverthis correspondence is not requiredDirect mapsDirect maps list filesystems that do not share a common prefix such as usrsrcand cstools A direct map eg etcautodirect that described both of thesefilesystems to automount might look something like thisusrsrc harpusrsrccstools ro monkcstoolsBecause they do not share a common parent directory these automounts musteach be implemented with a separate autofs mount This configuration requiresmore overhead but it has the added advantage that the mount point and directorystructure are always accessible to commands such as ls Running ls on a directoryfull of indirect mounts can be confusing to users because automount doesnt show The other side of this issue is the time required to mount a filesystem System response is faster andsmoother if filesystems arent being continually remountedthe subdirectories until their contents have been accessed ls doesnt look insidethe automounted directories so it does not cause them to be mountedMaster mapsA master map lists the direct and indirect maps that automount should pay attention to For each indirect map it also specifies the root directory to be used by themounts defined in the mapA master map that referenced the direct and indirect maps shown in the previousexamples would look something like this Directory Mapharp etcautoharp prototcp etcautodirectThe first column is a local directory name for an indirect map or the special token for a direct map The second column identifies the file in which the map is storedYou can have several maps of each type When you specify mount options at theend of a line they set the defaults for all mounts within the map Linux administrators should always specify the fstypenfs mount flag for NFS version serversOn most systems the default options set on a master map entry do not blend withthe options specified in the direct or indirect map to which it points If a map entry has its own list of options the defaults are ignored Linux merges the two setshowever If the same option is specified in both places the map entrys value overrides the defaultExecutable mapsIf a map file is executable its assumed to be a script or program that dynamicallygenerates automounting information Instead of reading the map as a text file theautomounter executes it with an argument the key that indicates which subdirectory a user has attempted to access The script prints an appropriate map entryif the specified key is not valid the script can simply exit without printing anythingThis powerful feature makes up for many of the deficiencies in automounters ratherstrange configuration system In effect you can easily define a sitewide automountconfiguration file in a format of your own choice You can write a simple scriptto decode the global configuration on each machine Some systems come with ahandy etcautonet executable map that takes a hostname as a key and mounts allexported filesystems on that hostSince automount scripts run dynamically as needed its unnecessary to distributethe master configuration file after every change or to convert it preemptively to theautomounter format in fact the global configuration file can have a permanenthome on an NFS serverAutomount visibilityWhen you list the contents of an automounted filesystems parent directory thedirectory appears empty no matter how many filesystems have been automountedthere You cannot browse the automounts in a GUI filesystem browserAn example ls portal ls portalphotosartclass florissant rmnpblizzard frozendeadguyOct rmnpboston greenville steamboatThe photos filesystem is alive and well and is automounted under portal Its accessible through its full pathname However a review of the portal directory doesnot reveal its existence If you had mounted this filesystem through the fstab file ora manual mount command it would behave like any other directory and would bevisible as a member of the parent directoryOne way around the browsing problem is to create a shadow directory that contains symbolic links to automount points For example if automountsphotos isa link to portalphotos you can ls the contents of automounts to discover thatphotos is an automounted directory References to automountsphotos are stillrouted through the automounter and work correctlyUnfortunately these symbolic links require maintenance and can go out of syncwith the actual automounts unless they are periodically reconstructed by a scriptReplicated filesystems and automountIn some cases a readonly filesystem such as usrshare might be identical on several different servers In this case you can tell automount about several potentialsources for the filesystem It then chooses a server according to its own idea ofwhich servers are closest given network routes NFS protocol versions and response times to an initial queryAlthough automount itself does not see or care how the filesystems it mounts areused replicated mounts should represent readonly filesystems such as usrshareor usrlocalX Theres no way for automount to synchronize writes across a setof servers so replicated readwrite filesystems are of little practical useYou can assign explicit priorities to determine which replica to select first The priorities are small integers with larger numbers indicating lower priority The defaultpriority is most eligibleAn autodirect file that defines usrman and cstools as replicated filesystemsmight look like thisusrman ro harpusrshareman monkusrmancstools ro leopardmonkcstoolsNote that server names can be listed together if the source path on each is the sameThe after monk in the first line sets that servers priority with respect to usrmanThe lack of a priority after harp indicates an implicit priority of Automatic automounts V all but LinuxInstead of listing every possible mount in a direct or indirect map you can tellautomount a little about your filesystem naming conventions and let it figure thingsout for itself The key piece of glue that makes this work is that the mountd runningon a remote server can be queried to find out what filesystems the server exports InNFS version the export is always which eliminates the need for this automationAutomatic automounts can be configured in several ways the simplest of whichis the hosts mount type on FreeBSD If you list hosts as a map name in yourmaster map file automount then maps remote hosts exports into the specifiedautomount directorynet hosts nosuidsoftFor example if harp exported usrshareman that directory could then be reachedthrough the automounter at the path netharpusrsharemanThe implementation of hosts does not enumerate all possible hosts from whichfilesystems can be mounted that would be impossible Instead it waits for individual subdirectory names to be referenced then runs off and mounts the exportedfilesystems from the requested hostA similar but finergrained effect can be achieved with the and wild cards inan indirect map file Also a number of macros available for use in maps expand tothe current hostname architecture type and so on See the automountM manpage for detailsSpecifics for LinuxThe Linux implementation of automount has diverged a bit from the original Sunstandards The changes mostly relate to the naming of commands and filesFirst automount is the daemon that actually mounts and unmounts remote filesystems It fills the same niche as the automountd daemon on other systems and generally does not need to be run by handThe default master map file is etcautomaster Its format and the format of indirectmaps are as described previously The documentation can be hard to find howeverThe master map format is described in automaster and the indirect map formatin autofs be careful or youll get autofs which documents the syntax of theautofs command As one of the man pages says The documentation leaves a lotto be desired To cause changes to the master map to take effect run the commandetcinitdautofs reload which is equivalent to automount in SunlandThe Linux implementation does not support the Solarisstyle hosts clause forautomatic automounts Recommended readingTable lists the various RFCs for the NFS protocolTable NFSrelated RFCsRFC Title Author Date Network File System Protocol Specification Sun Microsystems Mar NFS Version Protocol Specification B Callaghan et al Jun NFS Version and Version Security Issues M Eisler Jun NFS Version Design Considerations S Shepler Jun NFS Version Protocol S Shepler et al April NFS Version Minor Version Protocol S Shepler et al Jan NFS Version Minor Version Protocol T Haynes Nov Chapter The Network File System covers the most popular system for sharingfiles among UNIX and Linux systems However UNIX systems also need to sharefiles with systems such as Windows that dont natively support NFS Enter SMBIn the early s Barry Feigenbaum created the BAF protocol to afford sharednetwork access to files and resources Before release the name was changed fromthe authors initials to Server Message Block SMB The protocol was rapidly embraced by Microsoft and the PC community because it gave just like local accessto files on remote systemsIn a version called the Common Internet File System CIFS was released byMicrosoft mostly as a marketing exercise CIFS introduced oftenbuggy changesto the original SMB protocol As a result Microsoft released SMB in andthen SMB in Although its common within the industry to refer to SMBfileshares as CIFS the truth is that CIFS was deprecated long ago only SMB lives onIf youre working in a homogeneous UNIX and Linux environment then this chapter probably isnt for you But if you need a way to share files between UNIX andWindows systems read on Sun Microsystems had also entered the fray in with its WebNFS offering and Microsoft saw anopportunity to market SMB with a more userfriendly implementation and name SMB Samba SMB server for UNIXSamba is a popular software package available under the GNU Public License thatimplements the server side of the SMB protocol on UNIX and Linux hosts It wasoriginally created by Andrew Tridgell who first reverseengineered the SMB protocol and published the resulting code in Here we focus on Samba version Samba is well supported and under active development to expand its functionalityIt offers a stable industrial strength way to share files between UNIX and Windowssystems The real beauty of Samba is that you install only one package on the serverside no special software is needed on the Windows sideIn the Windows world a filesystem or directory made available over the networkis known as a share It sounds a bit strange to UNIX ears but we follow this convention when referring to SMB filesystemsAlthough we explore only file sharing in this chapter Samba can also implement avariety of other crossplatform services including Authentication and authorization Network printing Name resolution Service announcement file server and printer browsingSamba can also perform the basic functions of a Windows Active Directory controller This configuration involves a certain amount of hubris though we suspectthat being an AD controller is probably a job best left to Windows serversThere is certainly value in getting your UNIX and Linux systems added to an ADdomain as clients however This arrangement lets you share identity and authentication information sitewide See Chapter Single SignOn for more informationLikewise we dont recommend Samba as a print server CUPS is probably yourbest bet there See Chapter for more information about printing in UNIXand Linux with CUPSMost of Sambas functionality is implemented by two daemons smbd and nmbdsmbd implements file and print services as well as authentication and authorization nmbd is responsible for the other major SMB components name resolutionand service announcementUnlike NFS which requires kernellevel support Samba requires no drivers or kernel modifications and runs entirely as a user process It binds to the sockets used forSMB requests and waits for a client to request access to a resource Once a requesthas been authenticated smbd forks an instance of itself that runs as the user whois making the requests As a result all normal file access permissions includinggroup permissions are obeyed The only special functionality that smbd adds ontop of this is a file locking service that gives Windows systems the locking semantics to which they are accustomedIf youre left wondering why youd use SMB over say a more UNIXintegrated remotefilesystem such as NFS the answer is ubiquity Almost all OSs support SMB at somelevel Table summarizes some of the main differences between SMB and NFSTable SMB vs NFSSMB NFSUserspace servers and processes Kernel server with threadsPeruser server processes Same server one process for all clientsUses underlying OS for access control Has its own access control systemMounters usually individual users Mounters usually systemsPretty good performance Best performanceChapter explores NFS in more detail Installing and configuring SambaSamba is available for all our example systems Most Linux distributions include itby default Patches documentation and other goodies are available from sambaorgMake sure you are using the most current Samba packages available for your systemsince many updates fix security vulnerabilitiesIf Samba is not already installed on your system you can install it on FreeBSD withpkg install samba On Linux systems grab the sambacommon package throughyour package manager of choiceYou configure Samba in the etcsambasmbconf file usrlocaletcsmbconf onFreeBSD The file specifies the directories to share their access rights and Sambasgeneral operational parameters Linux packages are kind enough to supply a heavily commented sample configuration thats a good starting point for new setupsSamba comes with sensible defaults for its configuration options and most sitesneed only a small configuration file Run the command testparm v for a listing ofall the Samba configuration options and the values to which they are currently setThis listing includes your settings from the smbconf or smbconf file as well as anydefault values you have not overridden Note that once Samba is running it checksits configuration file every few seconds and loads any changesno restart requiredThe most common use of Samba is to share files with Windows clients Access tothese shares must be authenticated through a user account by one of two optionsThe first option uses local accounts for which users specify a password that is managed separately from their other accounts such as their domain login The secondoption integrates Active Directory authentication and so piggybacks on the usersdomain login credentialsFile sharing with local authenticationThe simplest way to authenticate users who want to access Samba shares is by creating a local account for them on the UNIX or Linux serverBecause Windows passwords work quite differently from UNIX passwords Sambacannot control access to SMB shares by means of users existing account passwordsHence to use local accounts you must store and maintain a separate SMB password hash for every userSometimes however simplicity outweighs user convenience and this authentication system is really simple Heres the start of an example smbconf file that uses itglobalworkgroup ulsahsecurity usernetbios name freebsdbookThe security user parameter tells Samba to use local UNIX accounts Be sure theworkgroup name is set to fit your environment This is typically the Active Directorydomain if youre in a Windows environment If youre not you can omit this settingSamba has its own command smbpasswd for setting up Windowsstyle passwordhashes For example here we add the user tobi and set a password for him sudo smbpasswd a tobiNew SMB password passwordRetype new SMB password passwordThe UNIX account should already exist before you attempt to set its Samba passwordUsers can change their Samba password by running smbpasswd without any options smbpasswdNew SMB password passwordRetype new SMB password passwordThis example changes the Samba password of the current user on the Samba serverUnfortunately Windowsonly users must log in to a shell prompt on the server tochange their share password The ability to log in remotely must be set up separatelymost likely through SSHFile sharing with accounts authenticated by Active DirectoryAs simple as the basic process is maintaining a separate authentication databasefor shares with smbpasswd does seem archaic in todays hyperintegrated worldIn most cases youll want users to authenticate through some form of centralizedauthority such as Active Directory or LDAPRecent years have brought great advances to UNIX and Linux in this area Chapter Single SignOn covers the necessary components including directory servicessssd the nsswitchconf file and PAM Once you have deployed those componentsconfiguring Samba to take advantage of them is easyHeres an example of the start of an smbconf file for an environment in which Active Directory performs user authentication via sssdglobalworkgroup ulsahrealm ulsahexamplecomsecurity adsdedicated keytab file FILEetcsambasambakeytabkerberos method dedicated keytabIn this case the realm parameter should be the same as the local Active Directorydomain name The dedicated keytab file and kerberos method parameters enableSamba to work properly with Active Directorys Kerberos implementationConfiguring sharesAfter youve configured Sambas general settings and authentication you can specifyin the smbconf file which directories should be shared through SMB Each sharethat you expose needs its own stanza in the configuration file The name of thestanza becomes the share name that is advertised to SMB clientsHeres an examplebookshare path storagebookshare read only noHere SMB clients see a mountable share named sambaserverbookshare It yieldsaccess to the file tree located at storagebookshare on the serverSharing home directoriesYou can automatically convert users home directories into distinct SMB shares withthe magic stanza name homes in the smbconf filehomescomment Home Directoriesbrowseable novalid users Sread only noFor example this configuration would allow user janderson to access her homedirectory through the path sambaserverjanderson from any Windows systemon the network Historically winbind was used integrate Active Directory with Samba These days sssd is the preferred method This keytab file is created by sssd if youve set it up according to the instructions in Chapter Formore information about keytabs in Samba see googlZxCUKA deep link within wikisambaorgAt some sites the default permissions on home directories let users browse oneanothers files Because Samba relies on UNIX file permissions to implement access control Windows users coming in through Samba can then read one anothers home directories too However experience shows that this behavior tends toconfuse Windows users and make them feel exposedThe variable S listed as the value of valid users in the example above expandsto the username associated with each share it thus restricts access to the owner ofthe home directory Omit this line if that is not the behavior you wantSamba uses its magic homes section as a last resort If a particular users homedirectory has an explicitly defined share in the configuration file the parametersset there override the values set through homesSharing project directoriesSamba can map Windows access control lists ACLs to either traditional UNIX filepermissions or ACLs if the underlying filesystem supports them But in practicewe find that ACLs are too complex for most users to deal withInstead of using ACLs we normally set up a special share for each user group thatneeds a collective work area When a user attempts to mount this share Sambachecks that the applicant is in the appropriate UNIX group before allowing accessIn the example below a user must be a part of the eng group to mount the shareand access filesengcomment Group Share for engineering Everybody who is in the eng group may access this share People will have to log in with their Samba accountvalid users engpath homeeng Disable NT ACLs since we do not use them herent acl support no Make sure that all files have sensible permissions and that dirs have the setgid inherit group bit setcreate mask directory mask force directory mode force group eng Normal share parametersbrowseable noread only noguest ok noThis configuration does not require you to create a pseudouser to act as the ownerof the shared directory You just need a UNIX group here eng that includes theintended users of the shareSee page formore informationabout ACLsUsers mount the share under their own accounts but to facilitate collaboration wewould prefer that any files created within the share be owned by group eng Thatway other team members can access newly created files by defaultThe first step toward ensuring this behavior is to use the force group option tocoerce mounters effective group IDs to eng the UNIX group that controls accessto the share However this step alone is not enough to ensure that new files anddirectories are assigned a group owner of engAs explained on page the setgid option on a directory makes new files created within that directory inherit the directorys group owner We can ensure thatnew files are owned by eng by setting the group of the shares root to eng and thenturning on the setgid bit on that directory sudo chown rooteng homeeng sudo chmod urwxgrwxso homeengThese measures are sufficient to manage files created in the root of the share However to make the system work for complex hierarchies of files we also need toensure that newly created directories setgid bits are also turned on The exampleconfiguration above implements this requirement with the force directory modeand directory mask options Mounting SMB file sharesMounting for SMB file shares works quite differently from how its done for othernetwork filesystems In particular SMB volumes are mounted by a specific userrather than being mounted by the system itselfYou need local permission to perform an SMB mount You also need the passwordfor an identity that the remote SMB server will allow to mount the share A typicalcommand line on Linux is sudo mount t cifs o usernamejoe redmondjoes homejoemntAnd the equivalent on FreeBSD sudo mount t smbfs joeredmondjoes homejoemntWindows conceptualizes network mounts as being established by a particular userhence the usernamejoe option above whereas UNIX regards them as moretypically belonging to the system as a whole Windows servers generally cannotdeal with the concept that several different people might be accessing a mountedWindows shareFrom the perspective of the UNIX client all files in the mounted directory appearto belong to the user who mounted it If you mount the share as root then all files Or at least it does so on Linux FreeBSD does not honor the setgid bit on a directory however its default behavior is to inherit the group just as Linux does with the setgid bit turned on Setting the setgid bit on FreeBSD does no harm howeverbelong to root and gardenvariety users might not be able to write files on theWindows serverThe mount options uid gid fmask and dmask let you tweak these settings so thatownership and permission bits are more in tune with the intended access policy forthat share Check the mountcifs Linux or mountsmbfs FreeBSD man pagefor more information about these options Browsing SMB file sharesSamba includes a commandline utility called smbclient that lets you list file shareswithout actually mounting them It also defines an FTPlike interface for interactive access This feature can be useful when you are debugging or when a scriptneeds access to a shareFor example heres how to list shares available to user dan on the server hoarder smbclient L hoarder U danEnter dans password passwordDomainWORKGROUP OSUnix ServerSamba Sharename Type Comment Temp Disk Temp Storage Programs Disk Various Programs and Applications Docs Disk Shared Documents Backups Disk Backups of all sortsTo connect to a share and transfer files omit the L flag and include the share name smbclient hoarderDocs U danOnce youre connected type help for a list of available commands Ensuring Samba securityIts important to be aware of the security implications of sharing files and otherresources over a network For a typical site you need to do two things to ensure abasic level of security Explicitly specify which clients can access the resources shared by SambaThis part of the configuration is controlled by the hosts allow clause inthe smbconf file Make sure that it contains only the IP addresses addressranges or hostnames that it shouldYou can include a hosts deny clause in the smbconf file as well but notethat denials have priority If you include a hostname or address in boththe hosts deny clause and the hosts allow clause that host will not beable to access the resource Block access to the server from outside your organization Samba usesencryption only for password authentication It does not use encryptionfor its data transport In almost all cases you should block access fromoutside your organization to prevent users from accidentally downloading files in plain text across the InternetBlocking is typically implemented at the network firewall level Sambauses UDP ports and TCP ports and Since the release of Samba version excellent security documentation has beenavailable on Sambas wiki wikisambaorg Debugging SambaSamba usually runs without requiring much attention If you do experience a problem you can consult two primary sources of debugging information the smbstatuscommand and Sambas logging facilitiesQuerying Sambas state with smbstatussmbstatus shows currently active connections and locked files its the first placeto look when issues arise This information is especially useful for tracking downlocking problems eg Which user has file xyz open readwrite exclusive sudo smbstatus Some output condensed for claritySamba version UbuntuPID Username Group Machine clay atrust dan atrust Service pid machine Connected atadmin Wed Apr swdepot Wed Apr clients Wed Apr clients Fri Apr Locked filesPid Uid DenyMode RW Oplock SharePath Name DENYNONE RDONLY NONE atrustadmin New Hire Proces DENYALL RDONLY NONE homeclay DENYNONE RDONLY NONE atrustclients AcmeSupplyConThe first section of output lists the users that have connected The Service columnin the next section shows the actual shares theyve mounted The last section fromwhich weve removed a couple of columns to save space lists any active file locksIf you kill the smbd associated with a certain user all that users locks disappearSome applications handle this situation gracefully and reacquire any locks theyneed Others freeze and die a horrible death with much clicking required on theWindows side just to close the unhappy application As dramatic as this may soundwe have yet to see any file corruption resulting from such a procedureBe careful when Windows claims that files have been locked by another application it is often right Fix the problem on the client side by closing the offendingapplication instead of bruteforcing the locks on the serverConfiguring Samba loggingConfigure logging parameters in your smbconf fileglobal The m causes a separate file to be written for each clientlog file varlogsambalogmmax log size If you want Samba to log only through syslog then set the following parameter to yessyslog only no We want Samba to log a minimal amount of information to syslog Everything should go to varlogsambalogsmbdnmbd instead If you want to log through syslog increase the following parametersyslog Higher log levels produce more information Logging uses system resources sodont ask for too much detail unless you are actively debuggingThe following example shows log entries generated by an unsuccessful connectionattempt pid effective real classauth sourceauthauthcauthcheckntlmpassword checkntlmpassword Authentication for user dan dan FAILEDwith error NTSTATUSWRONGPASSWORD sourcesmbderrorcerrorpacketset NT error packet at sourcesmbdsesssetupc cmdSMBsesssetupX NTSTATUSLOGONFAILUREA successful attempt looks like this pid effective real classauth sourceauthauthcauthcheckntlmpassword checkntlmpassword PAM Account for user dan succeeded pid effective real classauth sourceauthauthcauthcheckntlmpassword checkntlmpassword authentication for user dan dan dansucceededThe smbcontrol command is handy for altering the debug level of a running Sambaserver without altering the smbconf file For example sudo smbcontrol smbd debug authThis command sets the global debug level to and sets the debug level for authenticationrelated matters to The smbd argument specifies that all smbd daemonson the system should have their debug levels set To debug a specific establishedconnection use the smbstatus command to determine which smbd daemon handles the connection then pass its PID to smbcontrol to debug just that one connection At log levels over youll start to see encrypted passwords in the logsManaging character setsStarting with version Samba encodes all filenames in UTF If your serverruns with a UTF locale which we recommend this a great match If you are inEurope and are still using one of the ISO locales on the server you may findthat Sambacreated filenames that include accented characters eg or do not display correctly when you run ls The solution is to tell Samba to use thesame encoding as your serverunix charset ISOdisplay charset ISOMake sure the filename encoding is correct right from the start Otherwise fileswith improperly encoded names accumulate Fixing them later is a surprisinglycomplex task Recommended readingRed Hat Red Hat Enterprise Linux System Administrators Guide File and PrintServers googlLPjNXa deep link into accessredhatcomdocumentationSamba Project Samba Wiki Page wikisambaorg This wiki is updated relatively frequently and is an authoritative source of information though portions aresomewhat disorganized Type echo LANG to see if your system is running in UTF modeSECTION FOUROPERATIONSA longstanding tenet of system administration is that changes should be structuredautomated and applied consistently among machines But thats easier said thandone when youre confronted with a heterogeneous fleet of systems and networksin various states of healthConfiguration management software automates the management of operating systems on a network Administrators write specifications that describe how serversshould be configured and the configuration management software then brings reality into conformance with the specifications Several open source implementationsof configuration management are in widespread use In this chapter we introducethe basics of configuration management and describe the major playersAs an automation tool configuration management is closely affiliated with theDevOps philosophy of IT operations which we describe in more detail startingon page People sometimes conflate DevOps and configuration managementso you may occasionally hear these terms used interchangeably Nevertheless theyare distinct In this chapter we show several examples that demonstrate how configuration management enablesbut not is not identical toseveral key elementsof DevOps Configuration ManagementConfiguration management system is a bit of a handful to read and write so weoften shorten that term to CM system or even simply CM Unfortunately theabbreviation CMS is already in widespread use for content management system Configuration management in a nutshellThe traditional approach to sysadmin automation is an intricate complex of homegrown shell scripts supplemented by ad hoc fire fighting when scripts fail Thisscheme works about as well as you might expect Over time systems managed inthis way usually degenerate into a chaotic wreckage of package versions and configurations that cant be reliably reproduced Its sometimes called the snowflakemodel of system administration because no two systems are ever alikeConfiguration management is a better approach It captures desired state in the formof code Changes and updates can then be tracked over time in a version controlsystem which creates an audit trail and a point of reference The code also acts asinformal documentation of a network Any administrator or developer can readthe code to determine how the system is configuredWhen all a sites servers are under configuration management the CM system effectively acts as both an inventory database and a commandandcontrol center forthe network CM systems also offer orchestration features which let you applychanges and run ad hoc commands remotely You can target groups of hosts whosehostnames match particular patterns or whose configuration variables match a givenset of values Managed clients report information about themselves to the centraldatabase for analysis and monitoringMost configuration management code uses a declarativeas opposed to proceduralidiom Rather than writing scripts that tell the system what changes to makeyou describe the state you want to achieve The configuration management systemthen uses its own logic to adjust target systems as necessaryUltimately the job of a CM system is to apply a series of configuration specificationsaka operations to an individual machine Operations vary in granularity but theyare typically coarse enough to correspond to items that might plausibly appear ona sysadmins todo list create a user account install a software package and so onA subsystem such as a database might require anywhere between and operations to fully configure Full configuration of a freshly booted system might entaildozens or hundreds of operations Dangers of configuration managementConfiguration management is a major improvement over the ad hoc approach butit is not a magic wand A few sharp edges are particularly important for administrators to be aware of in advanceSee Chapter formore informationabout shell scriptingAlthough all major CM systems use similar conceptual models they describe thesemodels with different lexicons Unfortunately the terminology used by a particular CM system often has more to do with conforming to a marketing theme thanwith maximizing clarityThe result is a general lack of conformity and standardization among systems Mostadministrators will encounter several CM systems throughout the course of theircareers and will develop preferences derived from that experience Unfortunatelyknowledge of one system is not directly portable to anotherAs a site grows so too must the infrastructure needed to support its configurationmanagement system A site with a few thousand servers needs a handful of systemsdedicated to running the CM workloads This overhead imposes both direct andindirect costs in the form of hardware resources and ongoing maintenance CMsystem upgrades can be major projects in their own rightA certain level of operational maturity and rigor is necessary for a site to fully embrace configuration management Once a host is under the control of a CM system it must not be modified manually or it immediately reverts to the status of asnowflake systemAlthough some CM systems are easier to pick up than others they are all notoriousfor having a steep learning curve especially for administrators who lack prior experience with automation If you match this description consider practicing on a labof virtual machines to hone your skills before you tackle your production network Elements of configuration managementIn this section we review the components of a CM system and the concepts used toconfigure it at a greater level of detail Then starting on page we survey fourof the most commonly used CM systems Ansible Salt Puppet and ChefRather than adopt any particular CM systems terminology we use the clearest andmost directly descriptive term we can find for each concept Table on page maps the correspondences between our vocabulary and those of the four CMsystems listed above If youre already familiar with one of those CM systems youmight find it helpful to refer to this table as you read the material belowOperations and parametersWeve already introduced the concept of operations which are the smallscale actionsand checks used by a CM system to achieve a particular state Every CM systemincludes a large set of supported operations and more arrive with each new release On more than one occasion we have seen cases in which hurried or lazy administrators have manually updated a configurationmanaged host and set their changes to be immutable thus overridingthe expected state and preventing the CM system from applying future changes This kind of hack results in great confusion when an admins colleagues cannot quickly determine why the expected configuration is not being applied In one case it caused a major service outageHere are some sample operations that all CM systems can handle right out of the box Create or remove a user account or set its attributes Copy files to or from the system being configured Synchronize directory contents Render a configuration file template Add a new line in a configuration file Restart a service Add a cron job or systemd timer Run an arbitrary shell command Create a new cloud server instance Create or remove a database account Set database operating parameters Perform Git operationsThis is just a sampling most CM systems define hundreds of operations includingmany that perform potentially complex niche operations such as setting up specificdatabases runtime environments or even pieces of hardwareIf operations seem suspiciously similar to shell commands your intuition is accurate They are scripts usually written in the implementation language of the CMsystem itself and exploiting the systems standard tools and libraries In many casesthey run standard shell commands under the hood as part of their implementationJust as UNIX commands accept arguments most operations accept parameters Forexample a package management operation would accept parameters that specifythe package name version and whether the package is to be installed or removedParameters vary according to the operation As a convenience they usually havedefault values that are suitable for the most common use casesCM systems let you use variable values see the next section to define parametersThey can also infer parameter values according to the environment of the systemsuch as the network it lives on whether a particular configuration property is present or whether the systems hostname matches a given regular expressionA wellbehaved operation knows nothing about the host or hosts to which it mighteventually be applied The implementation is written to be relatively generic andOSagnostic Binding operations to specific systems occurs at a higher level of theconfiguration management hierarchyDespite CM systems focus on declarative configuration operations must ultimatelyrun like any other command Execution has a start and an end It can succeed orfail It reports its status back to the calling environmentHowever operations differ from typical UNIX commands in a few important ways Most operations are designed to be applied repeatedly without causingproblems Borrowing a term from linear algebra youll sometimes seethis latter property referred to as idempotence Operations know when they change the systems actual state Operations know when the system state needs to be changed If the current configuration already conforms to the specification the operationexits without doing anything Operations report their results to the CM system Their report data isricher than a UNIXstyle exit code and can aid in debugging Operations strive to be crossplatform They usually define a constrainedset of functions that are common to all supported platforms and they interpret requests in accordance with the local systemSome operations cant be made idempotent without a little help from a sysadmin whoknows more about the context For example if an operation runs a gardenvarietyUNIX command the CM system has no direct way of knowing what effect thatcommand had on the systemYou also have the option of writing your own custom operations Theyre just scriptsand the CM system typically provides a wellgreased path for integrating your custom operations with the standard onesVariablesVariables are named values that influence how configurations are applied to individual machines They commonly set parameter values and fill in the blanks inconfiguration templatesVariable management in CM systems is often quite rich A few points of note Variables can typically be defined in many different places and contextswithin the configuration base Each definition has a scope in which its visible Scope types vary by CMsystem and might encompass a single machine a group of machines ora particular set of operations Multiple scopes can be active in any given context Scopes can be nestedbut more commonly they are simply coactive Because multiple scopes can define values for the same variable someform of conflict resolution is necessary Some systems merge values butmost use precedence rules or definition order to pick a winning valueVariables are not limited to having scalar values arrays and hashes are also acceptable variable values in all CM systems Some operations accept nonscalar parametervalues directly but such values are more typically used above the level of individualoperations For example an array might be enumerated in a loop to apply the sameoperation more than once with different parametersFactsCM systems investigate each configuration client to determine descriptive factssuch as the IP address of the primary network interface and the OS type This information is then accessible from within the configuration base through variablevalues As with any other variable these values can be used to define parametervalues or to expand templatesIt can take awhile to determine all the facts associated with a particular systemTherefore CM systems generally cache facts and they do not necessarily rebuildthe cache on every run If you find that a particular configuration flow is encountering stale configuration data you might need to explicitly invalidate the cacheAll CM systems let target machines add their own values to the fact database eitherby including a static file of declarations or by running custom code on the targetmachine This feature is useful both for extending the types of information that canbe accessed through the facts database and for moving static configuration information onto client machinesClientside hints can be particularly useful for managing cloud and virtual serversYou simply apply cloudlevel markers such as EC tags as an instance is createdand the configuration management system can then flesh out the appropriate configuration by checking the markers Keep in mind the security implications of thisapproach however the client controls the facts that it reports so make sure thata compromised client cant exploit the configuration management system to gainadditional privilegesDepending on the CM system you may be able to transcend your local contextwhen sniffing around in the variable or fact space In addition to accessing the configuration information for the current host you may also be able to access data forother hosts or even to introspect the state of the configuration base itself This isa useful feature for coordinating a distributed system such as a cluster of serversChange handlersIf you change a web servers configuration file you had better restart the web serverThats the basic concept behind handlers which are operations that run in responseto some sort of event or situation rather than as part of a baseline configurationIn most systems a handler runs whenever one or more of a designated set of operations reports that it has modified the target system The handler isnt told anything about the exact nature of the change but because the association betweenoperations and their handlers is fairly specific additional information isnt neededBindingsBindings complete the basic configuration model by associating specific sets ofoperations to specific hosts or groups of hosts You can also bind operations to adynamic set of clients thats defined by the value of a fact or variable CM systemscan also define host groups by looking up information in a local inventory systemor by calling a remote APIIn addition to their basic linking role bindings in most CM systems also act as variable scopes This feature lets you customize the behavior of the operations youreassigning by defining or customizing variable values for the clients that are targetedA given host can match criteria for many different bindings For example a nodemight live on a certain subnet be managed by a particular department or fill anexplicitly designated role eg Apache web server The CM system takes accountof all of these factors and activates the operations associated with each bindingOnce you set up the bindings for a host you can invoke your CM systems toplevelconfigure everything mechanism to make the CM system identify all the operations that should run on the target and execute them in orderBundles and bundle repositoriesA bundle is a collection of operations that perform a specific function such as installing configuring and running a web server CM systems let you package bundles into a format thats suitable for distribution or reuse In most cases a bundle isdefined by a directory and the name of the directory defines the name of the bundleCM vendors maintain public repositories that include both officially blessed andusercontributed bundles You can use these as is or modify them to suit yourneeds Most CM systems provide native commands for interacting with repositoriesEnvironmentsIts often useful to segregate configurationmanaged clients into multiple worldssuch as the traditional categories of development test and production Large installations can create even finer distinctions to support processes such as the gradualstaged rollout of new code into productionThese different worlds are known generically as environments both inside andoutside the configuration management context This seems to be the single termon which all configuration management systems agreeWhen properly implemented environments are not just groups of clients Theyrean additional axis of variation that can affect multiple aspects of the configurationThe development and production environments might both include web serversand database servers for example but the details of how those roles are definedmight vary among environmentsFor example its common for the database and web server to run on the same machine in the development environment However the production environmentusually has multiple servers of each type A production environment might alsodefine server types that dont exist in the development environment such as thosethat do load balancing or act as DMZ proxiesThe environment system is usually thought of as a sort of pipeline for configurationcode As a thought experiment you can imagine that fixed groups of clients run thedevelopment test and production environments As a given configuration base isvalidated it propagates from one environment to the next ensuring that changesare properly vetted before they reach the allimportant production systemsOn most CM systems different environments are just different versions of the sameconfiguration base If youre a Git user think of them as tags in a Git repository thedevelopment tag points to the most recent version of the configuration base andthe production tag might point to a commit thats several weeks old The tags moveforward as releases make their way through testing and deploymentDifferent environments can provide different variable values to clients For example the database credentials used in development likely differ from those used onproduction systems as do the details of the network configuration and perhaps theusers and groups who are permitted accessSee Chapter Continuous Integration and Delivery for more information aboutenvironmentsClient inventory and registrationBecause CM systems define lots of ways to segregate clients into categories theoverall universe of machines under configuration management must be well defined The inventory of managed hosts can live in a flat file or in a proper relationaldatabase In some cases it may even be entirely dynamicThe exact mechanism through which configuration code is distributed parsed andexecuted varies among CM systems Most systems actually give you several optionsin this regard Here are a few common approaches A daemon runs continuously on each client The daemon pulls its configuration code from a designated CM server or server group A central CM server pushes configuration data to each client This processcan run on a regular schedule or it can be initiated by administrators Each managed node runs a client that wakes up periodically reads configuration data from a local clone of the configuration base and applies therelevant configuration to itself There is no central configuration serverConfiguration information is sensitive and often includes secrets such as passwordsTo protect this data all CM systems define some way for clients and servers to authenticate each other and to encrypt private informationThe process of putting a new client under configuration management can be madeas simple as installing the appropriate clientside software If the environment hasbeen configured to support automatic bootstrapping a new client can automaticallycontact its configuration server authenticate itself and initiate the configurationprocess OSspecific initialization mechanisms typically kick off this chain of eventsthe first time the client is bootstrapped The flow is depicted in Exhibit AExhibit A Initialization process for a new CMmanaged clientNew client rst bootrole webenvironment productionClient downloads andinstalls agentCM agent automatically authenticatesand registers with theCM serverClient entersoperation leavingagent running as abackground daemonCM agent applies thelist of bindingsServer identiesthe bindings for aproduction web server Popular CM systems comparedCurrently four major players own the market for general configuration management on UNIX and Linux systems Ansible Salt Puppet and Chef Table showssome general information about these packagesTable Major configuration management systemsLanguages and formats DaemonsSystem Web site Impl Config Template Server ClientAnsible ansiblecom Python YAML Jinja No NoSalt saltstackcom Python YAML Jinja Optional OptionalPuppet puppetcom Ruby custom ERBa Optional OptionalChef chefio Ruby Ruby ERB Optional Yesa ERB embedded Ruby is a basic syntax for embedding Ruby code in templatesAll of these packages are relatively young The oldest Puppet debuted in Itstill claims the largest market share in large part because of its early head start Chefwas released in followed by Salt in and Ansible in The general category of configuration management software was pioneered by MarkBurgesss CFEngine in CFEngine is still around and is still actively developedbut the majority of its user base has been siphoned away by newer systems Seecfenginecom for current informationMicrosoft has its own CM solution in the form of PowerShell Desired State Configuration Although it originates in the Windows world and is primarily designedto configure Windows clients Microsoft has also published extensions for configuring Linux systems Its worth noting that all four of the systems in Table canconfigure Windows clients as wellA number of projects focus on specific subdomains of configuration managementnotably newsystem provisioning eg Cobbler and software deployment egFabric and Capistrano The general proposition behind these systems is that bymore closely modeling a specific problem domain they can provide a simpler andmore targeted set of featuresDepending on your needs you may or may not find that these specialized systemsprovide a reasonable rate of return on your learning investment Generic configuration management systems like those in Table are not perfectly suited to allpossible activitiesThe systems in Table work with pretty much any type of contemporaryUNIXcompatible client machine although theres always a support frontier Chefhas a modest edge in compatibility and supports even AIXOS support on the configuration server side for those systems that use a configuration server is more limited Chef for example requires RHEL or Ubuntu forits server Containerized versions of the server can run anywhere though so thisisnt as much an obstacle as it might seemTerminologyTable shows the terms used by each of our example CM systems for the entitiesoutlined in Elements of configuration managementBusiness modelsAll the products we discuss are freemiummodel packages which means that thebasic systems are open source and free but that each system has a corporate backerthat sells support consulting services and addon packagesIn theory vendors have a potential motivation to withhold useful functionality fromthe open source releases to motivate addon sales In the configuration managementspace however this effect has not been evident The open source versions of thesoftware are fullfeatured and more than adequate for most sitesAddon services are mostly of interest to large organizations If your site falls intothis category you may want to evaluate configuration management systems withrespect to the functionality and pricing of the fullstack offerings The main upsellsSee Chapter formore informationabout containersare usually support custom development training GUIs and reporting and monitoring solutions In this book we discuss only the basic free versionsArchitectural optionsIn theory CM systems dont require servers Software could run only on the machinesbeing configured Youd copy the configuration base to each target host and simplyrun a command to say Here configure yourself according to these specificationsIn practice its nice not to have to fuss with the details of getting configuration information pushed out to clients and executed CM systems always make some provision for centralized control even if the master machine is defined as whereveryou happen to be logged in and have a clone of the configuration baseAnsible uses no daemons at all other than sshd which is an appealing simplification Configuration runs happen when an administrator or cron job on the serverruns the ansibleplaybook command ansibleplaybook executes the appropriateremote commands over SSH leaving no trace of its presence on the client machineafter configuration has completed The only requirements for client machines arethat they be accessible through SSH and have Python installed Depending on the system you might need a Python addon or two as well For example Fedora requires the pythondnf packageTable Configuration management Rosetta StoneOur term Ansible Salt Puppet Chefoperation task state resource resourceop type module function resource typeproviderproviderop list tasks states class manifest recipeparameter parameter parameter property attribute attributebinding playbook top file classificationdeclarationrun listmaster host control master master serverclient host host minion agent node nodeclient group group nodegroup node group rolevariable variable variable parameter variable attributefact fact grain fact automatic attributenotification notification requisite notify notifieshandler handler state subscribe subscribesbundle role formula module cookbookbundle repo galaxy GitHub forge supermarketSalt Puppet and Chef include both master and clientside daemons Typical deployment scenarios run daemons on both sides of the relationship and this is theenvironment youll see described in most documentation Its possible to run eachof these without a server also but this configuration is less commonIts tempting to assume that CM systems with daemons must be more heavyweightand more complex than those without ie Ansible However that isnt necessarilytrue In Salt and Puppet the daemons are facilitators and accelerators Theyre useful but optional and they dont change the fundamental architecture of the systemalthough they do enable some advanced features If you prefer you are free to runthese systems without daemons and to replicate the configuration base by handSalt even has an SSHbased mode that works similarly to AnsibleGiven that why would you want to mess around with a bunch of optional daemons Several reasons Its faster Ansible works hard to overcome the performance limits imposedby SSH and by its lack of clientside caching but it is still noticeably moresluggish than Salt When youre reading a system administration book tenseconds sounds insignificant In the midst of resolving an outage it feelsendless especially when repeated across dozens or hundreds of clients Some features cant exist without central coordination For example Saltlets clients notify the configuration master of events such as full disks Youcan then respond to these events through the normal configuration management facilities Having a central connection point facilitates a varietyof interclient datasharing features Only the masterside daemon is really a potential source of administrativecomplexity CM systems work hard to make client bootstrapping a oneline operation regardless of whether a daemon is involved The presence of active agents on both client and server opens a variety ofarchitectural options not available in onesided configurationsIn terms of architecture Chef is the outlier among configuration management systems in that its server daemon is a toptier entity within the conceptual model Saltand Puppet serve configuration data directly from plaintext files on disk to makechanges you simply edit the files By contrast the Chef server is an opaque and authoritative source of configuration information Changes must be uploaded to theserver with the knife command or they will not be available to clients Howevereven Chef has a serverless mode of operation in the form of chefsoloWe mention all this not to promote serverful systems per se but simply to point outthat the main fault line among CM systems runs between Chef and everything elseAnsible Salt and Puppet all have about the same modest level of overall complexity Chef requires significantly more investment to maintain and master especiallywhen its extensive line of addon modules is added to the mixBecause of its serverless model Ansible is often tagged as a sort of easy option forconfiguration management But in fact the basic architectures of Salt and Puppetare similarly approachable Dont write them off as advanced optionsThe converse is also true Ansible is more than just a gimpedout starter system forshortattentionspan sysadmins Its a legitimate option for complex sites althoughits sluggish performance becomes increasingly more apparent in this contextLanguage optionsAnsible and Salt are written in Python Puppet and Chef are written in Ruby Butexcept in the case of Chef this information is probably less relevant than it mightinitially appearNo Python code appears in the average Ansible or Salt configuration Both of thesesystems use YAML an alternate syntax for expressing JavaScript object notationaka JSON as their primary configuration language YAML is just structured datanot code so it has no inherent behavior other than the interpretation assigned bythe configuration management systemHeres a simple example from Salt that keeps the SSH service enabled and runningsshserverrunsshservice name sshd running enable trueTo make YAML files more dynamically expressive both Ansible and Salt augmentthem with a templating system Jinja as a preprocessor Jinja has its roots in Python but its not just a simple Python wrapper In use it feelsmore like a templatesystem than a real programming language Even Salt which relies more heavilyon Jinja than does Ansible cautions against putting too much logic into Jinja codeThe bottom line is that unless you write your own custom operation types or useexplicit escapes into Python you wont be encountering much Python in the Ansible and Salt worlds Extending the CM system with your own code can in factbe quite easy and quite helpful howeverBoth Puppet and Chef use Rubybased domainspecific languages as their primaryconfiguration systems Chef s version is a lot like a configuration management an A strong case could be made that Salt is the simplest system of all if you disregard its advanced facilities and somewhat peculiar documentation In fairness Salt is actually format and preprocessoragnostic and it supports several input pipelinesincluding raw Python right out of the box However departing from the greased path of Jinja andYAML means leaving the documentation and the rest of the world behind Its probably best deferreduntil youre quite familiar with Salt Jon Corbet one of our technical reviewers agrees that these systems dont expose much Pythonuntil things go horribly wrong At that point he adds familiarity with Python tracebacks and datastructure representations helps a lotalog of Rails from the web development world That is it has been extended with afew concepts that are designed to facilitate configuration management but its stillrecognizably Ruby For exampleservice sshd dosupports restart true status trueaction enable startendMost configuration management tasks can be achieved without delving below thesurface of Ruby but Rubys full power is available if you need it Youll appreciatethis hidden depth more and more as your comfort with Ruby and Chef increasesBy contrast Puppet has put in quite a bit of work to be conceptually independent ofRuby and to use it only as an implementation layer Although the language remainsRuby under the hood and is amenable to the insertion of Ruby code the Puppetlanguage has its own idiosyncratic structure that is more akin to a declarative system such as YAML than a programming languageservice ssh ensure running enable trueIn our opinion Puppet hasnt done administrators any favors with this architectureInstead of letting you leverage your existing knowledge of Ruby or parlay yourPuppet experience into a more general familiarity with Ruby Puppet just definesits own insulated worldDependency management optionsNo matter how your configuration management system structures its data the worklist for a given client ultimately boils down to a set of operations for the client to execute Some of those operations have executionorder dependencies and some dontFor example consider the following Ansible tasks for installing a www user accountsuch as might be used to own the files for a web application name Ensure that www group existsgroup namewww statepresent name Ensure that www user existsuser namewww groupwww statepresent createhomenoWe want the www user to have its own dedicated group also named www Ansiblesuser module does not create groups automatically so we must do that in a separatestep And the group creation must precede the creation of the www account its anerror to specify a nonexistent group in a user operationAnsible executes operations in the order in which they are presented by the configuration so this configuration snippet works just fine Chef works this way too inpart because its much harder to rearrange code than data Even if it wanted to Chefcouldnt reliably break your code into pieces and reassemble the pieces as it sees fitBy contrast Puppet and Salt allow dependencies to be explicitly declared For example in Salt the equivalent set of states would bewwwuseruserpresent name www gid www createhome false require wwwgroupwwwgroupgrouppresent name wwwWe inverted the order of the operations here for dramatic effect But because ofthe require declaration the operations run in the correct order regardless of howthey appear in the source file The following command applies the configuration sudo salt testsystem stateapply ordertesttestsystem ID wwwgroup Function grouppresent Name www Result True Comment Group www is present and up to date Started Duration ms Changes ID wwwuser Function userpresent Name www Result True Comment User www is present and up to date Started Duration ms ChangesSummary for testsystemSucceeded Failed Total states run The require parameter can be added to any operation state in Salt to ensurethat the named prerequisites run before the current operation Salt defines severaltypes of dependency relationships and declarations can appear on either side ofa relationshipPuppet works similarly It also helps to ease the pain of declaring dependencies byinferring them automatically in some common circumstances For example a userconfiguration that names a particular group automatically becomes dependent onthe resource that configures that group NiceSo Why would you want to declare your dependencies explicitly when configuration order seems natural and effortless Apparently lots of administrators have beenasking this question as both Salt and Puppet have moved to a hybrid dependencymodel in which presentation order is significant However its only a factor withina given configuration file interfile dependencies must still be explicitly declaredThe main benefit of declaring dependencies is that it makes configurations moreresilient and explicit The CM system is not obliged to abort the configuration process at the first sign of trouble because it knows which subsequent operations mightbe affected by a failure It can abort one dependency chain while allowing othersto continue Nice but in our view not a significant payback for the extra work ofdeclaring dependenciesIn theory a CM system that knows dependency information can parallelize theexecution of independent operation chains on a particular host However neitherSalt nor Puppet attempts this featGeneral comments on ChefWeve seen deployments of the mainline CM packages at organizations of varioussizes and they all display something of a tendency toward entropy The Ansible access options section starting on page includes some hints for keeping thingsorganized However an even more fundamental rule is to avoid taking on morecomplexity than is helpful for your environmentIn practice this means you need to be clear about whether youre living in Chefterritory or not Chef thinks big To get the most out of Chef you should have Hundreds or thousands of machines under configuration management An administrative staff of nonuniform privileges and experience Chef sinternal permissions system and multiple interfaces are quite helpful here Specific reporting compliance or regulatory requirements to enforce The patience to train new team members without prior Chef experienceSure you can run Chef standalone on a single machine for free Nobodys stopping you But youll still have to pay the cognitive overhead for many of the enterpriselevel features you arent using Theyre baked into the architecture and thedocumentationWe like Chef Its complete robust and scalablemore so than the alternativesBut at heart its just another configuration management system that does the samebasic stuff as Ansible Salt and Puppet Keep Chef in perspective and resist thetemptation to adopt it just because its the most powerful or because it usesRuby for that matterWe have found that bringing beginners up to speed with Chef can be a significantchallenge especially for those without prior configuration management experienceChef requires a developer mindset more than the other systems do Prior programming experience is helpfulChefs attribute precedence system is powerful but can also be a source of frustrationIts peculiar combination of foodie and Internetmeme nomenclature is annoyingand uninformative Resolving dependencies among cookbooks can be challengingsometimes an upstream dependency breaks and all your systems develop problems unless you remembered to pin all your dependencies to a particular versionGeneral comments on PuppetPuppet is the oldest of the four main CM systems and the one with the largest installed base It has lots of users lots of contributed modules and a free web GUIStill its losing market share fairly steadily to more recent competitorsAs a determinedly middleoftheroad option Puppet is under pressure from bothends of the market Its famous for serverside bottlenecks that cause problemswhen managing thousands of hosts and several major Puppet deployments haveabandoned it over the last couple of years most publicly Lyft which adopted SaltThese days such largescale scenarios seem to be better handled with a tiered Chefor Salt networkIn the arena of small deployments Ansible and Salt are mounting a serious challenge with their relatively low barriers to entry As discussed on page Puppetis not complex at heart However it does drag along some historical baggage thattends to impede newcomers For example relatively few operations are built intothe Puppet core Most sites will need to go prospecting for usercontributed modules to complete their basic configurationsOur subjective impression is that Puppet went through some false starts early in itsdesign and development Although Puppet has worked hard to correct these issueshistory and backward compatibility take an inevitable toll on the current productIt doesnt help that Puppet transmutes the golden treasure of Ruby into the lumpof coal that is the Puppet configuration language That was probably a sensible decision back in when Ruby was obscure and Rails had not yet appeared onthe scene to propel it to stardom These days the Puppet configuration languagejust seems gratuitousNone of these issues is a deal breaker but Puppet seems to have no clear and compelling advantage that might counterbalance such concerns We are not aware ofany bakeoff or comparative review written within the last few years in which Puppet emerged as a recommended optionOf course if youve inherited an existing Puppet installation theres no need to startlooking for an immediate replacement Puppet works fine the distinctions amongthese systems are mostly a matter of style and marginal advantageGeneral comments on Ansible and SaltAnsible and Salt are both nice systems and we recommend one of these optionsfor the majority of sitesWeve taken a deeper look at both of these systems in Introduction to Ansible andIntroduction to Salt which begin on pages and respectively Each ofthose sections reviews the systems configuration syntax and the general flavor ofdaytoday useAnsible and Salt look deceptively similar on the surface mostly because they bothuse YAML and Jinja as their default formats Under the hood however they almostcouldnt be more different Accordingly we defer our headtohead comparison ofAnsible and Salt until page once weve discussed them both in a bit more detailBefore we look into the systems themselves however we cast a jaundiced eye onYAML itselfYAML a rantAs mentioned earlier YAML is just an alternate syntax for JSON For example thisYAML for Ansible name Install cpdf on cloud servershosts cloudbecome yestasks name Install OCAML packages package name item statepresent withitems gmake ocaml ocamlopam In theory a YAML document should start with three dashes on a line by themselves and the Ansibledocumentation often follows this convention However this start YAML document line is essentially vestigial As far as we are aware it can safely be omitted in all casesmaps to the following JSONname Install cpdf on cloud servershosts cloudbecome yestasks name Install OCAML packages package name item state present withitems gmake ocaml ocamlopam In the JSON world brackets enclose lists and curly braces enclose hashes A colonseparates a hash key from its value These delimiters can appear directly in YAMLbut YAML also understands indentation to indicate structure much like PythonYAML marks items in a list with a preceding dashTake a moment to verify that you understand how the YAML example above mapsinto JSON because Ansible and Salt are actually JSONbased worlds The YAMLis just a shorthand We pick on Ansible below as its version of YAML is a bit moreidiosyncratic but most of the general points apply to Salt as wellClearly the YAML version is more readable than the JSON version The problemisnt YAML per se but rather the compromises inherent in trying to force data ofthe complexity found in configuration management systems into the mold of JSONYAML is good for representing simple data structures but its not a tool that scaleswell to arbitrary complexity When cracks appear in the model they have to beputtied over with a variety of ad hoc fixesThe example above already contains such a patch Did you spot it package name item statepresentIgnore the item part thats just a Jinja expansion The crime here is thenamevalue syntax which is really just a nonstandard shorthand for defining asubhash package name item state present Once again Salt partisans will protest that Salt cant be blamed for YAML and Jinja because it has noactual dependencies on these systems Youre free to use any one of a number of alternatives Thats alltrue At the same time its a lot like saying that youre not responsible for the countrys governmentbecause you didnt voteOr is it Actually not because Ansible doesnt allow hash values that start with aJinja expansion That Jinja term now must sport quotes package name item state presentAnd what if the operation accepts a free form argument name Cry for helpshell echo Please sir I just want the syntax to be consistentargs warn noVisually this doesnt look so bad But think about whats actually going on shell isthe operation type and warn is a parameter for shell just as state is a parameter forpackage in the previous example So what is that extra args dictionary doing thereWell shell typically has a complex string as its main argument the shell commandto run so its been made a special type of operation that accepts a string insteadof a parameter hash as its value The args dictionary is actually a property of thetaskitem wrapper not the shell operation Its contents are covertly stuffed downinto the shell operation on your behalf to make the whole construction workNo problem keep calm and carry on But its a confusing subtlety that muddles arelatively basic exampleThe problem isnt this specific scenario Its the constant drip of edge cases ambiguities and compromises that are needed to coerce configuration data into JSONformat Does this particular argument go in the operation In the state In the binding Its all just a big JSON hierarchy so the answer is rarely obviousLook again at the Install cpdf on cloud servers playbook on page Is it obvious that withitems should be at the same level as package and not at the samelevel as name and state which are in fact logically beneath package Probably notThe underlying intent behind the YAML approach is praiseworthy use an existingformat that people already know and represent configuration information as datainstead of code Still these systems have syntactic warts that would probably notbe permitted in a real programming language Introduction to AnsibleAnsible has no server daemon and installs no software of its own on clients so itsreally just a set of commands most notably ansibleplaybook ansiblevault andansible that you install on any system from which you wish to manage clients Despite the looseness of YAML as used in configuration management systems its specification is actually quite lengthy In fact its longer than the specification for the entire Go programming languageStandard OSlevel packages are widely available for Ansible although the packagenames vary from system to system On RHEL and CentOS make sure you havethe EPEL repository enabled on the master systems As with most things OSspecific packages are often somewhat out of date with respect to the trunk If you dontmind forgoing package management Ansible is easy to install from the GitHubrepository ansibleansible or through pipThe default location of Ansibles master configuration file is etcansibleansiblecfgAs with most addons FreeBSD moves the ansible directory to usrlocaletcThe default ansiblecfg file is short and sweet The only change wed recommend isto add the following lines to the end sshconnectionpipelining trueThese lines turn on pipelining an SSH feature that significantly improves performance Pipelining requires that sudo on clients not be configured to require interactive terminals however that is the defaultIf you keep your configuration data under etcansible youll need to use sudo tomake changes and you tie yourself to one particular server machine Alternativelyyou can easily set up Ansible for use under your own account The server just runsssh to reach other systems so root privileges are unnecessary unless you need torun privileged commands on the server sideFortunately Ansible makes it a snap to combine systemwide and personal configurations Dont remove the systemwide configuration just shadow it by creatingansiblecfg and setting the location of your inventory file and roles directorydefaultsinventory hostsrolespath rolesThe inventory is the list of client systems and roles are bundles that abstract variousaspects of client configuration We return to both of these topics shortlyHere we define both locations as relative paths which assumes that youll cd toyour clone of the configuration base and that youll follow the stated naming conventions Ansible also understands the shells notation for home directories ifyou prefer to use fixed paths Ansible allows pretty much everywhere else tooAnsible exampleBefore we delve into too much more detail we first look at a small example thatdemonstrates a few basic Ansible operations pip is a package manager for Python Try pip install ansible to pull the latest version from PyPI thePython Package Index You might need to install pip from your distributions packaging system first It is curious that ansiblecfg uses ini format instead of YAML as do several other Ansible configuration files We dont know the reason for thisSee page formore informationabout EPELSee page toallow sudo withouta control terminalThe following set of steps would set up sudo on a new system as might be requiredeg on FreeBSD which does not include sudo by default Install the sudo package Copy a standard sudoers file from a server and install it locally Make sure the sudoers file has appropriate permissions and ownerships Create a UNIX group called sudo Add every system administrator with an account on the local machineto the sudo groupThe Ansible code below implements these steps Because this code is designed toillustrate several points about Ansible it isnt necessarily an example of idiomaticAnsible code name Install sudo packagepackage namesudo statepresent name Install sudoers filetemplate dest sudoerspath src sudoersj owner root group wheel mode name Create sudo groupgroup namesudo statepresent name Get current list of usernamesshell cut d f etcpasswdregister userlist name Add administrators to the sudo groupuser name item groupssudo appendtruewithitems admins when item in userliststdoutlines The statements are applied in order much as they would be in a shell scriptThe expressions enclosed in double curly braces eg admins are variableexpansions Ansible interpolates facts in a similar manner This kind of parametermanagement flexibility is a common characteristic of configuration managementsystems and its one of their main advantages over raw scripts You define the generalprocedure in one place and the configuration specifics elsewhere The CM systemthen collapses the global specification and ensures that the proper parameters areapplied to each target hostThe file sudoersj is a Jinja template that expands to become the sudoers file onthe target machine The template can consist of static text or it can have internallogic and variable expansions of its ownTemplates are usually kept along with configurations in the same Git repositoryallowing for onestop shopping when configurations are applied Theres no needto maintain a separate file server from which templates can be copied The configuration management system uses its existing access to the target host to installtemplates so credential management need be set up only onceWe had to work around a couple of rough edges Ansibles user module used hereto add system administrators to the sudo UNIX group normally ensures that thespecified account exists and it creates the account if it does not In this scenariowe want to affect only accounts that already exist so were forced to manually checkfor the existence of each account before we permit user to modify itTo do that the configuration runs the shell command cut d f etcpasswd toobtain a list of existing accounts and captures registers the output under thename userlist Its similar in principle to the sh lineuserlistcut d f etcpasswdEach account listed in the admins variable withitems admins is considered separately During its turn the account name is assigned to the variableitem The name item is an Ansible convention the configuration does not specify it For each account found within the output of the cut command the whenclause the user clause is invokedTheres a bit of extra glue we havent shown that binds this configuration to a particular set of target hosts and that tells Ansible to make the changes as root When weactivate that binding by running ansibleplaybook exampleyml Ansible startsworking to configure several target hosts in parallel If any operation fails Ansiblereports the error and stops working on the host that generated it Other hosts cancontinue until theyre doneClient setupAnsible needs three things from each configuration management client SSH access sudo permission A Python interpreterIf the client is a Linux cloud server it may be Ansibleaccessible right out of the boxSystems like FreeBSD that dont install sudo or Python by default might need a bitmore tweaking but you can do some of the initial bootstrapping through Ansible In a more typical scenario the configuration management system would be responsible for setting upadministrators accounts as well as sudo access The configuration specifications for both functionswould likely refer to the same admins variable and so there would be no possibility of conflict andno need to validate each account name Ansible does not actually require sudo access per se Its only needed if you want to run privileged operations But you typically willwith raw operations which execute commands remotely without the usual Pythonwrapper Or you can just write your own bootstrapping scriptSeveral choices must be made when Ansible clients are set up We suggest a reasonablegame plan in Ansible access options starting on page but for now lets assumeyouve created a dedicated ansible user on the client that the appropriate SSH keyis in your default set and that youre willing to enter the sudo password by handClients dont introduce themselves to Ansible so you need to add them to Ansibleshost inventory By default the inventory is a single file called etcansiblehostsOne nice feature of Ansible is that you can replace any flat configuration file with adirectory of the same name Ansible then merges the contents of the files the directory contains This feature is useful for structuring your configuration base but itsalso Ansibles way of incorporating dynamic information if a particular file is executable Ansible runs it and captures the output instead of reading the file directlyThis aggregation feature is so useful and so commonly used that we recommendbypassing the larval flatfile stage of most configuration files and skipping directlyto directories For example we can define an Ansible client by adding the following line to etcansiblehostsstatic or to hostsstatic within a personal configuration basenewclientexamplecom ansibleuseransibleFreeBSD clients put Python in an unusual location so youll need to tell Ansibleabout thatfreebsdexamplecom ansiblepythoninterpreterusrlocalbinpythonansibleuseransibleThis should all be on a single line in the hosts file On page we present amuch better way to set these variables but that method is just a generalization ofthis same ideaTo check connectivity with a new host run the setup operation which returns theclients fact catalog ansible newclientexamplecom m setupnewclientexamplecom SUCCESS ansiblefacts ansibleallipvaddresses more lines omitted Actually Ansible is even smarter than this It ignores certain file types entirely eg ini files So notonly can you put in scripts but also configuration files for scriptsThe name setup is unfortunate as no explicit setup is actually required You cango directly to actual configuration operations if you wish In addition you can runthe setup operation as often as you like to review the clients fact catalogCheck to be sure that privilege escalation through sudo is also working correctly ansible newclientexamplecom a whoami become askbecomepassSUDO password passwordnewclientexamplecom SUCCESS rc rootHere the command operation which runs shell commands is the default We couldhave said m command explicitly with equivalent results The a flag introducesoperation parameters in this case the actual command to executeBecoming is Ansibles odd locution for privilege escalation you become anotheruser The other user is root by default but you can specify a different one with theu option Unfortunately you have to force Ansible to ask you for the sudo passwordwith askbecomepass and it does so regardless of whether the remote systemactually prompts for the passwordClient groupsGroups are defined within the hosts directory as well although the syntax can become a bit awkwardclientfourexamplecomwebserversclientoneexamplecomclienttwoexamplecomdbserversclientoneexamplecomclientthreeexamplecomIf this doesnt look so bad thats because weve skirted the main problem areas Theini format is flatish so some tricks are needed if you want to define hierarchicalgroups or add extras directly to the hosts file eg variable assignments for a groupThese features arent actually that important in practice howeverNote that we had to list clientfour at the top of the file because that host is notincluded in any groups We cant just append to the hosts file because that wouldmake clientfour a member of the dbservers group even if we added a blank lineas a separatorThis is another reason why configuration directories are helpful In practice wedprobably want to put each group definition in a separate fileAnsible lets you freely intermix client names and group names on command linesand within the configuration base Neither is specially marked and both can besubject to globbing Regularexpressionstyle matching is also available for bothjust start the pattern with a Theres also a set algebra notation for combining cohorts of clients in various waysFor example the following command uses a globbing expression to select the webservers group It executes the ping operation on each member of that group ansible web m pingclientoneexamplecom SUCCESS changed falseping pongclienttwoexamplecom SUCCESS changed falseping pongVariable assignmentsAs we saw on page variable values can be assigned within inventory files Butthats gauche dont do it that wayEvery host and group can have its own collection of variable definitions in YAMLformat By default these definitions are stored under etcansiblehostvars andetcansiblegroupvars in files named for the host or group You can use a ymlsuffix if you want Ansible finds the appropriate files either wayAs with other Ansible configurations these files can be converted to directories ifyoud like to add some additional structure or scripting Ansible does its usual routine of ignoring configuration files running scripts and combining all the resultsinto a final packageAnsible automatically defines a group called all for you Just like other groups allcan have its own group variables For example if you standardize on using clientaccounts named ansible for configuration management thats a good fact to putin the global configuration here in say groupvarsallbasicsansibleuser ansibleIf multiple value declarations exist for a variable Ansible selects a final value according to precedence rules rather than declaration order Ansible currently has different precedence categories but the relevant point in this case is that hostvariables trump group variablesConflicts among overlapping groups are resolved at random which can make forinconsistent behavior and tricky debugging Try to structure your variable declarations so that theres no possibility for overlapsDynamic and computed client groupsAnsibles grouping system really comes into its own when dynamic scripting is addedto the mix The dynamic inventory scripts used with cloud providers for exampledont simply list all the available servers They also slice and dice those servers intoad hoc groupings according to metadata from the cloudFor example Amazons EC lets you assign arbitrary tags to each instance Youmight assign the tag webserver to every instance that needs an NGINX stack anddbserver to every instance that needs PostgreSQL The ecpy dynamic inventoryscript would then create groups named tagwebserver and tagdbserver Thesegroups can have their own group variables and can be named in bindings playbooks just like any other groupThe situation gets murkier when it comes to grouping clients on criteria internalto Ansible such as the values of facts You cant do this directly What you can doinstead is target playbooks to broader groups such as all and apply conditional expressions to individual operations which when the proper conditions do notapply cause the operation to be skippedFor example the following playbook ensures that etcrcconf contains a line toconfigure the hostname on each FreeBSD client name Set hostname at startup on FreeBSD systemshosts alltasks lineinfile dest etcrcconf line hostname hostname regexp hostname when ansibleosfamily FreeBSDIf the last line looks like it needs some your instinct is good This is actuallyjust a bit of Ansible syntactic sugar to help keep configurations tidy when clausesare always going to be Jinja expressions so Ansible surrounds their contents withdouble braces for you automatically This feature is helpful but its just one of a fairlyextensive list of irregularities in Ansibles YAML parsingIn this example every host in inventory is considered for the lineinfile operationBut thanks to the when clause only FreeBSD hosts actually run it This approachworks fine but it doesnt make the FreeBSD hosts into a true group They cant forexample have a normal groupvars entry although you can simulate the effectwith some juryriggingA structurally preferable but slightly more verbose alternative is to use a groupbyoperation which runs locally and classifies hosts according to an arbitrary key value for which you designate a template name Group hosts by OS typehosts alltasks groupby key ansibleosfamily name Set hostname at startup on FreeBSD systemshosts FreeBSDtasks lineinfile dest etcrcconf line hostname hostname regexp hostnameThe basic game plan is similar but the classification occurs in a separate play Ansibles term for what we call a binding see page We then start a new play sothat we can specify a different set of target hosts this time using the FreeBSD groupthat the first play defined for usThe advantage of using groupby is that we perform the classification only once Wecan then hang any number of tasks off the second play with confidence that weretargeting only the intended clientsTask listsAnsible calls operations tasks and a collection of tasks in a separate file is calleda task list Like all but a few parts of an Ansible configuration task lists are justYAML so the files have a yml suffixThe binding of task lists to specific hosts is done in higherlevel objects called playbooks which are described on page For now lets focus on the operationsthemselves and not worry about how they come to be applied to a particular hostAs an example we revisit the install sudo example from page with a slightlydifferent focus and implementation This time we create the administrator accountsfrom scratch and give each one its own UNIX group of the same name We then setup a sudoers file that lists the administrators explicitly instead of just assigningprivileges to a sudo UNIX group A task or state file should normally have a welldefined domain and a clear objective whereas thisagenda is kind of a jumble We chose these operations to illustrate some general points rather than asan example of appropriate configuration base structureSome input data is needed to drive these operations in particular the location ofthe sudoers file and the names and usernames of administrators We should putthis information in a separate variable file say groupvarsalladminsymlsudoerspath etcsudoersadmins username manny fullname Manny Calavera username moe fullname Moe Money The value of admins is an array of hashes we iterate through this array to create allthe accounts Heres what the complete task list would look like name Install sudo packagepackage namesudo statepresent name Create personal groups for adminsgroup name itemusername withitems admins name Create admin accountsuser name itemusername comment itemfullname group itemusername groups wheelwithitems admins name Install sudoers filetemplate dest sudoerspath src templatessudoersj owner root group wheel mode From the perspective of YAML and JSON the tasks form a list Each dash at theleft margin starts a new task which is represented by a hashIn this example every task has a name field that describes its function in EnglishThe names are technically optional but if you dont include them Ansible tells youvery little about what its doing when you run the configuration other than listingthe module names package group etcEach task must have among its keys the name of exactly one operation module Thevalue of that key is itself a hash that enumerates the operation parameters Parameters that you do not explicitly set assume default valuesThe notation name Install sudo packagepackage namesudo statepresentis an Ansible extension to YAML thats essentially equivalent to name Install sudo packagepackage name sudo state presentTheres some potential weirdness here in the case of operations like shell that havefreeform arguments but we wont rehash that here See the YAML rant on page The oneline format is not only more compact but it also lets you set parameterswhose values are Jinja expressions without quotes as seen in the task that createspersonal groups for admins In the normal syntax a Jinja expression cannot appearat the start of a value unless the entire value is in quotes The quoting is benign butit does add visual noise Despite appearances the quotes do not force the value tobe a stringNow were ready to break out a few of the more notable aspects of this example tasklist in the sections belowstate parametersIn Ansible operation modules can often perform several different tasks depending on the state you request For the package module for example statepresentinstalls the package stateabsent removes it and statelatest ensures that thepackage is both present and up to date Operations often look for different sets ofparameters depending on the state being invokedIn a few cases eg the service module with staterestarted which restarts adaemon this model wanders a bit from what might normally be conceived of as astate but overall it works well The state can be omitted as shown here when creating the sudo group in which case it assumes a default value usually somethingpositive and empowering such as present configured or runningIterationwithitems is an iteration construct that repeats a task once for each element itssupplied with For quick reference heres another copy of the two tasks in our example that use withitems name Create personal groups for adminsgroup name itemusername withitems admins name Create admin accountsuser name itemusername comment itemfullname group itemusername groups wheelwithitems admins Note that withitems is an attribute of the task not the operation that the task runsOn each pass through a loop Ansible sets the value of item to one of the itemssupplied to withitems In this case we assigned the variable admins a list ofhashes so item is always a hash The notation itemusername is shorthand foritemusername Use whichever you preferEach of these tasks loops through the admins array separately One pass createsUNIX groups and the other creates user accounts Although Ansible does define agrouping mechanism for tasks called a block that construct unfortunately doesnot support withitemsIf you really need the effect of a single loop that executes multiple tasks in sequenceyou can achieve it by moving the loop body into a separate file and including itinto the main task list include sudosubtasksymlwithitems admins withitems is not the only loop available in Ansible There are also loop formsdedicated to iterating over hashes termed dictionaries in Python over lists offiles and over globbing patternsInteraction with JinjaThe Ansible documentation is not very specific about how YAML and Jinja interactbut its important to understand the details As constructs like withitems demonstrate Jinja is not simply a preprocessor thats run over a file before it is handed offto YAML as is the case in Salt In fact Ansible parses YAML with Jinja expressionsintact It then Jinjaexpands each string value immediately before use Parametersof iterated operations are reevaluated during each iterationJinja has control structures of its own including loops and conditionals Howeverthey are inherently incompatible with Ansibles delayedevaluation architectureand so they are not allowed in Ansibles YAML files although they can be used intemplates Ansible constructs such as when and withitems are not just windowdressing for the equivalent Jinja They represent a rather different approach tostructuring the configurationTemplate renderingAnsible uses the Jinja template language both to add dynamic features to YAMLfiles and to flesh out configuration file templates installed by the template moduleWe use a template in this example to set up the sudoers file Here are the variabledefinitions again for referencesudoerspath etcsudoersadmins username manny fullname Manny Calavera username moe fullname Moe Money And the task code name Install sudoers filetemplate dest sudoerspath src templatessudoersj owner root group wheel mode The file sudoersj is a mix of plain text and Jinja code for the dynamic bits Forexample heres a skeletal example that gives sudo ALL privileges to each adminDefaults envkeep HOME for admin in admins admin ALLALL ALL endfor The for loop wrapped by is Jinja syntax Unfortunately you cant indentloop bodies sensibly as you might in a real programming language because doingso would cause the output of the template to be indented as wellThe expanded version looks like thisDefaults envkeep HOMEmanny ALLALL ALLmoe ALLALL ALLNote that variable values automatically flow through to templates The values areavailable to configuration files under exactly the same names used to define themno prefix or additional hierarchy is imposed Autodiscovered fact variables are inthe toplevel namespace too but to forestall potential name conflicts they all beginwith the prefix ansibleAnsibles module for installing static files is called copy However you may as welltreat all configuration files as templates even if their contents initially consist ofstatic text You can then add customizations in the future without having to touchthe configuration code just edit the template Reserve copy for binary files and forstatic files that will never need expansion such as public keysBindings plays and playbooksBindings are the mechanism through which tasks become associated with sets ofclient machines Ansibles binding object is called a play Heres a simple example name Make sure NGINX is installed on web servershosts webserverstasks package namenginx statepresentJust as multiple tasks can be concatenated to form a task list multiple plays in sequence form a playbookAs in other systems the basic elements of a binding are a set of hosts and a set oftasks However Ansibles system allows several additional options to be specifiedat the play level Theyre listed in Table Table Ansible play elementsKey Format What it specifiesname string Name to print out when executing the play optionalhosts list string Client systems on which to run associated tasks and rolesvars hash Variable values to set for the scope of this playvarsfiles list Files from which to read variable valuesbecome strings Privilege escalation eg sudo optionstags list Categories for selective execution see page tasks list Operations to run may include separate fileshandlers list Operations to run in response to notifyroles list Bundles roles to invoke for these hosts see page The biggies here are the variablerelated options not so much because they appearin plays per se but because theyre available pretty much anywhereeven whenexecuting includes Ansible can activate the same task list or playbook again andagain with different sets of variable values Its a lot like defining a function egmake a user account and then calling it with different sets of argumentsAnsible formalizes this system in its implementation of bundles called roles whichwe discuss on page Roles are powerful but under the hood theyre just a setof standardized conventions for doing includes so theyre also easy to understandHeres a simple play that demonstrates the use of handlers name Update cowclicker web apphosts clickeraclickerbtasks name rsync app files to srv synchronize mode pull src webrepositescowclicker dest srvcowclicker notify restart nginxhandlers name restart nginx service namenginx staterestartedThis playbook runs on hosts clickera and clickerb It mirrors files from a centrallocal repository by running rsync using the synchronize module then restartsthe NGINX web server if any updates were madeWhen a task with a notify clause makes changes to the system Ansible runs thehandler of the requested name Handlers themselves are just tasks but theyre declared in a separate section of the playPlaybooks are the primary unit of execution in Ansible You run them withansibleplaybook ansibleplaybook globalyml asksudopassAnsible approaches multihost execution task by task As it reads a playbook eachtask is run in parallel on the targeted hosts When every host has completed the taskAnsible continues to the next task By default Ansible runs tasks simultaneously onup to five hosts but you can set a different limit with the f flagWhen debugging problems its often helpful to include the vvvv argument to increase the amount of debugging output Youll see the exact commands that areexecuted on the remote system and their detailed responsesRolesAs we described generically starting on page bundles our term are thepackaging mechanism defined by a CM system to facilitate reuse and sharing ofconfiguration fragmentsAnsible calls its bundles roles and they are in fact nothing but a structured systemof include operations and variable precedence rules They make it easy to put thevariable definitions task lists and templates associated with a configuration into asingle directory making them readily available for reuse and sharingEach role is a subdirectory of a directory called roles thats normally found at thetop level of your configuration base You can also add sitewide role directories bysetting the rolespath variable in ansiblecfg as shown on page All knownrole directories are searched whenever you include a role in a playbookRole directories can have the subdirectories shown in Table Roles are invoked through playbooks and nowhere else Ansible looks for a filecalled mainyml within each of the roles subdirectories If it exists the contentsare automatically incorporated into any playbook that invokes the role For example the playbook name Set up cowclicker app throughout East regionhosts webserverseastroles cowclickeris roughly equivalent to name Set up cowclicker app throughout East regionhosts webserverseastvarsfiles rolescowclickerdefaultsmainyml rolescowclickervarsmainymltasks include rolescowclickertasksmainymlhandlers include rolescowclickerhandlersmainymlHowever variable values from the default folder do not override values that havealready been set In addition Ansible makes it easy to refer to files from the filesand templates directories and it subincludes any roles mentioned as dependencies in the metamainyml fileFiles other than mainyml are ignored by the roles system so you can break theconfiguration into whatever pieces are appropriate and just include those partsinto mainymlAnsible lets you pass a set of variable values to a particular instance of a role Ineffect this makes the role act as a sort of parameterized function For exampleyou might define a bundle thats used to deploy a Rails app You could invoke thatbundle several times within a playbook supplying the parameters of a differentapp for each invocation name Install ULSAH Rails appshosts ulsahserverroles role railsapp appname ulsahreviews role railsapp appname admincom In this example the railsapp role would probably depend on a role for nginxor some other web server so it would not be necessary to mention the web serverrole explicitly If you wanted to customize the web server installation you couldTable Subdirectories of an Ansible roleSubdir Contentsdefaults Default values for variables overridablevars Variable definitions not overridable but can reference overridestasks Tasks lists sets of operationshandlers Operations that respond to notificationsfiles Data files typically used as a source for copy operationstemplates Templates to be processed by Jinja before installationmeta List of bundles to run in preparation for this bundlesimply include the appropriate variable values in the railsapp invocation andthose values would be propagated downwardAnsibles public role repository is located at galaxyansiblecom You can search forroles with the ansiblegalaxy command but youre better off using the web siteIt lets you sort by rating or download count and you can easily click through tothe GitHub repo that hosts the actual code for each role Several roles are usuallyavailable to address most common scenarios so its worth examining the code todetermine which version will serve your needs bestOnce youve settled on a role implementation copy the files to your roles directoryby running ansiblegalaxy install For example ansiblegalaxy install ANXSpostgresql downloading role postgresql owned by ANXS downloading role from httpsgithubcomANXSpostgresqlvtargz extracting ANXSpostgresql to etcansiblerolesANXSpostgresql ANXSpostgresql was installed successfullyRecommendations for structuring the configuration baseMost configuration bases are organized hierarchically That is various pieces of theconfiguration feed into a master playbook that controls the global state Howeveryou can also define taskspecific playbooks that are unrelated to the global schemeTry to keep task lists and handlers out of playbook files Instead put them in separate files and interpolate them with include This structure makes a clean separation between bindings and behavior and it puts all tasks on an equal footing Forextra style points avoid freestanding task lists entirely and standardize on rolesIts sometimes recommended that a single playbook should cover all the tasks thatrelate to each logically distinct group of hosts For example all the roles and tasksthat relate to web servers should be included in a single webserveryml playbookThis approach avoids replication of host groups and provides a clear locus of control for each host groupOn the other hand following this rule means that theres no direct way to run aportion of the global configuration even for debugging Ansible can run only playbooks there is no simple command that runs a specific task list on a given machineThe official solution for this issue is tagging which works fine but requires somesetup You can include a tags field in or above any task to classify it At the command line use ansibleplaybooks t option to specify the subset of tags you wantto run In most debugging scenarios youll also want to use the l option to limitexecution to a specified test hostAssign tags at as high a level as you can within the configuration hierarchy Undernormal circumstances you should have no temptation to assign tags to individualtasks If you do it may be a sign that the particular task list should be split upInstead attach tags to the include or roles clause that incorporates a specific tasklist or role into the configuration The tags then cover all the included tasksAlternatively you can just construct scratch playbooks that run parts of the configuration base on a test host Setting up these scratch playbooks is a minor annoyance but so is taggingAnsible access optionsAnsible needs SSH and sudo access on every client system which sounds straightforward and familiar until you consider that the configuration management systemholds the master keys to the entire organization Its hard for daemonbased systemsto be more secure than the root account on the configuration server but Ansiblecan potentially do better than this with some thoughtful planningFor simplicity its best if SSH access is funneled through a dedicated account suchas ansible that has the same name on each client That account should use a simple shell and should have a minimal dotfile configurationOn cloud servers you can use a standard bootstrapping account such as ecuser onEC for Ansible control Just make sure that after the initial setup the account hasbeen properly locked down and does not allow eg su to root without a passwordYou have some flexibility regarding the actual security design But keep the following points in mind Ansible needs one credential password or private key to gain access to aremote system and another to escalate privileges with sudo Proper securityhygiene suggests that these be separate credentials A single compromisedcredential should not grant an intruder root access to a target machine If both credentials are stored in the same place with the same form of protection encryption file permissions they are effectively a single credential Credentials can be reused on machines that are peers eg web servers ina farm but it should not be possible to use credentials from one server toaccess a more sensitiveor even substantially differentserver Ansible has transparent support for encrypted data through the ansiblevaultcommand but only if the data is contained in a YAML or ini file Administrators can remember only a few passwords Its unreasonable to demand more than one password for a given operation Some sites set up clientside ansible accounts with the NOPASSWD option in the sudoers file suchthat no password is required for the ansible account to run sudo This is a terribly insecure configuration If you cant bring yourself to type a password at least install the PAM SSH agent module andrequire a forwarded SSH key for sudo access See page for more information about PAMSites will arrive at their own tradeoffs but we suggest the following system as a robust but usable baseline that conforms to these guidelines SSH access is controlled by key pairs that are used by Ansible only Passwordbased SSH access is prohibited on client systems by settingPasswordAuthentication no in etcsshsshdconfig SSH private keys are protected by a passphrase set with sshkeygen pAll private keys have the same passphrase Private SSH keys are kept in a known location on the Ansible master machine They do not live within the configuration base and administratorsagree not to copy them elsewhere Remote accounts ansible accounts have random UNIX passwords thatare listed in the configuration base in encrypted form All of them are encrypted with the same passphrase but its different from the passphraseused for SSH private keys You will need to add some Ansible glue to makesure the right passwords are used with the right client hostsIn this scheme both sets of credentials are encrypted which makes them resistantto simple violations of file permissions This layer of indirection also lets you changethe master passphrases easily without changing the underlying keysAdministrators need remember only two passphrases the passphrase that givesaccess to SSH private keys and the Ansible vault password which allows Ansibleto decrypt the hostspecific sudopasswords as well as any other confidential information included in your configuration baseIf you require more granularity for administrator permissions which is likely youcan encrypt multiple sets of credentials with different passphrases If the sets arecumulative as opposed to disjoint no individual administrator needs to remember more than two passphrasesIts assumed in this system that administrators will use sshagent to manage accessto private keys All keys can be activated with a single sshadd command and theSSH password need be entered only once per session To work on a system otherthan the usual Ansible master admins can use SSHs ForwardAgent option to tunnel keys through to the machine on which work is being done All other securityinformation is included in the configuration base itselfIts true that sshagent and key forwarding are only as secure as the machines onwhich they run Less so really like sudo with a grace period they are only as secure as your personal account However the risk is mitigated by limits on timeand context Use the t argument to sshagent or sshadd to cap the lifetime ofactivated keys and terminate connections that have access to forwarded keys onceyou are no longer using themSee page for moredetails on sshagentIf possible private keys should never be deployed onto client systems If clients needprivileged access to controlled resources eg to clone a controlled Git repositoryuse the proxying features built into SSH and Ansible or use sshagent to make private keys temporarily available to the client without copying themFor some reason Ansible cannot currently recognize encrypted files in the configuration base and prompt you to enter the passphrase for decryption You haveto force its hand with the askvaultpass argument to the ansibleplaybook andansible commands Theres a vaultpasswordfile option available for noninteractive use but of course that reduces security If you decide to use a password fileit should be accessible only to the dedicated ansible account Introduction to SaltOut in the world you might see Salt referred to as Salt SaltStack or Salt Open Theseterms are essentially interchangeable The vendors name is SaltStack and they useSaltStack as a generic term to refer to the complete product line which includessome enterprise addons that we dont discuss in this book However many peoplecall the open source system SaltStack tooSalt Open is a more recently introduced name that designates only the open sourcecomponents of Salt But currently that name doesnt seem to be used anywhereoutside of saltstackcomSaltStack maintains its own package repository at reposaltstackcom which hostsuptodate packages for every Linux packaging system See the web site for instructions on how to add the repo to your configuration Some distributions include freerange Salt packages of their own but its generally best to go directly to the sourceYoull need the saltmaster package on the configuration server the master Ifyou have any dealings with cloud providers also install the saltcloud packageIt wraps a variety of cloud providers into a standard interface and simplifies theprocess of creating new cloud servers to be managed through Salt Its essentiallysimilar to cloud providers native CLI tools but it handles machines at both theSalt and cloud layers New machines are automatically bootstrapped enrolled andapproved Deleted machines are removed from Salt as well as the providers cloudSaltStack doesnt host a package repo for FreeBSD but it is a supported platformThe web installer is FreeBSD aware curl L httpsbootstrapsaltstackcom o tmpsaltboot sudo sh tmpsaltboot P MBy default the web installer installs clientside software as well as the master serverIf you dont want that pass the N option to saltbootSalts configuration files go in etcsalt both on the master server and on clientsminions Its theoretically possible to run the server daemon as an unprivilegeduser but that requires manually chowning a bunch of system directories that SaltSee Chapter formore informationabout containersexpects to interact with If youre tempted to head down this road youre probablybetter off using a containerized version of the server or saving the configurationinto a prebaked machine imageSalt has a simple access control system that you can configure to allow unprivilegedusers to initiate Salt operations on minions However you must do manual permission hacking similar to that required for nonroot operation Considering thatthe master has direct root access to all minions we find this feature rather suspectfrom a security perspective If you do use it keep a tight lid on the permissionsthat are grantedSalt maintains a separation between configuration files that set variable values thepillar and configuration files that define operations states The distinctiongoes all the way to the top you must set separate locations for these configurationhierarchies They both default to living under srv which is equivalent to the following etcsaltmaster filefilerootsbase srvsaltpillarrootsbase srvpillarHere base is a required common environment on top of which additional environments eg development can be layered Variable definitions go in the srvpillarroot and everything else lives in srvsaltNote that the paths themselves are list elements since theyre prefixed with dashesYou can include multiple directories which makes the saltmaster daemon serve amerged view of the listed directories to minions This is a useful feature when youare organizing a large configuration base since it permits you to add structure thatSalt wouldnt natively understandTypically youll want to manage the configuration base as a single Git repositorythat includes both the salt and pillar subdirectories This isnt a good fit for thedefault layout because it means that srv would be the repo root consider movingeverything down a level to srvsaltsalt and srvsaltpillarThe Salt documentation doesnt do a very good job of explaining why the pillarand the states have to be completely separate but in fact this distinction is centralto Salts architecture The saltmaster daemon doesnt pay the slightest attentionto state files it simply makes them available to minions who are responsible forparsing and executing themThe pillar is entirely different Its evaluated on the master and propagated to minions as a single unified JSON hierarchy Each minion sees a different view of thepillar but none of them can see the implementation machinery behind these viewsIn part this is a security measure Salt makes a strong guarantee that minions cannot access each others pillars Its also a datasourcing distinction as dynamic pillarcontent always originates from the master This makes for a nice complementaritywith grains Salts version of facts which originate on minionsSalts communication bus uses TCP ports and on the server Make surethese ports are allowed through any firewalls or packet filters that lie between theserver and the prospective clients The clients themselves do not accept networkconnections so this step needs to be done only once for the serverWhen first investigating Salt you might find it informative to run saltmaster l debugin a terminal window instead of as a system service This makes saltmaster runin the foreground and print out activity on Salts communication bus as it occursMinion setupAs on the master side you have a choice of native packages from SaltStacks repoor a universal bootstrap script The repo is hardly worth fussing with on minionsso we recommend the latter curl o tmpsaltboot sL httpsbootstrapsaltstackcom sudo sh tmpsaltboot PThe bootstrap script works on any supported system On systems without curl wgetand fetch also work fine See the saltstacksaltbootstrap repository on GitHub forspecific installation scenarios and source codeBy default the saltminion daemon tries to register itself with a master machinenamed salt This magic name system was first popularized by Puppet You canuse DNS wizardry to make the name resolve appropriately or you can set an explicitmaster in etcsaltminion usrlocaletcsaltminion on FreeBSDmaster saltexamplecomRestart saltminion after modifying this file usually service saltminion restartnote the underscore rather than a dash For production systems that are started automatically minimize your exposure to external eventsby downloading a locally cached version of the boot script Install a specific version of the Salt clientalso from a local cache or preload it on the machine image Run the boot script with a h option tosee all the options it supportsSee page for moreinformation aboutnetwork firewallsSee Chapter for more information about DNSsaltmaster accepts client registrations from any random machine that can reach itbut you must approve each client with the saltkey command on the master configuration server before it becomes active sudo saltkey l unacceptedUnaccepted Keysnewclientexamplecom If everything looks good accept all pending keys sudo saltkey yAThe following keys are going to be acceptedUnaccepted KeysnewclientexamplecomKey for minion newclientexamplecom acceptedYou can now check connectivity from the server with the test module sudo salt newclientexamplecom testpingnewclientexamplecomTrueIn this example newclientexamplecom looks suspiciously like a hostname but itreally isnt Its just the machines Salt ID a string that defaults to the hostname butcan be set to anything you like in the clients etcsaltminion filemaster saltexamplecomid newclientexamplecomIDs and IP addresses have nothing to do with each other For example even if were the clients actual IP address you could not directly target itthat way with Salt commands sudo salt testpingNo minions matched the target No command was sent no jid was assignedERROR No return receivedVariable value binding for minionsAs we saw in the server setup section Salt has separate filesystem hierarchies forstate bindings and variablevalue bindings the pillar Each of these directorytrees has a topsls file at the root that binds groups of minions to files within thetree The two topsls files both use the same layout sls is just Salts standard extension for YAML files Of course you can do IPbased matching It just has to be explicit See page As an example heres the layout of a simple Salt configuration base that shows boththe salt and pillar roots tree srvsaltsrvsalt salt topsls hostnamesls bootstrapsls sshdsls baselinesls pillar topsls baselinesls webserversls freebsdsls directories filesTo bind the variables defined in pillarbaselinesls and pillarfreebsdsls to ourexample client we could include the following lines in pillartopslsbasenewclientexamplecom baseline freebsdAs in the master file base is a required common environment that can be overlaidin more sophisticated setups See page for more about thisIts possible for baselinesls and freebsdsls to define some of the same variablevalues For scalar and array values the last source listed in topsls is the one thattakes effect Hashes however are mergedFor example if a minion binds to one variable file that looks like thisadminusersmanny uid moe uid and one that looks like thisadminusersjack uid then Salt merges the two versionsThe pillar data presented to minions isadminusersmanny uid moe uid jack uid Minion matchingIn the scenario above what we probably want is to apply baselinesls to all clientsand to apply freebsdsls to all clients that are running FreeBSD Heres how we cando that with selection patterns in the pillartopsls filebaseexamplecom baselineGosFreeBSD freebsdThe star matches all client IDs in examplecom We could have just used herebut we wanted to emphasize that its a globbing pattern The G prefix requests amatch on grain values The grain being inspected is named os and the value soughtis FreeBSD Globbing is allowed here tooA less magical way to write the matching expression for FreeBSD would beosFreeBSD match grain freebsdThe choice is up to you but the notation expands cleanly to complex expressionsthat involve parentheses and Boolean operations Table lists most of the common matching types although a few have been omittedIf Table looks disturbingly complex take heart these are just options Realworldselectors look a lot more like our simple examplesIf youre wondering what all those grains or pillar values are that you can matchagainst its easy to find out Just use sudo salt minion grainsitemsor sudo salt minion pillaritemsto obtain a complete listYou can define named groups in the etcsaltmaster file Theyre called nodegroupsand they are useful for moving complex group selectors out of topsls files Howevertheyre not really a true grouping mechanism so much as a way to name patternsfor reuse As a result their behavior is a bit squirrelly They can only be defined interms of compoundtype selectors not for example by a simple list of clients unless you use an L clause and you must use an explicit match type of nodegroupto match against them Theres no global shorthand notationSalt statesSalt operations are called states As in Ansible theyre defined in YAML formatand in fact they look vaguely similar to Ansible tasks However the finegraineddetails are quite different You can include a series of state definitions in a sls fileStates are bound to specific minions in the topsls file at the root of the salt arm ofthe configuration base This file looks and functions exactly like the topsls file forvariable bindings see the examples on page Take a look at the following Salt version of the same example we worked throughwith Ansible starting on page we install sudo and create a corresponding sudogroup to which we assign administrators who should have sudo privileges We thencreate a group of administrator accounts each of which has its own UNIX group ofthe same name Finally we then copy in a sudoers file from the configuration baseAs it happens we can use exactly the same variable file for Salt that we used for Ansiblesudoerspath etcsudoersadmins username manny fullname Manny Calavera username moe fullname Moe Money Table Salt minion match typesCode Target Match type match Examplea ID glob glob cloudexamplecomE ID regex pcre EnwwclinkdL ID list list LhostahostbhostcbG grain glob grain GdomainexamplecomE grain regex grainpcre EvirtualxenVMWareI pillar glob pillar IscalingtypeautoscaleJ pillar regex pillarpcre JserverclasswebdatabaseS IP address CIDR block ipcidr Sc compound compound compound not GosfamilyRedHata This is the default No matchtype code is needed or definedb Note the lack of spaces individual expressions cant include themc Codes are used to label individual termsTo make these definitions available to all minions we put them in the configurationbase at pillarexamplesls and add a binding to topslsbase exampleHeres a Salt version of the operationsinstallsudopackagepkginstalled name sudo refresh truecreatesudogroupgrouppresent name sudo for admin in pillaradmins creategroup adminusername grouppresent name adminusername createuser adminusername userpresent name adminusername gid adminusername groups wheel sudo fullname adminfullname endfor installsudoersfilefilemanaged name pillarsudoerspath source saltfilessudoers user root group wheel mode This version shows the operations in their most canonical form for easier comparison with the equivalent Ansible task list that starts on page We can make afew additional changes to clean things up a bit but first a look at this longer versionSalt and JinjaThe first thing to notice is that the file includes a Jinja loop delimited by and These delimiters are similar to and except that and do not returnvalues The contents of the loop are interpolated into the YAML file as many timesas the loop runsAlthough Jinja uses Pythonlike syntax YAML already owns the indentation in asls file so Jinja is forced to define blockending tokens such as endfor In straightPython blocks would normally be defined through indentationSalt defines only a rudimentary iteration construct in its basic YAML scheme seethe comments regarding names on page Conditionals and robust iterationhave to be provided by Jinja or by whatever template language the sls file is runthrough In fact Salt does not care about YAML either It just expands configuration files through a designated pipeline and consumes the final JSON outputwhich must be fully literalOn one hand this approach is clean Theres no conceptual ambiguity about whatsgoing on and its easy to examine an expanded sls file to make sure it means whatyou intended On the other hand it means youll be using Jinja to provide any logicrequired by your configuration The mix of templating code and YAML can easilybecome somewhat dazzling Its a bit like writing the logic of a web app using onlyHTML templatesSeveral rules of thumb can help keep Salt configurations tidy First Salt has usableand welldefined mechanisms for implementing variablevalue overlays Use theseto keep as much configuration as possible in the domain of data rather than codeMany examples in the Salt documentation use Jinja conditionals when they arentthe best solution for example The following sls file installs the Apache web serverwhich has different package names on different distributions apachepkgslsapachepkginstalled if grainsos RedHat name httpd elif grainsos Ubuntu name apache endif This variation could be dealt with more elegantly through the pillar apachepkgsls pillarapachepkg pkginstalled pillartopslsbase defaultsGosUbuntu ubuntu In fairness the examples are usually designed to illustrate some point other than general tidinessSee page forgeneral informationabout Python pillardefaultsslsapachepkg httpd pillarubuntuslsapachepkg apacheAlthough replacing one file with four might not initially seem like a simplificationits now an extensible and codefree system MultiOS environments will encountermany such variations and they can all be dealt with in one placeIf a value has to be dynamically calculated consider whether you can put the codeat the top of the sls file and simply memorialize it for later use in a variable Forexample another way to write the Apache package installation above would be set pkgname httpd if grainsos Ubuntu set pkgname apache endif pkgname pkginstalledThis at least has the advantage of separating the Jinja logic from the actual configurationIf you must intermix Jinja logic with YAML consider whether you can break outsome of the YAML segments into separate files You can then interpolate thesesegments as appropriate Once again the idea is simply to separate the code andYAML rather than alternating back and forth between themFor nontrivial calculations you can abandon YAML altogether and replace it withpure Python or with one of the Pythonbased DSLs that Salt includes by defaultSee the Salt documentation for renderers for more informationState IDs and dependenciesTo return to our sudo example from page here are its first two states againfor referenceinstallsudopackagepkginstalled name sudo refresh truecreatesudogroupgrouppresent name sudoYou can see that the individual states are not items in a list as they are in Ansiblebut rather the elements of a hash The hash key for each state is an arbitrary stringcalled the ID As usual with hashes IDs must be unique or theyll collideBut wait The potential domain for collisions is not just this particular file but theentire client configuration State IDs must be globally unique because Salt is eventually going to stuff them all together into one big hashIts a bit of a funny hash though because it preserves the order of keys In a standard hash keys emerge in random order when the hash is enumerated Thats theway that Salt used to work too and as a result all dependencies among states hadto be explicitly declared These days the hash preserves the order of presentation bydefault although that can still be overridden if explicit dependencies are declaredor if this behavior is turned off in the master fileTheres still some trickiness though In the absence of other constraints order ofexecution conforms to the original sls files However Salt still presumes that statesare not logically dependent on one another unless you say so If a state fails to execute Salt notes the error but then continues and runs the next stateIf you want a dependent state not to run if its ancestors fail you can declare thatexplicitly For examplecreatesudogroupgrouppresent name sudo require installsudopackageIn this configuration Salt wont try to create a sudo group unless the sudo packagewas successfully installedRequisites also come into play when ordering states from multiple files UnlikeAnsible Salt does not interpolate the contents of an include file at the point theinclude was encountered It simply adds the file to its toread list If multiple filesattempt to include the same source there is still be only one copy of the source inthe final assembly and the order of states might not be what you expected Inorderexecution is guaranteed only within a file if any states depend on externally definedoperations they must declare explicit requisitesThe requisite mechanism is also used to achieve an effect analogous to Ansiblesnotifications Actually a handful of alternatives to require are syntactically interchangeable with it but imply subtle shadings of behavior One of those watch isparticularly useful for doing things when another state makes changes to the systemFor example the following configuration sets the systems time zone and the arguments to be passed to ntpd when it starts up This configuration always makes surethat ntpd is running and configured to start at boot time In addition it restartsntpd if either the system time zone or the ntpd flags are updatedsettimezonetimezonesystem name AmericaLosAngelessetntpdoptsaugeaschange context filesetcrcconf lens shellvarslns changes set ntpdflags gntpdservicerunning enable true watch setntpdopts settimezoneState and execution functionsIn a sls file the names that appear directly under state IDs are the operations thosestates should run Some specific cases from our example scenario are pkginstalledand grouppresentThese names include both a module part and a function part Together theyare roughly analogous to an Ansible module name together with a state value Forexample Ansible uses a package module with statepresent for installing packages whereas Salt uses a dedicated pkginstalled function within the pkg moduleSalt makes a big whoopdedo of distinguishing operations that do things to targetsystems execution functions from those that idempotently enforce a particularconfiguration state functions State functions usually call their associated execution functions when they need to make changesThe general idea is that only state functions should be mentioned in sls files andonly execution functions should appear on command lines Salt primly enforcesthese rules sometimes to confusing effectState and execution functions live in separate Python modules but related modulesusually share the same name For example theres both a timezone state moduleand a timezone execution module There cant be any overlap in function namesbetween the two modules though because that would create ambiguity The endresult is that to set the time zone from a sls file you must use timezonesystemsettimezonetimezonesystem name AmericaLosAngeles Augeas is a tool that understands many different file formats and facilitates automated changes As evidenced by this line quoting issues in YAML can be subtleBut to set a minions time zone from the command line you use timezonesetzone sudo salt minion timezonesetzone AmericaLosAngelesIf you get it wrong and need to consult the documentation youll find the two halvesof timezone in different sections of the manual Its also not always clear from behavior exactly which type of function is which For example gitconfigset whichsets Git repository options is a state function but stateapply which idempotentlyenforces configurations is an execution functionUltimately you just have to know which functions are which and the contexts towhich they belong If you need to call a function from the wrong contextwhichis sometimes necessaryyou can use the adapter functions modulerun runs anexecution function from a state context and statesingle runs a state functionfrom an execution context For example the adapted timezone calls above would besettimezonemodulerun name timezonesetzone timezone AmericaLosAngelesand salt minion statesingle timezonesystem nameAmericaLosAngelesParameters and namesOnce again here are the first two states from page for referenceinstallsudopackagepkginstalled name sudo refresh truecreatesudogroupgrouppresent name sudoIndented under the name of each operation that is the modulefunction construction is its list of parameters In Ansible the parameters for an operation form onebig hash Salt wants them as a list with each entry prefaced by a dash More specifically Salt wants a list of hashes though theres typically only one key in each hashMost parameter lists include a parameter called name which is the standard labelfor the thing this operation is configuring Alternatively you can supply a list oftargets in a parameter called names For examplecreategroupsgrouppresent names sudo rvmIf you provide a names parameter Salt reruns the operation multiple times substituting one item from the names list into the name parameter on each pass Thisis a mechanical process and the operation itself is not aware of the iteration Its aruntime as opposed to parsetime operation much like Ansibles withitemsconstruction But because Jinja expansion has already completed theres no opportunity to base the values of other parameters on the name If you need to adjustmultiple parameters ignore names and just iterate with a Jinja loopSome operations can handle multiple arguments at once For example pkginstalledcan hand off multiple package names at once to the underlying OS package manager which may be useful for efficiency or dependency resolution Because Salt hidesnames iteration such operations are forced to use a separate parameter name toenable bulk operations For example the statesinstallpackagespkginstalled names sudo curl andinstallpackagespkginstalled pkgs sudo curl both install sudo and curl The first version does it in two distinct operations andthe second does it in oneWe stress this seemingly minor point because its easy to make mistakes with namesBecause its mechanical names iterates even operations that pay no attention to thename parameter On reviewing the Salt log youll see that multiple executions haverun successfully but somehow the target system still doesnt seem to be properlyconfigured So its helpful to understand exactly whats going onIf you dont specify an explicit name for a state Salt copies the state ID to this fieldYou can use this behavior to simplify state definitions a bit For examplecreatesudogroupgrouppresent name sudobecomessudogrouppresentor even justsudo grouppresentYAML doesnt allow hash keys without values so now that grouppresent no longer has any listed parameters it has to become a simple string instead of a hash keywith a parameter list as a value Thats fine Salt checks for this explicitlyThe shorthand style is usually clearer than the long form A separate ID field cantheoretically serve as a comment or an explanation but most IDs seen in the wildsimply restate behavior that is already obvious If you want comments add commentsThe shorthand form has a potential problem though since state IDs must be globally unique short IDs named for common system entities are more vulnerable toID collisions Salt detects and reports conflicts so this is really more an annoyancethan a serious issue But if youre writing a Salt formula with the intention of reusing it in several configuration bases or you are planning to share it with the Saltcommunity stick with IDs that are less likely to clashSalt allows several operations to be included in a single state Since the two operationsabove share a name field we can combine them into a single state without havingto state any explicit names However theres yet another YAML snare awaiting ussudopkginstalled refresh truegrouppresent The value of the sudo key now has to be a hash it cant be a hash with the stringgrouppresent somehow tacked on Accordingly we now have to treat grouppresentas a hash key and provide an explicit parameter list as a value even though that listis empty Thats true even if we drop the refresh parameter from pkginstalledsudopkginstalled grouppresent Just as we collapsed these two states we can collapse our two states that do user account management A more idiomatic version of the state list from page is thussudopkginstalled grouppresent for admin in pillaradmins adminusername grouppresent userpresent gid adminusername groups wheel sudo fullname adminfullname endfor pillarsudoerspath filemanaged source saltfilessudoers user root group wheel mode State binding to minionsAs it happens theres not much more to say about Salt state bindings They workexactly like pillar bindings Theres a topsls file at the root of the state hierarchyand it maps minion groups to state files Heres a skeletal examplebase bootstrap sitebaseGosUbuntu ubuntuGwebserver nginx webappsIn this configuration all hosts apply states from bootstrapsls and sitebasesls fromthe root of the state hierarchy Ubuntu systems also run ubuntusls and web servers that is minions that have a toplevel webserver entry in their grains databasesrun states to configure NGINX and local web appsOrder in topsls corresponds to the general order of execution on each minion Butas usual explicit dependency information within states overrides the default orderHighstatesSalt refers to the bindings in topsls as a minions highstate You activate thehighstate by telling the minion to run the stateapply function with no arguments sudo salt minion stateapplyThe statehighstate function is equivalent to stateapply with no argumentsYoull see both forms usedEspecially when debugging new state definitions you might want a minion to runonly a single state file Thats easily accomplished with stateapply sudo salt minion stateapply statefileLeave out the sls suffix on the state file name Salt will add it Also keep in mindthat the path to the state file has nothing to do with your current directory Its always interpreted relative to the state root as defined in the minions configurationfile This command does not redefine the minions highstate in any way it simplyruns the specified state fileThe salt command accepts a variety of flags for targeting different sorts of miniongroups but its easiest to just remember C for compound and use one of theshorthands from Table on page Theres a bit of potential terminological confusion in that Salt also uses highstate to mean a parsedand assembled JSON tree of states which it then processes to form a lowstatealso a JSON treewhich is the lowlevel input to the execution engineFor example to highstate all Red Hat minions sudo salt C GosRedHat statehighstateThe default match type is ID globbing so the command sudo salt statehighstateis the command for validate the entire sites configurationIn keeping with Salts minioncentric execution model all parallel executions beginsimultaneously and minions do not report back until they have completed executionThe salt command prints each minions results as soon as it receives them There isno way to display incremental results while a state file is executingIf you have lots of minions or a complex configuration base the salt commandsdefault output can be quite a lot to look through because it reports on every operation considered by every minion Add the option stateoutputmixed to reducethis output to one line for operations that succeed and cause no changes The optionstateverbosefalse suppresses output for nochange operations entirely but saltstill prints a header and summary for each minionSalt formulasSalt calls its bundles formulas well formula really Like Ansible roles theyrejust a directory of files although Salt formulas have an outer wrapper that includessome metadata and versioning information as well In actual use you just need theinner formula directoryFormula directories go in one of the salt roots defined in the master file If youwant you can create a root just for formulas Formulas sometimes include examplepillar data but youre responsible for installing that yourselfSalt does nothing special to support formulas except that if you name a directory ina topsls file or include statement Salt looks for an inityml file within that directory and reads that This convention provides a clear default path into the formulaMany formulas also include standalone states that you can reference by specifyingboth the directory and filenameNothing in Salt can be included in a configuration more than once and that includesformulas You can make multiple inclusion requests but theyll be coalesced As a result formulas cannot be instantiated multiple times in the way that Ansible roles canIt doesnt matter anyway because Salt defines no way to pass parameters to a formulaother than by putting variable values in the pillar Jinja expressions can set the values of variables but those settings exist only within the context of the current fileTo simulate the effect of invoking a formula repeatedly you can supply pillar datain the form of a list or hash that the formula can iterate through on its own However the formula must be explicitly written with this structure in mind You cantimpose it after the factThe central Salt repository for communitycontributed formulas is currently justGitHub Look for the username saltformulas Each formula is a separate projectEnvironmentsSalt makes several gestures toward explicit support for environments eg theseparation of development test and production universes Unfortunately its environment facilities are somewhat peculiar and they dont map straightforwardly to the most common realworld use cases Its possible to get environments upand running with a little bit of determination and a tube of Jinja glue but we findthat in practice many sites simply punt and run separate master servers for eachenvironment instead This jibes well with security and compliance standards thatrequire separation of environments at the network layerAs we saw back on page the etcsaltmaster file enumerates the various placeswhere configuration information can be stored It also associates an environmentwith each set of pathsfilerootsbase srvsaltpillarrootsbase srvpillarHere srvsalt and srvpillar are the state and pillar root directories for the defaultenvironment called base For simplicity we have omitted mention of pillar datain the discussion below environment management works the same way for botharms of the configuration baseSites with more than one environment will typically add an additional layer to theconfiguration directory hierarchy to represent that factfilerootsbase srvbasesaltdevelopment srvdevelopmentsaltproduction srvproductionsaltEvidently these example cowboys have no test environment Dont try this at homeAn environment can list multiple root directories If theres more than one theserver transparently merges their contents However each environment performsa separate merge and the final results remain segregatedInside topsls files the bindings that associate minions to particular states andpillar files toplevel keys are always environment names So far weve only seenSee page formore informationabout environmentsexamples that used the base environment but of course any valid environment cango in this spot For examplebase globaldevelopmentdev webserver databaseproductionwebprod webserverdbprod databaseThe exact import of an environments appearance in a topsls file depends on howyouve configured Salt In all cases environments must already be defined in themaster file top files cannot create new environments In addition state files arerequired to originate from the environment context in which they are mentionedBy default Salt does not associate minions with any particular environment andminions can receive state assignments from any or all of the environments in topslsIn the snippet above for example all minions run the globalsls state from the baseenvironment Depending on their IDs individual minions may also receive statesfrom the production or development environmentsThe Salt documentation encourages this way of configuring environments but wehave some reservations One potential issue is that minions end up as frankenserversthat draw configuration elements from multiple environments You cant trace anygiven minions configuration back to one particular environment at one particularpoint in time because every minion has multiple parentsThis distinction is important because a single base environment must be sharedamong all other environments Which one should it be The development versionof the base environment The production version A completely separate and stagedconfiguration base Exactly when should you migrate the base environment to anew releaseTheres also some additional complexity lurking under the covers Each environment is a fullfledged Salt configuration hierarchy so it can in theory have its owntopsls file Each of those topsls files can in theory refer to multiple environmentsWhen confronted with this situation Salt tries to merge all the top files into one When you set a minions ID to match the development or production pattern you are functionally associating it to the corresponding environment However Salt itself does not make an explicit associationat least not in this configurationcomposite frankenconfiguration Environments can demand the execution ofone anothers statesstates that they dont own control or know anything aboutIt would be horrifying if it werent so sillyIts not clear exactly what use cases this architecture attempts to enable Althoughtop file merging is the default behavior the docs repeatedly warn you away fromsetting things up this way Instead youre encouraged to designate a single topslsfile most likely in base to control all environmentsIf you do that though it soon becomes apparent that theres some organizationalfriction between this external top file and the rest of the environments The topfile is an integral part of an environments configuration so states and top files arenormally codeveloped a change to one often requires changes to the other Witha separate top file you must effectively separate each environment into two piecesthat must be manually kept synchronized with each other In addition the mastertop file must be shared with synchronized with and compatible with all other environments When you promote the test environment to production for exampleyou must make sure the master topsls is adjusted to reflect the proper settings forthat specific version of the new production releaseAlternatively you can hardwire minions to a given environment either by settingthe value of environment in the minions etcsaltminion file or by including theflag saltenvenvironment on salt command lines Under this regime a minion seesonly the topsls file of its assigned environment Within that top file its view is alsofurther limited to entries that appear under that environmentFor example a machine pinned to the development environment might see thetopsls file from page in the following abbreviated form assuming that thetopsls file was found at the root of the development state treedevelopmentdev webserver databaseThis mode of operation is quite a bit closer than the default to the traditional concept of environments There can be no unintended crosstalk among environmentswhich limits the potential for unintended behavior It also has the advantage that asa particular version of the configuration base is promoted through the environmentchain different portions of the topsls file automatically apply themselves to clientsThe main disadvantage is that you lose the ability to factor out parts of the configuration that are common to more than one environment Theres no builtin wayto see outside the context of the current environment so elements of the baselineconfiguration must be replicated into every environment Merging occurs at the YAML level though so youd better hope that multiple top files dont try to assign states to the same matching pattern within the same environment If they do some states will besilently discarded OopsRewritten to work in the context of this approach the topsls file from page would look something like thisdevelopment globaldev webserver databaseproduction globalwebprod webserverdbprod databaseThe base environment itself is now vestigial so weve dropped it from the topslsfile and copied that keys former contents directly into the development and production environmentsKeep in mind that were now operating in a world where every environment treehas its own topsls file For this example we assume that the topsls file hasnt diverged between the two environments so the same contents would appear in bothcopies of topslsOf course manually reproducing the elements of the common configuration insideeach environment is prone to error A better option is to define the common configuration as a Jinja macro so that it can automatically be repeated macro baseline global endmacro development baseline dev webserver databaseproduction baseline webprod webserverdbprod databaseWere assuming in this scenario that all minions are pinned to specific environmentsso we can now potentially remove the environment indicators from minion IDsHowever its a good idea to retain them for securityThe issue is that minions control their own environment settings If a minion in thedevelopment environment were compromised for example it could declare itselfto be a production server and potentially gain access to the keys and configurationsused in the production environment This is perhaps one reason why the Saltdocumentation seems a bit skittish about recommending environment pinningMaking environmentspecific configurations contingent on both the environmentsettings and the minion IDs protects against this line of attack If a minion changesits ID the master no longer recognizes it as an approved client and ignores it untilan administrator approves the change with the saltkey commandIf you prefer not to use IDs in this way an alternative is to use pillar data as a crosscheck Whatever you do you cant just drop the suffix and turn dev into because the shared portion of the configuration already uses as a key Duplicatepatterns within an environment are a YAML violationWhen debugging environments youll find a couple of execution functions especially helpful configget shows the value that a particular minion or set of minions is using for a configuration option sudo salt newclientdev configget environmentnewclientdevdevelopmentHere we can see that the minion with ID newclientdev has been pinned to thedevelopment environment just as its ID would suggest To see what the topslsconfiguration looks like from that minions perspective use stateshowtop sudo salt newclientdev stateshowtopnewclientdevdevelopment global webserver databaseThe output shows only the states that are active and selected for the target minionIn other words they are the states that would run if you invoked statehighstateon that minionNote that all the displayed states come from the development environment Becausethe minion is pinned that will always be the caseDocumentation roadmapSalts documentation docssaltstackcom will likely earn your admiration butperhaps only after a period of frustration The main sticking point is that topics arenested several layers deep but the headings at the top two layers do not necessarily The issue isnt really state configuration since minions have free access to all state files The problemis with pillar data which is assembled on the master side and should generally be kept securehint at what youll find at layer three The Architecture section for example containsno information about Salts architecture its really about multiserver deploymentsSome of the most useful reference material lies within sections that are organizedas scenarios or tutorials Fronttoback reading can sometimes evoke a dying sysadmins fever dream themes cyclically loom and recede without fully resolvingOnce in a while youll experience a moment of lucidity in which to appreciate theseverity of your conditionSome pointers The toplevel Using Salt section is an overview by concept and most ofConfiguration Management is labeled as a tutorial Because of their formats these sections look like supplemental materials But thats not truethey are pretty much the primary documentation for the material theycover Dont skip The best reference information is beneath State System Reference underConfiguration Management A lot of the stuff in here is not important fora first reading but Highstate data structure definitions Requisites and other global state arguments and The top file are particularly worth readingThe top file is also the authoritative documentation for environments The docs youll use most frequentlythe ones covering state and executionfunctionsare concealed under Salt Module Reference and camouflagedamong other module types that are of interest mostly to module developers Bookmark the sections for Full list of builtin state modules andFull list of builtin execution modules Ansible and Salt comparedWe like both Ansible and Salt Each of them has some friction points however andwe recommend them for different environments The sections below comment ona few of the factors you might consider when choosing between themDeployment flexibility and scalabilitySalt covers a broader range of deployment environments than does Ansible Itssimple enough that you can reasonably use it to manage a single server but it alsoscales effortlessly and essentially without limit If you want to learn one system thatcovers the broadest possible range of use cases Salt is a good choiceIn part thats because Salts architecture makes relatively few demands of the masterserver Minions receive their instructions and dont report back until theyre donewith all status information being reported at once Minions call the server to obtain configuration data but aside from serving pillar data the server itself performsrelatively little computationOnce your site outgrows a single Salt master you can convert your infrastructureto a tiered or replicated server scheme We dont cover those options in this bookbut theyre easy to set up and work wellLarge deployments are a comparative weak spot for Ansible It does include somefeatures to help you implement multitier server systems but the transition to thismodel is not as transparent as it is in SaltAnsible is an order of magnitude slower than Salt and because of its architecture itmust handle clients in batches However most servers can handle far more than thedefault simultaneous clients You can also change Ansibles execution strategy sothat clients arent kept in strict lockstep with each other Even a tuned Ansible systemwont approach the speed of Salt but its better than one might navely anticipateBuiltin modules and extensibilityBakeoffs of configuration management software sometimes attempt to compare thenumber of operation types that various systems support out of the box Howeverthese comparisons are hard to get right because of underlying structural differencesFunctions that are spread across several modules in Ansible might be addressedby one in Salt for example An atomic operation in one system may correspond toseveral operations in anotherAt present Salt and Ansible are roughly comparable in this respect In addition toextensive standard libraries both systems have a structure in place for absorbingcommunitywritten modules into the core or an easily accessible addon packIn any event total module count doesnt matter nearly as much as coverage of thesystems and software your site is actually using All CM systems cover basic operations pretty well but as you move into the long tail offerings vary dramaticallyIts likely that youll eventually want to tackle some tasks for which your CM system doesnt have an offtheshelf solution Fortunately Salt and Ansible are botheasy to extend with your own Python code Embrace this extensibility early on andmake it part of your repertoireSecurityAs outlined in Ansible access options starting on page Ansible can be madealmost arbitrarily secure The only limit to security is your own willingness to retype passwords and deal with security red tapeAnsibles vault system lets you keep configuration data in an encrypted formatThats actually a pretty big deal because it means that neither the Ansible servernor the configuration base needs to be particularly secure Salts modular architecture probably makes this an easy feature to add but it doesnt come in the boxBy contrast Salt can only be as secure as the root account on the master serverAlthough the master daemon itself is simple to set up the server on which it runsshould receive your sites most aggressive securement Ideally the master shouldbe a machine or virtual server dedicated to this taskIn practice administrators hate intrusive security protocols as much as anyoneelse does Most realworld Ansible installations have relatively lax security Justas Ansible can be made arbitrarily secure it can also be made arbitrarily insecureEven if you strive to keep Ansible fully secured you may have trouble maintaining this approach once your site grows beyond the point at which configurationmanagement can be handled by an administrator typing commands in a terminalwindow Nothing that runs out of cron for example can depend on the presenceof an administrator to enter passwords Working around that constraint inevitablyends up lowering security to the level of the root accountThe bottom line on security is that Ansible gives you both more options and moreopportunities to shoot yourself in the foot Its more securable but that doesnt necessarily mean that its more secure Either system is fine for the average site Keepyour own needs and constraints firmly in mind when evaluating these systemsMiscellaneousTable below and Table on the next page summarize some of the additional strengths and weakness of both Ansible and SaltTable Ansible pros and consAdvantages DisadvantagesRequires only SSH and Python no daemons Very slowClear and concise documentation Serverheavy harder to scaleBuiltin loops and conditionals minimal Jinja Lots of files with identical namesWorks fine as a nonroot user Idiosyncratic YAML syntaxOperations can use each others output Handmanaged client inventoryClean and flexible use of config directories Minimal grouping facilitiesSecure general encryption facility Many different variable scopesRoles can be instantiated repeatedly No daemons means fewer optionsLarger user base than Salt No real support for environments Best practicesIf youve worked on a software project you might find many of the issues addressedby configuration management systems to be familiar from the development worldDevelopment environments encompass many of the same vagaries multiple platforms multiple products derived from the same code base multiple types of buildsTable Salt pros and consAdvantages DisadvantagesFast Relies heavily on JinjaAt heart simpler than Ansible Documentation oddly organizedFlexible consistent bindings Poor support for nonroot useIntegrated support for cloud servers Formulas cant be instantiatedConcise configuration syntax No builtin encryption solutionMultitier server deployments No access to results of operationsStructured event monitoring Minimal support for variable value defaultsExecution logs easily exported Requires explicit dependency declarationsFanatically modularized Fanatically modularizedand configurations and deployment through successive steps of development testing and productionThese are complex issues and development environments are only tools Developersuse a variety of additional controlsdevelopment guidelines design reviews coding standards internal documentation and clear architectural boundaries amongothersto limit the slide toward entropyUnfortunately administrators often wander into configuration management territory without a proper suit of developers armor At first glance configurationmanagement seems deceptively straightforward like a slightly more general andsophisticated way to approach routine scripting tasks Configuration managementvendors work hard to reinforce this impression Their web sites are siren songs ofease and grace each one features a tutorial in which you deploy a web server byrunning ten lines of configuration codeIn reality the edge of the abyss may be closer than it seems particularly when multiple administrators contribute to the same configuration base over time Realworldspecifications for even a singlepurpose server run to hundreds of lines of codesegmented into multiple different functional roles Without coordination its easyto turn the CM system into a muddle of conflicting or parallel codeBest practices vary by configuration management system and by environment buta few rules apply to most situations Keep the configuration base under version control This isnt a best practice so much as a basic requirement for CM sanity Not only does Gitprovide change tracking and history but it has already solved many ofthe mechanical problems involved in coordinating projects across administrative boundaries Configuration bases are inherently hierarchical at least in a logical senseSome standards apply sitewide some apply to every server in a particular department or region and some are specific to particular hosts Inaddition youll most likely need the ability to make exceptions in certaincases Depending on your sites operations you might also need to maintain multiple independent hierarchiesPlan for all of this structure in advance and consider how you mightmanage scenarios in which different groups control different parts of theconfiguration base At the very least conventions for classifying hostseg EC instances Internetfacing hosts database servers should becoordinated sitewide and adhered to consistently CM systems allow different parts of the configuration base to be kept indifferent directories or repositories However this structure provides little actual benefit and it complicates daytoday configuration work Werecommend one big integrated configuration base Manage hierarchy andcoordination at the Git layer Sensitive data keys passwords should not be put under version controlunless encrypted even in private code repositories Git in particular isnot designed to maintain security Your CM system may have some encryption features built in but if not roll your own Because it effectively has root access to many other hosts a configurationserver is one of the most concentrated sources of security risk in your organization Its reasonable to dedicate a server to this role and it shouldreceive your most stringent security hardening Configurations should run without reporting spurious changes Scriptsand shell commands are usually the biggest sticking points Check yourCM systems documentation for advice on this topic as its one of the mostfrequent issues that users encounter Dont test on production servers But do test Its easy to spin up a testsystem in the cloud or in Vagrant Chef even provides an elaborate testing and development system in the form of Kitchen Make sure your testsystem matches your real systems by using the same machine image andnetwork configuration Read the code for addon bundles that you obtain from public repositoriesIts not that these sources are particularly suspicious its just that systemsand conventions differ widely In many cases youll find that a few localtweaks are needed If you can bypass the CM systems package managerand clone bundles directly from a Git repo then you can easily upgradeto later releases without losing your customizations Subdivide configurations ruthlessly Every file should have a clear andsingle purpose Ansible users might want to select a text editor that dealswell with different files all named mainyml Configurationmanaged servers should be managed That is thereshould be no penumbra of administrative work that was performed byhand and that no one knows how to replicate This issue appears primarilywhen moving existing servers onto configuration management Do not allow yourself or your team to temporarily disable the CM system on a node or to use a heavyhanded method of overriding the CMsystem for example by setting the immutable attribute on a configuration file that is typically under CM control These changes are inevitablyforgotten and confusion or outages ensue Its not hard to open a gateway from existing administrative databases intoa configuration management system and theres a lot of value in doing soCM systems are designed for this kind of interfacing For example youmight identify system administrators and their zones of activity in yoursitewide LDAP database and make this information available within theconfiguration management environment through gateway scripts Ideallyevery piece of information should have a single authoritative source CM systems are excellent for managing the state of a machine They arenot intended for stateful coordinated activities such as software deployment operations although the documentation and even some examplesmight lead you to believe that they are In our experience a dedicatedcontinuous deployment system is more suitable In elastic cloud environments where computational capacity is added inresponse to realtime demand the time that it takes for a new node tobootstrap through configuration management can be agonizingly slowOptimize by including packages and longrunning configuration itemswithin the baseline machine image rather than downloading and installing them at boot timeIf you use configuration management to set configuration parameters foran application make sure that that step comes early in the bootstrappingprocess so that the application comes online more quickly We try to limitthe CM run time to less than seconds for dynamically scaled nodes As an administrator working with a CM system you will allot much ofyour time to writing CM code testing changes against a representative setof systems committing the updates to a repository and applying changesto your site in a staged fashion To be most effective you should perfectthis process by investing time up front to learn best practices and tricksfor your system of choice When converting an existing snowflake server to configuration management you may find it usefulto clone the original system for use as a basis for comparison It can take multiple cycles of configuration management and testing to home in on all the systems particulars Recommended readingCowie Jon Customizing Chef Getting the Most Out of Your Infrastructure Automation Sebastopol CA OReilly Media Frank Felix and Martin Alfke Puppet Essentials nd Edition BirminghamUK Packt Publishing Geerling Jeff Ansible for DevOps Server and configuration management for humans St Louis MO Midwestern Mac LLC This book is focused mostly onbasic Ansible wrangling but it does include some helpful material about combiningAnsible with specific systems such as Vagrant Docker and JenkinsHochstein Lorin Ansible Up and Running nd Edition Sebastopol CA OReillyMedia Like Ansible for DevOps this book covers both the Ansible basics andinteractions with common environments such as Vagrant and EC Some highlightsare the inclusion of a largerscale example configuration a chapter on writing yourown Ansible modules and tips on debuggingMorris Kief Infrastructure as Code Managing Servers in the Cloud SebastopolCA OReilly Media This book includes few specifics about configurationmanagement per se but its helpful for understanding how configuration management integrates into the larger scheme of DevOps and structured administrationSebenik Craig and Thomas Hatch Salt Essentials Sebastopol CA OReillyMedia This is a short and rather skeletal book that sticks pretty closely tothe basics of Salt Its not the style of book wed ordinarily recommend but giventhe unevenness of the official documentation its a potentially useful reference forthose who seek a second opinionTaylor Mischa and Seth Vargo Learning Chef A Guide to Configuration Management and Automation Sebastopol CA OReilly Media Uphill Thomas and John Arundel Puppet Cookbook rd Edition Birmingham UK Packt Publishing Server virtualization makes it possible to run multiple operating system instancesconcurrently on the same physical hardware Virtualization software parcels outCPU memory and IO resources dynamically allocating their use among severalguest operating systems and resolving resource conflicts From the users point ofview a virtual server walks and talks like a fullfledged physical serverThis decoupling of hardware from the operating system affords numerous luxuriesVirtualized servers are more flexible than their bare metal kin Theyre portableand can be programmatically managed The underlying hardware is used moreefficiently because it can service multiple guests simultaneously And if that isntenough virtualization technology underpins both cloud computing and containersImplementations of virtualization have changed over the years but the core conceptsare not new to the industry Big Blue used virtual machines on early mainframeswhile researching timesharing concepts in the s The same techniques wereused throughout the mainframe heyday of the s until the clientserver boomof the s when the difficulty of virtualizing the Intel x architecture led to ashort period of relative dormancyThe evergrowing size of server farms rekindled interest in virtualization for modernsystems VMware and other providers conquered the challenges of x and made VirtualizationSee Chapter formore informationabout containersit easy to automatically provision operating systems These facilities eventually ledto the rise of ondemand Internetconnected virtual servers the infrastructure wenow know as cloud computing More recently advances in OSlevel virtualizationhave ushered in a new era of OS abstraction in the form of containersIn this chapter we begin by clarifying the terms and concepts you need in orderto understand virtualization for UNIX and Linux We then introduce the leadingvirtualization solutions used on our example operating systems Virtual vernacularThe terminology used to describe virtualization is somewhat opaque largely because of the way the technology evolved Competing vendors worked independentlywithout the benefit of standards yielding a bewildering array of ambiguous phrasesand acronymsTo further confuse the issue virtualization itself is an overloaded term that describes more than the scenario described above in which guest operating systemsrun within the context of virtualized hardware OSlevel virtualizationmore commonly referred to as containerizationis a related but distinct set of facilities thathas become just as ubiquitous as server virtualization For those without handsonexposure to these technologies it can often be difficult to grasp the differences Wecontrast the two approaches later in this sectionHypervisorsA hypervisor also known as a virtual machine monitor is a software layer thatmediates between virtual machines VMs and the underlying hardware on whichthey run Hypervisors are responsible for sharing system resources among the guestoperating systems which are isolated from one another and which access the hardware exclusively through the hypervisorGuest operating systems are independent so they neednt be the same CentOScan run alongside FreeBSD and Windows for example VMware ESX XenServerand FreeBSD bhyve are examples of hypervisors The Linux kernelbased virtualmachine KVM converts the Linux kernel into a hypervisorFull virtualizationThe first hypervisors fully emulated the underlying hardware defining virtual replacements for all the basic computing resources hard disks network devices interrupts motherboard hardware BIOSs and so on This mode called full virtualization runs guests without modification but incurs a performance penalty becausethe hypervisor must constantly translate between the systems actual hardware andthe virtual hardware exposed to guests Conways Law comes to mind Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizationsSimulating an entire PC is a complex task Most hypervisors that offer full virtualization separate the task of maintaining multiple environments virtualizationfrom the task of simulating the hardware within each environment emulationThe most common emulation package used in these systems is an open source project called QEMU You can find more information at qemuorg but in most casesthe emulator doesnt require much attention from administratorsParavirtualizationThe Xen hypervisor introduced paravirtualization in which modified guest operating systems detect their virtualized state and actively cooperate with the hypervisor to access hardware This approach improves performance by an order ofmagnitude or more However guest operating systems need substantial updates torun this way and the exact modifications depend on the specific hypervisor in useHardwareassisted virtualizationIn and Intel and AMD introduced CPU features Intel VT and AMDVthat facilitate virtualization on the x platform These extensions gave rise tohardwareassisted virtualization also known as accelerated virtualization In thisscheme the CPU and memory controller are virtualized by the hardware albeitunder the control of the hypervisor Performance is very good and guest operating systems need not know that theyre running on a virtualized CPU These dayshardwareassisted virtualization is the assumed baselineAlthough the CPU is a primary point of contact between the hardware and guestoperating systems it is only one component of the system The hypervisor stillneeds some way to present or emulate the rest of the systems hardware Eitherfull virtualization or paravirtualization can be used for this task In some cases amix of approaches is used it depends on the virtualizationawareness of the guestParavirtualized driversOne great advantage of hardwareassisted virtualization is that it largely restrictsthe need for paravirtualization support to the level of device drivers All operatingsystems allow addon drivers so setting up a guest with paravirtualized disk drivesdisplay cards and network interfaces is as simple as installing the appropriate driversThe drivers know the secret handshake that lets them connect with the hypervisorsparavirtualization support and the guest OS remains none the wiserA few pesky aspects of the PC architecture such as the interrupt controller and theBIOS resources fall into the domain of neither the CPU nor the device drivers Inthe past the predominant approach has been to implement these remaining components through full virtualization For example Xens HVM Hardware VirtualMachine mode combines support for CPUlevel virtualization extensions with acopy of the QEMU PC emulator And PVHVM ParaVirtualized HVM mode addsto this scheme paravirtualized drivers on guest operating systems greatly reducingthe amount of full virtualization needed to keep the system running However thehypervisor still needs an active copy of QEMU for each virtual machine so that itcan cover the odds and ends not addressed by the paravirtualized driversModern virtualizationThe most recent versions of Xen and other hypervisors have more or less eliminatedthe need to emulate legacy hardware Instead they rely on CPUlevel virtualizationfeatures paravirtualized guestOS drivers and a few additional sections of paravirtualized code within guest kernels Xen calls this mode PVH ParaVirtualizedHardware and its considered to be a closetoideal blend that yields optimal performance but imposes the lowest possible requirements on guest kernelsYou might encounter any of the varieties of virtualization described above in the wildor when reading documentation However its not worth memorizing any particular taxonomy or worrying too much about virtualization modes The boundariesamong these modes are porous and the hypervisor generally works out the bestoptions for a given guest If you keep your software updated you automatically benefit from the latest enhancements The only reason to choose anything other thanthe default operating mode is to support older hardware or ancient hypervisorsType vs type hypervisorsMany reference materials draw a somewhat dubious distinction between type and type hypervisors A type hypervisor runs directly on the hardware without a supporting OS and for that reason is sometimes called a baremetal or native hypervisor Type hypervisors are userspace applications that run on top ofanother generalpurpose OS Exhibit A depicts these two modelsExhibit A Comparison of type and type hypervisorsGuestOSGuestOSGuestOSMemoryHardwareIO CPUsType hypervisorGuestOSGuestOSGuestOSMemoryHardwareIO CPUsHost OSType hypervisorType Type VMware ESXi and XenServer are considered type and FreeBSDs bhyve is type Likewise workstationoriented virtualization packages such as Oracles VirtualBoxand VMware Workstation are also type Its true that type and type systems are different but the delineation is not alwaysso binary KVM for example is a Linux kernel module that gives virtual machinesdirect access to CPU virtualization features Differentiating among types of hypervisor is more an academic exercise than a point of practiceLive migrationVirtual machines can move between hypervisors running on different physicalhardware in real time in some cases without interruptions in service or loss of connectivity This feature is called live migration The magic lies in a memory dancebetween the source and target hosts The hypervisor copies changes from the sourceto the destination and as soon as the memory is identical between the two themigration completes Live migration is helpful for highavailability load balancingdisaster recovery server maintenance and general system flexibilityVirtual machine imagesVirtual servers are created from images which are templates of configured operating systems that a hypervisor can load and execute The image file format variesby hypervisor Most hypervisor projects maintain a collection of images that youcan download and use as a basis for your own customizations You can also take asnapshot of a virtual machine to create an image either as a backup of importantdata or to use as the basis for creating more virtual machinesBecause the virtual machine hardware presented by the hypervisor is standardizedimages are portable among systems even if their actual hardware differs Imagesare specific to a particular hypervisor but conversion tools that port images amonghypervisors are availableContainerizationOSlevel virtualizationor containerizationis a different approach to isolationthat does not use a hypervisor Instead it relies on kernel features that isolate processes from the rest of the system Each process container or jail has a privateroot filesystem and process namespace The contained processes share the kerneland other services of the host OS but they cannot access files or resources outsideof their containers Exhibit B illustrates this architectureBecause it does not require virtualization of the hardware the resource overheadof OSlevel virtualization is low Most implementations offer nearnative performance This type of virtualization precludes the use of multiple operating systemsSee Chapter formore informationabout containersExhibit B ContainerizationMemoryHardwareIO CPUsJailed processesJailed processesJailed processesHost OS kernelUser spacehostprocesseshowever because the host kernel is shared by all containers Linuxs LXC Dockercontainers and FreeBSD jails are implementations of containersIts easy to confuse containers with virtual machines Both define portable isolatedexecution environments and both look and act like full operating systems with rootfilesystems and running processes Yet their implementations are entirely differentA true virtual machine has an OS kernel an init process drivers to interact withhardware and the full trappings of a UNIX operating system A container on theother hand is merely the facade of an operating system It uses the strategies described above to give individual processes a suitable execution environment Table on the next page illustrates some of the practical differencesIts common to use containers in combination with virtual machines Virtual machines are the best way to subdivide physical servers into manageable chunks Youcan then run applications in containers atop the VMs to achieve optimal systemdensity this procedure is sometimes called bin packing The containersonVMsarchitecture is standard for containerized applications that need to run on publiccloud instancesWe focus on true virtualization for the rest of this chapter See Chapter Containers for more details on containerization Virtualization with LinuxXen and KVM are the leading open source virtualization projects for Linux Xennow a project of the Linux Foundation powers some of the largest public cloudsincluding Amazon Web Services and IBMs SoftLayer KVM is the kernelbasedvirtual machine integrated into the mainline Linux kernel Both Xen and KVM havedemonstrated their stability through many production installations at large sites This is not entirely true FreeBSDs Linux emulation layer permits Linux containers on FreeBSD hostsTable Comparing virtual machines with containersVirtual machine ContainerA fullfledged OS that shares underlyinghardware through a hypervisorAn isolated group of processes managedby a shared kernelRequires a complete boot procedure toinitialize starts in minutesProcesses run directly by the kernel noboot required starts in secondLonglived Frequently replacedHas one or more dedicated virtual disksattached through the hypervisorFilesystem view is a layered constructdefined by the container engineImages measured in gigabytes Images measured in megabytesA few dozen or fewer per physical host Many per virtual or physical hostComplete isolation among guests OS kernel and services shared with hostMultiple independent operating systemsrunning side by sideMust run the same kernel as the host OSdistribution may differXenInitially developed by Ian Pratt as a research project at the University of Cambridgethe Linuxfriendly Xen has grown to become a formidable virtualization platformthat challenges even the commercial giants in terms of performance security andespecially costAs a paravirtual hypervisor Xen claims a mere overhead far less thanfully virtualized solutions Because Xen is open source a variety of managementtools are available with varying levels of feature support The Xen source code isavailable from xenprojectorg but many Linux distributions include native supportXen is a baremetal hypervisor that runs directly on the physical hardware A running virtual machine is called a domain There is always at least one domain referred to as domain zero or dom Dom has full hardware access manages theother domains and runs all the hypervisors own device drivers Unprivileged domains are referred to as domUDom typically runs a Linux distribution It looks just like any other Linux systembut includes the daemons tools and libraries that complete the Xen architectureand enable communication among domU dom and the hypervisorThe hypervisor is responsible for CPU scheduling and memory management for thesystem as a whole It controls all domains including dom However the hypervisoritself is in turn controlled and managed from dom What a tangled web we weaveTable lists the most interesting puzzle pieces of a Linux domTable Xen components in domPath Contentsetcxen Primary configuration directoryauto Guest OS config files to autostart at boot timescripts Utility scripts that create network interfaces etcvarlogxen Xen log filesusrsbinxl Xen guest domain management toolEach Xen guestdomain configuration file in etcxen specifies the virtual resourcesavailable to a domU including disk devices CPU memory and network interfacesEach domU has a separate configuration file The format is flexible and gives administrators granular control over the constraints applied to each guest If a symboliclink to a domU configuration file is added to the auto subdirectory that guest OSis automatically started at boot timeXen guest installationIt takes several steps to get a guest server up and running under Xen We recommendthe use of a tool such as virtmanager virtmanagerorg to simplify the processvirtmanager was originally a Red Hat project but it has now been deproprietarized and is available for most Linux distributions virtinstall its commandlineOS provisioning tool accepts installation media from a variety of sources includingSMB or NFS mounts physical CDs or DVDs and HTTP URLsGuest domains disks are normally stored in virtual block devices VBDs in domThe VBD can be connected to a dedicated resource such as a physical disk driveor logical volume Or it can be a loopback file also known as a filebacked VBDcreated with dd Performance is better with a dedicated disk or volume but filesare more flexible and can be managed with normal Linux commands such as mvand cp in dom Backing files are sparse files that grow as neededUnless the system is experiencing performance bottlenecks a filebacked VBD isusually the best choice Its a simple process to transfer a VBD onto a dedicateddisk if you change your mindThe installation of a guest domain might look like this sudo virtinstall n chef f vmchefimg l httpexamplecommyosr nographicsThis is a typical Xen guest domain with the name chef a disk VBD location ofvmchefimg and installation media obtained through HTTP The instance hasMiB of RAM and uses no X Windows graphics support during installationvirtinstall downloads the files needed to start the installation and then kicks offthe installer processWhen the screen clears install Linux through the standard textbased processwhich includes network configuration and package selection After the installationcompletes the guest domain reboots and is ready for use To disconnect from theguest console and return to dom just press ControlIts worth noting that although this example uses textbased installation graphicsbased installation through Virtual Network Computing VNC is also availablevirtinstall saves the domains configuration in etcxenchef Heres what it looks likename chefuuid aefdbdfbddmaxmem memory vcpus bootloader usrbinpygrubonpoweroff destroyonreboot restartoncrash restartvfb disk vmchefdskxvdaw vif maceebridgexenbr You can see that the NIC defaults to bridged mode In this case the VBD is a blocktap file that affords better performance than does a standard loopback file Thewritable disk image file is presented to the guest as devxvdaThe xl tool is convenient for daytoday management of virtual machines It letsyou start and stop VMs connect to their consoles and investigate their currentstate Below we show the running guest domains then connect to the console forthe chef domU IDs are assigned in increasing order as guest domains are createdand they are reset when the host reboots sudo xl listName ID MemMiB VCPUs State TimesDomain r chef b sudo xl console To change the configuration of a guest domain eg to attach another disk or tochange the network to NAT mode instead of bridged edit the guests configurationfile in etcxen and reboot the guestKVMKVM the Kernelbased Virtual Machine is a full virtualization platform that isthe default for most Linux distributions Like Xens HVM mode KVM takes advantage of the Intel VT and AMDV CPU extensions and relies in a typical setupon QEMU to implement a fully virtualized hardware system Although the systemis native to Linux it has also been ported to FreeBSD as a loadable kernel moduleSince KVM defaults to full virtualization many guest operating systems are supported including Windows Paravirtualized Ethernet disk and graphics carddrivers are available for Linux FreeBSD and Windows Their use is optional butrecommended for performanceUnder KVM the Linux kernel itself serves as the hypervisor Memory management and scheduling are handled through the hosts kernel and guest machinesare normal Linux processes Enormous benefits accompany this unique approachto virtualization For example the complexity introduced by multicore processorsis handled by the kernel and no hypervisor changes are required to support themLinux commands such as top ps and kill show and control virtual machines justas they would for other processes The integration with Linux is seamlessKVM guest installationAlthough the technologies behind Xen and KVM are fundamentally different thetools that install and manage guest operating systems are eerily similar As underXen you can use virtinstall to create new KVM guests Use the virsh commandto manage themThe flags passed to virtinstall vary slightly from those used for a Xen installationTo begin with the hvm flag says that the guest should be hardware virtualized asopposed to paravirtualized In addition the connect argument guarantees thatthe correct default hypervisor is chosen since virtinstall supports more than onehypervisor Finally the use of accelerate is recommended to take advantage of theacceleration capabilities in KVM Ergo a full command for installing an Ubuntuserver guest from DVDROM looks something like this sudo virtinstall connect qemusystem n UbuntuYakketyr f ubuntuYakketyimg s c devdvd ostype linuxaccelerate hvm vncWould you like to enable graphics support yes or noAssuming that the Ubuntu installation DVD has been inserted this commandlaunches the installation and stores the guest in the file ubuntuYakketyimgallowing it to grow to GB Since we specified neither nographics nor vncvirtinstall asks whether to enable graphicsThe virsh utility spawns its own shell from which you can run commands To openthe shell type virsh connect qemusystem The following series of commandsdemonstrates some of the core functionality of virsh Type help in the shell to seea complete list or see the man page for the nittygritty details sudo virsh connect qemusystemvirsh list all Id Name State UbuntuYakkety running CentOS running WindowsServer shut offvirsh start WindowsServerDomain WindowsServer startedvirsh shutdown CentOSDomain CentOS is being shutdownvirsh quit FreeBSD bhyveFreeBSDs virtualization software is bhyve a relatively new system first added inFreeBSD It can run BSD Linux and even Windows guests However it runson a limited set of hardware and is missing some of the core features found in other implementationsWith so many virtualization platforms that support FreeBSD on the market alreadyits unclear why the bhyve effort started when it did Unless you are developing acustom platform that requires embedded FreeBSD virtualization we recommendchoosing another solution until this project matures VMwareVMware is the biggest player in the virtualization industry and was the first vendorto develop techniques to virtualize the fractious x platform VMware is a commercial entity but some of its products are free Theyre all worthy of considerationwhen you are choosing a sitewide virtualization technologyThe primary product of interest to UNIX and Linux administrators is ESXi whichis a baremetal hypervisor for the Intel x architecture ESXi is free but some useful functionality is limited to paid licenseesIn addition to ESXi VMware offers some powerful advanced products that facilitate centralized deployment and management of virtual machines They also havethe most mature live migration technology weve seen Indepth coverage of the fullVWware product suite is beyond the scope of this chapter however ESXi stands for Elastic Sky X integrated Cant make this stuff up Free like a box of puppies VirtualBoxVirtualBox is a consumergrade crossplatform type hypervisor It performs probably good enough virtualization of systems typically for individuals Its popularamong developers and end users because it is free easy to install easy to use andoften simplifies the creation and management of test environments Performanceand hardware support are both weak points VirtualBox is generally not suitablefor production virtualization useThe history of VirtualBox is long and sordid It originally began as a commercialproduct of Innotek GmbH but was released as open source before Innotek was acquired by Sun Microsystems in After Oracle swallowed Sun in the productwas rebranded as Oracle VM VirtualBox VirtualBox lives on today available underthe GPLv open source license and remains under active development at OracleVirtualBox runs on Linux FreeBSD Windows macOS and Solaris Oracle doesnot publish or support the FreeBSD version of the host but its available as a community port Supported guest OSs include Windows Linux and FreeBSDBy default you wrangle virtual machines through VirtualBoxs GUI If youre interested in running VMs on a system that doesnt run a GUI explore VBoxHeadlessthe morbid name for VirtualBoxs CLI tool You can download VirtualBox and readmore about it at virtualboxorg PackerPacker packerio from the esteemed open source company HashiCorp is a toolfor building virtual machine images from a specification file It can build imagesfor a variety of virtualization and cloud platforms Integrating Packer into yourworkflow lets you be more or less virtualizationplatformagnostic You can easilybuild your customized image for whatever platform youre using on a given dayTo create an image Packer launches an instance from a source image of yourchoosing It then customizes the instance by running scripts or invoking otherprovisioning steps that you specify Finally it saves a copy of the virtual machinesstate as a new imageThis process is particularly helpful for supporting an infrastructure as code wayof managing servers Instead of manually applying changes to images you modifya template that describes the image in abstract terms You then check the specification into a repository as you would with traditional source code This techniquesupplies you with outstanding transparency repeatability and reversibility It alsocreates a clear audit trail The VirtualBox web site does claim that its a professional solution which is licensed for enterpriseuse This may in fact be the case with regard to Oracle operating systems which are the only prebuiltVMs availablePacker configurations are JSON files Most administrators agree that JSON is a poorchoice of format since its notoriously picky about quotes and commas and doesntallow comments With luck HashiCorp will soon convert Packer to their muchimproved custom configuration format but until then youre stuck editing JSONIn a template builders define how to create an image and provisioners configure and install software for the image Builders exist for AWS GCP DigitalOceanVMware VirtualBox and Vagrant among others Provisioners can be shell scriptsChef cookbooks Ansible roles or other configuration management toolsThe following template customamijson demonstrates AWSs amazonebs builder and the shell provisionerbuilders type amazonebs accesskey AKIAIOSFODNNEXAMPLEsecretkey wJalrXUtnFEMIKMDENGbPxRfiCYEXAMPLEKEY region uswest sourceami amidae instancetype tmedium sshusername ubuntu sshtimeout m subnetid subnetefa vpcid vpcb associatepublicipaddress true amivirtualizationtype hvm amidescription ULSAH AMI aminame ULSAHE tags Name ULSAHE Demo AMI provisioners type shell source customizeamish Just as the CLI tool needs certain parameters to launch an instance the amazonebsbuilder needs data such as API credentials instance type the source AMI on whichto base the new image and a VPC subnet where the instance should be locatedPacker uses SSH to execute the provisioning step so we make sure the instancehas a public IP addressIn this case the provisioner is a shell script called customizeamish Packer copiesthe script to the remote system with scp and runs it Theres nothing special aboutsuch a script it can do anything youd normally do from a script For example it canadd new users download and configure software or execute security hardening stepsTo create the AMI invoke packer build packer build customamijsonpacker build notes each step of the creation process on the console The amazonebsbuilder takes the following steps It automatically creates a key pair and a security group It starts the instance and waits for it to become accessible on the network It uses scp and ssh to perform the requested provisioning steps It creates an AMI by calling the EC CreateImage API It cleans up by terminating the instanceIf everything works correctly Packer prints the AMI ID as soon as it is available foruse If a problem occurs during the build Packer prints a magentacolored errormessage and exits after cleaning up after itselfThe debug argument to packer build pauses at each step to let you troubleshootproblems You can also use the null builder to fix any errors without launching aninstance each time you try to run a build VagrantAlso developed by HashiCorp Vagrant is a wrapper that sits on top of virtualization platforms such as VMware VirtualBox and Docker However it is not itselfa virtualization platformVagrant simplifies virtual environment provisioning and configuration Its missionis to quickly and easily create disposable preconfigured development environmentsthat closely mirror production environments This glue function lets developers writeand test code with minimal involvement from sysadmins or an operations teamIts possible but not required to use Vagrant in combination with Packer For example you might standardize the base image that you use for your production platforms through Packer then distribute a Vagrant build of that image to developersThe developers can then spin up an instance of the image on their laptop or cloudprovider of choice with any necessary customizations This method balances theneed for centralized management of production images with developers need foraccess to a similar environment that they can directly control Recommended readingThe web site virtualizationinfo is an excellent source of current news trends andgossip in the virtualization and cloud computing sectorsHashimoto Mitchell Vagrant Up and Running Create and Manage VirtualizedDevelopment Environments Sebastopol CA OReilly Media Kusnetsky Dan Virtualization A Managers Guide Big Picture of the Who Whatand Where of Virtualization Sebastopol CA OReilly Media Mackey Tim and J K Benedict XenServer Administration Handbook PracticalRecipes for Successful Deployments Sebastopol CA OReilly Media Senthil Nathan VirtualBox at Warp Speed Virtualization with VirtualBox Seattle WA Amazon Digital Services Troy Ryan and Matthew Helmke VMware Cookbook A RealWorld Guide toEffective VMware Use nd Edition Sebastopol CA OReilly Media Few technologies have generated as much excitement and hype in recent years asthe humble container whose explosion in popularity coincided with the releaseof the open source Docker project in Containers are of particular interest tosystem administrators because they standardize software packaging an ambitionthat has long been tantalizingly out of reachTo illustrate the utility of containers consider a typical web application developedin any modern language or framework At a minimum the following ingredientsare needed to install and run the app The code for the application and its correct configuration Libraries and other dependencies potentially numbered in the dozenseach pinned to a specific version that is known to be compatible An interpreter eg Python or Ruby or run time JRE to execute thecode also version pinned Localizations such as user accounts environment settings and servicesprovided by the operating system ContainersA typical site runs dozens or hundreds of such applications Maintaining uniformity in each of these areas across multiple application deployments is a constantchallenge even with the assistance of the tools discussed in Chapter Configuration Management and Chapter Continuous Integration and Delivery Incompatible dependencies required by separate applications lead to systems that areunderutilized because they cannot be shared In addition at sites where softwaredevelopers and system administrators are functionally separated careful coordination is needed because its not always straightforward to identify whos responsiblefor what parts of the operating environmentA container image simplifies matters by packaging an application and its prerequisites into a standard portable file Any host with a compatible container runtimeengine can create a container by using the image as a template Tens or hundreds ofcontainers can run simultaneously without conflicts With images typically beinga few hundred megabytes in size or less its practical to copy them among systemsThis easy application portability is perhaps the primary reason for the popularityof containersThis chapter focuses on Docker The eponymous business behind Docker has playeda central role in bringing containers into mainstream use and the Docker ecosystem is the one youre most likely to encounter as a system administrator DockerInc offers several products related to containers but we limit our discussion tothe main container engine and the Swarm cluster managerSeveral viable alternative container engines are available rkt from CoreOS is themost complete It has a cleaner process model than Docker and a more secure default configuration rkt integrates well with the Kubernetes orchestration systemsystemdnspawn from the systemd project is another option for lightweight containers It has fewer features than Docker or rkt but in some cases that can be a goodthing rkt cooperates with systemdnspawn to configure container namespaces Background and core conceptsThe containers rapid rise to grace can be attributed more to timing than to theemergence of any single technology Containers are a fusion of numerous existingkernel features filesystem tricks and networking hacks A container engine is themanagement software that pulls it all togetherIn essence a container is an isolated group of processes that are restricted to a privateroot filesystem and process namespace The contained processes share the kerneland other services of the host OS but by default they cannot access files or systemresources outside their container Applications that run within a container are notaware of their containerized state and do not require modificationAfter you read the following sections it should be clear that containers containno magic In fact they rely on some features of UNIX and Linux that have beenaround for many years See Chapter Virtualization for a description of howcontainers differ from virtual machinesKernel supportThe container engine uses several kernel features that are essential for isolatingprocesses In particular Namespaces isolate containerized processes from the perspective of several operating system facilities including filesystem mounts process management and networking The mount namespace for example showsprocesses a customized view of the filesystem hierarchy Containers canrun with varying levels of integration with the host operating system depending on how these namespaces have been configured Control groups contextually abbreviated to cgroups limit the use of system resources and prioritize certain processes over others Cgroups prevent runaway containers from consuming all available CPU and memory Capabilities allow processes to execute certain sensitive kernel operationsand system calls For example a process might have a capability that permits it to change the ownership of a file or to set the system time Secure computing mode usually shortened to seccomp restricts accessto system calls It allows more finegrained control than do capabilitiesDevelopment of these features was driven in part by the Linux Containers projectLXC which began at Google in LXC was the basis of Borg Googles internalvirtualization platform LXC supplies the raw functions and tools needed to createand run Linux containers but with more than commandline tools and configuration files its quite complicated The first few releases of Docker were essentiallyuserfriendly wrappers that made LXC easier to useDocker now relies on an improved standardsbased container run time dubbedcontainerd It too relies on Linux namespaces cgroups and capabilities to isolatecontainers Learn more at containerdioImagesA container image is akin to a template for a container Images rely on union filesystem mounts for performance and portability Unions overlay multiple filesystemsto create a single consistent hierarchy Container images are union filesystemsthat are organized to resemble the root filesystem of a typical Linux distributionThe directory layout and the locations of binaries libraries and supporting files all This is similar in principle to the chroot system call which irreversibly sets a processs apparent rootdirectory and thereby disables access to files and directories above the level of the chroot The LWNnet article A brief history of union mounts describes the relevant background The relatedarticles are interesting reading too See lwnnetArticlesconform to standard Linux filesystem hierarchy specifications Specialized Linuxdistributions have been developed for use as the basis of container imagesTo create a container Docker points to the readonly union filesystem of an imageand adds a readwrite layer that the container can update When containerizedprocesses modify the filesystem their changes are transparently saved within thereadwrite layer The base remains unmodified This is known as a copyonwritestrategyMany containers can share the same immutable base layers thus improving storageefficiency and reducing startup times Exhibit A depicts the schemeExhibit A Docker images and the union filesystemContainer frombase imageReadwrite lesystem layerContainer frombase imageReadwrite lesystem layerContainer frombase imageReadwrite lesystem layerLayer Layer Layer Base imageReadonlyNetworkingThe default way to connect containers to the network is to use a network namespaceand a bridge within the host In this configuration containers have private IP addresses that arent reachable from outside the host The host acts as a poor mansIP router and proxies traffic between the outside world and the containers Thisarchitecture gives administrators control over which container ports are exposedto the outside worldIts also possible to forgo the private container addressing scheme and expose entire containers directly to the network This is called host mode networking andit means that the container has unfettered access to the hosts network stack Thismight be desirable in some situations but it also presents a security risk becausethe container is not fully isolatedSee Docker networks on page for more details Docker the open source container engineDocker Incs primary product is a clientserver application that builds and managescontainers The Docker container engine written in Go is highly modular Separate individual projects manage pluggable storage networking and other featuresDocker Inc is not without controversy Its tools tend to evolve rapidly and newversions have sometimes been incompatible with existing deployments Some sitesworry that relying on Dockers ecosystem will result in vendor lockin And as withany new technology containers introduce complexity and require some study tounderstandTo counter these sources of resistance Docker Inc became one of the foundingmembers of the Open Container Initiative a consortium whose mission is to guidethe growth of container technology in a healthily competitive direction that fostersstandards and collaboration You can learn more at opencontainersorg In Docker founded the Moby project and contributed the primary Docker Git repository to it to facilitate easier community development of the Docker execution engine Refer to mobyprojectorg for detailsOur discussion of Docker is based on version Docker maintains an exceptionally rapid pace of development and the current features are a moving target We focus on the nuts and bolts here but be sure to supplement our tutorialwith the reference material at docsdockercom You might also dip your toes intothe Moby sandbox at playwithmobycom and the Docker lab environment atlabsplaywithdockercomBasic architecturedocker is an executable command that handles all management tasks for the Dockersystem dockerd is the persistent daemon process that implements container andimage operations docker can run on the same system as dockerd and can communicate with it through UNIX domain sockets or it can contact dockerd from aremote host over TCP The architecture is depicted in Exhibit B on the next pagedockerd owns all the scaffolding needed to run containers It creates the virtualnetwork plumbing and maintains the data directory in which containers and imagesare stored varlibdocker by default Its responsible for creating containers byinvoking the appropriate system calls setting up union filesystems and executingprocesses In short it is the container management softwareExhibit B Docker architectureUbuntu FreeBSDUbuntu Debian FreeBSDdockerddockerContainersHost operating systemLocal imagesDocker image registryInstances ofManages CachesPushes and pulls images Controlsthrough network orlocal domain socketAdministrators interface with dockerd from the command line by running subcommands of the docker client You can create a container with docker run forexample or view information about the server with docker info Table summarizes some frequently used subcommandsAn image is the template for a container It includes the files that processes runningwithin the container instance depend on such as libraries operating system binaries and applications Linux distributions can function as convenient base imagesbecause they define a complete operating environment However an image is notnecessarily based on a Linux distribution The scratch image is an explicitly emptyimage intended as a basis for creation of other more practical imagesA container relies on the image template as a basis for execution When dockerdruns a container it creates a writable filesystem layer that is separate from the sourceimage The container can read any of the files and other metadata stored within theimage but any writes are confined to the containers own readwrite layerAn image registry is a centralized collection of images dockerd communicates withregistries when you docker pull an image that isnt already present or when youdocker push one of your own images The default registry is Docker Hub whichstockpiles images for many popular applications Most standard Linux distributionsalso publish Docker imagesYou can run your own registry or you can add your custom images to private registries that are hosted on Docker Hub Any system with Docker can pull images froma registry as long as the registry server is accessible over the networkInstallationDocker runs on Linux macOS Windows and FreeBSD but Linux is the flagshipplatform FreeBSD support is considered experimental Visit dockercom to choosethe installation method that best suits your environmentUsers in the docker group can control the Docker daemon through its socket whicheffectively gives those users root privileges This is a significant security risk so wesuggest that you use sudo to control access to docker rather than adding users tothe docker group In the examples below we run docker commands as the root userThe installation process may not immediately start the daemon If it isnt runningstart it through the systems normal init system On CentOS for example run sudosystemctl start dockerClient setupIf youre connecting to a local dockerd and youre in the docker group or havesudo privileges no client configuration is necessary The docker client connectsto dockerd through a local socket by default You can modify the default client behavior by setting environment variablesTo connect to a remote dockerd set the DOCKERHOST environment variableThe usual HTTP port for the daemon is and the TLS version is Table Frequently used docker subcommandsSubcommand What it doesdocker info Displays summary information about the daemondocker ps Displays running containersdocker version Displays extensive version info about the server and clientdocker rm Removes a containerdocker rmi Removes an imagedocker images Displays local imagesdocker inspect Displays the configuration of a container JSON outputdocker logs Displays the standard output from a containerdocker exec Executes a command in an existing containerdocker run Runs a new containerdocker pullpush Downloads images from or uploads images to a remote registrydocker startstop Starts or stops an existing containerdocker top Displays containerized process statusFor example export DOCKERHOSTtcpAlways use TLS to communicate with remote daemons If you use plain HTTP youmay as well hand out root privileges freely to anyone on your network You can findadditional details on Docker TLS configuration in Use TLS starting on page We also suggest enabling the content trust export DOCKERCONTENTTRUSTThis feature validates the integrity and publisher of Docker images Enabling thecontent trust prevents the client from pulling images that are not trustedIf you run docker through sudo you can prevent sudo from purging your environment variables with the E flag You can also whitelist specific environmentvariables by setting the value of the envkeep variable in etcsudoers For exampleDefaults envkeep DOCKERCONTENTTRUSTThe container experienceTo create a container you need an image to use as a template The image has all thefilesystem bits needed to run programs A new installation of Docker has no imagesTo download images from the Docker Hub use docker pull docker pull debianUsing default tag latestlatest Pulling from librarydebianfff Download completedbdbf Download completeDigest shaedbacebffecaedStatus Downloaded newer image for debianlatestThe hex strings are the layers of the union filesystem If the same layer is used by morethan one image Docker needs only a single copy We didnt request a specific tagor version of the Debian image so Docker downloaded the latest tag by defaultExamine the locally available images with docker images docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest ccdc weeks ago MBubuntu wily beecd days ago MBubuntu trusty df days ago MBubuntu dbfd weeks ago MBcentos defcac weeks ago MBcentos latest defcac weeks ago MBdebian jessie fff weeks ago MBdebian latest fff weeks ago MB You can review the available images by browsing hubdockercomThis machine has the images for several Linux distributions including the justdownloaded Debian image The same image can be tagged more than once Notice thatdebianjessie and debianlatest share an image ID they are two different names forthe same imageArmed with an image its remarkably simple to run a basic container docker run debian binecho Hello WorldHello WorldWhat just happened Docker created a container from the Debian base image andran the command binecho Hello World inside it The container stops running when the command exits in this case immediately after echo completes Ifthe debian image didnt already exist locally the daemon would attempt to automatically download it before running the command We didnt specify a tag sothe latest image was used by defaultWe start an interactive shell with the i and t flags to docker run The commandbelow starts a bash shell within the container and connects the outer shells IOchannels to it We also assign the container a hostname which is helpful for identifying it in logs Otherwise wed see the containers random ID in log messagesbenhost sudo docker run hostname debian it debian binbashrootdebian lsbin dev home lib mnt proc run srv tmp varboot etc lib media opt root sbin sys usrrootdebian ps auxUSER PID CPU MEM VSZ RSS TTY STAT START TIME COMMANDroot Ss binbashroot R ps auxrootdebian uname relxrootdebian exitexitbenhost uname relxThe experience is oddly similar to accessing a virtual machine There is a completeroot filesystem but the process tree appears nearly empty binbash is PID because its the command that Docker started in the containerThe result of uname r is the same both inside and outside the container That willalways be the case we show it as a reminder that the kernel is sharedProcesses in containers cannot see other processes running on the system becauseof PID namespacing However processes on the host can see the containerizedprocesses The PID of a process as seen from within a container differs from thePID that is visible from the host This is GNU echo not to be confused with the echo command built into most shells They do exactlythe same thingFor real work you need longlived containers that run in the background and acceptconnections over the network The following command runs in the backgroundd a container named nginx thats generated from the official NGINX imageWe tunnel port from the host into the same port within the container docker run p hostname nginx name nginx d nginxUnable to find image nginxlatest locallylatest Pulling from librarynginxfdddf Already existsaedcaeb Pull completeeadab Pull completeaffb Pull completeDigest shaaabfbafcbcfaccbdcfeedcaabeStatus Downloaded newer image for nginxlatestccbebaacfcfbdfaaedfeddcdWe didnt have the nginx image locally so Docker had to pull it from the registryOnce the image was downloaded Docker started the container and printed its IDa unique character hexadecimal stringdocker ps shows a brief summary of running containers docker psIMAGE COMMAND STATUS PORTSnginx nginx g daemon off Up minutes tcpWe didnt tell docker what to run in the container so it used the default commandthat was specified when the image was created The output shows this command tobe nginx g daemon off which runs nginx as a foreground process rather than asa background daemon The container has no init to manage processes and if thenginx server were started as a daemon the container would run but immediatelyexit when the nginx process forked and exited to enter the backgroundMost server daemons offer a commandline flag that forces them to run in theforeground If your software doesnt run in the foreground or if you need to runseveral processes in a container you can assign a process control system such assupervisord to act as a lightweight init for the containerWith NGINX running in the container and port mapped from the host we canmake HTTP requests to the container with curl NGINX serves a generic HTMLlanding page by defaulthost curl localhostDOCTYPE htmlhtmlheadtitleWelcome to nginxtitleWe can use docker logs to view the STDOUT from the container which in thiscase is the NGINX access log The only traffic is our curl request docker logs nginx Feb GET HTTP curl We can also use docker logs f to get a realtime stream of a containers output justlike running tail f on a growing log filedocker exec creates a new process in an existing container For example to debugor troubleshoot we could start an interactive shell in a container docker exec ti nginx bashrootnginx aptget update aptget y install procpsrootnginx ps ax PID TTY STAT TIME COMMAND Ss nginx master process nginx g daemon off S nginx worker process Ss bash R ps axContainer images are as slim as possible and are often missing common administrative utilities In this sequence we first updated the package index and then installed ps which is part of the procps packageThe process list reveals the nginx master daemon an nginx worker and our bashshell When we exit the shell created with docker exec the container continues torun If PID exited while our shell was active the container would terminate andour shell would also exitWe can stop and start the container docker stop nginxnginx docker psIMAGE COMMAND STATUS PORTS docker start nginx docker psIMAGE COMMAND STATUS PORTSnginx nginx g daemon off Up minutes tcpdocker start starts the container with the same arguments that were passed whenthe container was created with docker runWhen containers exit they remain on the system in a dormant state You can list allcontainers including those that are stopped with docker ps a Its not particularlyharmful to keep unneeded old containers lying around but its considered poorhygiene and might cause name collisions if you reuse container namesWhen we finish with the container we can stop and remove it docker stop nginx docker rm nginxdocker run rm runs a container and removes it automatically when it exits butthis works only for containers that are not daemonized with dVolumesThe filesystem layers for most containers consist of static application code librariesand other supporting or OS files The readwrite filesystem layer allows containersto make local modifications to these layers However heavy reliance on the overlay filesystem isnt the best storage solution for dataintensive applications such asdatabases For those kinds of apps Docker has the notion of volumesA volume is an independent writable directory within a container thats maintainedseparately from the union filesystem If the container is removed the data in thevolume persists and can be accessed from the host Volumes can also be sharedamong multiple containersWe add a volume to a container with dockers v argument docker run v data rm hostname web name web d nginxIf data already exists within the container any files found there are copied to thevolume We can find the volume on the host by running docker inspect docker inspect f json Mounts web Mounts Name febbccdafbfdcefffd Source varlibdockervolumesfebbccdadata Destination data Driver local Mode RW true Propagation The inspect subcommand returns verbose output we applied a filter so that onlythe mounted volumes would be printed If the container terminates or needs tobe removed we can find the data volume at the Source directory on the host TheName looks more like an ID but its useful if we need to identify the volume laterFor a higherlevel overview of volumes on the system we run docker volume lsDocker also supports bind mounts which mount volumes on the host and incontainers simultaneously For example we can bindmount mntdata from thehost to data in the container with the following command docker run v mntdatadata rm name web d nginxWhen the container writes to data the changes are also visible in mntdata onthe hostFor bindmounted volumes Docker does not copy existing files from the containers mount directory to the volume As with a traditional filesystem mount the volumes contents supersede the original contents of the containers mount directoryWhen running containers in the cloud we suggest combining bind mounts withthe block storage options offered by cloud providers For example AWSs ElasticBlock Storage volumes make great backing stores for Docker bind mounts Theyhave builtin snapshot facilities and can move among EC instances They canalso be copied between nodes which makes it straightforward for other systemsto retrieve a containers data You can leverage EBSs native snapshotting facilitiesto create a simple backup systemData volume containersOne helpful pattern that has emerged from realworld experience is the dataonlycontainer Its purpose is to hold a volume configuration on behalf of other containers so that those containers can be easily restarted and replacedCreate a data container by using either a normal volume or a bindmounted volumefrom the host The data container never actually runs docker create v mntdatadata name nginxdata nginxNow you can use the data containers volume in the nginx container docker run volumesfrom nginxdata p name web d nginxThe web container has read and write access to the data volume of the dataonlynginxdata container web can be restarted removed or replaced but so long asit is started with volumesfrom the files in data will remain persistentIn truth combining data persistence with containers is a bit of an impedance mismatch Containers are meant to be created and removed at a moments notice inresponse to external events The ideal is to have a fleet of identical servers that rundockerd with containers being deployable to any of the servers Once you addpersistent data volumes however the container becomes coupled to a particularserver As much as wed like to be living in the ideal world many applications doneed persistent dataDocker networksAs discussed in Networking on page there is more than one way to connectcontainers to the network During installation Docker creates three default networking options List them with docker network ls docker network lsNETWORK ID NAME DRIVERe bridge bridgeaceb none nullefecc host hostIn the default bridge mode containers reside on a private namespaced networkwithin the host The bridge connects the hosts network to the container namespaceWhen you create a container and map a port from the host with docker run pDocker creates iptables rules that route traffic from the hosts public interface tothe containers interface on the bridge networkWith host networking no separate network namespace is used Instead the container shares the network stack with the host including all its interfaces Ports exposed by the container are also exposed on the interfaces of the host Some softwarebehaves better when running with host networking but this configuration can alsolead to port conflicts and other problemsNone networking indicates that Docker shouldnt take any steps whatsoever toconfigure networking It is intended for advanced use cases that have custom networking requirementsPass the net argument to docker run to select a containers networkNamespaces and the bridge networkA bridge is a Linux kernel feature that connects two network segments During installation Docker quietly creates a bridge called docker on the host Docker choosesan IP address space for the far side of the bridge that it calculates as unlikely to collide with any networks reachable by the host Each container is given a namespacedvirtual network interface that has an IP address within the bridged network rangeThe address selection algorithm is practical but not perfect Your network may haveroutes that arent visible from the host If a collision occurs the host will no longerbe able to access the remote network that has the overlapping address space butit will be able to reach local containers If you find yourself in this situation or ifyou need to customize the bridges address space for some other reason use thefixedcidr argument to dockerdNetwork namespaces rely on virtual interfaces strange constructs that are createdin pairs where one side is in the hosts namespace and the other is in the containers Data flows in one end of the pair and out the other end thus connecting thecontainer to the host In most cases a container has only one such pair Exhibit Cillustrates the conceptOne half of each pair is visible from the hosts networking stack For example hereare the visible interfaces on a CentOS host with just one container runningcentos ip addr show enps BROADCASTMULTICASTUPLOWERUP mtu qdisc pfifofaststate UP qlen linkether cf brd ffffffffffff inet brd scope global dynamic enps validlft sec preferredlft sec inet feafffecf scope link validlft forever preferredlft foreverSee page formore informationabout iptables docker BROADCASTMULTICASTUPLOWERUP mtu qdisc noqueuestate UP linkether d brd ffffffffffff inet scope global docker validlft forever preferredlft forever inet fedfffe scope link validlft forever preferredlft forever vethaif BROADCASTMULTICASTUPLOWERUP mtu qdiscnoqueue master docker state UP linkether dabdbfeb brd ffffffffffff linknetnsid inet fedafffebdbfeb scope link validlft forever preferredlft foreverThe output shows enps the primary interface on the host and docker the virtualEthernet bridge which uses the range The veth interface is the hostside of the virtual interface pair that connects the container to the bridged networkThe containers side of the bridged pair is not visible from the host without lowlevelinspection of the networking stack This invisibility is just a side effect of the waynetwork namespaces work However we can find the interface by inspecting thecontainer itself docker inspect f json NetworkSettingsNetworksbridge nginx bridge Gateway IPAddress IPPrefixLen MacAddress ac The containers IP address is and the default gateway is the dockerbridge interface This is the bridge network depicted in Exhibit CIn the default bridge configuration all containers can communicate with one another because they are all on the same virtual network However you can create additional network namespaces to isolate containers from one another That way youcan serve multiple isolated environments from the same set of container instancesExhibit C A docker bridge networkContainer eth Container eth vethdavethaeddockerenpsExternalnetworkHost operating systemNetwork overlaysDocker has lots of additional networking flexibility available to help with advanceduse cases For example you can create userdefined private networks that automatically have container linking With network overlay software containers runningon separate hosts can route traffic to each other through a private network addressspace Virtual eXtensible LAN VXLAN technology described in RFC is onesystem that can be combined with containers to implement advanced networkingcapabilities See the Docker networking documentation for more detailsStorage driversUNIX and Linux systems offer multiple ways to implement a union filesystemDocker is technologyagnostic in this regard and filters all filesystem operationsthrough a storage driver that you selectThe storage driver is configured as part of the docker daemon launch options Yourchoice of storage engine has important consequences for performance and stabilityespecially in production environments that support many containers Table shows the current menu of driversThe VFS driver effectively disables the use of a union filesystem Docker creates acomplete copy of an image for each container resulting in higher disk usage andlonger container start times However this implementation is simple and robust Ifyour use case involves longlived containers VFS is a reliable choice Weve neverencountered a site that uses VFS in production howeverBtrfs and ZFS are also not true union filesystems However they implement overlays efficiently and reliably because they natively support copyonwrite filesystemclones Docker support for Btrfs and ZFS is currently limited to a few specific Linuxdistributions and FreeBSD for ZFS but these are good options to keep an eye onfor the future The less filesystem magic specific to the container system the betterStorage driver selection is a nuanced topic Unless you or somebody on your teamhas comprehensive knowledge of one of these filesystems we recommend that youstick with the default for your distribution The Docker storage driver documentation has further informationdockerd option editingYoull inevitably need to modify some of dockerds settings Tuning options includethe storage engine DNS options and the base directory in which images and metadata are stored Run dockerd h to see a complete list of argumentsYou can examine a running daemons configuration with docker infocentos docker infoContainers Running Paused Stopped Images Server Version Storage Driver overlay Backing Filesystem xfsLogging Driver jsonfilePlugins Volume local Network bridge null hostKernel Version elxOperating System CentOS Linux CoreOSType linuxArchitecture xTable Docker storage driversDriver Description and commentsaufs A reimplementation of the original UnionFSThe original Docker storage engineDefault for Debian and UbuntuNow deprecated because its not part of the mainline Linux kernelbtrfs Uses the Btrfs copyonwrite filesystem see page Btrfs is stable and is included in the mainline Linux kernelDockers use is distributionlimited and somewhat experimentaldevicemapper Default for RHELCentOS Direct LVM mode strongly recommended but needs configurationHas a history of bugsStudy Dockers devicemapper documentationoverlay Based on OverlayFSConsidered the replacement for AuFSThe default in CentOS if the overlay kernel module is loadedvfs Not a real union filesystemSlow but stable suitable for some production environmentsGood as a proof of concept or as a testbedzfs Uses the ZFS copyonwrite filesystem see page Default for FreeBSDConsidered experimental on LinuxThis is a good place to check that any customizations you made have taken effectDocker conforms to the operating systems native init system for managing daemon processes including settings for startup options For example on a distribution that uses systemd the following command edits the Docker service unit toset a nondefault storage driver a set of DNS servers and a custom address spacefor the bridge network systemctl edit dockerServiceExecStartExecStartusrbindocker daemon D storagedriver overlay dns dns bip The redundant ExecStart is not a mistake Its a systemdism that clears the default setting ensuring that the new definition is used exactly as shown Once theedits are complete we restart the daemon with systemctl and review the changescentos sudo systemctl restart dockercentos sudo systemctl status docker dockerservice Loaded loaded etcsystemdsystemdockerservice static vendor preset disabled DropIn etcsystemdsystemdockerserviced overrideconf Active active running since Wed UTC s ago Main PID docker CGroup systemslicedockerservice usrbindocker daemon D storagedriver overlaydns dns bip On systems running upstart configure daemon options in etcdefaultdockerFor older systems with SysVstyle init use etcsysconfigdockerBy default dockerd listens for connections from docker on the UNIX domainsocket at varrundockersock To set the daemon to listen on a TLS socket instead use the daemon option H tcp See page for more detailsabout how to set up TLSImage buildingYou can containerize your own applications by building images that include yourapplication code The build process begins with a base image You add your application by committing any changes as new layers and saving the image to the localimage database You can then create containers from the image You can also pushyour image to a registry to make it accessible to other systems running DockerEach layer of an image is identified by a cryptographic hash of its contents Thehash serves as a validation system that lets Docker confirm that no corruption ormalicious intervention has modified the contents of the imageSee Chapter formore informationabout editing unitfiles with systemctlChoosing a base imageBefore creating a custom image choose a suitable base The rule of thumb for baseimages is that the smaller footprint the better The base should have what you needto run your software and nothing moreMany of the official images are based on a distribution called Alpine Linux whichweighs in at a lean MB but may have library incompatibilities with some applications The Ubuntu image is larger at MB but still small in comparison to atypical server installation You might be able to find a base image that has your application runtime components already configured Default base images exist forthe most common languages runtimes and application platformsThoroughly vet your base image before you make a final decision Examine thebases Dockerfile see the next section and any nonobvious dependencies to avoidsurprises Base images may have unexpected requirements or include vulnerableversions of software In some circumstances you may need to copy the Dockerfileof a base image and rebuild it to suit your needsWhen dockerd downloads an image it downloads only the layers that it doesntalready have If all your applications use the same base there is less data for thedaemon to download and containers start faster when first runBuilding from a DockerfileA Dockerfile is a recipe for building an image It contains a series of instructionsand shell commands The docker build command reads the Dockerfile runs itsinstructions in sequence and commits the result as an image Software projectsthat have a Dockerfile usually keep it in the root directory of the Git repository tofacilitate building new images that contain that softwareThe first instruction in a Dockerfile specifies an image to use as the base Each subsequent instruction commits a change to a new layer which is used in turn as thebase for the next instruction Each layer includes only the changes from the previouslayer The union filesystem merges the layers to create a containers root filesystemHere is a Dockerfile that builds the official NGINX image for DebianFROM debianjessieMAINTAINER NGINX Docker Maintainers dockermaintnginxcomENV NGINXVERSION jessieRUN aptget update aptget install y cacertificates nginxNGINXVERSION rm rf varlibaptlists forward request and error logs to docker log collectorRUN ln sf devstdout varlognginxaccesslog ln sf devstderr varlognginxerrorlogEXPOSE CMD nginx g daemon off From githubcomnginxincdockernginx Slightly simplifiedNGINX uses the debianjessie image as a base After declaring a maintainer the filesets an environment variable NGINXVERSION thats then available to every subsequent instruction in the Dockerfile and also to any process that runs inside thecontainer once the image has been built and instantiated The first RUN instructiondoes the heavy lifting by installing NGINX from a package repositoryBy default NGINX sends log data to varlognginxaccesslog but the convention for containers is to log messages to STDOUT In the final RUN command themaintainers use a symbolic link to redirect the access log to the STDOUT devicefile Similarly errors are redirected to the containers STDERRThe EXPOSE command tells dockerd which ports the container listens on The exposedports can be overridden at container run time with the p option to docker runThe final instruction in the NGINX Dockerfile is the command that dockerd shouldexecute when it starts the container In this case the container runs the nginx binary as a foreground processSee Table for a rundown of common Dockerfile instructions The referencemanual at docsdockercom is the authoritative documentationComposing a derived DockerfileWe can use a very simple Dockerfile to build a derived NGINX image that adds acustom indexhtml replacing the default from the official image cat indexhtmlDOCTYPE htmltitleULSAH indexhtml filetitlepA simple Docker image brought to you by ULSAHp cat DockerfileFROM nginx Add a new indexhtml to the document rootADD indexhtml usrsharenginxhtmlOther than having a custom indexhtml our new image will be identical to thebase image Heres how we build the customized image docker build t nginxulsah Step FROM nginx fddcStep ADD indexhtml usrsharenginxhtml cceafRemoving intermediate container ccfdbSuccessfully built cceafWe use docker build with t nginxulsah to create an image with the name nginx andthe tag ulsah to distinguish it from the official NGINX image The trailing dot tellsdocker build where to search for the Dockerfile in this case the current directoryNow we can run the image and see our customized indexhtml docker run p name nginxulsah d nginxulsah curl localhostDOCTYPE htmltitleULSAH indexhtml filetitlepA simple Docker image brought to you by ULSAHpWe can check that our image is listed among the local images by running the command docker images docker images grep ulsahREPOSITORY TAG IMAGE ID CREATED SIZEnginx ulsah cceaf minutes ago MBTo remove images run docker rmi You cant remove an image until youve stoppedand removed any containers that are using it docker ps grep nginxulsahIMAGE COMMAND STATUS PORTSnginxulsah nginx g daemon off Up seconds tcp docker stop nginxulsah docker rm nginxulsahnginxulsahnginxulsah docker rmi nginxulsahTable Abbreviated list of Dockerfile instructionsInstruction What it doesADD Copies files from the build host to the image aARG Sets variables that can be referenced during the build but notfrom the final image not intended for secretsCMD Sets the default commands to execute in a containerCOPY Like ADD but only for files and directoriesENV Sets environment variables available to all subsequent buildinstructions and containers spawned from this imageEXPOSE Informs dockerd of the network ports exposed by the containerFROM Sets the base image must be the first instructionLABEL Sets image tags visible with docker inspectRUN Runs commands and saves the result in the imageSTOPSIGNAL Specifies a signal to send to the process when told to quit withdocker stop defaults to SIGKILLUSER Sets the account name to use when running the container andany subsequent build instructionsVOLUME Designates a volume for storing persistent dataWORKDIR Sets the default working directory for subsequent instructionsa The source can be a file directory tarball or remote URLBoth docker stop and docker rm echo the name of the container they affect resulting in nginxulsah being printed twiceRegistriesA registry is an index of Docker images that dockerd can access through HTTPWhen an image is requested that doesnt exist on the local disk dockerd pulls itfrom the registry Images are uploaded to a registry with docker push Althoughimage operations are initated by the docker command only dockerd actually interacts with registriesDocker Hub is a hosted registry service run by Docker Inc It hosts images for manydistributions and open source projects including all our example Linux systemsThe integrity of these official images is verified through a content trust system thusensuring that the image you download is provided by the vendor whose name is onthe label You can also publish your own images to Docker Hub for others to useAnyone can download public images from Docker Hub but with a subscription youcan also create private repositories Once you have a paid account at hubdockercomlog in from the command line with docker login to access the private registry sothat you can push and pull your own custom images You can also trigger an imagebuild whenever a commit is detected on a GitHub repositoryDocker Hub is not the only subscriptionbased registry Others include quayioArtifactory Google Container Registry and the Amazon EC Container RegistryDocker Hub is the generous benefactor of the greater image ecosystem and it alsobenefits from being the default registry when nothing more specific is requestedFor example the command docker pull debianjessiefirst looks for a local copy of the image If the image isnt available locally the nextstop is Docker Hub You can tell docker to use a different registry by including ahostname or URL in the image specification docker pull registryadmincomdebianjessieSimilarly when building an image to push to a custom registry you must tag it withthe registrys URL and you must authenticate before you push docker tag debianjessie registryadmincomdebianjessie docker login httpsregistryadmincomUsername benPassword password docker push registryadmincomdebianjessieDocker saves the login details to a file in your home directory called dockercfg sothat you need not log in again the next time you interact with the private registryFor performance or security reasons you might prefer to run your own image registry The registry project is open source githubcomdockerdistribution and asimple registry is easy to run as a container docker run d p name registry registryThe registry service is now running on port You can pull an image from it byqualifying the name of the image youre seeking docker pull localhostdebianjessieThe registry implements two authentication methods token and htpasswd tokendelegates authentication to an external provider which is likely to require customdevelopment effort htpasswd is simpler and allows HTTP basic authenticationfor registry access Alternatively you can set up a proxy eg NGINX to handleauthentication Always run the registry with TLSThe default private registry configuration is not appropriate for a largescale deployment Considerations for production use include storage space authentication andauthorization requirements image cleanup and other maintenance tasksAs your containerized environment expands your registry will be inundated withnew images For users working in the cloud an object store such as Amazon S orGoogle Cloud Storage is one possible way to store all this data The registry nativelysupports both servicesBetter yet you can outsource your registry functions to the registries built into yourcloud platform of choice and have one less thing to worry about Both Google andAmazon run managed container registry services You pay for storage and for thenetwork traffic to upload and download images Containers in practiceOnce youre comfortable with the general way that containers work youll find thatcertain administrative chores need to be approached differently in a containerizedworld For example how do you manage log files for containerized applicationsWhat are some security considerations How do you troubleshoot errorsThe list below offers a few rules of thumb to help you adjust to life inside a container When your application needs to run a scheduled job dont run cron ina container Use the cron daemon from the host or a systemd timer toschedule a shortlived container that runs the job and exits Containersare meant to be disposable Need to log in and check out what a process is doing Dont run sshd inyour container Log in to the host with ssh then use docker exec to openan interactive shell The registry tag differentiates the latestgeneration registry from the previous version which implements an API that is incompatible with current versions of Docker If possible set up your software to accept its configuration informationfrom environment variables You can pass environment variables to containers with the e KEYvalue argument to docker run Or set up manyvariables at once from a separate file with envfile filename Ignore the commonly dispensed advice one process per container Thatsnonsense Split processes into separate containers only when it makessense to do so For example its usually a good idea to run an applicationand its database server in separate containers because they are separatedby clear architectural boundaries But its perfectly OK to have more thanone process in a container when thats appropriate Use common sense Focus on the automatic creation of containers for your environmentWrite scripts to build images and upload them to registries Make surethat software deployment procedures involve replacing containers notupdating them in place On that note avoid maintaining containers If youre accessing a containermanually to fix something figure out what the problem is resolve it inthe image then replace the container Immediately update your automation tooling if necessary Stuck Ask questions on the Docker User mailing list on the DockerCommunity Slack or in the docker IRC channel on freenodeEverything an application needs to function should be available within its container the filesystem network access and kernel facilities The only processes that runin a container are the ones that you start It is atypical of containers to run normalOS services such as cron rsyslogd and sshd although it is certainly possible todo so Those duties are best left to the host OS If you find yourself needing theseservices within a container reconsider your problem and see if you can solve it ina more containerfriendly wayLoggingUNIX and Linux applications traditionally use syslog now the rsyslogd daemonto process log messages Syslog handles log filtering sorting and routing to remotesystems Some applications dont use syslog and instead write directly to log filesContainers do not run syslog Instead Docker collects the logs for you throughlogging drivers Container processes need only write logs to STDOUT and errorsto STDERR Docker collects those messages and sends them to a configurabledestinationIf your software supports logging only to files apply the same technique as theNGINX example on page create symbolic links from log files to devstdoutand devstderr when you build the imageDocker forwards the log entries it receives to a selectable logging driver Table lists some of the more common and useful logging driversTable Docker logging driversDriver What it doesjsonfile Writes JSON logs in the daemons data directory default asyslog Writes logs to a configurable syslog destination bjournald Writes logs to the systemd journal agelf Writes logs in the Graylog Extended Log Formatawslogs Writes logs to the AWS CloudWatch servicegcplogs Writes logs to Google Cloud Loggingnone Does not collect logsa Log entries stored this way are accessible through the docker logs commandb Supports UDP TCP and TCPTLSWhen using jsonfile or journald you can access log data from the command linethrough docker logs containeridYou set the default logging driver for dockerd with the logdriver option You canalso assign a logging driver at container run time with docker run loggingdriverSome drivers accept additional options For example the logopt maxsize option configures log file rotation for the jsonfile driver Use this option to avoidfilling up the disk with log files Refer to the Docker logging documentation forcomplete detailsSecurity adviceContainer security relies on processes within containers being unable to access filesprocesses and other resources outside their sandbox Vulnerabilities that allow attackers to escape containersknown as breakout attacksare serious but rare Thecode that underlies container isolation has been in the Linux kernel since at least its mature and stable As with baremetal or virtualized systems insecureconfigurations are a far more likely source of compromises than are vulnerabilitiesin the isolation layerDocker maintains an interesting list of known software vulnerabilities that are and arenot mitigated by containerization See docsdockercomenginesecuritynoneventsRestrict access to the daemonAbove all protect the docker daemon Because dockerd necessarily runs with elevated privileges its trivial for any user with access to the daemon to gain full rootaccess to the hostThe following sequence of commands demonstrates the risk iduidben gidben groupsbendocker docker run rm v host t i debian bashrooteaecfb cd hostrooteaecfbhost lsbin dev home lib mnt proc run srv test usrboot etc lib media opt root sbin sys tmp varThis transcript shows that any user in the docker group can mount the hosts rootfilesystem to a container and gain full control of its contents This is just one ofmany possible ways to elevate privileges through DockerIf you use the default UNIX domain socket to communicate with the daemon addonly trusted users to the docker group which has access to the socket Better yetcontrol access through sudoUse TLSWe said it before and well say it again if the docker daemon must be remotely accessible dockerd H require the use of TLS to encrypt network communicationsand to mutually authenticate the client and serverSetting up TLS involves having a certificate authority issue certificates to the dockerdaemon and clients Once the key pairs and certificate authority are in place actually enabling TLS for docker and dockerd is a simple matter of supplying the rightcommandline arguments Table lists the essential settingsTable TLS arguments common to docker and dockerdArgument Meaning or argumenttlsverify Require authenticationtlscert a Path to a signed certificatetlskey a Path to a private keytlscacert a Path to the certificate of a trusted authoritya Optional The default locations are dockercertkeycapemSuccessful use of TLS relies on a mature certificate management processes Certificate issuance revocation and expiration are a few of the issues that need attentionHeavy is the burden of a securityconscious administratorRun processes as unprivileged usersProcesses in containers should run as nonroot users just as they should on a fullfledged operating system This practice limits an attackers ability to launch breakoutSee page for information about TLSattacks When you are writing a Dockerfile use the USER instruction to run futurecommands in the image under the named user accountUse a readonly root filesystemTo further restrict containers you can specify docker run readonly therebylimiting the container to a readonly root filesystem This works well for statelessservices that never need to write You can also mount a readwrite volume that yourprocess can modify but leave the root filesystem readonlyLimit capabilitiesThe Linux kernel defines separate capabilities that can be assigned to processesBy default Docker containers are granted a large subset of these You can enablean even greater subset by starting a container with the privileged flag Howeverthis option disables many of the isolation benefits of using Docker You can tune thespecific capabilities that are available to containerized processes with the capaddand capdrop arguments docker run capdrop SETUID capdrop SETGID debianjessieYou can also drop all privileges and add back just the ones you need docker run capdrop ALL capadd NETRAW debianjessieSecure imagesThe Docker content trust feature validates the authenticity and integrity of imagesin a registry The publisher of the image signs it with a secret key and the registryvalidates it with the corresponding public key This process ensures that the imagewas produced by the expected creator You can use content trust to sign your ownimages or to validate the images in a remote registry The feature is available onDocker Hub and on some third party registries such as ArtifactoryUnfortunately most of the content on Docker Hub is unsigned and should be considered untrustworthy Indeed most images on the Hub are never patched updated or audited in any wayThis lack of a proper chain of trust associated with many Docker images is representative of the miserable state of security on the Internet in general Its quitecommon for software packages to depend on third party libraries with little or noconcern being given to the trustworthiness of the content thats pulled in Somesoftware repositories have no cryptographic signatures whatsoever Its also commonto find articles that actively encourage disabling validation Responsible system administrators are highly suspicious of unknown and untrusted software repositoriesSee page for moreinformation aboutLinux capabilitiesDebugging and troubleshootingContainers bring with them a particularly heinous complement of obscure debugging techniques When an application is containerized its symptoms become moredifficult to characterize and their root causes more puzzling Many applications canrun without modification inside a container but in some scenarios they may behave differently You might also encounter bugs within Docker itself This sectionhelps navigate these treacherous watersErrors usually manifest themselves in log files so thats the first place to look forinformation Use the advice in Logging on page to configure logging for containers and always review the logs when you encounter issuesIf you experience problems with a running container trydocker exec ti containername bashto open an interactive shell From there you can attempt to reproduce the problemexamine the filesystem for evidence and search for configuration errorsIf you see errors related to the docker daemon or if you have trouble starting itsearch the issues list at githubcommobymoby You may find others that have thesame problem and one of them may have identified a potential fix or workaroundDocker doesnt automatically clean up images or containers When neglected theseremnants can consume an inordinate amount of disk space If your container workload is predictable configure a cron job to clean up by running docker systemprune and docker image pruneA related annoyance are dangling volumes volumes that were once attached toa container but for which the container has since been removed Volumes are independent of containers so any files within them will continue to consume diskspace until the volumes are destroyed You can use the following incantation toclean out orphaned volumes docker volume ls f danglingtrue List dangling volumes docker volume rm docker volume ls qf danglingtrue Remove emBase images you depend on may have a VOLUME instruction in their Dockerfile Ifyou dont notice this case you might end up with a full disk after running a fewcontainers from that image You can show the volumes associated with a containerby running docker inspect docker inspect f Volumes containername Container clustering and managementOne of the great promises of containerization is the prospect of colocating manyapplications on the same host while avoiding interdependencies and conflictsthereby making more efficient use of servers This is an appealing vision but theDocker engine is responsible only for individual containers not for answering thebroader question of how to run many containers on distributed hosts in a highlyavailable configurationConfiguration management tools such as Chef Puppet Ansible and Salt all supportDocker They can ensure that hosts run a certain set of containers with declaredconfigurations They also support image building registry interfaces network andvolume management and other containerrelated chores These tools centralize andstandardize container configuration but they do not solve the problem of conductingthe deployment of many containers across a network of servers Note that althoughconfiguration management systems are useful for a variety of containerrelatedtasks you will rarely need to use configuration management inside of containersFor networkwide container deployments you need container orchestration software also known as container scheduling or container management software Anentire symphony of open source and commercial tooling is available to handlelarge numbers of containers Such tools are crucial for running containers at scalein a production contextTo understand how these systems work think of the servers on the network as afarm of compute capacity Each node in the farm offers CPU memory disk andnetwork resources to the scheduler When the scheduler receives a request to run acontainer or set of containers it places the container on a node that has sufficientspare resources to meet the containers needs Because the scheduler knows wherecontainers have been placed it can also assist in routing network requests to thecorrect nodes within the cluster Administrators interact with the container management system rather than dealing with any individual container engine ExhibitD illustrates this architectureExhibit D Basic container scheduler architectureContainerContainerContainerContainerAgentContainerengineContainer schedulerAdministrative controlInbound requestsControlRouting mesh NodesContainer management systems supply several helpful features Scheduling algorithms select the best node in light of a jobs requestedresources and the utilization of the cluster For example a job with highbandwidth requirements might be slotted onto a node with a Gbsnetwork interface Formal APIs allow programs to submit jobs to the cluster opening thedoor to integration with external tools Its easy to use container management systems in conjunction with CICD systems to simplify softwaredeployments Container placement can accommodate the needs of highavailability configurations For example an application may need to run on host nodesin several distinct geographical regions Health monitoring is built in The system can terminate and rescheduleunhealthy jobs and can route jobs away from unhealthy nodes Its easy to add or remove capacity If your compute farm doesnt haveenough resources available to satisfy demand you can simply add another node This facility is especially well suited to cloud environments The container management system can interface with a load balancerto route network traffic from external clients This facility obviates thecomplex administrative process of manually configuring network accessto containerized applicationsOne of the most challenging tasks in a distributed container system is mappingservice names to containers Remember that containers are typically ephemeral innature and may have dynamic ports assigned How do you map a friendly persistentservice name to multiple containers especially when the nodes and ports changefrequently This problem is known as service discovery and container managementsystems have various solutionsIt helps to be familiar with the underlying container execution engine before divinginto orchestration tooling All the container management systems were aware ofrely on Docker as the default container execution engine although some systemsalso support other enginesA synopsis of container management softwareDespite their relative youth the container management tools we discuss below aremature beyond their years and can be considered production grade In fact manyare already used in production at highprofile largescale technology companiesMost are open source and have sizable user communities Based on recent trendswe anticipate substantial development in this area in the coming yearsIn the upcoming sections we highlight the functionality and features of the mostwidely used systems We also mention their integration points and common use casesKubernetesKubernetessometimes shortened to ks because there are eight letters betweenthe leading k and the trailing shas emerged as a leader in the container management space It originated within Google and was launched by some of the samedevelopers that worked on Borg Googles internal cluster manager Kuberneteswas released as an open source project in and now has more than a thousandactive contributors It has the most features and the fastest development cycle ofany system were aware ofKubernetes consists of a few separate services that integrate to form a cluster Thebasic building blocks include The API server for operator requests A scheduler for placing tasks A controller manager for tracking the state of the cluster The Kubelet an agent that runs on all cluster nodes cAdvisor for monitoring container metrics A proxy for routing incoming requests to the appropriate containerThe first three items on this list run on a set of masters which can optionally servedual duty as nodes for high availability The Kubelet and cAdvisor processes runon each node handling requests from the controller manager and reporting statistics about the health of their tasksIn Kubernetes containers are deployed as a pod which contains one or morecontainers All containers in a pod are guaranteed to be colocated on the samenode Pods are assigned a clusterwide unique IP address and they are labeled foridentification and placement purposesPods are not meant to be longlived If a node dies the controller schedules a replacement pod on a different node with a new IP address Therefore you cannotuse the address of a pod as a durable nameServices are collections of related pods with an address that is guaranteed not tochange If a pod within a service dies or fails a health check the service removesthat pod from its rotation You can also use the builtin DNS server to assign resolvable names to servicesKubernetes has integrated support for service discovery secret management deployment and pod autoscaling It has pluggable networking options to enable containernetwork overlays It can support stateful applications by migrating volumes amongnodes as needed Its CLI tool kubectl is one of the most intuitive that weve everworked with In short it has more advanced features than we can possibly coverin this short sectionAlthough Kubernetes has the most active and engaged community and the mostadvanced features those assets are accompanied by a steep learning curve Recentversions have improved the experience for firsttime users but a fullfledged customized Kubernetes deployment is not for the timid Production ks deploymentsimpose a substantial administrative and operational burdenThe Google Container Engine service is implemented with Kubernetes and it offers one of the best experiences for teams that want to run containerized workloadswithout the operational overhead of cluster managementMesos and MarathonMesos is an entirely different breed It was conceived at the University of California at Berkeley around as a generic cluster manager It quickly made its wayto Twitter where it now runs on thousands of nodes Today Mesos is a toplevelproject from the Apache Foundation and boasts a large number of enterprise usersThe major conceptual entities in Mesos are masters agents and frameworks Amaster is a proxy between agents and frameworks Masters relay offers of systemresources from agents to frameworks If a framework has a task to run it choosesan offer and instructs the master to run the task The master sends along the taskdetails to the agentMarathon is a Mesos framework that deploys and manages containers It includesa handsome user interface for managing applications and a simple RESTful APITo run an application you write a request definition in JSON format and submitit to Marathon through the API or the UI Because its an external framework thedeployment of Marathon is flexible Marathon can run on the same node as themaster for convenience or it can run externallySupport for multiple coexisting frameworks is Mesoss biggest differentiator ApacheSpark the bigdata processing tool and Apache Cassandra a NoSQL databaseboth offer Mesos frameworks thus allowing you to use Mesos agents as nodes ina Spark or Cassandra cluster Chronos is a framework for scheduled jobs ratherlike a version of cron that runs on a cluster instead of an individual machine Theability to run so many frameworks on the same set of nodes is a nice feature andhelps create a unified and centralized experience for administratorsUnlike Kubernetes Mesos does not come with batteries included For exampleload balancing and traffic routing are pluggable options that depend on your preferred solution Marathon includes a tool the Marathonlb that implements thisservice or you can choose your own Weve had success using HashiCorps Consuland HAProxy Designing and implementing an exact solution is left as an exercisefor the administratorLike Kubernetes Mesos requires some contemplation to understand and use Mesos and most of its frameworks rely on Apache Zookeeper for cluster coordinationZookeeper is somewhat difficult to administer and is known for complex failurecases In addition a highavailability Mesos cluster requires a minimum of threenodes which may be an onerous burden at some sitesDocker SwarmNot to be left behind Docker offers Swarm a container cluster manager built directly into Docker The current incarnation of Swarm emerged in as an answerto the growing popularity of Mesos Kubernetes and other cluster managers thatused Docker containers under the hood Container orchestration is now a majorfocus for Docker IncSwarm is easier to get started with than is Mesos or Kubernetes Any node that runsDocker can join the swarm as a worker node and any worker node can also be amanager There is no need to run separate nodes as masters Starting a swarm is assimple as running docker swarm init There are no additional processes to manageand configure and there is no state to track It works out of the boxYou can use familiar docker commands to run services as in Kubernetes collections of containers on the swarm You declare the state you want to achieve threecontainers running my web application and Swarm schedules the tasks on thecluster It automatically handles failure states and zerodowntime updatesSwarm has a builtin load balancer that adjusts automatically as containers are addedor removed The Swarm load balancer is not as fullfeatured as tools such as NGINXor HAProxy but on the other hand it doesnt require any administrative attentionSwarm supplies a secure Docker experience by default All connections betweennodes in a swarm are TLSencrypted and no configuration is required on the partof the administrator This is a major differentiator for Swarm when compared toits competitorsAWS EC Container ServiceAWS offers ECS a container management service designed for EC instances AWSsnative virtual servers In a manner reminiscent of many Amazon services AWSlaunched ECS with minimal functionality but has steadily enhanced it over timeECS has matured into a fine choice for sites that are already invested in AWS andwant to stick to EZ modeECS is a mostly managed service The cluster manager components are operatedby AWS Users run EC instances that have Docker and the ECS agent installed Theagent connects to the central ECS API and registers its resource availability To runa task on your ECS cluster you submit a task definition in JSON format throughthe API ECS then schedules the task on one of your nodesBecause the service is mostly managed the barrier to entry is low You can getstarted with ECS in just a few minutes The service scales well to at least hundredsof nodes and thousands of concurrent tasks Strictly speaking this is true for Kubernetes and Mesos as well but weve found it to be common practice to separate masters from agents in highavailability configurationsECS integrates with other AWS services For example load balancing among multiple tasks along with the requisite service discovery are handled by the Application Load Balancer service You can add resource capacity to your ECS cluster bytaking advantage of EC autoscaling ECS also integrates with AWSs Identity andAccess Manager service to grant permissions for your container tasks to interactwith other servicesOne of the most polished parts of ECS is the included Docker image registry Youcan upload Docker images to the EC Container Registry where theyre storedand made available to any Docker client whether its running on ECS or not Ifyoure running containers on AWS use the container registry in the same regionas your instances Youll achieve far better reliability and performance than withany other registryThe ECS user interface although functional shares the limitations of other AWSinterfaces The AWS CLI tool has complete support for the ECS API For management of applications on ECS we recommend turning to third party open sourcetools such as Empire githubcomremindempire or Convox convoxcomfor a more streamlined experience Recommended readingDocker Inc Official Docker Documentation docsdockercom Docker has gooddocumentation Its comprehensive and usually up to dateMatthias Karl and Sean Kane Docker Up Running Sebastopol CA OReilly Media This book focuses on running Docker containers in productionenvironmentsMouat Adrian Using Docker Developing and Deploying software with Containers Sebastopol CA OReilly Media This book covers topics from basic toadvanced and includes plenty of examplesTurnbull James The Docker Book wwwdockerbookcomThe Container Solutions blog at containersolutionscomblog includes technicalHOWTOs best practices and interviews with experts in the container spaceUntil the past decade or so updating software was a hairpulling timeconsumingexercise in frustration Release processes typically involved ad hoc homegrownscripts that were invoked in enigmatic order and saddled with outdated and incomplete documentation Testingif it existed at allwas performed by a qualityassurance team that was far removed from the development cycle and often becamea major obstacle to shipping code Administrators developers and project managers would plan dayslong marathons for the final stages of releasing updates to liveusers Service outages were often scheduled weeks in advanceGiven this unsavory context it should come as no surprise that some very smartpeople were working diligently to improve the situation After all where some seeonly problems others see opportunityAt top of mind is Martin Fowler an oracle of the software industry and chief scientist of the influential development shop ThoughtWorks In an insightful articlegooglYlisI Fowler describes continuous integration as a software developmentpractice where members of a team integrate their work frequently thus removing one of the major pain points of software work the tiresome task of reconcilingcode fragments that have diverged dramatically over a long period of independent Continuous Integrationand Deliverydevelopment The practice of continuous integration is now ubiquitous amongsoftware development teamsHot on the heels of this innovation came continuous delivery which is similar inconcept but which targets a separate goal reliably deploying updated software to livesystems Continuous delivery embraces the release of small incremental changesto IT infrastructure If something breaks that is if a regression is introduced itbecomes straightforward to isolate and resolve the issue because the changes between versions are small At the extreme end some sites aim to deploy new codeto users multiple times per day Bugs and security issues can be resolved in hoursrather than weeksIn combination continuous integration and continuous delivery henceforth denoted CICD encompass the tools and processes needed to facilitate frequent incremental software and configuration updatesCICD is a pillar of the DevOps philosophy Its the glue that holds together developers and operations folks It is as much a business asset as a technical innovationOnce introduced CICD becomes the bedrock of an IT organization because itimposes logic and organization on release processes that were previously chaoticSysadmins are central to the design implementation and ongoing maintenance ofCICD systems Administrators install configure and operate the tools that makeCICD function They are responsible for ensuring that software build processesare fast and reliableTesting is an important element of CICD and although administrators may notwrite the tests though they sometimes do they are often responsible for settingup the infrastructure and the systems on which the tests are performed Perhapsmost importantly it is ultimately system administrators who are responsible fordeployments the delivery component of CICDAn effective CICD system is implemented not with a solitary tool but rather with acollection of software that works in unison to form a cohesive environment Myriadopen source and commercial tools are available to coordinate the various elementsof CICD These coordination tools rely on other software packages to do the actual work eg compiling code or setting up servers in a particular configurationIndeed there are so many options that the initial approach to CICD can be overwhelming If nothing else the recent proliferation of tools in this space is evidenceof CICDs growing importance to the industryIn this chapter we attempt to navigate the maze of CICD concepts terminologyand tools We cover the basics of a CICD pipeline the various types of testing andtheir relevance to CICD the practice of running multiple environments in parallel and some of the most popular open source tools At the end of the chapter wedissect an example CICD pipeline that uses some of the most popular tools Whenyoure finished with this chapter you should understand some of the principles andtechniques that go into creating a powerful flexible CICD systemSee page for morecomments on DevOps CICD essentialsMany terms related to CICD sound similar and have overlapping meanings Solets first take a closer look at the differences between continuous integration delivery and deployment Continuous integration is the process of collaborating on a shared codebase merging disparate code changes into a version control system andautomatically creating and testing builds Continuous delivery is the process of automatically deploying builds tononproduction environments after the continuous integration processcompletes Continuous deployment closes the loop by deploying to live systems thatserve real users without any operator interventionContinuous deployment without any human supervision can be intimidating butthats precisely the point the idea is to reduce the fear factor by deploying as oftenas possible eliminating more and more issues until the team has enough confidencein the testing and tooling to enable automatic releasesContinuous deployment need not be the ultimate goal of all sites There may becompliance or risk reasons to pause at any point in the pipeline If thats the caseyou can still benefit from making each stage of the process as simple as possiblefor the human who pushes the final button Every organization should choose itsown boundariesPrinciples and practicesBusiness agility is one of the key benefits of CICD Continuous deployment facilitates the release of welltested features to production in minutes or hours insteadof weeks or months Because every change is built tested and deployed immediately the delta between versions is much smaller And that decreases the risk ofdeployment and helps narrow the range of possible root causes if something goeswrong Rather than staging a small number of bigbang deployments per year youmight find yourself releasing new code multiple times per week or even per dayCICD stresses the release of more features more often This goal is achievable onlywhen developers write and commit code in smaller chunks To realize continuousintegration developers need to push code changes at least once per day after running tests locallyFor administrators CICD processes greatly reduce the amount of time spent preparing for and implementing releases They also reduce the time spent debuggingproblems when deployments inevitably fail Few things are more satisfying thanwatching a new feature release itself to production without any human interventionThe next sections cover some basic rules of thumb to keep in mind as you developyour CICD processesUse revision controlAll code should be tracked in a source control system We recommend Git butthere are lots of options Most software development teams use source control asa matter of courseFor sites that embrace the infrastructureascode concept as we demonstrate inthe section CICD in practice starting on page you can track infrastructurerelated code alongside your applications You can even store documentation andconfiguration settings in source controlMake sure version control is the single source of truth Nothing can be managedmanually or off the recordBuild once deploy oftenA CICD pipeline begins with a build The output of the build the artifact isused from that point forward for testing and deployment The only way to confirmthat a specific build is ready to go to production is to run all tests against that buildDeploy the same artifact to at least one or two environments that match production as closely as possibleAutomate endtoendBuilding testing and deploying code without manual intervention is the key toreliable and reproducible updates Even if youre not planning to deploy code continuously to production the final production deployment step should run fullyunattended after being triggered by a humanBuild every integration commitAn integration merges changes made by multiple developers or teams of developersThe product is a composite code base that incorporates everyones updatesAn integration does not randomly snatch work in progress out of developers handsand stick it in the mainline code base thats a recipe for disaster Individual developers are responsible for managing their own development stream When theyreready they initiate an integration Integrations occur as frequently as possibleIntegrations are performed through the source control system The exact workflowvaries Individual developers might be responsible for merging their work back to thetrunk or a designated release overseer might integrate several developers or teamsat once The merge process can be largely automated but theres always the potential for two sets of changes to conflict That situation requires human interventionThe idea behind continuous integration is that commits to the revision control systems integration branch automatically result in a build The integration branchpart is important because source control serves several purposes In addition tobeing a vehicle for collaboration and integration its also useful as a backup systemas a checkpoint for work in progress and as a system that lets developers work onseveral updates while keeping the changes related to those updates logically separate Therefore only commits to the integration branch result in a buildFrequent integrations make it easy to trace a broken build back to the exact lines ofcode that caused the problem The revision control system can then determine theidentity of the responsible developer But note a broken build should carry little orno stigma The goal is just to get the build running again Encourage a blamefreeculture within your teamsShare responsibilityWhen something goes wrong the pipeline needs to be fixed No new code can bepushed until the previous problem has been resolved Its the equivalent of haltingthe assembly line in a factory It is the responsibility of the entire team to fix thebuild before resuming development workCICD shouldnt be a mysterious system that runs in the background and occasionally sends email when something is broken Every team member should haveaccess to the CICD interface to view dashboards and logs Some sites create humorous widgets such as RGB lighting fixtures that act as a visual indicator of thepipelines current statusBuild fast fix fastCICD is designed to yield feedback as quickly as possible ideally within minutesafter pushing code to source control This rapid response guarantees that developers pay attention to the result If the build fails the developers will likely be able tofix the problem quickly because the changes they just committed are fresh in theirminds Slow build processes are counterproductive Strive to eliminate redundantand timeconsuming steps Ensure that your build system has enough agents andthat the agents have sufficient system resources to build quicklyAudit and verifyPart of the CICD system includes a detailed history of every software release including its progression from development to production This auditability can beuseful to ensure that only authorized builds are deployed The settings and eventtimelines related to each environment can be irrefutably verifiedEnvironmentsApplications do not run in isolation They depend on external resources such as databases caches network filesystems DNS records remote HTTP APIs other applications and external network services An execution environment includes all theseresources and anything else that the application needs so it can run Building andmaintaining such environments is a target of substantial administrative attentionMost sites run at least three environments listed here in ascending order of importance Development dev for short for integrating updates from multiple developers testing infrastructure changes and checking for obvious failuresDevelopment is used mostly by the technical staff and not by business typesor end users In the context of CICD the development environment maybe created and reset multiple times per day Staging or stage for manual and automated testing and for furthervetting of changes and software updates Some organizations call this thetest environment Testers product owners and other business stakeholders use the staging environment to review new features and bug fixesStaging can also be used for penetration testing and other security checks Production prod for implementing service for real users The production environment usually includes extensive measures to ensure highperformance and strong security An outage in production is an allhandsondeck emergency that must be resolved immediatelyA typical CICD system promotes software through each of these environments insuccession filtering out errors and software defects along the way You can deployto production with confidence because you know that changes have already beentested in two other environmentsEnvironment parity is a subject of some complexity for administrators The purposeof the nonproduction or lower environments is to prepare and scrutinize changesof all types before they are made in production Substantive differences among environments can result in unforeseen incompatibilities that might ultimately causedegraded performance downtime or even the destruction of dataFor example imagine that the development and staging environments have undergone an operating system upgrade but production still runs the older OS versionIts time for a software deployment The new software is thoroughly tested in devand stage and it seems to work fine However an unexpected incompatibility becomes evident during the production rollout because the older version of a certainlibrary is missing functionality used by the new codeThis scenario is quite common and its one reason why administrators must bevigilant about keeping environments in sync The closer that lower environmentsmatch production the higher your chances of maintaining high availability anddelivering software successfullyRunning multiple environments at full capacity can be expensive and timeconsumingBecause production serves far more users than the lower environments its usuallynecessary to run a larger number of more expensive systems in that environmentProduction data sets tend to be larger and the provisioned disk space and serversize are beefed up to compensateEven this type of difference among environments can cause unanticipated problemsA load balancer misconfiguration that didnt matter in dev or stage may reveal adefect or a database query that runs quickly in dev and stage might turn out to befar slower when applied to productionscale dataMatching production capacity in lower environments is a tricky problem Strive tohave at least one lower environment that has redundancy in all the same places thatproduction does eg multiple web servers fully replicated databases and matching failover strategies for any clustered systems Its fine for the staging servers tobe smaller in size although any tests you run to check performance will not reflectproduction numbersFor best results data sets in lower environments should be similar in size and content to those of production One strategy is to create nightly snapshots of all relevant production data and copy it to the lower environment For compliance andgood security hygiene sensitive user data must be anonymized before its used thisway For truly massive data sets that are not practical to copy import a smaller butstill meaningful sampleDespite your best efforts lower environments will never be exactly like the production environment Some configuration settings such as credentials URLs addressesand hostnames will differ Use configuration management to track these configuration items among environments When the CICD system runs a deploymentconsult your source of truth to find the relevant configuration for that environmentand make sure that all environments are deployed in the same wayFeature flagsA feature flag enables or disables an application feature depending on the value ofa configuration setting Developers can build support for feature flags into theirsoftware You can use feature flags to enable certain features in specific environments For example you can enable a feature for the staging environment whilekeeping it disabled in production until its fully tested and ready for the user baseFor instance consider an ecommerce application that has a shopping cart Thebusiness wants to run a promotion that requires some changes to the code Thedevelopment team can build the feature and release it to all three environments inadvance but enable it only on dev and stage When the business is ready to advertise and activate the promotion enabling the feature becomes a simple lowriskconfiguration change rather than a software release If the feature has a bug its easyto disable it without updating the software PipelinesA CICD pipeline is a series of steps called stages that run in sequence Each stageis essentially a script that performs tasks specific to your software projectAt the most basic level a CICD pipeline Reliably builds and packages software Runs a series of automated tests to search for bugs and configuration errors Deploys code to one or more environments culminating in productionExhibit A illustrates the stages in a simple yet mature CICD pipelineExhibit A A basic CICD pipelineDeploy todev envRunTestsBadbuildBadbuildBadbuildBuild Deploy tostage envRunTestsDeploy toprod envRunTestsRollbackOngoingTestsCommitFAIL FAIL FAIL FAILMaturityThe following sections break down the three stages in further detailThe build processA build is a snapshot of the current status of a software project Its typically the firststage of any CICD pipeline possibly after a code analysis stage that monitors codequality and searches for security risks The build step transforms the code into aninstallable piece of software Builds can be triggered by a commit to the integrationbranch of the code repository or they can run on a regular schedule or on demandEvery pipeline run starts with a build but not every build reaches production Oncea build passes testing it becomes a release candidate If the release candidate isactually deployed to production it becomes a release If you do continuous deployment every release candidate is also a release Exhibit B illustrates these categoriesExhibit B Builds release candidates and releasesBuilds Releasecandidates ReleasesEvery timeyou change code All tests passed Deployed to productionThe precise steps of the build process depend on the language and software For aprogram in C C or Go the build process is a compilation often initiated by makethat results in an executable binary For languages that do not require compilationsuch as Python or Ruby the build stage might involve packaging the project withall relevant dependencies and assets including libraries images templates andmarkup files Some builds might involve only configuration changesThe output of the build stage is a build artifact The nature of that artifact dependson the software and the configuration of the rest of the pipeline Table listssome of the common types of artifacts Whatever the format the artifact is the basis for deployments throughout the rest of the pipelineTable Common types of build artifactsType What its forjar or war file Java archive or Java web application archiveStatic binary Statically compiled programs commonly C or Gorpm or deb file OSnative software packages for Red Hat or Debianpip or gem package Packaged Python or Ruby applicationsContainer image Applications that run under DockerMachine image Virtual servers especially for public or private cloudsexe file Windows executableBuild artifacts are saved to an artifact repository The type of repository dependson the type of artifact At its simplest a repository can be a directory on a remoteserver thats accessible through SFTP or NFS It can also be a yum or APT repository a Docker image repository or in the cloud an object store such as an AWS Sbucket The repository must be available to all the systems that need to downloadand install the artifact during a deploymentTestingEach stage in a CICD pipeline runs tests to catch buggy code and bad builds sothat the code that makes it through to production is free of defects or at least asfree as possible Testing is the linchpin of this process It engenders trust that arelease is ready to deployIf a build fails any test the remaining stages of the pipeline are pointless The teammust determine why the build failed and address the underlying issue Because buildsare created for every code push its easy to isolate the problem to the latest commitThe fewer lines of code changed between builds the easier the problems isolationFailures do not always stem from software bugs They can occur because of network conditions or infrastructure errors that require administrative attention Ifthe application depends on outside resources such as third party APIs there can beupstream failures in the external resource Some tests can run in isolation but othertests require the same infrastructure and data that will be present in productionConsider adding each of the following types of tests to your CICD pipeline Static code analysis examines code for syntax errors duplication violations of coding guidelines security problems or excessive code complexity These checks are fast and do not involve executing the actual code Unit tests are written by the same developers who write the application codeThey reflect the developers view of how the code is supposed to functionThe intent is to test the input and output of every method and functionunit in the code Code coverage is a sometimes misleading metricthat describes what portion of the code is being unittested Integration tests take unit tests one step further by running the applicationin its intended execution environment Integration tests run the application with its underlying frameworks and with external dependencies suchas outside APIs databases queues and caches Acceptance tests simulate typical use In contrast to unit tests acceptancetests reflect a users point of view For webbased software this stage mightinclude remotecontrolling browser page loads through tools such as Selenium For mobile software the build artifact might go to a device farmthat runs the app on many different mobile devices Different browsersand versions make acceptance tests challenging to create but in the endthese tests have meaningful results Performance tests search for performance problems introduced by thelatest code To identify bottlenecks these socalled stress tests shouldinvoke the application within a perfect clone of your production environment with real traffic patterns Tools such as JMeter or Gatling cansimulate thousands of concurrent users interacting with an application ina programmed pattern To gain the most from performance testing ensure that monitoring and graphing instrumentation are in place Thosetools clarify both the applications typical performance and its behaviorunder a new build Infrastructure tests go handinhand with programmatically provisionedcloud infrastructure If you create temporary cloud infrastructure as partof your CICD pipeline you can write test cases to verify the proper configuration and operation of the infrastructure itself Does the system runthrough configuration management successfully Are only the expecteddaemons running Serverspec serverspecorg is one interesting tool inthis space Sometimes the code thats difficult to test is also the most likely to have defects Your code may have code coverage which is quite high by industry standards but if the most complex code isnttested bugs might be missed Code coverage alone is not an adequate measure of code qualitySee Chapter forinformation aboutmonitoring andgraphing systemsDepending on the characteristics of your project some tests are more importantthan others For example software that implements a REST API has no need forbrowserbased acceptance tests Instead youll likely focus on integration tests Onthe other hand for shopping cart software browser tests for all the important userpaths catalog product pages cart checkout are mandatory Consider the needsof your project and implement testing accordinglyThese workflows dont have to be linear Actually because one of the goals is to getfeedback as quickly as possible its a good idea to run as much of the testing as possible in parallel But keep in mind that some tests might depend on the results ofother tests others might potentially interfere with each other Ideally tests shouldhave no crossdependenciesAvoid the temptation to ignore or overlook broken tests Its easy to get in the habitof understanding the reason for a failure considering it to be harmless or inapplicable and suppressing the test However this thinking is dangerous and can leadto a less reliable testing system Keep in mind the golden rule of CICD fixing abroken pipeline is the top priorityTo reinforce this tenet make it difficult to ignore failed tests It should be a technicalrequirement enforced through the CICD software that production deploymentscannot occur if there are any broken testsDeploymentDeployment is the act of installing software and preparing it for use within a serverenvironment The specifics of how this is done depend on the technology stack Adeployment system must understand how to retrieve the build artifact eg froma package repository or container image registry how to install it on the serverand what setup steps if any are necessary A deployment concludes when a newversion of software is running and the old version has been disabledA deployment might be as simple as updating some HTML files on disk No restartor further configuration required But for most cases a deployment involves at leastinstalling a package and restarting an application Complex largescale productiondeployments might involve installing code on multiple systems while serving livetraffic without pausing for a service outageSystem administrators play an important role in the deployment process They areusually responsible for creating deployment scripts monitoring important application health indicators during deployments and ensuring that the infrastructureand configuration needs of other team members are metThe following list describes just a few of the possible ways to deploy software Run a basic shell script that invokes ssh to log in to each system downloads and installs the build artifact and then restarts the application Thesetypes of scripts are usually home grown and do not scale to more than ahandful of systems Use a configuration management tool to orchestrate the installation process across a managed set of systems This strategy is more organized andscalable than the use of shell scripts Most configuration managementsystems are not designed specifically to facilitate deployments althoughthey can be used for this purpose If the build artifact is a container image and the application runs on a container management platform such as Kubernetes Docker Swarm or AWSECS the deployment might be nothing more than a quick API call to thecontainer manager The container service manages the rest of the deployment process on its own See Containers and CICD starting on page A few open source projects standardize and streamline deploymentCapistrano capistranorbcom is a Rubybased deployment tool that extends Rubys Rake system to run commands on remote systems Fabricfabfileorg is a similar tool written in Python These tools by developersfor developers are essentially elaborate shell scripts Software deployment is a wellexplored problem for users of public cloudsMost cloud ecosystems include both integrated and third party deployment services that can be used from a CICD pipeline Some examplesinclude Google Deployment Manager AWS CodeDeploy and HerokuTailor the deployment technique to your sites technology stack and service needs Ifyou have a simple environment with a few servers and a small handful of applicationsa configuration management tool might be appropriate At sites with a large numberof servers spread among data centers a specialized deployment tool is called forAn immutable deployment codifies the principle that servers should never bemodified or mutated once theyve been initialized To deploy a new release theCICD tooling creates entirely new servers with the updated build artifact includedin the image In this model servers are considered disposable and temporary Thisstrategy is predicated on a programmable infrastructure such as a public or privatecloud where instances can be allocated through an API call Some of the largestusers of the public cloud embrace immutable deploymentsIn CICD in practice starting on page we walk through a sample immutabledeployment that uses HashiCorps Terraform tool to create and update infrastructureZerodowntime deployment techniquesAt some sites services must continue to run even while they are being upgraded orredeployed either because an outage poses unacceptable risk health care government services or because it might have substantial financial costs highvolumeecommerce or financial services Updating live software without service interruptions is the Xanadu of software deployments and its the source of much anxiety and yakshavingOne common way to achieve a zerodowntime release is a bluegreen deploymentThe concept is straightforward stage the new software on a standby system or setof systems run tests to confirm its functionality then switch traffic from the livesystem to the standby once the tests are completeThis strategy works particularly well when traffic is proxied through a load balancer The live systems handle all the user connections while the standby systems arebeing prepared When the time is right the standby systems can be added to theload balancer and the previously live systems removed The deployment is complete when all the old systems are out of the rotation and all the transactions theywere handling have concludedA rolling deployment updates existing systems in a stepwise fashion modifyingsoftware on one system at a time Each system is removed from the load balancerupdated then added back to the rotation to accept user traffic This type of deployment can cause problems if the application cannot tolerate two different versionsrunning simultaneouslyBoth the bluegreen and rolling deployment strategies can accommodate a canaryakin to the hapless canary in a coal mine You first allocate a small amount of trafficto a single system or small percentage of systems that runs the new release If thenew release has problems you back it out and correct the problem having impactedonly a handful of users Of course the canary systems need precise telemetry andmonitoring so that you can determine whether problems have been introduced Jenkins the open source automation serverJenkins is an automation server written in Java Its by far the most popular software used to implement CICD With broad adoption and an extensive ecosystemof plugins Jenkins is well suited to a variety of use casesIts easy to dabble with Jenkins by running it in a Docker container docker run p name jenkins jenkinscijenkinsOnce the container starts you can access the Jenkins user interface in a web browseron port Youll find the initial administrator password buried in the containeroutput In practice you would need to change that password immediatelyA singlecontainer configuration is fine for learning the ropes but youll likely needa more robust solution in production environments The Jenkins download pagejenkinsiodownload has installation instructions that we neednt rehash hereRefer to those docs for installation on Linux and FreeBSD CloudBees the makerof Jenkins also offers a highavailability version called Jenkins EnterpriseJenkins has plugins for every conceivable task Use plugins to outsource buildsto different types of agents send notifications coordinate releases and executescheduled jobs Plugins integrate with open source tools and with all the majorcloud platforms and external SaaS providers Plugins give Jenkins superpowersSee page formore informationabout load balancersMost Jenkins configuration is done through the web UI and being merciful to yourattention span we dont attempt to cover the UIs nooks and crannies Instead weintroduce the fundamentals of Jenkins along with some of its most important featuresBasic Jenkins conceptsAt its core Jenkins is a coordination server that links a series of tools into a chainor to use CICD terminology a pipeline Jenkins is an organizer and facilitator allactual work depends on outside services such as source code repositories compilers build tools testing harnesses and deployment systemsA Jenkins job or project is a collection of linked stages Creating a project is the firstorder of business for a new installation You can link the projects steps together sothat they run in sequence or in parallel You can even set up conditional steps thatdo different things depending on the results of previous stepsEvery project should be connected to a source code repository Jenkins has nativesupport for pretty much every version control system Git Subversion Mercurialeven ancient systems such as CVS There are also integration plugins for higherlevelversion control services such as GitHub GitLab and BitBucket Youll need to giveJenkins the appropriate credentials to allow it to download code from your repositoryThe build context is the current working directory on the Jenkins system thatsexecuting a build Source code is copied into the build context along with any supporting files that are needed for the buildOnce youve wired up Jenkins to a version control repository you can create a buildtrigger This is the signal for Jenkins to copy the current source code and start thebuild process Jenkins can poll the source repository for new commits and initiatea build whenever it finds one It can also start builds on a schedule or be triggeredby a web hook a feature supported by GitHubAfter setting up the trigger create the build steps that is the specific tasks that willcreate a build Steps can be codebasespecific or they can be generic shell scriptsFor example Java projects are usually built with a tool called Maven A Jenkinsplugin supports Maven directly so you can simply add a Maven build step For aproject written in C the first build step might just be a shell script that runs makeThe remaining build steps depend on your goals for the project The most commonbuilds include steps that initiate the testing tasks discussed in Testing starting onpage You may need a step to create a custom build artifact such as a tarball OSpackage or container image You can also include steps that trigger administratornotifications take deploymentrelated actions or coordinate with outside toolingFor a CICD project the build steps can address all the stages of a pipeline buildthe code run tests upload the artifact to a repository and kick off a deploymentEach stage of the pipeline is a just a build step within the Jenkins project The Jenkins interface presents an overview of the status of each step so its easy to see at aglance whats happening in the pipelineSites that have many applications should have separate Jenkins projects for eachEach project will have a distinct code repository and build steps The Jenkins system needs all the tools and dependencies to run a build for any of its projects Forexample if you have configured both a Java project and a C project your Jenkinssystem must have both Maven and make installedProjects can depend on other projects Use this to your advantage by structuringprojects as generic inheritable templates For example if you have a variety of applications that are built differently but deployed in the same way eg as containersrunning on a server cluster you can create a generic deploy project that manages the common deployment stage Individual application projects can execute thedeploy project thereby eliminating a nowredundant build stepDistributed buildsAt sites that support dozens of applications each with its own dependencies andbuild steps its easy to inadvertently create dependency conflicts and bottlenecksbecause too many pipelines are running at once To compensate Jenkins lets yougraduate to a distributed build architecture This mode of operation uses a buildmaster a central system that keeps track of all the projects and their current stateand build agents which run the actual build steps for a project If you use Jenkinsa lot youll move to this configuration pretty quicklyBuild agents run on hosts that are separate from the build master The Jenkins master logs in to the slaves usually through SSH to start the agent process and to addlabels that document the slaves capabilities For example you might distinguishyour Javacapable agents from your Ccapable agents by applying appropriate labelsFor best results run agents in containers remote VMs or ephemeral cloud instancesthat scale out and back on demand If you have a container cluster you can use Jenkins plugins to run agents in the cluster through a container management systemPipeline as codeThus far weve described the process of setting up Jenkins projects by stringingtogether individual build steps in the web UI This is the quickest way to get started with Jenkins but from an infrastructure perspective its also a bit opaque Thecodein this context the contents of each build stepis managed by Jenkins Youcant check graphical build steps into a code repository and if you lose the Jenkinsserver theres no easy way to replace it youll need to restore your projects froma recent backupJenkins version introduced a major new feature called the Pipeline that affordsfirstclass support for CICD pipelines A Jenkins pipeline codifies the steps of aproject in a declarative domainspecific language thats based on the Groovy programming language You can commit the Jenkins pipeline code called a Jenkinsfilealongside the code thats associated with the pipelineThe following Jenkinsfile demonstrates a basic buildtestdeploy cyclepipeline agent any stages stageBuild steps sh make stageTest steps sh make test stageDeploy steps sh deploysh The agent any notation instructs Jenkins to prepare a workspace for this pipelineon any available build agent The Build Test and Deploy stages parallel the conceptual stages of a CICD pipeline In our example each stage has a single step thatinvokes a shell sh to execute a commandThe Deploy stage runs a custom script deploysh that handles the entire deployment including copying the build artifact generated by the Build stage to a set ofservers and restarting server processes In practice deployment would usually bedivided into multiple stages to afford better visibility and control over the full process CICD in practiceWe now turn to a contrived example to illustrate the concepts presented so far Weveconcocted a simple application UlsahGo thats a lot more basic than anything youmight need to manage in the real world Its entirely selfcontained and has no dependencies on other applications A workspace is the same as a build context a location on the agents local disk that contains all thefiles needed by the build including the source code and dependencies Every build has a privateworkspaceOur example includes the following elements The UlsahGo web application with just one small feature Unit tests for the application A virtual machine image for DigitalOcean which contains the application A singleserver development environment created on demand A loadbalanced multiserver staging environment created on demand A CICD pipeline that ties all these parts togetherWe use several popular tools and services in this example GitHub as the code repository DigitalOcean virtual machines and load balancers HashiCorps Packer for provisioning the DigitalOcean image HashiCorps Terraform to create deployment environments Jenkins to manage the CICD pipelineYour applications might use a different technology stack but the general conceptsare similar regardless of the toolingExhibit C depicts the first several stages of the example pipeline The diagramshows the pipeline polling GitHub for new commits to the UlsahGo project Whena commit is found Jenkins runs the unit test suite If the tests pass Jenkins buildsthe binary If the binary builds successfully the pipeline continues to create the deployment artifact a DigitalOcean machine image that includes the binary If any ofthe stages fail the pipeline reports an errorExhibit C Demonstration pipeline part oneErrorBuildUlsahGobinaryBuildDigitalOceanimageUnittestsCommitpollingBuild step failsUnit tests fail Image build failsJenkins PipelineFoundcommitUnittestspassBuildsucceedsUlsahGo GitHubrepositoryContinue todeploymentstagesWe describe the deployment stages in detail later But first we should review theseinitial stagesUlsahGo a trivial web applicationOur example application is a web service with a single feature It returns as JSONthe authors associated with a specified edition of this book For example the following query shows the authors for this edition curl ulsahgoadmincomedition authors Evi Garth Trent Ben Dan number We do some sanity checking to make sure users arent getting too carried away forexample by requesting implausible editions curl vs ulsahgoadmincomedition HTTP Not Found ContentType applicationjson error th edition is invalidOur application also has a healthcheck endpoint Health checks are an easy wayfor monitoring systems to ask the application Hey are you working OK curl ulsahgoadmincomhealthy healthy trueDevelopers typically work closely with administrators to create the build and teststages of a CICD pipeline In this case since the application is written in Go wecan use the standard Go tools go build and go test in our pipelineUnit testing UlsahGoUnit tests are the first test suite to run because they operate at the source code levelUnit tests involve testing the applications functionality at the finest possible granularity its functions and methods Most languages have testing frameworks thatoffer native support for unit testsLets examine one unit test for UlsahGo Consider the following functionfunc ordinaln int string suffix th switch n case suffix st case suffix nd case suffix rd return strconvItoan suffixThe function takes an integer as input and determines the corresponding ordinalexpression For example when passed a the function returns st UlsahGo usesthis function to format the text in the error message for invalid editionsUnit tests try to prove that given some input the function returns the expectedoutput Heres a unit test that exercises this functionfunc TestOrdinalt testingT ord ordinal exp st if ord exp tErrorexpected s got s exp ord ord ordinal exp th if ord exp tErrorexpected s got s exp ord This unit test runs the function on two values and and confirms that the actual response matches the expectation We can run the tests through Gos builtintesting framework go testPASSok githubcombwhaleyulsahgo sIf some part of the application changes in the futurefor example if updates aremade to the ordinal functionthe tests report any divergence from the expectedoutput Developers are responsible for updating unit tests as they adjust the codeExperienced developers by design write code that is easy to test They aim to havecomplete coverage of each method and function The ordinal function implements three special cases and a general case A full set of unit testswould exercise each of these possible paths through the codeTaking first steps with the Jenkins PipelineWith the code ready to ship and the unit tests in place the first step in our CICDjourney is to configure the project in Jenkins The GUI interface walks us throughthe process Here are our selections Our new project is a Pipeline project defined by code as opposed to atraditional freestyle project with build steps mostly defined throughuser interface elements We want to track our pipeline alongside our source code repository ina Jenkinsfile so we choose Pipeline script from SCM for the Pipelinedefinition We trigger the build by polling GitHub for commits We add credentialsso that Jenkins can access the UlsahGo repository and configure Jenkinsto poll GitHub for changes every five minutesThe initial setup takes just a few moments In real life we would use a GitHub webhook to notify Jenkins that a new commit was available thus avoiding polling andsparing GitHub from our unnecessary calls to their APIWith this setup Jenkins executes the pipeline described by the Jenkinsfile in therepository whenever a new commit is pushed to GitHubConsider how your repositories are organized In this project we chose to combineCICD and application code in the same repository with all CICDrelated files being kept in the pipeline subdirectory The UlsahGo repository is laid out as follows tree ulsahgoulsahgo pipeline Jenkinsfile packer provisionersh ulsahgojson ulsahgoservice production tfprodsh ulsahgotf testing tftestingsh ulsahgotf ulsahgogo ulsahgotestgoAn integrated structure works well for a small project like this one Jenkins PackerTerraform and other tools can look in the pipeline subdirectory for their configuration files Modifying the deployment pipeline is a simple matter of updating therepository For more complex environments where multiple projects share a common infrastructure it makes sense to have a dedicated infrastructure repositoryWith the project in place we can commit our first Jenkinsfile The first step in anypipeline is to check out the source code Heres a complete Jenkinsfile pipelinescript that does just thatpipeline agent any stages stageCheckout steps checkout scm The checkout scm line instructs Jenkins to check out the code from software configuration management a generic industry term for source controlWith Jenkins polling GitHub and the checkout stage complete we can move on tosetting up the test and build stages Our Go project has no external dependenciesThe only requirement for building and testing our code is the go binary We havealready installed go on our Jenkins system with aptget y install golanggo sowe need only add the test and build stages to the JenkinsfilestageUnit tests steps sh go test stageBuild steps sh go build After we commit the changes Jenkins discovers the new commit and executes thepipeline Jenkins emits friendly log output indicating that it has done soMar PM hudsontriggersSCMTriggerRunner runINFO SCM changes detected in UlsahGo Triggering Mar PM orgjenkinscipluginsworkflowjobWorkflowRunfinishINFO UlsahGo completed SUCCESSThe Jenkins GUI uses a weather metaphor to indicate the health of recent builds Asun icon represents a project that is building successfully and a stormy cloud iconrepresents failures You can debug build failures by inspecting the console outputwhich is found under the build details It shows the STDOUT printed by any partof the buildHeres a snippet from the go test and go build steps of our pipelinePipeline stagePipeline Unit TestsPipeline shUlsahGo Running shell script go testPASSok varjenkinshomeworkspaceUlsahGo sPipeline Pipeline stagePipeline stagePipeline BuildPipeline shUlsahGo Running shell script go buildPipeline Pipeline stagePipeline Pipeline nodePipeline End of PipelineFinished SUCCESSYou can normally pinpoint the cause of a failed build by reviewing the log Lookfor error messages that identify the failed step You can also add your own log messages to supply clues about the state of the system such as the values of variablesor the contents of a script at a given point in the execution Writing output for thepurpose of debugging is a timehonored programming traditionThe output of our build is a single binary file ulsahgo which contains our entireapplication This incidentally is one of the primary benefits of Go programs andone of the reasons Go is popular with sysadmins its easy to create static binariesthat run on multiple architectures and have no external dependencies Installing aGo application is often as simple as copying it to the systemBuilding a DigitalOcean imageWith ulsahgo ready to ship we next build a virtual machine image for the DigitalOcean cloud We start with a vanilla Ubuntu image install the latest updatesand then install ulsahgo The resulting image becomes the deployment artifact forthe remaining stages of the pipelineIf youre unfamiliar with the tool packer which creates virtual machine imagesrefer to the section Packer on page before continuingpacker reads its image configuration from a template that has two primary sectionsbuilders which interact with remote APIs to create machines and images and optional provisioners which run custom configuration stepsThe template for our UlsahGo image has only one builderbuilders type digitalocean apitoken rjFsrMIvqTlBqqBnfxQedJkkZJcqJcBOnmOihz region sfo size mb image ubuntux snapshotname ulsahgolatest sshusername rootThe builder tells packer which platform to build the image on and how to authenticate to the API among other providerspecific detailsThree provisioning steps followprovisioners type file source ulsahgo destination tmpulsahgo type file source pipelinepackerulsahgoservice destination etcsystemdsystemulsahgoservice type shell script pipelinepackerprovisionersh The first two provisioning steps add files to the image The first file is the applicationitself ulsahgo which is uploaded to tmp for later use The second is a systemddropin unit file that manages the serviceThe last provisioner executes a custom shell script on the remote system The scriptprovisionersh updates the system and then sets up the applicationusrbinenv bashappulsahgo Update the OS and add a useraptget update aptget y upgradeusrsbinuseradd s usrsbinnologin app Set up the working directory and appmkdir optapp chown app optappcp tmpapp optappappchown app optappapp chmod optappapp Enable the systemd unitsystemctl enable appSee page for moreinformation aboutsystemd unit filesIn addition to shell scripts packer lets you use all the popular configuration management tools as provisioning steps Call out to Puppet Chef Ansible or Salt toprovision your images in a more structured and scalable mannerFinally we can add an imagebuilding stage to our JenkinsfilestageBuild image steps sh packer build pipelinepackerulsahgojson packertxt sh grep ID packertxt grep E o doimagetxt The first step invokes packer and saves the output to packertxt in the builds working directory The tail end of that output includes an ID for the new image digitalocean Gracefully shutting down droplet digitalocean Creating snapshot ulsahgolatest digitalocean Waiting for snapshot to complete digitalocean Destroying droplet digitalocean Deleting temporary ssh keyBuild digitalocean finished Builds finished The artifacts of successful builds are digitalocean A snapshot was created ID The second step greps the ID from packertxt and saves it to a new file in the buildcontext Because the image is the deployment artifact we will need to refer to itsID from later stages of the pipelineProvisioning a single system for testingAt this point we have a process for continuously running unit tests building theapplication and creating a virtual machine image as a build artifact The remaining build stages focus on deploying the artifact and testing it in the wild Exhibit Dpicks up where Exhibit C on page leaves offExhibit D Demonstration pipeline part twoErrorCreate singleDigitalOceandropletRun testsagainstdropletCreateload balancedenvironmentRun testsagainst loadbalancerConguration failsDropletcreationfailsTestsfailTestsfailJenkins PipelineDOimagereadyWeve chosen to use terraform another gem from HashiCorp to create and manage the UlsahGo infrastructure terraform reads its configuration from plansJSONlike configuration files that describe a desired infrastructure configurationIt then creates the cloud resources described in the plan by making an appropriate series of API calls terraform supports dozens of cloud providers and a widevariety of servicesThe following terraform configuration ulsahgotf requisitions a single DigitalOcean droplet running the image we created in the previous stage of the pipelinevariable dotoken variable sshfingerprint variable doimage provider digitalocean token vardotokenresource digitaloceandroplet ulsahgolatest image vardoimage name ulsahgolatest region sfo size mb sshkeys varsshfingerprintMost of this is selfexplanatory use DigitalOcean as the provider and authenticatewith the provided token Create the droplet in the sfo region from the specifiedimage IDIn the Packer template on page we directly embedded parameters such as theAPI token in the builder configuration One big problem with that approach isthat the API key is saved in the source code repository even though its supposedlysecret The key gives access to the cloud providers API and hence would be dangerous in the wrong hands Keeping secrets in revision control is a security antipatternfor reasons we describe in more detail on page In this example we instead read the parameters as variables The three variables are The DigitalOcean API token The fingerprint of an SSH key that will be permitted to access the droplet The ID of the image to use for the new system which we captured duringthe previous stage of the pipelineJenkins can store secrets such as the API token in its credential store an encrypted area thats intended for exactly this kind of sensitive data The pipeline can readvalues from the credential store and save them as environment variables The values then become accessible throughout the pipeline without being saved in theversion control systemHeres how we set this up in the Jenkinsfilepipeline environment DOTOKEN credentialsdotoken SSHFINGERPRINT credentialssshfingerprint Recall that we saved the ID of the DigitalOcean machine image to a file within thebuild area doimagetxt We need that ID in our new pipeline stage which creates the actual DigitalOcean droplet The code for the new stage just runs a scriptfrom the project repositorystageCreate droplet steps sh bash pipelinetestingtftestingsh Its easier and more maintainable to separate the code of complex scripts from therest of the pipeline as weve done here tftestingsh contains the following linescp doimagetxt pipelinetestingcd pipelinetestingterraform apply var doimagedoimagetxt var dotokenDOTOKEN var sshfingerprintSSHFINGERPRINTterraform show terraformtfstate grep ipvaddress awk print doiptxtThis script copies the saved image ID to a temporary directory pipelinetestingthen runs terraform from that directory terraform looks for files in the currentdirectory that have tf extensions so we dont have to explicitly name the plan fileIts the same ulsahgotf file that we looked at on page A few explanations The DOTOKEN and SSHFINGERPRINT environment variables areavailable to any shell commands in the pipeline The environment clauseshown above can appear either at the level of the overall pipeline or withina particular stage depending on the scope you want doimagetxt reads the contents of the saved DigitalOcean imageID from the text file saved in the previous stage The final line of the tftestingsh script inspects the terraformcreateddroplet obtains its IP address and saves the address to a text file for usein the next stage The terraformtfstate file is terraforms snapshot of thesystem state Its how terraform keeps track of resourcesLike packer terraform sends useful output to the Jenkins console output pageHere are the relevant bits from the terraform apply commandPipeline Create dropletPipeline shUlsahGo Running shell scriptdigitaloceandropletulsahgolatest Creating disk computed image ipvaddress computed ipvaddressprivate computed name ulsahgolatest region sfo resizedisk true size mb sshkeys sshkeys status computeddigitaloceandropletulsahgolatest Still creating s elapseddigitaloceandropletulsahgolatest Still creating s elapseddigitaloceandropletulsahgolatest Still creating s elapseddigitaloceandropletulsahgolatest Creation complete ID Apply complete Resources added changed destroyedWhen this stage completes the droplet is up and running with UlsahGoTesting the dropletWe have some confidence that the code is functional because it passed the unittesting step However we also need to make sure that it runs successfully as partof a DigitalOcean droplet Testing at this level is considered a form of integrationtest We want the integration tests to run each time a new image is created so weadd a new stage to the JenkinsfilestageTest and destroy the droplet steps sh binbash l curl D v doiptxtedition grep HTTP curl D v doiptxtedition grep HTTP terraform destroy force Sometimes a blunt heavy object is the right tool for the job This pair of curl commands queries ulsahgo on the remote droplets port where ulsahgo runsby default We check that a query for the fifth edition returns an HTTP code success and that a query for the sixth edition returns an HTTP failure Weknow to expect these specific status codes only because of our familiarity with theapplicationAt the conclusion of the tests we destroy the droplet because its no longer neededThe droplet is created tested and destroyed each time the pipeline runsDeploying UlsahGo to a pair of droplets and a load balancerThe final pipeline task is to deploy to our mock production environment whichconsists of two DigitalOcean droplets and a load balancer Once again Terraformis up to the taskWe can reuse some of the configuration from the singledroplet terraform planfile We still need the same variables and the droplet resource This time we add asecond droplet resourceresource digitaloceandroplet ulsahgob name ulsahgob size mb image vardoimage sshkeys varsshfingerprint region sfoWe also add a load balancer resourceresource digitaloceanloadbalancer public name ulsahgolb region sfo forwardingrule entryport entryprotocol http targetport targetprotocol http healthcheck port protocol http path healthy dropletids digitaloceandropletulsahgoaid digitaloceandropletulsahgobid The load balancer listens on port and forwards requests to each of the droplets onport where ulsahgo is listening We tell the load balancer to use the healthyendpoint to confirm that each copy of the service is running The load balancer addsa droplet to the rotation if it receives a status code when it queries this endpointNow we can add the production configuration as a new stage in the pipelinestageCreate LB steps sh bash pipelineproductiontfprodsh The load balancer stage is more or less identical to the single instance stage Eventhe external script is pretty much the same so we omit its contents here We couldeasily refactor these scripts so that a single version could handle both environmentsbut for now weve kept the scripts separateWe can add a testing stage as well this time running against the load balancers IPaddressstageTest load balancer steps sh binbash l curl D v s dolbiptxtedition grep HTTP curl D v s dolbiptxtedition grep HTTP The curl commands are similar to the previous set but they target port wherethe load balancer listensConcluding the demonstration pipelineThis demonstration CICD implementation captures several of the key elementsof a realworld pipeline The first two stages unit testing and building demonstrate continuousintegration Each time a developer commits code Jenkins runs unit testsand tries to build the project The third stage creating a DigitalOcean image as a build artifact is thebeginning of continuous delivery We can use the same image when deploying to each environment Deployment to a single droplet is considered a development or testingenvironment The final stage deploys ulsahgo to a highavailability productionlike environment thereby closing the loop on a continuous deployment pipeline If any stage in the pipeline fails the subsequent stages are skipped In thatcase console output is available to help debug the problemThis pipeline relies on open source tools throughout All the deployment code iscaptured in just a few text files that are kept in the same repository as the applications source codeAstute readers will think of a host of improvements that could be made to thesesteps To name just a few A bluegreen deployment to ensure no downtime in the production stage Status notifications to email or chat rooms for each stage Hooks to help monitoring systems note that a new deployment has occurred A better method of propagating data such as the image ID between stagesContinuous improvement is integral to CICD and to system administration ingeneral Over time a chain of incremental improvements results in a highly efficient and automated software delivery system Containers and CICDMost software relies on outside dependencies such as third party libraries a particular filesystem layout the availability of certain environment variables and otherlocalizations Conflicts among required dependencies often make it hard to runmultiple applications on a single virtual machineTo further complicate matters building an application requires resources differentfrom those running it For example the build process might require a compiler anda test suite but these extras are not needed at run timeContainers offer an elegant solution to these problems From an operations viewpoint the environment needs only the capability to run containers You can activate any given container on any containercompatible system without further configuration effort because all dependencies and localizations for an app are housedwithin its container Multiple containers can run on the same system simultaneously without conflictYou can use containers to simplify your CICD environment in several ways By running the CICD system itself within a container By building applications inside containers By using container images as build artifacts for deploymentThe first point is rather obvious you can run your CICD software including boththe master and any agents in containers thereby avoiding the overhead of havingto dedicate systems to the CICD infrastructureThe other two scenarios require a bit more explication We look at them in moredetail in the next sectionsSee Chapter formore informationabout containersContainers as a build environmentThe exact environment needed to build an application is projectspecific and sometimes quite complex Rather than installing all the necessary tools build softwareand dependencies directly on your CICD agent systems you can build your software within containers and leave the CICD agents in a clean and generic state Thebuild process then becomes portable and independent of the specific CICD agentConsider a typical application that depends on a PostgreSQL database and a Rediskeyvalue store To build and test the application in a traditional setting youd needseparate servers for each component the application itself the Redis daemon andPostgreSQL In a pinch you might run all these components on one system butyou probably wouldnt use that same server to build and test another service thathad different dependenciesInstead you can use shortlived containers for each component One container canbuild and run the application It can connect to separate containers on the samehost or a different host for PostgreSQL and Redis Once the build process is complete the containers can be stopped and discarded You can use the same CICDagent to build with no risk of conflicts applications that have other dependenciesThe container image used to build software should be distinct from the containerimage that runs it The build image is normally larger than the runtime image because it includes extra components such as compilers and testing toolsMost current CICD tools include native support for containers Jenkins has a Dockerplugin that integrates nicely with the pipeline Also check out Drone trydroneioa CICD platform designed around containersContainer images as build artifactsThe product of a build can be a container image deployable through a container orchestration system Containers are lightweight and highly portable Movingcontainer images among systems by way of an image registry is easy and fast AnyCICD tool can adopt the strategy of producing containersThe basic workflow becomes Build your application inside a buildspecific container Create a container image that includes the application and its dependencies Push the image to a registry Deploy that image to a containerready execution environmentIts generally best to use a container management platform such as Docker SwarmMesosMarathon Kubernetes or AWS EC Container Service to deploy images intoproduction Your pipelines deployment stage can call the appropriate APIs and letthe platform handle the specifics Exhibit E on the next page illustrates the procedureExhibit E Containerbased deployment processBuildcontainerimagePushto registryTriggerdeploymentMasterNodeNodeNodeCICD server KubernetesWeve found containers to be an excellent match for mature CICD pipelines Theirextremely fast cycle time makes it easy both to deploy new code and to revert to aprevious version in the event of a problem Both virtual machines and configuration management systems are an order of magnitude slower Recommended readingBeck Kent et al Manifesto for Agile Software Development agilemanifestoorgDuvall Paul M Steve Matyas and Andrew Glover Continuous Integration Improving Software Quality and Reducing Risk Upper Saddle River NJ AddisonWesley Farcic Viktor The DevOps Toolkit Automating the Continuous DeploymentPipeline with Containerized Microservices Seattle WA Amazon Digital ServicesLLC Fowler Martin Continuous Integration googlYlisI martinfowlercomHumble Jez and David Farley Continuous Delivery Reliable Software Releasesthrough Build Test and Deployment Automation Upper Saddle River NJ AddisonWesley Morris Kief Infrastructure as Code Managing Servers in the Cloud SebastopolCA OReilly Media jenkinscidocsgooglegroupscom Jenkins User Handbook jenkinsiodocbookComputer security is in a sorry state In contrast to the progress seen in virtuallyevery other area of computing security flaws have become increasingly dire andthe consequences of inadequate security more severe Computer security issuesdirectly influence and threaten societies around the worldIf youre tempted to skip over this chapter permit us to pique your curiosity byreminding you of a few computer security events that have occurred since the lastedition of this book The sophisticated Stuxnet worm discovered in attacked Irans nuclear program by damaging centrifuges at a uranium enrichment plant In Edward Snowden exposed the massive NSA surveillance machinerevealing that some major Internet companies were complicit in allowingthe government to spy on US citizens Around a new type of attack known as ransomware came to prominence Attackers compromise a target system and encrypt its data holdingit hostage Victims must pay a ransom for recovery They often do Security In the US Office of Personnel Management was breached compromising the sensitive and private details of more than million UScitizens many of whom had security clearances In Russian statesponsored hackers allegedly mounted a campaignto influence the outcome of the US presidential election In a ransomware attack of unprecedented scale took over Windowssystems in more than countries The attack used an exploit developedby the NSAThe stakes have never been higher We think it will get worse before it gets betterPart of the challenge is that security problems are not purely technical You cannotsolve them by buying a particular product or service from a third party Achievingan acceptable level of security requires patience vigilance knowledge and persistencenot just from you and other sysadmins but from your entire user andmanagement communitiesAs a system administrator you bear a heavy burden You must push an agenda thatsecures your organizations systems and networks ensures that they are vigilantlymonitored and properly educates your users and your staff Familiarize yourselfwith current security technology and work with experts to identify and resolvevulnerabilities at your site Security considerations should be part of every decisionStrike a balance between security and usability Remember thatSecurity ConvenienceThe more security measures you introduce the more constrained you and yourusers will be Implement the security measures suggested in this chapter only aftercarefully considering the implications for your usersIs UNIX secure Of course not UNIX and Linux are not secure nor is any otheroperating system that communicates on a network If you must have absolute totalunbreachable security then you need a measurable air gap between your computerand any other device Some people argue that you also need to enclose your computer in a special room that blocks electromagnetic radiation look up Faradaycage How fun is thatThis chapter examines the complex field of computer security the sources of attacks the basic ways to secure systems the tools of the trade and sources of additional information Sometimes even an air gap isnt enough In a paper Genkin Shamir and Tromer described atechnique to extract RSA encryption keys from laptops by analyzing the highpitched frequenciesthey emit when decrypting a file Elements of securityThe field of information security is quite broad but it is often best described by theCIA triad This acronym stands for Confidentiality Integrity AvailabilityConfidentiality concerns the privacy of data Access to information should be limited to those who are authorized to have it Authentication access control and encryption are a few of the subcomponents of confidentiality If a hacker breaks intoa system and steals a database of customer contact information a compromise ofconfidentiality has occurredIntegrity relates to the authenticity of information Data integrity technology ensures that information is valid and has not been altered in unauthorized ways Italso addresses the trustworthiness of information sources When a secure web sitepresents a signed TLS certificate it is proving to the user not only that the information it is sending is encrypted but also that a trusted certificate authority suchas VeriSign or Equifax has verified the identity of the source Technologies suchas PGP also offer some assurance of data integrityAvailability expresses the idea that information must be accessible to authorizedusers when they need it Otherwise the data has no value Outages not caused byintruders eg those caused by administrative errors or power outages also fallinto the category of availability problems Unfortunately availability is often ignoreduntil something goes wrongConsider the CIA principles as you design implement and maintain systems andnetworks As the old security adage goes security is a process How security is compromisedIn this section we take a general look at how realworld security problems tend tooccur Most security lapses fit into one of the following categoriesSocial engineeringHuman users and administrators of a computer system are the weakest links inthe chain of security Even in todays world of heightened security awareness unsuspecting users with good intentions are easily convinced to give away sensitiveinformation No amount of technology can protect against the user elementyoumust ensure that your user community has a high awareness of security threats sothat they can be part of the defenseThis problem manifests itself in many forms Attackers coldcall their victims andpose as legitimately confused users in an attempt to get help accessing the systemSomeone unintentionally posts sensitive information on a public forum whiletroubleshooting problems Physical compromises occur when seemingly legitimatemaintenance personnel show up to rewire the network closetThe term phishing describes attempts to collect information from users or to coaxthem to into doing something foolish such as installing malware Phishing beginswith deceptive emails instant messages text messages or social media contactsTargeted attacks so called spear phishing can be especially hard to defend againstbecause the communication often includes victimspecific information that lendsan appearance of authenticitySocial engineering is a powerful hacking technique and is one of the most difficultthreats to neutralize Your site security policy should include training for new employees Regular organizationwide communications are an effective way to informstaff about social media threats physical security email phishing multifactor authentication and good password selectionTo gauge your organizations resistance to social engineering you might find itinformative to attempt some social engineering attacks of your own Be sure youhave explicit permission to do this from your own managers however These exploits look very suspicious if they are performed without a clear mandate Theyrealso a form of internal spying so they have the potential to generate resentment iftheyre not handled forthrightlyMany organizations find it useful to communicate to users that administrators willnever request their passwords Tell users to report any such password requests tothe IT department immediatelySoftware vulnerabilitiesOver the years countless securitysapping bugs have been discovered in computersoftware By exploiting subtle programming errors or context dependencies hackers have been able to manipulate systems into a variety of compromising positionsBuffer overflows are an example of a programming error with complex security implications Developers often allocate a predetermined amount of temporary memory space called a buffer to store a particular piece of information If the code isntcareful about checking the size of the data against the size of the container thatssupposed to hold it the memory adjacent to the allocated space is at risk of beingoverwritten Crafty hackers can input carefully composed data that crashes theprogram or in the worst case executes arbitrary codeBuffer overflows are a subcategory of a larger class of software security bugs knownas input validation vulnerabilities Nearly all programs accept some type of inputfrom users eg commandline arguments parameters for an HTTP request Ifthe code processes such data without rigorously checking it for appropriate formatand content bad things can happenIn some ways open source operating systems have a leg up on security The sourcecode for Linux and FreeBSD is available to everyone and thousands of people canand do scrutinize each line of code for possible security threats This arrangement is widely believed to result in better security than that of closed operatingsystems where a limited number of people have the opportunity to examine thecode for weaknessesWhat can you as an administrator do to prevent this type of attack It depends onthe application but one obvious approach is to reduce the privileges that your applications run with to minimize the impact of security bugs A process running asan unprivileged user can do less damage than one that runs as root For the paranoid this approach can include a mandatory access control system such as SELinuxContainers with limited capabilities can also play a role hereOver time the open source community has developed a standard process for addressing software vulnerabilities Initial reports should go directly to the softwaredevelopers so that patches to address the issue can be developed and released before hackers formulate methods to exploit it Later details of the security issue arereleased publicly so that administrators become aware of it and so that the issue andthe patches can receive public scrutiny For this reason keeping up with patchesand security bulletins is an important part of most administrators job Fortunately modern operating systems strive to make software updates straightforward andeasy to automateDistributed denialofservice attacks DDoSA DDoS attack aims to interrupt a service or adversely impact its performancemaking the service unavailable to users Its usually achieved by flooding a site withnetwork traffic thereby consuming all the sites available bandwidth or system resources DDoS attacks can be financially motivated in which case the attacker holdsthe site for ransom or they can be either political or retaliatoryTo conduct an attack attackers plant malicious code on unprotected devices outside the victims network This code lets the attackers remotely command these intermediary systems forming a botnet In the most common DDoS scenario theminions of the botnet are instructed to pelt the victim with network trafficIn recent years botnets have been assembled from Internetconnected devices suchas IP cameras printers and even baby monitors These devices have essentiallyno security and the owners usually remain unaware that their devices have beencompromised Sophisticated commandandcontrol tools for managing botnetsare available on the dark web for anyone to purchase Some of them even includefree customer serviceSee Chapter formore informationabout containersIn the fall of the Mirai botnet targeted security researcher and blogger BrianKrebs slamming his site with Gbs of traffic from tens of thousands of sourceIP addresses Naturally his hosting provider asked him to kindly move somewhereelse The Mirai botnet code has since been open sourcedMost of the responsibility for preventing and mitigating DDoS attacks falls to thenetwork management layer Software and hardware are available to detect attacksand shut them down while keeping legitimate services online Public cloud providers and some colocation facilities are equipped with this technology Howeverthe mitigations arent perfect and the threats are constantly shiftingInsider abuseEmployees contractors and consultants are trusted agents of an organization andare granted special privileges Sometimes these privileges are abused Insiders cansteal or reveal data disrupt systems for financial gain or create havoc for politicalreasonsThis type of attack is often the hardest of all to detect Most security measures guardagainst external threats so they arent effective against users who have been grantedaccess Insiders are typically not under suspicion in the first place only the mostrigorous organizations systematically monitor their own employeesSystem administrators must never knowingly install back doors in the environmentfor their own use Such facilities are too easily misinterpreted or exploited by othersNetwork system or application configuration errorsSoftware can be configured securely or notsosecurely Software is developed to beuseful instead of annoying hence notsosecurely is too often the default Hackersfrequently gain access by exploiting features that would be considered helpful andconvenient in less treacherous circumstances accounts without passwords firewallswith overly relaxed rules and unprotected databases to name a fewA typical example of a host configuration vulnerability is the standard practice ofallowing Linux systems to boot without requiring a boot loader password GRUBcan be configured at installation to require a password but administrators almostnever activate this option This omission leaves the system open to physical attackHowever its also a perfect example of the need to balance security against usabilityRequiring a password means that if the system were unintentionally rebooted egafter a power outage an administrator would have to be physically present to getthe machine running againOne of the most important steps in securing a system is simply making sure thatyou havent inadvertently put out a welcome mat for hackers Problems in this category are the easiest to find and fix although there are potentially a lot of them andits not always obvious what to check for The port and vulnerability scanning toolscovered later in this chapter can help a motivated administrator identify problemsbefore theyre exploited Basic security measuresMost systems do not come secured out of the box Customizations made duringand after installation change the security profile for new systems Administratorsshould take steps to harden new systems integrate them into the local environmentand plan for their longterm security maintenanceWhen the auditors come knocking its useful to be able to prove that you havefollowed some kind of standard procedure especially if that procedure conformsto external recommendations and best practices for your industry Refer to Sources of security information on page for recommendations on selecting a systemhardening standardAt the highest level you can improve your sites security by keeping in mind a fewrules of thumb Apply the principle of least privilege by allocating only the minimumprivileges needed by each entity person or role This principle applies tofirewall rules user permissions file permissions and any other situationwhere access controls are used Layer security measures to achieve defense in depth For example dontrely solely on your external firewall for network protection Otherwiseyou are simply building a structure like a Tootsie Pop a hard crunchyoutside and a soft chewy center Minimize the attack surface The fewer interfaces exposed systems unnecessary services and unused or underused systems the lower the potential for vulnerabilities and security weaknessesAutomation is a close ally in the security war Use configuration management andscripting to create repeatably secure systems and applications The more securitysteps you automate the less room is available for human errorSoftware updatesKeeping systems updated with the latest patches is an administrators highestvalue security chore Most systems are preconfigured to point at the vendors packagerepository which makes applying patches as simple as running a few commandsLarger sites can use a local repository that mirrors that of the vendor thus savingexternal bandwidth and speeding updatesA reasonable approach to patching should include the following elements A regular schedule for installing routine patches that is followed diligentlyConsider the impact of patches on users when designing this scheduleMonthly updates are usually sufficient but be prepared to apply criticalpatches on short notice A change plan that documents the impact of each set of patches outlinespostinstallation testing steps and describes how to back out the changes inthe event of problems Communicate this change plan to all relevant parties An understanding of which patches pertain to the environment Administrators should subscribe to vendorspecific security mailing lists andblogs as well as to generalized security discussion forums such as Bugtraq An accurate inventory of applications and operating systems used in yourenvironment This census helps ensure complete coverage Use reportingsoftware to keep track of your installed baseSoftware patches sometimes introduce novel security problems and weaknessesof their own However most exploits target older vulnerabilities that are widelyknown Statistically speaking you are much better off with systems that are regularly updated Make sure its done methodically and consistentlyUnnecessary servicesStock systems come with lots of services running by default Disable and possiblyremove those that are unnecessary especially if they are network daemons Oneway to see which services are using the network is to use the netstat commandHeres partial output from a FreeBSD systemfreebsd netstat an grep LISTENtcp LISTENtcp LISTENtcp LISTENtcp LISTENLinux is transitioning to the ss command for this purpose but netstat still worksthere tooA variety of commands can help pinpoint the service thats using a port For example you can use lsoffreebsd sudo lsof iCOMMAND PID USER FD TYPE SIZEOFF NODE NAMEsshd root u IPv t TCP ssh LISTENsshd root u IPv t TCP ssh LISTENsshd root u IPv t TCP sshESTABLISHEDsshd vagrant u IPv t TCP sshESTABLISHEDOnce you have the PIDs you can then use ps to identify specific processes If a service is unneeded stop it and make sure that it wont be restarted at boot time Youcan also use the tools fuser or netstat p if lsof is not availableLimit your systems overall footprint The fewer packages the less vulnerable softwareThe industry as a whole is beginning to address this issue by reducing the numberof packages included in a default installation Some specialized distributions suchas CoreOS take this to the extreme and force nearly everything to run in a containerRemote event loggingThe syslog service forwards log information to files lists of users or other hosts onyour network Consider setting up a secure host to act as a central logging machinethat parses forwarded events and responds appropriately A single centralized logaggregator can capture logs from a variety of devices and alert administrators whenever meaningful events occur Remote logging also prevents hackers from coveringtheir tracks by rewriting or erasing log files on systems that have been compromisedMost systems come configured to use syslog by default but you will need to customize the configuration to set up remote loggingBackupsRegular tested system backups are an essential part of any site security plan Theyfall into the availability bucket of the CIA triad Make sure that all filesystemsare regularly replicated and that you store some backups offsite If a significantsecurity incident occurs youll then have an uncontaminated checkpoint fromwhich to restoreHowever backups can also be a security hazard Protect your backups by limitingand monitoring access and by encrypting backup filesViruses and wormsUNIX and Linux have been mostly immune from viruses Only a handful existmost of which are academic in nature and none have wreaked the kind of costly havoc that has become commonplace in the Windows world Nonetheless thisfact hasnt stopped certain antivirus vendors from predicting the demise of theplatform from malwareunless you purchase their antivirus product at a speciallow price of courseThe exact reason for the lack of malicious software is unclear Some claim that UNIXsimply has less market share than its desktop competitors and is therefore not aninteresting target for virus authors Others insist that UNIXs accesscontrolled environment limits the damage from selfpropagating worms and virusesThe latter argument has some validity Because UNIX restricts write access to systemexecutables at the filesystem level unprivileged user accounts cannot infect the restof the environment Unless virus code is being run by root the scope of infectionSee Chapter formore about startingprocesses at boot timeSee Chapter for more information about loggingis significantly limited The moral then is not to use the root account for daytoday activities See page for more comments on this issuePerhaps counterintuitively one valid reason to run antivirus software on UNIXservers is to protect your sites Windows systems from Windowsspecific virusesA mail server can scan incoming email attachments for viruses and a file servercan scan shared files for infectionClamAV by Tomasz Kojm is a popular free antivirus product for UNIX and LinuxThis widely used GPL tool is a complete antivirus tool kit that includes signaturesfor thousands of viruses You can download the latest version from clamavnetOf course one school of thought argues that antivirus software is itself counterproductive Its detection and prevention rates are mediocre and the cost of licensingand management are burdensome All too frequently antivirus software breaksother aspects of a system resulting in a variety of tech support problems Somecompromises have even resulted from attacks on the antivirus infrastructure itselfRecent versions of Microsoft Windows include a basic antivirus tool called WindowsDefender Its not the quickest to detect new forms of malware but its effective andrelatively unlikely to interfere with other aspects of the systemRoot kitsThe craftiest hackers try to cover their tracks and avoid detection Often they hopeto continue using your system to distribute software illegally probe other networksor launch attacks against other systems They often use root kits to help them remain undetected Sony is notorious for having included rootkitlike capabilitiesin the copy protection software included on millions of music CDsRoot kits are programs and patches that hide important system information suchas process disk or network activity They come in many flavors and vary in sophistication from simple application replacements such as hacked versions of ls andps to kernel modules that are nearly impossible to detectHostbased intrusion detection software such as OSSEC is an effective way to monitorsystems for the presence of root kits File integrity monitoring tools such as AIDEfor Linux can alert you to files that have changed unexpectedly Rootkitfindingscripts such as chkrootkit chkrootkitorg can identify known kitsAlthough programs are available to help administrators remove root kits from acompromised system the time it takes to perform a thorough cleaning would probably be better spent saving data and wiping the system The most advanced rootkits are aware of common removal programs and make attempts to subvert themSee Chapter for more information about emailcontent scanningPacket filteringIf youre connecting a system to a network that has Internet access you must install a packetfiltering router or firewall between the system and the outside worldThe packet filter should pass only traffic for services that you specifically want tooffer from that system Limiting the public exposure of your systems is a firstlinedefense Many systems do not need to be directly accessible to the public InternetIn addition to firewalling systems at the Internet gateway you can double up withhostbased packet filters such as ipfw for FreeBSD and iptables or ufw on LinuxDetermine which services run on the host and open ports only for those servicesIn some cases you can also limit which source addresses are allowed to connect toeach port Many systems need only one or two ports to be accessibleIf your systems are in the cloud you can use security groups rather than physicalfirewalls When designing security group rules be as granular as possible Consideradding outbound rules as well to limit an attackers ability to make outbound connections from your hosts See the platformspecific sections in Cloud networkingstarting on page for additional discussion of this topicPasswords and multifactor authenticationWere simple people with simple rules Heres one every account must have a password and it needs to be something that cant easily be guessed Password complexityrules may be a hassle but they exist for a reason Guessable passwords are one ofthe leading sources of compromiseOne of our favorite trends from recent years is the proliferation of support formultifactor authentication MFA systems These schemes validate your identityboth through something you know a password or passphrase and something youhave such as a physical device commonly a phone Almost any interface can beprotected with MFA from UNIX shell accounts to bank accounts Enabling MFAis an easy and powerful security winFor a variety of reasons MFA is now an absolute minimum requirement for anyInternetfacing portal that gives access to administrative privileges That includesVPNs SSH access and administrative interfaces to web applications An argumentcan be made that singlefactor passwordonly authentication is not acceptable forany user authentication but you must secure at least all administrative interfaceswith MFA Fortunately excellent cloudbased MFA services are available such asGoogle Authenticator and Duo duocomVigilanceTo ensure the security of your system regularly monitor its health network connections process table and overall status usually daily Do regular selfassessmentswith the power tools discussed later in this chapter Security compromises tend tostart with a small foothold and expand so the earlier you identify an anomaly thebetter off youll be This is much easier said than doneYou might find it beneficial to work with an external firm to perform a comprehensive vulnerability analysis These projects can draw your attention to issues that youhadnt previously considered At a minimum they establish a baseline understanding of the areas in which youre most exposed Such engagements often reveal thathackers have already been nesting in the clients networkApplication penetration testingApplications that are exposed to the Internet need their own security precautionsin addition to general system and network hygiene Because of the widespreadproliferation of vulnerability data and exploit tools its a good idea to have all applications penetration tested to verify that theyve been designed with security inmind and have appropriate controls in placeSecurity is only as strong as the weakest link in the chain If you have a secure network and system infrastructure but an application running on that infrastructureallows access to sensitive data without a password for example you have won thebattle but lost the warPenetration testing is a poorly defined discipline Many companies that tout theirpenetration testing services focus mostly on smoke and mirrors The Hollywoodscenes of adolescent kids in windowless basements filled with sera terminalsarent entirely inaccurate Buyer bewareFortunately the Open Web Application Security Project OWASP publishes information about common application vulnerabilities and methods for probing applications for these issues Our recommendation is that you have a professional thirdparty who specializes in application penetration testing perform a penetrationtest at launch and periodically throughout the life of an application Make sure theyadhere to the OWASP methodology Passwords and user accountsIn addition to securing all Internetfacing privileged access through multifactor authentication its important to select and manage passwords securely In the world ofsudo administrators personal passwords are just as important as root passwordsMore so in fact the more frequently a password is used the more opportunities thereare for it to be compromised through methods other than bruteforce decryptionFrom a narrowly technical perspective the most secure password of a given lengthconsists of a random sequence of letters punctuation and digits Years of propaganda and picky web site password forms have convinced most people that this is thesort of password they ought to be using But of course they never do unless theyuse a password vault to remember passwords on their behalf Random passwordsSee page for moreinformation aboutpassword crackingare simply impractical to commit to memory at the lengths needed to withstandbruteforce attacks characters or longerBecause password security increases exponentially with length your best bet is touse a very long password a passphrase that is unlikely to appear elsewhere butis easy to remember You can throw in a misspelling or modified character for extracredit but the general idea is to let the length do the heavy lifting for youFor example six guests drank Evis poisoned wine is an excellent passphrase Orat least it was until it appeared in this book Thats true despite the fact that it consists mostly of common lowercase dictionary words and despite the fact that thewords are logically related and grammatically orderedThe other core concept that all administrators and users must keep in mind is thata given passphrase should never be used for more than one purpose It is all toocommon that a large breach occurs and usernames with passwords are exposed Ifthose usernames and passwords were used elsewhere all those accounts are compromised now too Never use the same password across administrative boundarieseg your personal banking site vs social mediaPassword changesChange root and administrator passwords At least every six months Every time someone who had access to them leaves your site Whenever you wonder whether security might have been compromisedIn the past the conventional wisdom has been that passwords should be changedfrequently to guard against the possibility of undetected compromises Howeverpassword updates have their own risks and they disrupt life for administratorsCompetent hackers install backup access mechanisms as soon as they penetrate asite so password changes are less helpful than they might initially seemIts still advisable to make regularly scheduled changes but dont go overboard If youreally want to increase security youre better off obsessing about password qualityPassword vaults and password escrowIts often said that passwords should never be written down but its perhaps moreaccurate to say that they should never be left accessible to the wrong people As security maven Bruce Schneier has noted a scrap of paper in an administrators wallet is relatively secure in comparison to most forms of Internetconnected storageA password vault is a piece of software or a combination of software and hardwarethat stores passwords for your organization in a more secure fashion than Wouldyou like Windows to remember this password for youSeveral developments have made a password vault almost a necessity The proliferation of passwords needed not just to log in to computers butalso to access web pages configure routers and firewalls and administerremote services The increasing need for strong passwords as computers get so fast thatweak passwords are easily broken Regulations that require access to certain data to be traceable to a singlepersonno shared logins such as rootPassword management systems became more popular in the wake of US legislation that imposed additional requirements on sectors such as government financeand health care In some cases this legislation requires multifactor authenticationPassword vaults are also a great boon for sysadmin support companies who mustsecurely and traceably manage passwords not only for their own machines but alsofor their customers machinesPassword vaults encrypt the passwords they store Typically every user has a separate vault password Just when you thought your password travails were over nowyou have even more passwords to manage and worry aboutMany password vault implementations are available Free ones for individuals egKeePass store passwords locally give allornothing access to the password database and do no logging Appliances suitable for huge enterprises eg CyberArkcan cost tens of thousands of dollars Many of the commercial offerings charge either by the user or by the number of passwords they rememberThe vault system we particularly like is Password from AgileBits passwordcomPassword comes from the massmarket world so it includes polished crossplatform UIs and integration with common web browsers Password has a teamslayer that extends this foundation of personal password management into the domain of organizational secretsAnother system worth evaluating is Secret Server from Thycotic thycoticcom Thissystem is browserbased and was designed from the ground up to serve the needsof organizations It includes extensive management and auditing features alongwith rolebased access control see page and finegrained permission optionsOne useful feature to look for in a password management system is a break the glassoption so named in honor of the hotel fire alarm stations that tell you to break theglass and pull the big red lever in the event of an emergency In this case breakingthe glass means obtaining a password that you wouldnt normally have access towith loud alarms being forwarded to other administrators Its a nice compromisebetween parsimonious password sharing a normal best practice and the realitiesof emergency fire fightingPoor password management is a common security weakness By default the contentsof the etcpasswd and etcshadow files or on FreeBSD the etcmasterpasswdfile determine who can log in so these files are the systems first line of defenseagainst intruders They must be scrupulously maintained and free of errors security hazards and historical baggageUNIX allows users to choose their own passwords and although this is a great convenience it leads to many security problems The comments in the section Passwordsand user accounts on page apply equally to user passwordsIt is important to regularly verify preferably daily that every login has a passwordEntries in the etcshadow file that describe pseudousers such as daemon whoown files but never log in should have a star or an exclamation point in their encrypted password fields These do not match any password and thus prevent useof the accountAt sites that use a centralized authentication scheme such as LDAP or Active Directory the same logic applies Enforce password complexity requirements and lockout accounts after a few failed login attemptsPassword agingMost systems that have shadow passwords also let you compel users to change theirpasswords periodically a facility known as password aging This feature may seemappealing at first glance but it has several problems Users often resent having tochange their passwords and since they dont want to forget the new password theychoose something simple that is easy to type and remember Many users switchbetween two passwords each time they are forced to change or increment a digitin the password defeating the purpose of password aging PAM modules see page can help enforce strong passwords to avoid this pitfallOn Linux systems the chage program controls password aging Using chage administrators can enforce minimum and maximum times between password changespassword expiration dates the number of days to warn users before their passwordsexpire the number of days of inactivity that are permissible before accounts are automatically locked and more The following command sets the minimum numberof days between password changes to sets the maximum number to sets theexpiration date to July and warns the user for days that the expirationdate is approachinglinux sudo chage m M E W benUnder FreeBSD the pw command manages password aging parameters This example sets the password validity period to days and sets the expiration date toSeptember freebsd sudo pw user mod trent p e See page formore informationabout the passwd fileGroup logins and shared loginsAny login that is used by more than one person is bad news Group logins egguest or demo are sure terrain for hackers to homestead and are prohibited inmany contexts by federal regulations such as HIPAA Dont allow them at your siteHowever technical controls cant prevent users from sharing passwords so education is the best enforcement tacticUser shellsIn theory you can set the shell for a user account to be just about any program including a custom script In practice the use of shells other than standards such asbash and tcsh is a dangerous practice If you find yourself tempted to create sucha login you might consider a passphraseless SSH key pair insteadRootly entriesThe only distinguishing feature of the root login is its UID of zero Since there canbe more than one entry in the etcpasswd file that uses this UID there can bemore than one way to log in as rootA common way for a hacker to install a back door after having obtained a root shellis to edit new root logins into etcpasswd Programs such as who and w refer to thename stored in utmp rather than the UID that owns the login shell so they cannotexpose hackers that appear to be innocent users but are really logged in as UID Dont allow root to log in remotely even through the standard root account Under OpenSSH you can set the PermitRootLogin configuration option to No in theetcsshsshdconfig file to enforce this restrictionBecause of sudo see page its rare that youll ever need to log in as root evenon the system console Security power toolsSome of the timeconsuming chores mentioned in the previous sections can be automated with freely available tools Here are a few of the tools youll want to look atNmap network port scannerNmaps main function is to check a set of target hosts to see which TCP and UDPports have servers listening on them Since most network services are associatedwith well known port numbers this information tells you quite a lot about thesoftware a machine is running As described on page a port is a numbered communication channel An IP address identifies anentire machine and an IP address port number identifies a specific server or network conversationon that machineRunning Nmap is a great way to find out what a system looks like to someone onthe outside who is trying to break in For example heres a report from a production Ubuntu systemubuntu nmap sT ubuntubooklabatrustcomStarting Nmap httpinsecureorg at MSTInteresting ports on ubuntubooklabatrustcom Not shown closed portsPORT STATE SERVICEtcp open smtptcp open httptcp open rpcbindtcp open netbiosssntcp open microsoftdstcp open mysqlNmap finished IP address host up scanned in secondsBy default nmap includes the sT argument to try to connect to each TCP port onthe target host in the normal way Once a connection has been established nmapimmediately disconnects which is impolite but not harmful to a properly writtennetwork serverFrom the example above we can see that the host ubuntu is running two servicesthat are likely to be unused and that have historically been associated with security problems portmap rpcbind and an email server smtp An attacker wouldmost likely probe those ports for more information as a next step in the informationgathering processThe STATE column in nmaps output shows open for ports that have servers listening closed for ports with no server unfiltered for ports in an unknown state andfiltered for ports that cannot be probed because of an intervening packet filternmap does not classify ports as unfiltered unless it is running an ACK scan Hereare results from a more secure server securebooklabatrustcomubuntu nmap sT securebooklabatrustcomStarting Nmap httpinsecureorg at MSTInteresting ports on securebooklabatrustcom Not shown closed portsPORT STATE SERVICEtcp open smtptcp open httpNmap finished IP address host up scanned in secondsIn this case its clear that the host is set up to allow SMTP email and an HTTPserver A firewall blocks access to other ports Actually only privileged ports those with port numbers under and other wellknown portsare checked by default Use the p option to explicitly specify a range of ports to scanIn addition to straightforward TCP and UDP probes nmap also has a repertoire ofsneaky ways to probe ports without initiating an actual connection In most casesnmap probes with packets that look like they come from the middle of a TCP conversation rather than the beginning and waits for diagnostic packets to be sentback These stealth probes may be effective at getting past a firewall or at avoidingdetection by a network security monitor on the lookout for port scanners If yoursite uses a firewall see Firewalls on page its a good idea to probe it withthese alternative scanning modes to see what they turn upnmap has the magical and useful ability to guess what operating system a remotesystem is running by looking at the particulars of its implementation of TCPIP Itcan sometimes even identify the software thats running on an open port The Oand sV options respectively turn on this behavior For exampleubuntu sudo nmap sV O securebooklabatrustcomStarting Nmap httpinsecureorg at MSTInteresting ports on securebooklabatrustcom Not shown closed portsPORT STATE SERVICE VERSIONtcp open smtp Postfix smtpdtcp open http lighttpd Device type general purposeRunning Linux XXXOS details Linux Nmap finished IP address host up scanned in secondsThis feature can be useful for taking an inventory of a local network Unfortunatelyit is also useful to hackers who can base their attacks on known weaknesses of thetarget OSes and serversKeep in mind that most administrators dont appreciate your efforts to scan theirnetwork and point out its vulnerabilities however wellintentioned your motiveDo not run nmap on someone elses network without permission from one of thatnetworks administratorsNessus nextgeneration network scannerNessus originally released by Renaud Deraison in is a powerful and usefulsoftware vulnerability scanner At this point it uses more than plugins tocheck for both local and remote security flaws Although it is now a closed sourceproprietary product it is still freely available and new plugins are released regularly It is the most widely accepted and complete vulnerability scanner availableNessus prides itself on being the security scanner that takes nothing for grantedInstead of assuming that all web servers run on port for instance it scans forweb servers running on any port and checks them for vulnerabilities Instead ofrelying on the version numbers reported by the service it has connected to Nessus can attempt to exploit known vulnerabilities to see if the service is susceptibleAlthough a substantial amount of setup time is required to get Nessus running itrequires several packages that arent installed on a typical system its well worththe effort The Nessus system includes a client and a server The server acts as adatabase and the client handles the GUI presentation Nessus servers and clientsexist for both Windows and UNIX platformsOne of the great advantages of Nessus is the systems modular design which makesit easy for third parties to add new security checks Thanks to an active user community Nessus is likely to be a useful tool for years to comeMetasploit penetration testing softwarePenetration testing is the act of breaking into a computer network with the ownerspermission for the purpose of discovering security weaknesses Metasploit is anopen source software package written in Ruby that automates this processMetasploit is controlled by the USbased security company Rapid but its GitHubproject has hundreds of contributors Metasploit includes a database of hundreds ofreadymade exploits for known software vulnerabilities For those that have the desire and the skill its possible to write custom exploit plugins to add to the databaseMetasploit uses the following basic workflow Scan remote systems to discover information about them Select and execute exploits according to the information found If a target is penetrated use included tools to pivot from the compromised system to other hosts on the remote network Run reports to document the results Clean up and revert all changes to the remote systemMetasploit has several interfaces a command line a web interface and a full GUIclient Choose the format that you like the best they have equivalent functionalityLearn more at metasploitcomLynis onbox security auditingIf you were faced with finding holes in the walls of an old wooden barn you mightfirst walk around the outside of the barn and look for the large gaping holes Networkbased vulnerability scanning tools like Nessus give you this view of a systemssecurity profile Walking inside the barn on a sunny day highlights the pinpointsizedholes in walls To get this same level of inspection of a system you need a tool likeLynis that runs on the system itselfAlthough unfortunately named this security power tool performs both onetimeand scheduled audits of a systems configuration patching and hardening state Thisopen source tool runs on Linux and FreeBSD systems and performs hundreds ofautomated compliance checks Download it from cisofycomlynisJohn the Ripper finder of insecure passwordsOne way to thwart poor password choices is to try to break the passwords yourselfand to force users to change passwords that you have broken John the Ripper is asophisticated tool by Solar Designer that implements various passwordcrackingalgorithms in a single tool It replaces the tool crack which was covered in previous editions of this bookEven though most systems use a shadow password file to hide encrypted passwordsfrom public view its still wise to verify that your users passwords are crack resistant Knowing a users password can be useful because people tend to use the samepassword over and over again A single password might grant access to anothersystem decrypt files stored in a users home directory and allow access to financialaccounts on the web Needless to say its not securitysmart to reuse a passwordthis way But nobody wants to remember hundreds of passwordsConsidering its internal complexity John the Ripper is an extremely simple program to use Direct john to the file to be cracked most often etcshadow andwatch the magic happen sudo john etcshadowLoaded password hashes with different salts FreeBSD MD password jsmithbadpass tjonesIn this example unique passwords were read from the shadow file As passwordsare cracked john prints them to the screen and saves them to a file called johnpotThe output contains the password in the left column with the login in parenthesesin the right column To reprint passwords after john has completed run the samecommand with the show argumentAs of this writing the most recent stable version of John the Ripper is Itsavailable from openwallcomjohn Since John the Rippers output contains thepasswords it has broken carefully protect this output and delete it as soon as youare done checking to see which users passwords are insecureAs with most security monitoring techniques its important to obtain explicit management approval before cracking passwords with John the RipperBro the programmable network intrusion detection systemBro is an open source network intrusion detection system NIDS that monitorsnetwork traffic and looks for suspicious activity It was originally written by VernPaxson and is available from broorgBro inspects all traffic flowing into and out of a network It can operate in passivemode in which it generates alerts for suspicious activity or in active mode in whichit injects traffic to disrupt malicious activity Both modes likely require modificationof your sites network configuration Especially the passwords of system administrators who have sudo privilegesUnlike other NIDSs Bro monitors traffic flows rather than just matching patternsinside individual packets This method of operation means that Bro can detectsuspicious activity by observing who talks to whom even without matching anyparticular string or pattern For example Bro can Detect systems used as stepping stones by correlating inbound andoutbound traffic Detect a server that has a back door installed by watching for unexpectedoutbound connections immediately after an inbound one Detect protocols running on nonstandard ports Report correctly guessed passwordsSome of these features require substantial system resources but Bro includes clustering support to help you manage a group of sensor machinesThe configuration language for Bro is complex and requires significant coding experience to use Unfortunately there is no simple default configuration for a noviceto install Most sites require a moderate level of customizationBro is supported to some extent by the Networking Research Group of the International Computer Science Institute ICSI but its mostly maintained by the communityof Bro users If you are looking for a turnkey commercial NIDS you will probablybe disappointed by Bro However Bro can do things that no commercial NIDS cando and it can either supplement or replace a commercial solution in your networkSnort the popular network intrusion detection systemSnort snortorg is an open source network intrusion prevention and detectionsystem originally written by Marty Roesch and now maintained by Cisco a commercial entity It has become the de facto standard for homegrown NIDS deployments and is also the basis of many commercial and managed services NIDSimplementationsSnort itself is distributed for free as an open source package However Cisco chargesa subscription fee for access to the most recent set of detection rulesA number of third party platforms incorporate or extend Snort and some of thoseprojects are open source One excellent example is Aanval aanvalcom whichaggregates data from multiple Snort sensors in a web consoleSnort captures raw packets off the network wire and compares them with a set ofrules aka signatures When Snort detects an event thats been defined as interesting it can alert a system administrator or contact a network device to block theundesired traffic among other actionsAlthough Bro is a much more powerful system Snort is a lot simpler and easier toconfigure attributes that make it a good choice as a starter NIDS platformOSSEC hostbased intrusion detectionOSSEC is free software and is available as source code under the GNU GeneralPublic License OSSEC serves up the following Root kit detection Filesystem integrity checks Log file analysis Timebased alerting Active responsesOSSEC runs on the systems of interest and monitors their activity It can send alertsor take action according to a set of rules that you configure For example OSSECcan monitor systems for the addition of unauthorized files and send email notifications like this oneSubject OSSEC Notification courtesy Alert level Date Fri Feb From OSSEC HIDS ossecmcourtesyatrustcomTo courtesyadminatrustcomOSSEC HIDS Notification Feb Received From courtesysyscheckRule fired level File added to the systemPortion of the logsNew filecourtesyhttpdbarkingsealcomhtmlwpcontentuploadshbirdjpgadded to the file system END OF NOTIFICATIONIn this way OSSEC acts as your eyes and ears on the system We recommendrunning OSSEC on every production systemOSSEC basic conceptsOSSEC has two primary components the manager server and the agents clientsYou need one manager on your network and you should install that componentfirst The manager stores the fileintegritychecking databases logs events rulesdecoders major configuration options and system auditing entries for the entirenetwork A manager can connect to any OSSEC agent regardless of its operatingsystem The manager can also monitor certain devices that do not have a dedicated OSSEC agentAgents run on the systems you want to monitor and report back to the managerBy design they have a small footprint and operate with a minimal set of privilegesMost of the agents configuration is obtained from the manager Communicationbetween the server and the agent is encrypted and authenticated You need to create an authentication key for each agent on the managerOSSEC classifies alerts by severity at levels to is the highest severityOSSEC installationOSSEC packages for most distributions are available at ossecgithubioInstall the server on the system you want to be your OSSEC manager and then install the agent on that and all other systems you want to monitor The install scriptasks some additional questions such as to what email address alerts should be sentand which monitoring modules should be enabledOnce the installation has finished start OSSEC withserver sudo varossecbinosseccontrol startNext register each agent with the manager On the server runserver sudo varossecbinmanageagentsYoull see a menu that looks something like this OSSEC HIDS v Agent manager The following options are availableAdd an agent AExtract key for an agent EList already added agents LRemove an agent RQuitChoose your action AELR or QSelect option A to add an agent and then type in the name and IP address of theagent Next select option E to extract the agents key Heres what that looks likeAvailable agentsID Name linuxclient IP Provide the ID of the agent to extract the key or q to quit Agent key information for isMDAyIGxpbnVYxpZWMSAxOTIuMTYLjcLjMgZjkYjMyYzlkMjgMWJlMTFinally log in to the agent system and run manageagents thereagent sudo varossecbinmanageagentsOn the client you will see that the menu has somewhat different options OSSEC HIDS v Agent manager The following options are availableImport key from the server IQuitChoose your action I or QSelect option I and then cut and paste the key you extracted above After you haveadded an agent you must restart the OSSEC server Repeat the process of key generation extraction and installation for each agent you want to connectOSSEC configurationOnce OSSEC is installed and running youll want to tweak it so that it gives youjust enough information but not too much The majority of the configuration isstored on the server in the varossecetcossecconf file This XMLstyle file is wellcommented and fairly intuitive but it contains dozens of optionsA common item you might want to configure is the list of files to ignore when checking file integrity For example if you have a custom application that writes its logfile to varlogcustomapplog you can add the following line to the syschecksection of the filesyscheckignorevarlogcustomapplogignoresyscheckAfter youve made this change and restarted the OSSEC server OSSEC will stopalerting you every time the log file changes The many OSSEC configuration optionsare documented at ossecnetmainmanualconfigurationoptionsIt takes time and effort to get any HIDS system running and tuned But after a fewweeks youll have filtered out the noise and the system will start to generate valuable information about changing conditions in your environmentFailBan bruteforce attack response systemFailBan is a Python script that monitors log files such as varlogauthlog andvarlogapacheerrorlog It looks for suspicious occurrences such as multiplefailed login attempts or queries to unusually long URLs FailBan then takes action to address the threat For example it might temporarily block network trafficfrom a particular IP address or send email to an incident response team Learnmore at failbanorg Cryptography primerMost software is designed with security in mind and that implies a strong dose ofcryptography Security standards and regulations are opinionated about the selection of cryptographic algorithms and the type of data that must be protected withcryptography Nearly all network protocols in modern use rely on cryptographyfor security In short cryptography is a pillar of computer security and sysadminsencounter it every day Its well worth your time to understand the basicsCryptography applies mathematics to the problem of securing communicationsA cryptographic algorithm called a cipher is the set of mathematical steps takento secure a message Such algorithms are designed by committees of experts whorepresent academic government and research interests from around the world Acceptance of a new algorithm is a lengthy and tedious process By the time it makesits way to the masses it has been thoroughly vettedEncryption is the process of using a cipher to convert plain text messages to unreadable ciphertext Decryption is the reverse of that process Cryptographic messagesciphertext exhibit several advantageous properties Confidentiality messages are impossible to read for everyone except theintended recipients Integrity it is impossible to modify the contents without detection Nonrepudiation the authenticity of the message can be validatedIn other words cryptography lets you communicate secretly over unsecured channels with the added benefit of being able to prove the correctness of the messageand the identity of sender Very valuable indeedMathematics shows that strong cryptographic algorithms are reliably secure Howeversoftware that implements the algorithms might have weaknesses and the securityof systems that guard cryptographic secrets might also be vulnerable rendering thealgorithms impotent Protecting your secrets and choosing welldesigned easilyupdated cryptography software is therefore paramountCryptographers have traditional names for three subjects who participate in a simplemessage exchange Alice and Bob who wish to communicate privately and Mallorya bad actor who wants to compromise their secrets disrupt their communicationor impersonate one of the other principals Weve adopted this conventionThe upcoming subsections introduce several cryptographic primitives the associated ciphers and some common use cases for eachSymmetric key cryptographySymmetric key cryptography is sometimes called conventional or classic cryptography because the ideas behind it have been around for a long time Its simple Some ciphers offer only a subset of these Often multiple ciphers are used together to create the fullset thus forming a hybrid cryptosystemAlice and Bob share a secret key that they use to encrypt and decrypt messages Theymust find a way to exchange the shared secret privately Once they both know thekey they can reuse it as long as they wish Mallory can only inspect or interferewith messages if she also has the keySymmetric keys are relatively efficient in terms of CPU usage and the size of theencrypted payloads As a result symmetric cryptography is often used in applications where efficient encryption and decryption are necessary However the needto distribute the shared key in advance is a serious impediment to many use casesAES the Advanced Encryption Standard from the United States National Instituteof Standards and Technology NIST is perhaps the most widely used symmetrickey algorithm Twofish and its predecessor Blowfish designed by cryptographerand security expert Bruce Schneier are also options These algorithms play a rolein the security of every network protocol you can shake your fist at including SSHTLS IPsec VPNs PGP and many othersPublic key cryptographyA limitation of symmetric keys is the need to securely exchange the secret key inadvance The only way to do so with complete security is to meet in person withoutinterference a major inconvenience For centuries this requirement limited thepractical utility of cryptography The invention of public key cryptography whichaddresses this problem was therefore an extraordinary breakthrough when it occurred in the sThe scheme works as follows Alice generates a pair of keys The private key remains a secret but the public key can be widely known Bob similarly generatesa key pair and publishes his public key When Alice wants to send Bob a messageshe encrypts it with Bobs public key Bob who holds the private key is the onlyone who can decrypt the messageExhibit A Sending a ciphertext message with public key cryptographyPlaintextmessagePlaintextmessageAlice BobEncryptwith Bobspublic keyDecryptwith Bobsprivate key CiphertextmessageAlice can also sign the message with her private key Bob can use Alices signature andher public key to validate its authenticity This process simplified here for clarityis known as a digital signature It proves that Alice not Mallory sent the messageThe DiffieHellmanMerkle key exchange method was the first publicly availablepublic key cryptosystem Shortly thereafter the RSA public key cryptosystem wascirculated by the nowfamous team of Ron Rivest Adi Shamir and Leonard Adleman These techniques are the foundation of modern network securityPublic key ciphers also called asymmetric ciphers rely on the mathematical concept of trapdoor functions in which a value is easy to compute and yet it is difficultand expensive to derive the steps that produced that value The performance characteristics of asymmetric ciphers generally render them impractical for encryptinglarge quantities of data They are often paired with symmetric ciphers to realize thebenefits of both public keys establish a session and share a symmetric key and thesymmetric key encrypts the ongoing conversationPublic key infrastructureOrganizing a trustworthy and reliable way to record and distribute public keys isa messy business If Alice wants to send Bob a private message she must trust thatthe public key she has for Bob is in fact his and not Mallorys Validating the authenticity of public keys at Internet scale is a formidable challengeOne solution adopted by PGP is a socalled web of trust It boils down to a network of entities who trust each other to varying degrees By following indirectchains of trust outside your personal network you can establish that a public keyis trustworthy with a reasonable degree of confidence Unfortunately the general publics interest in attending keysigning parties and cultivating a network ofcryptofriends has been shall we say less than enthusiastic as evidenced by PGPscontinuing obscurityThe Public Key Infrastructure used to implement TLS on the web addresses thisproblem by trusting a third party known as a Certificate Authority CA to vouchfor public keys Alice and Bob may not know each other but they both trust the CAand know the CAs public key The CA signs certificates for Alice and Bobs publickeys with its own private key Alice and Bob can then check the CAs endorsementsto be sure the keys are legitimateThe certificates of major CAs such as GeoTrust and VeriSign are bundled with operating system distributions When a client begins an encrypted session it will seethat the peers certificate has been signed by an authority already listed in the clientslocal trust store Hence the client can trust the CAs signature and can trust thatthe peers public key is valid The scheme is depicted in Exhibit B on the next pageCertificate authorities charge a fee for signing services the price of which is setaccording to the reputation of the CA market conditions and various features ofthe certificate Some variations such as socalled wild card certificates for entiresubdomains or extended validation certificates with a more rigorous backgroundcheck for the requesting entity are more expensiveExhibit B Public key infrastructure process for the webSysadminWeb serverUserCerticate Authority Administratorsends certicatesigning request Administrator installssigned certicate and privatekey on servers Client requestspublic certicate Server replies withpublic certicate Client checkssignature againstlocal trust store CA returnssigned certicateCAThe CA is implicitly trusted in this system Initially there were only a few trustedCAs but many more have been added over time Modern desktop and mobile operating systems trust hundreds of certificate authorities by default The CAs themselves are therefore highvalue targets for attackers who would like to use the CAsprivate key to sign certificates of their own devisingWhen an authority is hacked the entire system of trust is broken Several CAs areknown to have been compromised by attackers and in other widely discussed incidents CAs are known to have conspired with governments We encourage readersto choose issuing CAs carefully when purchasing signing servicesIn Lets Encrypt was launched as a free service sponsored by organizationssuch as the Electronic Frontier Foundation the Mozilla Foundation Cisco SystemsStanford Law School and the Linux Foundation that issues certificates throughan automated system By the end of this service had issued over millioncertificates Given the wellpublicized operational issues at some of the commercialCAs we recommend Lets Encrypt as a probably just as secure free alternativeIts also easy to act as your own certificate authority You can create a CA withOpenSSL import the CAs certificate to the trust store throughout your site andthen issue certificates against that authority This is a common practice for securing services on an intranet where the organization has full control over the trustedcertificate store See page for more detailsOrganizations should be careful when deciding to implement their own trustedauthority on companyissued machines Unless you have the same rigorous andaudited security in place that the professional CAs do you might just be creatinga gaping vulnerability in your environment As a corollary if you work for an organization that installs its own certificate in your computers trusted store suspectthat your own security may be compromised and act accordinglyTransport Layer SecurityTransport Layer Security TLS uses public key cryptography and PKI to securemessages between nodes on a network It is the successor to SSL the Secure Sockets Layer and youll commonly see the acronyms SSL and TLS used interchangeably even though the old SSL is obsolete and deprecated TLS paired with HTTPis known as HTTPSTLS runs as a separate layer that wraps TCP connections It supplies only the security for the connection and does not involve itself in the HTTP transaction Because of this hygienic architecture TLS can secure not only HTTP but also otherprotocols such as SMTPOnce a client and server have established a TLS connection the contents of theexchange including the URL and all headers are protected by encryption Onlythe host and port can be determined by an attacker since those details are visiblethrough the encapsulating TCP connection In the OSI model TLS lies somewherebetween layers and Although the typical use case is oneway TLS encryption in which the client validates the server it is possible and increasingly common to use twoway TLS sometimes known as mutual authentication In this scheme the client must present tothe server a certificate that proves its own identity This is for example how Netflix clients settop boxes and anything else that streams video from Netflix areauthenticated to the Netflix APIThe latest revision of TLS is Disable all versions of SSL along with TLS version because of known weaknesses TLS is under active development and introduces major changes that will have significant implications for some industriesCryptographic hash functionsA hash function accepts input data of any length and generates a small fixedlengthvalue that is somehow derived from that data The output value is variously referredto as a hash value hash summary digest checksum or fingerprint Hash functionsare deterministic so if you run a particular hash function on a particular input youwill always generate the same hash valueBecause hashes have a fixed length only a finite number of possible output valuesexist For example an bit hash has only that is possible outputs Therefore some inputs necessarily generate the same hash value an event known asa collision Longer hash values reduce the frequency of collisions but can nevereliminate them entirely A representative from the financial services industry attempted to influence a technical decision onthe TLS development mailing list but he was about two years too late The concern was summarilyrejected in an entertaining email thread See the thread at googluAEwPNHundreds of different hash functions are used in software but the subset knownas cryptographic hash functions are of particular interest to sysadmins and mathematicians In this context cryptographic means real good These hash functionsare designed to have pretty much every desirable property you could want from ahash function including the following Entanglement every bit of the hash value depends on every bit of the input data On average changing one bit of input should cause of thehash bits to change Pseudorandomness hash values should be indistinguishable from random data Of course hash values are not random they are generateddeterministically and reproducibly from input data But they should stilllook like random data they should have no detectable internal structureshould have no apparent relationship to the input data and should passall known statistical tests of randomness Nonreversibility given a hash value it should be computationally infeasibleto discover another input that generates the same hash valueWith a sufficiently highquality hashing algorithm and a sufficiently long hash value length we can make the leap of faith of assuming that two inputs that generatethe same hash value are in fact the same input Of course that cant ever be theoretically certain because all hashes have collisions However it can be made likelyto any desired level of statistical proof by increasing the length of the hash valueCryptographic hashes verify the integrity of things They can certify that a givenconfiguration file or command binary has not been tampered with or that a messagesigned by an email correspondent has not been modified in transit For example toverify that a FreeBSD system and a Linux system are using identical sshdconfigfiles we can use the following commandsfreebsd sha etcsshsshdconfigSHA etcsshsshdconfig efddcfcbfeadlinux shasum etcsshsshdconfigefddcfcbfead etcsshsshdconfigWeve elided part of the hash values for simplicity As is typical for most use casesthe output values are shown here in hexadecimal notation But keep in mind thatthe actual hash values are just bags of binary data and that this data can be represented in multiple waysMany cryptographic hash algorithms exist but the only ones recommended for general use at this point are the SHA and SHA Secure Hash Algorithm familieswhich were selected through an extensive review process by NISTEach of these algorithms exists in a range of variants with different hash valuelengths For example SHA is the SHA algorithm configured to generate SHA has been compromised and should no longer be useda bit hash value A SHA algorithm without a version number eg SHAalways refers to a member of the SHA familyAnother common cryptographic hash algorithm MD remains widely supported by cryptographic software However its known to be vulnerable to engineeredcollisions in which multiple inputs yield the same hash value Although MD is nolonger considered safe for use in cryptography its still a wellbehaved hash function and is theoretically OK to use for lowsecurity applications But why botherJust use SHAOpen source software projects often publish hashes of the files they release to thecommunity The OpenSSH project for example distributes PGP signatures whichrely on cryptographic hash functions of its tarballs for verification To verify theauthenticity and integrity of a download you calculate the hash value of the file youactually downloaded and compare it to the published hash value thus ensuring thatyouve received a complete and unmolested copy with no bit errorsHash functions are also used as a component of message authentication codes akaMACs The hash value inside a MAC is signed with a private key The process ofvalidating the MAC checks both the authenticity of the MAC itself by decryptingit with the corresponding public key and the integrity of the content by checkingit against the content hash MAC schemes often play an important role in webapplication securityRandom number generationCryptographic systems need a source of random numbers from which to generatekeys But algorithms arent known for their random and unpredictable behaviorWhat to doThe gold standard for randomness is data from physically random processes suchas radioactive decay and RF noise from the galactic core These sources do exist seerandomorg for access to some actual random data and an explanation of how its derived Interesting but unfortunately not directly helpful for daytoday cryptographyTraditional pseudorandom number generators use methods similar to those ofhash functions to generate sequences of randomlooking data However the processis deterministic Once you know the internal state of the random number generatoryou can reproduce the output sequence exactly Ergo this is usually a poor optionfor cryptography When you generate a random bit key you want bitsworth of randomness not bits of number generator state thats been algorithmically massaged into occupying bitsFortunately kernel developers have put considerable effort into recording subtlevariations in system behavior and using these as sources of randomness Sourcesinclude everything from the timing of packets seen on a network to the timing ofhardware interrupts to the vagaries of communication with hardware devices suchas disk drives Even on virtual and cloud servers theres still enough entropy available in the environment to generate reasonably random numbersAll these sources feed forward into a secondary pseudorandom number generatorthat ensures the output stream of random data will have reasonable statistical properties That data stream is then made available through a device driver In Linuxand FreeBSD its presented as devrandom and devurandomTwo main things to know about random numbers Nothing that runs in user space can compete with the quality of the kernels random number generator Never allow cryptographic software togenerate its own random data always make sure it uses random datafrom devrandom or devurandom Most software does this by default The choice of devrandom vs devurandom is a matter of disputeand unfortunately the arguments are too subtle and mathematical tosummarize here The short version is that devrandom on Linux is notguaranteed to generate data at all if the kernel feels that the system hasnot been accumulating enough entropy Either get educated and pick oneside or the other or just use devurandom and dont worry your prettylittle head about this issue Most experts seem to recommend the latterapproach FreeBSD users are excused from battle as devrandom anddevurandom on the BSD kernel are identicalCryptographic software selectionThere is good reason to be highly suspicious of all security software and the packagesthat provide cryptographic services most of all Major international governmentsare rumored to have attempted to influence the design phases of cryptographicprotocols and algorithms It seems safe to assume that several wellfunded groupsare eager to compromise any cryptographic project that is not fully nailed downThat said we trust open source software more than closed Projects such as OpenSSLhave a history of serious vulnerabilities but those problems are disclosed mitigatedand released in a transparent open forum The project history and source code areexamined by thousands of peopleNever rely on homegrown cryptography of any sort It is difficult enough just touse libraries correctly Bespoke cryptosystems are doomed to vulnerabilityThe openssl commandopenssl is an administrators TLS multitool You can use it to generate publicprivatekey pairs encrypt and decrypt files examine the cryptographic properties of remote systems create certificate authorities convert among file formats and myriadother cryptographic operationsPreparing keys and certificatesOne of the most common administrative functions of openssl is to prepare certificates for signing by a CA Start by creating a bit private key openssl genrsa out admincomkey Use the private key to create a certificate signing request openssl prompts formetadata known as the Distinguished Name DN to include with the request Itsalso possible to present this information in an answers file instead of inline asshown below openssl req new sha key admincomkey out admincomcsrCountry Name letter code AUUSState or Province Name full name SomeStateOregonLocality Name eg city PortlandOrganization Name eg company Internet Widgits Pty LtdULSAHEOrganizational Unit Name eg section Crypto divisionCommon Name eg server FQDN or YOUR name serveradmincomSubmit the contents of admincomcsr to the CA The CA will perform a validation process to confirm that you are associated with the domain for which youreobtaining a certificate usually by sending email to an address within that domainand will subsequently return a signed certificate You can then use admincomkeyand the CAsigned certificate in your web server configurationMost of these fields are fairly arbitrary but the Common Name is important It mustmatch the name of the subdomain you want to serve If for instance you want toserve TLS for wwwadmincom make that your Common Name You can requestmultiple names for a single certificate or a wild card that matches all the names ina subdomain for example admincomOnce you have the certificate you can examine its properties Here are some of thedetails of a wild card certificate for googlecom openssl x noout text in googlecompemdepth CUSOGeoTrust IncCNGeoTrust Global CA Signature Algorithm shaWithRSAEncryption Issuer CUS OGoogle Inc CNGoogle Internet Authority G Validity Not Before Dec GMT Not After Mar GMT Subject CUS STCalifornia LMountain View OGoogle IncCNgooglecomThe validity period is from Dec through March Clients who connect outside of this window will see error messages that the certificate is no longervalid Tracking and managing certificate expiration dates is a common sysadminresponsibilityDebugging TLS serversUse openssl sclient to examine the TLS details of a remote server This information can be quite useful when you are debugging web servers having certificateproblems For example to examine the TLS properties of googlecom outputtruncated for brevity openssl sclient connect googlecomNew TLSvSSLv Cipher is AESSHAServer public key is bitSecure Renegotiation IS supportedCompression NONEExpansion NONESSLSession Protocol TLSv Cipher AESSHA SessionID FDCEEEFEEFFCDCFBBFD SessionIDctx MasterKey CDAFBBEEBACCACFACDEFABFD KeyArg None Start Time Timeout sec Verify return code okYou can use openssl sclient to check which versions of the TLS protocol a serversupports See also openssl sserver which starts a generic TLS server That can behandy for testing and debugging clientsPGP Pretty Good PrivacyPhil Zimmermanns PGP package provides a tool chest of breadandbutter cryptographic utilities focused primarily on email security It can encrypt data generatesignatures and verify the origin of files and messagesPGP has an interesting history that includes lawsuits criminal prosecutions andthe privatization of portions of the original PGP suite Recently PGP has been heavily criticized for exposing too much metadata in its most common usage modesExposed message length recipients and cleartext draft storage among otherthings are weaknesses that could potentially be exploited by attackers especially nationstate actors with generous resources That said PGP is still significantlybetter than sending information in plain textPGPs file formats and protocols are being standardized by the IETF under thename OpenPGP and multiple implementations of the proposed standard exist TheGNU project provides an excellent free and widely used implementation knownas GnuPG at gnupgorg For clarity we refer to the systems collectively as PGP eventhough individual implementations have their own namesSee page formore informationabout email privacyUnfortunately the UNIX and Linux versions are nutsandbolts enough that youhave to understand a fair amount of cryptographic background to use them Although you may find PGP useful in your own work we dont recommend that yousupport it for users because it has been known to spark many puzzled questionsWe have found the Windows version to be considerably easier to use than the gpgcommand with its dozens of different operating modesSoftware packages on the Internet are often distributed with a PGP signature filethat purports to guarantee the origin and purity of the software However it is difficult or impossible for people who are not diehard PGP users to validate thesesignatures Users must have collected a library of public keys from people whoseidentities they have personally verified Downloading a single public key along witha signature file and software distribution is approximately as secure as downloading the distribution aloneSome email clients add on a simple GUI for encrypted incoming and outgoingmessages Google Chrome users can install the end to end extension to incorporate PGP support for GmailKerberos a unified approach to network securityThe Kerberos system designed at MIT attempts to address some of the issues ofnetwork security in a consistent and extensible way Kerberos is an authenticationsystem a facility that guarantees that users and services are in fact who they claimto be It does not afford any additional security or encryption beyond thatKerberos uses symmetric and asymmetric cryptography to construct nested setsof credentials called tickets Tickets are passed around the network to certifyyour identity and to give you access to network services Each Kerberos site mustmaintain at least one physically secure machine called the authentication serveron which to run the Kerberos daemon This daemon issues tickets to users or services that present credentials such as passwords when they request authenticationIn essence Kerberos improves on traditional password security in only two ways itnever transmits unencrypted passwords on the network and it relieves users fromhaving to type passwords repeatedly making password protection of network services somewhat more palatableThe Kerberos community boasts one of the most lucid and enjoyable documents everwritten about a cryptosystem Bill Bryants Designing an Authentication Systema Dialogue in Four Scenes Despite its age it remains required reading for anyoneinterested in cryptography and is available atwebmitedukerberoswwwdialoguehtmlKerberos offers a better network security model than does the ignoring networksecurity entirely model but it is neither perfectly secure nor painless to install andrun It does not supersede the other security measures described in this chapterUnfortunately and perhaps predictably the Kerberos system distributed as partof Windows Active Directory uses proprietary undocumented extensions to theprotocols As a result it does not interoperate well with distributions based on theMIT code Fortunately the sssd daemon lets UNIX and Linux systems interactwith Active Directorys version of Kerberos See the sections starting on page for more information SSH the Secure SHellThe SSH system invented by Tatu Ylnen is a protocol for remote logins and forsecuring network services on an insecure network SSHs capabilities include remotecommand execution shell access file transfer port forwarding network proxy services and even VPN tunneling It is an indispensable tool a veritable Swiss Armyknife for system administratorsSSH is a clientserver protocol that uses cryptography for authentication confidentiality and integrity of communications between two hosts It is designed for algorithmic flexibility allowing the underlying cryptographic protocols to be updatedand deprecated as the industry evolves SSH is documented as a group of relatedprotocols in RFCs through In this section we discuss OpenSSH the open source SSH implementation that isincluded and enabled by default on nearly every version of UNIX and Linux Wealso mention a few alternative solutions for the adventurous and openmindedOpenSSH essentialsOpenSSH was developed by the OpenBSD project in and has since beenmaintained by that organization The software suite consists of several commands ssh the client sshd the server daemon sshkeygen for generating publicprivate key pairs sshadd and sshagent tools for managing authentication keys sshkeyscan for retrieving public keys from servers sftpserver the server process for file transfer over SFTP sftp and scp file transfer client utilitiesIn the most common and basic usage a client establishes a connection to the serverauthenticates itself and subsequently opens a shell to execute commands Authentication methods are negotiated according to mutual support and the preferencesof the client and server Many users can log in simultaneously A pseudoterminalis allocated for each connecting their input and output to the remote systemTo initiate this process a user invokes ssh with the remote host as the first argument ssh serveradmincomssh attempts a TCP connection on port the standard SSH port assigned by IANAWhen the connection is established the server sends its public key for verificationIf the server isnt already known and trusted ssh prompts the user to confirm theserver by presenting a hash of the servers public key called the key fingerprintThe authenticity of host serveradmincom cant be establishedECDSA key fingerprint is SHAquLdFoXBIOpUHwnUyKcRUuUAre you sure you want to continue connecting yesnoThe intent is that a server administrator can communicate the host key to users inadvance Users can then compare the information they received from the administrator to the servers proffered fingerprint when they first connect If the two matchthe hosts identity is provedOnce the user accepts the key the fingerprint is added to sshknownhosts forfuture use ssh wont mention the servers key again unless the key changes in whichcase ssh displays a nasty warning message that the servers identity has changedIn practice this server verification dance is routinely ignored Administrators rarelysend the host key to users and users blindly accept the host key without verification This rubberstamping of new host keys subjects users to maninthemiddleattacks Fortunately the process can be automated and streamlined We discuss thisissue in Host key verification with SSHFP on page Once the host key has been accepted the server lists the authentication methodsit supports OpenSSH implements all the methods described by the SSH RFCs including simple UNIX password authentication trusted hosts public keys GSSAPIfor integration with Kerberos and a flexible challengeresponse scheme to supportPAM and onetime passwords Of these public key authentication is the mostcommonly used and is the method we recommend for most sites It offers the bestbalance of strong security and convenience We discuss the use of public keys withSSH in more detail in Public key authentication starting on page ssh and sshd can be tuned for varying needs and security types Configuration isfound in the etcssh directory an uncharacteristically standard location among allflavors of UNIX and Linux Table enumerates the files found in that directoryTable Configuration files found in etcsshFile Permissions Contentssshconfig Sitewide client configurationsshdconfig Server configurationmoduli Prime numbers and generators for the DH key exchangekey Private keys for every algorithm supported by the serverkeypub A public key to match each private keyIn addition to etcssh OpenSSH uses ssh for storing public and private keys forperuser client configuration overrides and for a few other purposes The sshdirectory is ignored unless its permissions are set to OpenSSH has a respectable though not impeccable track record for security vulnerabilities According to the CVE database cvemitreorg several critical vulnerabilities were discovered in early versions The last of these was documented in Occasional denialofservice and bypass vulnerabilities continue to be announcedbut most of them are considered relatively low risk Still its wise to update theOpenSSH packages as part of your regular patching scheduleThe ssh clientGetting started with ssh is straightforward but its power and versatility lie in itsmany options Through configuration you can choose cryptographic algorithmsand ciphers create convenient host aliases set up port forwarding and much moreThe basic syntax isssh options usernamehost commandFor example to check the disk space of varlog ssh serveradmincom df h varlogIf you specify a command ssh authenticates itself to the host runs the commandand exits without opening an interactive shell If you do not specify a usernamessh uses your local username on the remote hostssh reads configuration settings from the sitewide file etcsshsshconfig andprocesses additional options and overrides on a peruser basis from sshconfigTable lists some of the more interesting options that you can tune in these filesWe discuss some of these options in more detail later in this chapterWhen ssh assembles a final configuration commandline arguments take precedenceover entries in sshconfig The global configuration set in etcsshsshconfigis the lowestpriority source of configuration optionsssh sends the current username as the login name if another value is not specifiedYou can supply a different username with the l flag or the syntax ssh l hsolo serveradmincom ssh hsoloserveradmincomClient options that are not available as direct arguments to ssh can still be set onthe command line with the o flag For example you could disable host checks fora server ssh o StrictHostKeyCheckingno serveradmincomThe v option prints debug messages Specify it multiple times maximum of threeto increase verbosity Youll find this flag to be invaluable when debugging authentication problemsFor convenience ssh returns the exit status of the remote command Use this behavior to check for error conditions when invoking ssh from scriptsConsult man ssh and man sshconfig to familiarize yourself with available optionsand features Run ssh h for a succinct summaryPublic key authenticationOpenSSH and the SSH protocol generally can use public key cryptography to authenticate users to remote systems As a user you start by creating a publicprivatekey pair You give the public key to the server administrator who adds it to theserver in the file sshauthorizedkeys You can then log in to the remote serverby running ssh with the remote username and matching private key ssh i sshidecdsa hsoloserveradmindomUse sshkeygen to generate a key pair You can specify which cryptographic algorithm to use as well as bit length and other characteristics For example to generatean ECDSA key pair with a bit elliptic curve size sshkeygen t ecdsa b Generating publicprivate ecdsa key pairEnter file in which to save the key homebensshidecdsa returnEnter passphrase empty for no passphrase returnEnter same passphrase again returnYour identification has been saved in homebensshidecdsaYour public key has been saved in homebensshidecdsapubThe key fingerprint isSHAVRhraUfpnYdtMmGURbIoyfcpnpbwhsmvsdrlhK benTable Useful SSH client configuration optionsOption Meaning DefaultAddKeysToAgent Automatically add keys to sshagent noConnectTimeout Connection timeout in seconds varies aControlMaster Allow connection multiplexing noDynamicForward Set up a SOCKS or SOCKS proxy ForwardAgent Enable sshagent forwarding noHost Marker for a new host alias IdentityFile Path to an authentication private key sshidrsa bPort Port to connect on RequestTTY Specify whether a TTY is needed autoServerAliveInterval Pings for connections to the server disabledStrictHostKeyChecking Require yes or ignore no host keys aska The default is determined by the kernels TCP defaults which vary widelyb The precise name depends on the authentication algorithm By default all keys begin with idThe public key sshidecdsapub and private key sshidecdsa files arebaseencoded ASCII files Never share the private key sshkeygen sets the permissions on the public and private key correctly as and respectively Thisexample uses ECDSA but its also fine to use t rsa with or bitssshkeygen prompts for an optional passphrase to encrypt the private key If youuse a passphrase you must type it to decrypt the key before ssh can read it A passphrase improves security because the authentication process gains an additionalverification step you must both have the key file and know the passphrase thatdecrypts it before you can authenticateWe suggest setting a passphrase on all privileged accounts that is those with sudoprivileges If you need to use a key without a passphrase to enable an automatedprocess limit the corresponding server accounts permissionsIf youre the server administrator and you need to add a public key for a new userfollow these steps Ensure that the user has an active account with a valid shell Get a copy of the users public key from the user Create the users ssh directory with permissions Add the public key to usersshauthorizedkeys and set the permissions of that file to For example if user hsolos public key were saved in tmphsolopub the processwould look like this grep hsolo etcpasswdhsoloxHan Solohomehsolobinbash mkdir p hsolossh chmod hsolossh cat tmphsolopub hsolosshauthorizedkeys chmod hsolosshauthorizedkeysIf you do this more than once youll almost certainly find it prudent to script theprocedure Configuration management systems like Ansible and Chef handle thistask cleanlyThe sshagentThe sshagent daemon caches decrypted private keys You load your private keysinto the agent and ssh then automatically offers those keys when it connects tonew servers simplifying the process of connectingUse the sshadd command to load a new key If the key requires a passphraseyoull be prompted to enter it To list the currently loaded keys type sshagent l sshadd sshidecdsaEnter passphrase for sshidecdsa passphraseIdentity added sshidecdsa sshidecdsa sshadd l SHAVRbIoyfcpnpbwhsmvsdrlhK sshidecdsa ECDSAYou can have many keys active at once Remove a key with sshadd d path or purgeall loaded keys with sshadd DOddly to remove the private key from the agent the public key must be in the samedirectory and have the same filename but with a pub extension If the public key isnot available you might receive a confusing error message that the key does not exist sshadd d sshidecdsaBad key file homebensshidecdsa No such file or directoryYou can easily fix this problem by extracting the public key with sshkeygen andsaving it to the expected filename This extraction is possible because the privatekey file contains a copy of the public key as well as the private key keyhomebensshidecdsa sshkeygen yf key keypubEnter passphrase passphrasesshagent is even more useful when you leverage its key forwarding feature whichmakes the loaded keys available to remote hosts while you are logged in to themthrough ssh You can use this feature to jump from one server to another withoutcopying your private key to remote systems See Exhibit CExhibit C sshagent forwardingClient Jump server Private serversaka bastion hostConnect from hereto private serverswithout copying the keySSH connection withagent forwardingRuns sshagentTo enable agent forwarding either add ForwardAgent yes to your sshconfigfile or use ssh AUse key forwarding only on servers that you trust Anyone in control of the serveryouve forwarded to can assume your identity and access remote systems They cannot read your private keys directly but they can use any that are available throughthe forwarding agentHost aliases in sshconfigYoull undoubtedly encounter many different SSH configurations if you interact withor administer a large number of servers To simplify your life the sshconfig filelets you set up aliases and overrides for individual hostsFor example consider two systems The first is a web server with IP address where sshd listens on port Your username on that server ishan and you have a private key for authentication The other is debianadmincomwhere your username is hsolo Youd prefer to disable password authentication entirely but the Debian server requires itTo connect to these servers from the command line you could use optionlardedcommands such as these ssh l han p i homehansshidecdsa ssh l hsolo debianadmincomYour client in this case must leave password authentication enabled the defaultbecause its a hassle to type o PasswordAuthenticationno all the timeThe following sshconfig sets up aliases for these hosts and has the added benefitof disabling password authentication by defaultPasswordAuthentication noHost web HostName User han IdentityFile homehansshidecdsa ForwardAgent yes Port Host debian Hostname debianadmincom User hsolo PasswordAuthentication yes Now you can use the much simpler commands ssh web and ssh debian to reachthese hosts The client reads the aliases and sets options automatically for each systemssh also understands some basic patterns for matching hosts For exampleHost ServerAliveInterval m ServerAliveCountMax Host User lukeThis example tells ssh to keep idle connections open for minutes on all serversIt also sets username luke when connecting to hosts on the networkHost aliases become more powerful than you can possibly imagine when combinedwith other tricks of the OpenSSH tradeConnection multiplexingControlMaster is a nifty ssh feature that enables connection multiplexing thusconsiderably improving SSH performance over WAN links When enabled the firstconnection to a host creates a socket that can be reused Subsequent connectionsshare the socket but require separate authenticationTurn on multiplexing with the ControlMaster ControlPath and ControlPersistoptions in a Host aliasHost web HostName User han Port ControlMaster auto ControlPath sshcmsocketrhp ControlPersist mControlMaster auto enables the feature ControlPath creates a socket at the designated location See man sshconfig for the substitutions that can be used in theControlPath filename In this case the file is named according to the remote loginusername host IP address and port Connecting to this host results in a socketlike this one ls l sshcmsocketsrw ben ben Jan hanSuch a pattern guarantees a unique filename for each socket ControlPersist savesthe socket for the specified period of time even if the first connection the master disconnectsSpend the seconds it takes to set this up then make a donation to the OpenBSDfoundation to thank them for implementing multiplexing and saving you timePort forwardingAnother useful ancillary feature of SSH is its ability to tunnel TCP connections securely through an encrypted channel thereby allowing connectivity to insecure orfirewalled services at remote sites Exhibit D on the next page shows a typical useof an SSH tunnel and should help clarify how it worksIn this scenario a remote userlets call her Alicewants to establish an HTTPconnection to a web server on an enterprise network Access to that host or to port is blocked by the firewall but having SSH access Alice can route the connectionthrough the SSH serverTo set this up Alice logs in to the remote SSH server with ssh On the ssh command line she specifies an arbitrary but specific in this case local port thatssh should forward through the secure tunnel to the remote web servers port ssh L webserver serveradmincomExhibit D An SSH tunnel for HTTPFIREWALLExternal systemsshChromerandomportrandomportportport port randomportSSH serversshdWeb serverhttpdEnterprisesideInternetsideAll source ports in this example are marked as random since programs choose anarbitrary port from which to initiate connectionsTo access the web server Alice can now connect to port on her own machineThe local ssh receives the connection and tunnels Alices traffic over the existingSSH connection to the remote sshd In turn sshd forwards the connection to theweb server on port Of course tunnels such as these can be intentional or unintentional back doors aswell Use tunnels with caution and also watch for unauthorized use of this facilityby users You can disable port forwarding in sshd with the AllowTCPForwardingno configuration optionsshd the OpenSSH serverThe OpenSSH server daemon sshd listens on port by default for connectionsfrom clients Its configuration file etcsshsshdconfig boasts myriad optionssome of which may need to be tuned for your sitesshd runs as root It forks an unprivileged child process for each connected client with the same permissions as the connecting user If you make changes to thesshdconfig file you can force sshd to reload by sending a HUP signal to the parent process sudo kill HUP sudo cat varrunsshdpidIn Linux you can also run sudo systemctl reload sshd The changes take effectfor new connections Existing connections are preserved without interruption butcontinue to use their previous settingsThe following example sshdconfig includes some commonly adjusted optionsconfigured to balance server security with users convenience Set to inet for IPvonly or inet for IPvonlyAddressFamily any Allows only the named users and groups to log in Somewhat draconian Addingremoving users requires reloadAllowUsers foo bar hsoloAllowGroups admins TCP forwarding is convenient but can be abusedAllowTcpForwarding yes Display a message before users authenticate Important for inane legal reasons and compliance requirementsBanner etcbanner We prefer to allow public key authentication onlyChallengeResponseAuthentication noPasswordAuthentication noRSAAuthentication noGSSAPIAuthentication noHostbasedAuthentication noPubkeyAuthentication yes Disconnect inactive clients after minutesClientAliveInterval ClientAliveCountMax Allow compression at all timesCompression yes Do not allow remote hosts to use forwarded portsGatewayPorts no Record failed login attemptsLogLevel VERBOSE Reduced from the default of MaxAuthTries Do not allow root to log in encourages use of sudoPermitRootLogin no Prevent users from setting their environment in an authorizedkeys filePermitUserEnvironment no Use the auth facility for syslog messagesSyslogFacility AUTH Kill the session if a TCP connection is lostTCPKeepAlive no Do not allow X forwarding if your site does not use XXForwarding noWe encourage you to list the acceptable ciphers and key exchange algorithms explicitly We dont include the details here because the names are quite long and area moving target anyway Follow Mozillas OpenSSH configuration guidelines whichcan be found at googlXxgxH deep link into wikimozillaorgHost key verification with SSHFPRecall from earlier in this section that SSH server host keys are routinely ignoredby server administrators and users alike Cloud instances exacerbate the problembecause even the administrator has no knowledge of the host key before logging inFortunately a DNS record known as SSHFP has been developed to address thisissue The premise is that the servers key is stored as a DNS record When a client connects to an unknown system SSH looks up the SSHFP record to verify theservers key rather than asking the user to verify itThe sshfp utility available from githubcomxelerancesshfp generates SSHFP DNSresource records either by scanning a remote server the s flag or by parsing a previously accepted key from a knownhosts file the k flag this is also the defaultOf course either choice assumes that the source of the key is known to be correctFor example the following command generates a BINDcompatible SSHFP recordfor serveradmincom sshfp serveradmincomserveradmincom IN SSHFP aeeafafadbdserveradmincom IN SSHFP cfdedfabcfdeaiAdd these records to the domains zone file be mindful of the names and theORIGIN reload the domain and use dig to verify the key dig serveradmincom IN SSHFP grep SSHFP DiG P serveradmincom IN SSHFP serveradmincom IN SSHFPserveradmincom IN SSHFP aeeafafserveradmincom IN SSHFP cfdedfabcfssh does not consult SSHFP records by default Add the VerifyHostKeyDNS optionto etcsshsshconfig to enable checking As with most SSH client options youcan also pass o VerifyHostKeyDNSyes on the ssh command line when you firstaccess a new systemYou can automate this process by generating the SSHFP record in the servers initialization scripts Use dynamic DNS or your favorite DNS providers API to createthe recordFile transfersOpenSSH has two utilities for transferring files scp and sftp On the server sidesshd runs a separate process called sftpserver to handle file transfers SFTP hasno relationship to the older and insecure File Transfer Protocol FTPYou can use scp to copy files from your system to a remote host from a remote hostto your system or between remote hosts The syntax mirrors that of cp with someextra decorations to designate hosts and usernames scp file serveradmincom scp serveradmincomfile file scp serveradmincomfile serveradmincomfilesftp is an interactive experience similar to a traditional FTP client You can alsofind graphical SFTP interfaces for most desktop operating systemsAlternatives for secure loginsMost systems and sites rely on OpenSSH for secure remote access but it is not theonly choiceDropbear is an SSH implementation with a focus on maintaining a compact footprintIt compiles to a statically linked KiB binary perfect for consumergrade routersand other embedded devices It includes some of the same features as OpenSSHsuch as public key authentication and agent forwardingGravitationals Teleport is another alternative SSH server that offers several advantages Its authentication model relies on expiring certificates which eliminatesthe problem of distributing and configuring users public keys Among Teleportsimpressive features are an optional audit trail for each connection and a nifty collaboration system that lets multiple users share a session Compared to OpenSSHTeleport is relatively new and unproven but to date there have been no reportedvulnerabilities We expect Gravitational to continue their rapid pace of developmentMosh developed by a brilliant team at MIT is a replacement for SSH Unlike SSHMosh operates on encrypted and authenticated UDP datagrams It is designed forbetter performance over WAN connections and for roaming For example youcan resume connections if you move from one IP address to another or if yourconnection drops First released in Mosh has a much shorter history thanOpenSSH but in its first few years it has had no reported security vulnerabilitiesLike Dropbear it has a much smaller footprint than OpenSSH FirewallsIn addition to protecting individual machines you can also implement securityprecautions at the network level The basic tool of network security is the firewalla device or piece of software that prevents unwanted packets from accessing networks and systems Firewalls are ubiquitous today and are found in devices rangingfrom desktop systems and servers to consumer routers and enterprisegrade network appliancesPacketfiltering firewallsA packetfiltering firewall limits the types of traffic that can pass through your Internet gateway or through an internal gateway that separates domains within yourorganization according to information in the packet header Its much like drivingyour car through a customs checkpoint at an international border crossing Youspecify which destination addresses port numbers and protocol types are acceptable and the gateway simply discards and in some cases logs packets that dontmeet the profilePacketfiltering software is included in Linux systems in the form of iptables andits easiertouse front end ufw and on FreeBSD as ipfw See the details beginningon page for more informationAlthough these tools are capable of sophisticated filtering and bring a welcome extra dose of security we generally discourage the use of UNIX and Linux systems asnetwork routers and most especially as enterprise firewall routers The complexity of generalpurpose operating systems makes them inherently less secure andless reliable than taskspecific devices Dedicated firewall appliances such as thosemade by Check Point and Cisco are a better option for sitewide network protectionFiltering of servicesMost wellknown services are associated with a network port in the etcservicesfile or its vendorspecific equivalent The daemons responsible for these servicesbind to the appropriate ports and wait for connections from remote sites Most ofthe wellknown service ports are privileged meaning that their port numbersare in the range to These ports can be used only by a process running asroot or with an appropriate Linux capability Port numbers and higher arereferred to as nonprivileged portsServicespecific filtering is predicated on the assumption that the client the machine that initiates a TCP or UDP conversation uses a nonprivileged port tocontact a privileged port on the server For example if you wanted to allow onlyinbound HTTP connections to a machine with the address youwould install a filter that allowed TCP packets destined for port at that addressand that permitted outbound TCP packets from that address to anywhere Theexact way that such a filter is installed depends on the kind of router or filteringsystem you are usingModern securityconscious sites use a twostage filtering scheme One filter is agateway to the Internet and a second filter lies between the outer gateway and therest of the local network The idea is to terminate all inbound Internet connections Port is the standard HTTP port as defined in etcserviceson systems that lie between these two filters If these systems are administrativelyseparate from the rest of the network they can handle a variety of services for theInternet with reduced risk The partially secured network is usually called the demilitarized zone or DMZThe most secure way to use a packet filter is to start with a configuration that allowsno inbound connections You can then liberalize the filter bit by bit as you discoveruseful things that dont work and hopefully move any Internetaccessible servicesonto systems in the DMZStateful inspection firewallsThe theory behind stateful inspection firewalls is that if you could carefully listento and understand all the conversations in all languages that were taking place ina crowded airport you could make sure that someone wasnt planning to bomb aplane later that day Stateful inspection firewalls are designed to inspect the trafficthat flows through them and compare the actual network activity to what shouldbe happeningFor example if the packets exchanged in an H video sequence name a port tobe used later for a data connection the firewall should expect a data connection tooccur only on that port Attempts by the remote site to connect to other ports arepresumably bogus and should be droppedSo what are vendors really selling when they claim to deliver stateful inspectionTheir products either monitor a limited number of connections or protocols or theysearch for a particular set of bad situations Not that theres anything wrong withthat clearly some benefit is derived from any technology that can detect trafficanomalies In this particular case however remember that the claims are mostlymarketing hypeFirewalls safeA firewall should not be your only means of defense against intruders Its only onecomponent of what ought to be a carefully considered multilayered security strategy Firewalls often confer a false sense of security If a firewall lulls you into relaxing other safeguards it will have had a negative effect on the security of your siteEvery host within your organization should be individually patched hardened andmonitored with tools such as Bro Snort Nmap Nessus and OSSEC Likewise yourentire user community needs to be educated about basic security hygieneIdeally local users should be able to connect to any Internet service they like butmachines on the Internet should only be able to connect to a limited set of localservices hosted within your DMZ For example you might want to allow SFTP access to a local archive server and allow SMTP connections to a server that receivesincoming emailTo maximize the value of your Internet connection we recommend that you emphasize convenience and accessibility when deciding how to set up your networkAt the end of the day its the system administrators vigilance that makes a networksecure not a fancy piece of firewall hardware Virtual private networks VPNsIn its simplest form a VPN is a connection that makes a remote network appear as ifit were directly connected even if it is physically thousands of miles and many routerhops away For increased security the connection is not only authenticated in someway usually with a shared secret such as a passphrase but the endtoend trafficis also encrypted Such an arrangement is usually referred to as a secure tunnelHeres a good example of the kind of situation in which a VPN is handy Supposethat a company has offices in Chicago Boulder and Miami If each office has aconnection to a local ISP the company can use VPNs to transparently and for themost part securely connect the offices across the untrusted Internet The companycould achieve a similar result by leasing dedicated lines to connect the three officesbut that would be considerably more expensiveAnother good example is a company whose employees telecommute from theirhomes VPNs let those users reap the benefits of their highspeed and inexpensivecable modem service while making it appear that they are directly connected tothe corporate networkBecause of the convenience and popularity of this functionality everybody is offering some type of VPN solution You can buy it from your router vendor as a plugin for your operating system or even as a dedicated VPN device for your networkDepending on your budget and scalability needs you may want to consider one ofthe many commercial VPN solutionsIf youre without a budget and looking for a quick fix SSH can do secure tunnelingfor you See Port forwarding starting on page IPsec tunnelsIf youre a fan of IETF standards or of saving money and need a real VPN solutiontake a look at IPsec Internet Protocol security IPsec was originally developed forIPv but it has also been widely implemented for IPv IPsec is an IETFapprovedendtoend authentication and encryption system Almost all serious VPN vendorsship a product that has at least an IPsec compatibility mode Linux and FreeBSDinclude native kernel support for IPsecIPsec uses strong cryptography to implement both authentication and encryptionservices Authentication ensures that packets are from the right sender and havenot been altered in transit and encryption prevents the unauthorized examinationof packet contentsIn tunnel mode IPsec encrypts the transport layer header which includes the sourceand destination port numbers Unfortunately this scheme conflicts with most firewalls For this reason most modern implementations default to transport modein which only the payloads of packets the data being transported are encryptedTheres a gotcha involving IPsec tunnels and MTU size You must ensure that oncea packet has been encrypted by IPsec nothing fragments it along the network paththe tunnel traverses To achieve this feat you might have to lower the MTU on thedevices in front of the tunnel In the real world bytes usually works Seepage in the TCP chapter for more information about MTU sizeAll I need is a VPN rightSadly theres a downside to VPNs Although they do build a mostly secure tunnel across the untrusted network between the two endpoints they dont usuallyaddress the security of the endpoints themselves For example if you set up a VPNbetween your corporate backbone and your CEOs home you may inadvertentlybe creating a path for your CEOs yearold daughter to have direct access to everything on your networkBottom line you need to treat connections from VPN tunnels as external connections and grant them additional privileges only as necessary and only after carefulconsideration Think about adding a special section to your site security policy tocover the rules applying to VPN connections Certifications and standardsIf the subject matter of this chapter seems daunting to you dont fret Computersecurity is a complicated and vast topic as countless books web sites and magazines can attest Fortunately much has been done to help quantify and organize theavailable information Dozens of standards and certifications exist and mindfulsystem administrators should reflect on their guidanceCertificationsLarge corporations often employ many fulltime employees whose job is guardinginformation To gain credibility in the field and keep their knowledge current theseprofessionals attend training courses and obtain certifications Prepare yourself foracronymfu as we work through a few of the most popular certificationsOne of the most widely recognized security certifications is the CISSP or Certified Information Systems Security Professional It is administered by ISC theInternational Information Systems Security Certification Consortium say that tentimes fast One of the primary draws of the CISSP is ISCs notion of a commonbody of knowledge CBK essentially an industrywide best practices guide forinformation security The CBK covers law cryptography authentication physicalsecurity and much more Its an incredible reference for security folksOne criticism of the CISSP has been its concentration on breadth and consequentlack of depth So many topics in the CBK and so little time To address this ISChas issued CISSP concentration programs that focus on architecture engineeringand management These specialized certifications add depth to the more generalCISSP certificationThe SANS Institute created the Global Information Assurance Certification GIACsuite of certifications in Three dozen separate exams cover the realm of information security with tests divided into five categories The certifications range indifficulty from the moderate twoexam GISF to the hour expertlevel GSE TheGSE is notorious as one of the most difficult certifications in the industry Many ofthe exams focus on technical specifics and require quite a bit of experienceFinally the Certified Information Systems Auditor CISA credential is an auditand process certification It focuses on business continuity procedures monitoringand other management content Some consider the CISA an intermediate certification that is appropriate for an organizations security officer role One of its mostattractive aspects is that it involves only a single examAlthough certifications are a personal endeavor their application to business is undeniable More and more companies now recognize certifications as the mark of anexpert Many businesses offer higher pay and promotions to certified employees Ifyou decide to pursue a certification work closely with your organization to have ithelp pay for the associated costsSecurity standardsBecause of the everincreasing reliance on data systems laws and regulations havebeen created to govern the management of sensitive businesscritical information Major pieces of US legislation such as HIPAA FISMA NERC CIP and theSarbanesOxley Act SOX have all included sections on IT security Although therequirements are sometimes expensive to implement they have helped give theappropriate level of focus to a onceignored aspect of technologyUnfortunately the regulations are filled with legalese and can be difficult to interpret Most do not contain specifics on how to achieve their requirements As a result standards have been developed to help administrators reach the lofty legislative requirements These standards are not regulationspecific but following themusually ensures compliance It can be intimidating to confront the requirementsof all the various standards at once so its usually best to first work through onestandard in its entiretyISO The ISOIEC formerly ISO standard is probably the most widelyaccepted in the world First introduced in as a British standard it is pages long and is divided into sections that run the gamut from policy throughphysical security to access control Objectives within each section define specificFor a broader discussion of industry andlegal standards thataffect IT environments see page requirements and controls under each objective describe the suggested best practice solutions The document costs about The requirements are nontechnical and can be fulfilled by any organization in away that best fits its needs On the downside the general wording of the standardleaves the reader with a sense of broad flexibility Critics complain that the lack ofspecifics leaves organizations open to attackNonetheless this standard is one of the most valuable documents available to theinformation security industry It bridges an often tangible gap between managementand engineering and helps focus both parties on minimizing riskPCI DSSThe Payment Card Industry Data Security Standard PCI DSS is a different beastentirely It arose from the perceived need to improve security in the credit cardprocessing industry following a series of dramatic exposures For example in the US government revealed the exposure of million credit card numbers byvarious Visa licensees including JCPenney This is the largest cybercrime case inUS history its estimated that more than million was lostThe PCI DSS standard is the result of a joint effort between Visa and MasterCardthough it is currently maintained by Visa Unlike ISO it is freely availablefor anyone to download It focuses entirely on protecting cardholder data systemsand has sections that define requirements for protectionBecause PCI DSS is focused on card processors it is not generally appropriate forbusinesses that dont deal with credit card data However for those that do strictcompliance is necessary to avoid hefty fines and possible criminal prosecution Youcan find the document at pcisecuritystandardsorgNIST seriesThe fine folks at NIST have created the Special Publication SP series of documents to report on their research guidelines and outreach efforts in computersecurity These documents are most often used in connection with measuringFISMA compliance for those organizations that handle data for the US federalgovernment More generally they are publicly available standards with excellentcontent and have been widely adopted by industryThe SP series includes more than documents All of them are available fromcsrcnistgovpublicationsPubsSPshtml Table on the next page lists a few thatyou might want to consider starting with The Federal Information Security Management Act of Table Recommended publications in the NIST SP seriesPub Title An Introduction to Computer Security The NIST Handbook Generally Accepted Principles and Practices for Securing IT Systems R Contingency Planning Guide for Information Technology Systems Managing Risk from Information Systems An Organizational Perspective R Recommended Security Controls for Federal IT and Organizations Guide to General Server SecurityThe Common CriteriaThe Common Criteria for Information Technology Security Evaluation commonly known as the Common Criteria is a standard for evaluating the securitylevel of IT products These guidelines were established by an international committee consisting of members from various manufacturers and industries Seecommoncriteriaportalorg to learn more about the standardOWASP the Open Web Application Security ProjectOWASP is a nonprofit worldwide organization focused on improving the securityof application software It is best known for its top list of web application security risks which helps remind all of us where to focus our energies when securingapplications Find the current list and a bunch of other great material at owasporgCIS the Center for Internet SecurityCIS has excellent resources for administrators Perhaps the most valuable are theCIS benchmarks a collection of technical configuration recommendations for securing operating systems You can find benchmarks for each of our example UNIXand Linux systems CIS also has benchmarks for cloud providers mobile devicesdesktop software network devices and more Learn more at cisecurityorg Sources of security informationHalf the battle of keeping your systems secure consists of staying abreast ofsecurityrelated developments in the world at large If your site is broken into thebreakin probably wont happen through the use of a novel technique More likelythe chink in your armor will turn out to have been a known vulnerability that hasbeen widely discussed in vendor knowledge bases on securityrelated newsgroupsand on mailing listsSecurityFocuscom the BugTraq mailing list and the OSS mailing listSecurityFocuscom specializes in securityrelated news and information The newsincludes current articles on general issues and on specific problems The site alsoincludes an extensive technical library of useful papers nicely sorted by topicSecurityFocuss archive of security tools contains software for a variety of operating systems along with blurbs and user ratings It is the most comprehensive anddetailed source of tools that we are aware ofThe BugTraq list is a moderated forum for the discussion of security vulnerabilitiesand their fixes To subscribe visit securityfocuscomarchive Traffic on this list canbe fairly heavy however and the signaltonoise ratio is poor A database of BugTraq vulnerability reports is also available from the web siteThe osssecurity mailing list openwallcomlistsosssecurity is an excellent sourceof security tidbits from the open source communitySchneier on SecurityBruce Schneiers blog is a valuable and sometimes entertaining source of information about computer security cryptography and squid Schneier is the author ofthe wellrespected books Applied Cryptography and Secrets and Lies among others Information from the blog is also captured in the form of a monthly newsletterknown as the CryptoGram Learn more at schneiercomcryptogramhtmlThe Verizon Data Breach Investigations ReportReleased annually this report is packed with statistics about the causes and sourcesof data breaches and its an entertaining read to boot The edition suggestsbased on an analysis of incidents that around of data breaches are financially motivated Espionage comes in a distant second This publication includes auseful breakdown of the types of attacks being seen in the wildThe SANS InstituteThe SANS SysAdmin Audit Network Security Institute is a professional organization that sponsors securityrelated conferences and training programs as wellas publishing a variety of security information Their web site sansorg is a usefulresource that occupies something of a middle ground between SecurityFocus andCERT Its neither as frenetic as the former nor as stodgy as the latterSANS offers several weekly and monthly email bulletins that you can sign up foron their web site The weekly NewsBites are nourishing but the monthly summaries seem to contain a lot of boilerplate Neither is a great source of latebreakingsecurity newsDistributionspecific security resourcesBecause security problems have the potential to generate a lot of bad publicityvendors are usually eager to help customers keep their systems secure Most largevendors have an official mailing list to which securityrelated bulletins are postedand many maintain a web site about security issues as well Its common for securityrelated software patches to be distributed for free even by vendors that normallycharge for software supportSecurity portals on the web such as SecurityFocuscom contain vendorspecificinformation and links to the latest official vendor dogmaUbuntu maintains a security mailing list athttpslistsubuntucommailmanlistinfoubuntusecurityannounceFor Red Hat security information subscribe to the enterprise watch list to getannouncements about the security of Red Hats product line Find it athttpsredhatcommailmanlistinfoenterprisewatchlistAlthough CentOS advisories typically always mirror Red Hat security advisoriesits probably worthwhile to subscribe to the CentOS list athttpslistscentosorgpipermailcentosannounceFreeBSD has an active security group with a mailing list athttpslistsfreebsdorgmailmanlistinfofreebsdsecurityOther mailing lists and web sitesThe contacts listed above are just a few of the many security resources available onthe net Given the volume of information thats now available and the rapidity withwhich resources come and go we thought it would be most helpful to point youtoward some metaresourcesOne good starting point is linuxsecuritycom which logs several posts each dayon pertinent Linux security issues It also maintains a running collection of Linuxsecurity advisories upcoming events and user groupsINSECURE magazine is a free bimonthly magazine that includes news about current security trends product announcements and interviews with notable securityprofessionals Some of the articles should be read with a vial of salt nearby and always check the bylines In many cases authors are just pimping their own productsThe Linux Weekly News is a tasty treat that includes regular updates about the kernel security distributions and other topics LWNs security section can be foundat lwnnetsecurityRHEL When your site has been attackedThe key to handling an attack is simple dont panic Its very likely that by the timeyou discover an intrusion most of the damage has already been done In fact it hasprobably been going on for weeks or months The chance that youve discovered abreakin that just happened an hour ago is slim to noneIn that light the wise owl says to take a deep breath and begin developing a carefully thought out strategy for dealing with the breakin You need to avoid tippingoff the intruder by announcing the breakin or performing any other activity thatwould seem abnormal to someone who may have been watching your sites operations for many weeks Hint performing a system backup is usually a good ideaat this point and hopefully will appear to be a normal activity to the intruderThis is also a good time to remind yourself that some studies have shown that of security incidents involve an insider Be very careful with whom you discuss theincident until youre sure you have all the factsHeres a quick step plan that may assist you in your time of crisis Dont panic In many cases a problem isnt noticed until hours or daysafter it took place Another few hours or days wont affect the outcomeThe difference between a panicky response and a rational response willMany recovery situations are exacerbated by the destruction of important log state and tracking information during an initial panic Decide on an appropriate level of response No one benefits from anoverhyped security incident Proceed calmly Identify the staff and resources that must participate and leave others to assist with the postmortem after its all over Hoard all available tracking information Check accounting files andlogs Try to determine where the original breach occurred Back up allyour systems Make sure that you physically writeprotect removablemedia if you connect them to a live system Assess your degree of exposure Determine what crucial informationif any has left the company and devise an appropriate mitigationstrategy Determine the level of future risk Pull the plug If necessary and appropriate disconnect compromisedmachines from the network Close known holes and stop the bleedingCERT recommends steps for analyzing an intrusion The document canbe found at certorgtechtipswinUNIXsystemcompromisehtml Devise a recovery plan With a creative colleague draw up a recoveryplan on nearby whiteboard This procedure is most effective when performed away from a keyboard Focus on putting out the fire and min If system backups are not a normal activity at your site you have much bigger problems than thesecurity intrusionimizing the damage Avoid assigning blame or creating excitement Inyour plan dont forget to address the psychological fallout your usercommunity may experience Users inherently trust others and blatantviolations of trust make many folks uneasy Communicate the recovery plan Educate users and management aboutthe effects of the breakin the potential for future problems and yourpreliminary recovery strategy Be open and honest Security incidents arepart of life in a modern networked environment They are not a reflection on your ability as a system administrator or on anything else worthbeing embarrassed about Openly admitting that you have a problem is of the battle as long as you can demonstrate that you have a planto remedy the situation Implement the recovery plan You know your systems and networksbetter than anyone Follow your plan and your instincts Speak with acolleague at a similar institution preferably one who knows you wellto keep yourself on the right track Report the incident to authorities If the incident involved outsideparties report the matter to CERT They have a hotline at and can be reached by email at certcertorg Include as muchinformation as you canA standard form is available from certorg to help jog your memory Here are someof the more useful pieces of information you might include The names hardware and OS versions of the compromised machines The list of patches that had been applied at the time of the incident A list of accounts that are known to have been compromised The names and IP addresses of any remote hosts that were involved Contact information if known for the administrators of remote sites Relevant log entries or audit informationIf you believe that a previously undocumented software problem may have beeninvolved report the incident to the software vendor as well Recommended readingDykstra Josiah Essential Cybersecurity Science Build Test and Evaluate SecureSystems Sebastopol CA OReilly Media Fraser B Editor RFC Site Security Handbook rfceditororg Garfinkel Simson Gene Spafford and Alan Schwartz Practical UNIX andInternet Security rd Edition Sebastopol CA OReilly Media Kerby Fred et al SANS Intrusion Detection and Response FAQ SANS sansorgresourcesidfaqLyon Gordon Fyodor Nmap Network Scanning The Official Nmap ProjectGuide to Network Discovery and Security Scanning Nmap Project How touse nmap from the author of nmapRisti Ivan Bulletproof SSL and TLS Understanding and Deploying SSLTLS andPKI to Secure Servers and Web Applications London UK Feisty Duck Schneier Bruce Liars and Outliers Enabling the Trust that Society Needs to ThriveNew York NY Wiley Thompson Ken Reflections on Trusting Trust in ACM Turing Award LecturesThe First Twenty Years Reading MA ACM Press AddisonWesley A commitment to monitoring is the distinguishing characteristic of a professionalsystem administrator Inexperienced sysadmins often leave systems unmonitoredand allow failures to be detected when a frustrated angry user calls the help deskbecause theyre unable to complete an intended task Slightly more cluedin administrative groups set up a monitoring platform but disable afterhours notificationsbecause they are too bothersome In either case fire fighting and hilarity ensueThese approaches adversely affect the enterprise complicate recovery efforts andgive the sysadmin team a bad reputationProfessional sysadmins adopt monitoring as their religion Every system is addedto the monitoring platform before it goes live and the battery of checks is regularly tested and tuned Metrics and trends are evaluated proactively so that problemscan be spotted before they affect users or put data at riskA major online video streaming service you may have heard of values their telemetrysystem so much that theyd rather have a service outage than a monitoring outageWithout monitoring theyd have no idea what was happening anywayA monitoringfirst philosophy along with its associated tools makes you a sysadmin superhero You develop a better understanding of your software and applications fix small problems before they snowball into catastrophic failures and Monitoringbecome more effective at finding error conditions debugging problems and understanding the performance of complex systems Monitoring also improves yourquality of life by letting you fix most issues at your convenience rather than at am on Thanksgiving Day An overview of monitoringThe goals of monitoring are to ensure that the IT infrastructure as a whole operatesas expected and to compile in an accessible and easily digested form data that areuseful for management and planning Simple right But this highlevel descriptioncovers a potentially vast territoryRealworld monitoring systems vary in every possible dimension but they all sharethis same basic structure Raw data is harvested from systems and devices of interest The monitoring platform reviews the data and determines what actionsare appropriate usually by applying administratively set rules The raw data and any actions decided on by the monitoring system flowthrough to back ends that take appropriate actionRealworld monitoring systems range from trivially simple to arbitrarily complexFor example the following Perl script includes all the elements listed aboveusrbinenv perlloadavg split s uptime If load is greater than notify sysadminif loadavg system mail s Server load is too high danadmincom devnullThe script runs the uptime command to obtain the systems load averages If theoneminute load average is larger than it sends mail to an administrator Dataevaluation reactionOnce upon a time a fancy monitoring setup involved collections of scripts likethis that ran from cron and commandeered a modem to send messages to sysadmins pagers Today you have multiple options available at every stage of themonitoring pipelineOf course you can still write individual monitoring scripts and run them from cronIf this is really all you need by all means keep things simple But unless you are responsible for only one or two servers this ad hoc approach is normally not sufficientThe following sections review the stages of the pipeline in a bit more detailInstrumentationA wide range of data that may prove useful to your organization includes performance figures response time utilization transfer rate availability figures reachability and uptime capacity state changes log entries and even business metricssuch as average shopping cart value or click conversion rateBecause anything one might do on a computer is potentially of monitoring interestmonitoring systems are usually datasource agnostic They often come with builtin support for a variety of inputs Even data sources that lack direct support cannormally be brought in with a few lines of adapter code or a separate data gatewaysuch as StatsD see page With so much data out there begging to be collected the hard part of designing acollection system can be knowing what to ignore Avoid collecting data that doesnot have a clear and actionable purpose Data overcollection loads down both themonitoring system and the entities being monitored It also tends to obscure thevalues that are truly important drowning them in a sea of noiseUnfortunately its often not easy to distinguish useful data from dross You mustcontinually reevaluate what is monitored and rethink how that data will be actedon throughout a systems lifeData typesAt the highest level monitoring data can be grouped into three general categories Realtime metrics which characterize the operational state of the environment These are typically numbers or Boolean values In general itsthe responsibility of the monitoring system to test these metrics againstexpectations and generate an alert if a current value exceeds a predefinedrange or threshold Events which often take the form of log file entries or push notificationsfrom subsystems These events sometimes known as patternbased metrics can indicate that a state change alarm condition or other action hasoccurred Events can be processed to form numeric metrics eg a totalor a rate or they can trigger monitoring responses directly Aggregated and summarized historic trends which are often timeseriescollections of realtime metrics They allow for analysis and visualizationof changes over time Many of the data points collected by application monitoring software fall into the event categorysometimes they have quantitative data attached as well Interrelationships among events eg theuser looked at the Settings page but then canceled without changing anything are often helpful toinvestigate Generalpurpose monitoring platforms tend not to be very good at this sort of crossreferencing which is one reason that application monitoring is a category of its ownIntake and processingMost monitoring systems revolve around a central monitoring platform that absorbs data from monitored systems performs appropriate processing and appliesadministrative rules to determine what should happen in responseFirstgeneration platforms such as Nagios and Icinga focused on detecting and responding to problems as they occurred These systems were revolutionary for theirday and led us into the modern world of monitoring Nevertheless they have beeneclipsed over time by the industrys gradual realization that all monitoring data istimeseries data If values didnt vary you wouldnt be monitoring themClearly a more dataoriented approach was needed However monitoring data isusually so voluminous that you cant simply dump it all into a traditional database andallow it to accumulate Thats a recipe for poor performance and overflowing disksThe modern approach is to organize monitoring around a data store thats specialized for handling timeseries data All data is stored for an initial period but asthe data ages the store applies increasingly high levels of summarization to limitstorage requirements For example the store might keep an hours worth of dataat onesecond resolution a weeks worth of data at oneminute resolution and ayears worth of data at onehour resolutionHistorical data is useful not only for dashboard presentations but also as a baseline for comparison Is the current network error rate or more above its historical averageNotificationsOnce you have a monitoring framework in place put careful thought into what todo with the monitoring results The first priority is usually to notify administratorsand developers about a problem that needs attentionNotifications must be actionable Structure your monitoring system so that everyone who receives a given notification must potentially do something in responseeven if the action is something as general as check later to be sure this was takencare of Notifications that are purely informational train staff to ignore notificationsIn most cases notifications need to extend beyond email to be optimally effectiveFor critical issues SMS notifications that is text messages to administrators cellphones are easy and efficient Recipients can set their ring tones and phone volumeso that theyll be awakened in the middle of the night if desiredNotifications should also be integrated with your teams ChatOps implementationLess critical notifications such as job statuses login failures and informational notices can be sent to one or more chat rooms so that interested parties can activelyreceive subsets of alerts in which they might be interestedBeyond these basic channels the possibilities are endless An LED lighting systemthat changes colors according to system status can be useful for ataglance statusSee page for morecomments on ChatOpsindication in a data center or network operations center for example Other optionsfor responding to situations identified by the monitoring systems include Automated actions such as dumping a database or rotating logs Calling an administrator on the phone Sending data to a wall board for public display Storing data in a timeseries database for later analysis Doing nothing and allowing later review through the system itselfDashboards and UIsBeyond alerting for clearly exceptional circumstances one of the main goals ofmonitoring is to present the state of the environment in a manner thats morestructured and easier to assimilate than a bunch of raw data Such displays are generically termed dashboardsDashboards are designed by administrators or by other stakeholders with an interest in particular aspects of the environment They use several different techniquesto transform raw data into infographic goldFirst theyre selective in what they present They concentrate on the most important metrics for a given domain the ones that indicate general states of health orperformance Second they give context clues to the significance and import of thedata thats shown For example problematic numbers and states are typically shownin red and primary metrics are depicted in larger font sizes Relationships amongvalues are shown through grouping Third dashboards display data series as chartsmaking them easy to assess at a glanceOf course most data thats collected never shows up on a dashboard Its helpful ifyour monitoring system also has a generalized UI that facilitates investigation andmodification of the data schema allows you to make arbitrary database queriesand charts arbitrarily defined sequences of data on the fly The monitoring cultureThis chapter is mostly about tools but culture is at least as important When youembark on a monitoring journey embrace the following tenets If someone cares about or depends on a system or service it must bemonitored Full stop Nothing in the environment that a service or userdepends on can remain unmonitored If a production device system or service exposes monitorable attributesthose attributes should be monitored Dont let a server with a fancy lightsout hardware management interface spend weeks futilely trying to notifyyou that a fan has failed All highavailability constructs must be monitored It would be unfortunate to learn that a primary server had failed only after the backup serverfailed too Monitoring is not optional The work plans of every sysadmin developerops staff member manager and project manager should include provisions for monitoring Monitoring data especially historical data is useful to everyone Makedata easily accessible and visible so that everyone can use it to help withroot cause analysis planning life cycle management and architecturalimprovement opportunities Put effort and resources into creating andpromoting monitoring dashboards Everyone should respond to alerts Monitoring is not just an ops problem All technical roles should receive notifications and work together toresolve issues This approach encourages bona fide root cause analysis bywhichever individuals are most suited to fix the underlying issue Properly implemented monitoring impacts quality of life in a positiveway A solid monitoring regimen frees you from the burden of worryingabout what state your systems are in and empowers others to support youWithout monitoring and appropriate documentation you are essentiallyon call Train responders to fix alerts not just suppress them Evaluate falsepositiveor noisy alerts and tune them so that they no longer trigger inappropriately Spurious alerts encourage everyone to ignore the monitoring system The monitoring platformsIf you plan to monitor multiple systems and more than a few metrics its worth investing some time into the deployment of a fullservice monitoring platform Theseare generalpurpose systems that collect data from multiple sources facilitate thedisplay and summarization of status information and establish a standard way todefine actions and alertsThe good news is that there are a variety of choices The notasgood news is thatno single perfect platform as yet exists When selecting from among the availableoptions consider the following issues Datagathering flexibility All platforms can absorb data from a variety ofsources However that doesnt mean that all platforms are equivalent inthis regard Consider the data sources you want to actually use Will youneed to read data from an SQL database From DNS records From anHTTP connection User interface quality Many systems offer customizable GUIs or web interfaces Most wellmarketed packages today tout their ability to understandJSON templates for data presentation A UI is not just marketing hypeyou need an interface that relays information clearly simply and comprehensibly Will you need different user interfaces for different groupswithin your organization Cost Some commercial management packages come at a stiff price Manycorporations find value in being able to say that their site is managed by ahighend commercial system If that isnt so important to your organization look at free options such as Zabbix Sensu Cacti and Icinga Automated discovery Many systems offer to discover your networkThrough a combination of broadcast pings SNMP requests ARP tablelookups and DNS queries they identify all your local hosts and devicesAll the discovery implementations we have seen work pretty well but accuracy is lower on complex or heavily firewalled networks Reporting features Many products can send alert email integrate withChatOps send text messages and automatically generate tickets for popular troubletracking systems Make sure that the platform you chooseaccommodates flexible reporting Who knows what electronic devicesyoull be dealing with in a few yearsOpen source realtime platformsAlthough the platforms in this sectionNagios Icinga and Sensu Coredo a littlebit of everything theyre known for their strength in handling instantaneous orthresholdbased metricsThese systems have their proponents but as firstgeneration monitoring toolstheyre gradually losing favor to timeseries systems which we describe startingon page Most sites starting from scratch would be better advised to opt fora timeseries systemNagios and IcingaNagios and Icinga specialize in realtime notification of error conditions Althoughthey do not help you determine how much your bandwidth utilization has increasedover the last month they can track you down when your web server goes offlineNagios and Icinga were originally forks of a single source tree but moderndayIcinga has been completely rewritten However it remains compatible with Nagios in most respectsBoth systems include scores of scripts for monitoring services of all shapes and sizesalong with extensive SNMP monitoring capabilities Perhaps their greatest strengthis their modular and heavily customizable configuration system which allows youto write custom scripts to monitor any conceivable metricYou can whip up new monitors in Perl PHP Python or even C if youre feeling ambitious and masochistic Many standard notification methods are built inemailweb reports text messages etc And as with monitoring plugins its easy to rollyour own notification and action scriptsNagios and Icinga both work well for networks of fewer than a thousand hosts anddevices They are easy to customize and extend and include powerful features suchas redundancy remote monitoring and notification escalationIf you are deploying new monitoring infrastructure from scratch we recommendIcinga over Nagios Its code base is generally cleaner and it has been rapidly accreting fans and community support From a functional perspective its UI is cleanerand faster and its able to autobuild service dependencies which can be essentialin complex environmentsSensuSensu is a fullstack monitoring framework thats available both as an open sourceedition Sensu Core and with paid commercially supported addons It has an ultramodern UI and can run any legacy Nagios Icinga or Zabbix monitoring pluginIt was designed as a replacement for Nagios so plugin compatibility is one of itsmost attractive features Sensu allows for easy integration with Logstash and Slacknotifications and its installation process is particularly easyOpen source timeseries platformsDetecting and responding to current problems is just one aspect of monitoring Itsoften equally important to know how values are changing over time and how theyrelate to other values Four popular timeseries platforms aim to scratch this itchGraphite Prometheus InfluxDB and MuninThese systems put the database front and center within the monitoring ecosystemThey vary in their degree of completeness as standalone monitoring systems andin general are designed for a more modular world than traditional systems such asIcinga You may need to supply some additional components to build a completemonitoring platformGraphiteGraphite was arguably the vanguard of the new generation of timeseries monitoring platforms At its core is a flexible timeseries database with an easytouse query language The reason for the monitoringlove movement and for the enormousinfluence that Graphite has had on frontend UIs is the way that it aggregates andsummarizes metrics It started the move away from perminute monitoring andtoward subsecond monitoringAs you might guess from the name Graphite includes graphing features for webvisualization However this aspect of the package has been somewhat eclipsed byGrafana Graphite is better known these days for its other components Carbon andWhisper which form the core of the data management systemGraphite can be combined with other tools to create a scalable distributed clustered monitoring environment that is capable of absorbing and reporting on hundreds of thousands of metrics Exhibit A shows an architectural diagram of suchan implementationExhibit A Clustered Graphite architecturecarbonrelayWhispercarbonwebappcarbonrelayWhispercarbonwebappcarbonrelayWhispercarbonwebappSQLite Apache httpdcarbonrelayGraphiteLoad balancercollectd collectd collectd collectd collectdUNIX and Linux serversGraphite Graphite GraphiteGrafana Web connections fromusers and administratorsMetricsPrometheusOur favorite timeseries platform today is Prometheus Its a comprehensive platform that includes integrated collection trending and alerting components Thecomponents are both sysadmin and developerfriendly which makes it a greatchoice for a DevOps shop It does not allow for clustering however which maymean that its not the right fit for sites that require high availabilityInfluxDBInfluxDB is an extraordinarily developerfriendly timeseries monitoring platform that supports a broad array of programming languages Much like GraphiteInfluxDB is really just a timeseries database engine Youll need to complete thepackage with external components such as Grafana to form a complete monitoringsystem that includes features like alertingThe data management features of InfluxDB are much richer than those of the alternatives listed above However InfluxDBs additional features also add some unwelcome complexity for typical installationsInfluxDB has had a somewhat troubled history of bugs and incompatibilitiesHowever the current version appears to be stable and is probably the best currentalternative to Graphite if you are seeking a standalone data management systemMuninMunin has historically been quite popular especially in Scandinavia Its built ona clever architecture in which the data collection plugins not only provide databut also tell the system how the data should be presented Although Munin is stillperfectly usable modern alternatives such as Prometheus should be considered fornew deployments Munin is still a useful tool for applicationspecific monitoringin some cases see page Open source charting platformsThe two main choices for creating dashboards and charts are the graphing featuresbuilt into Graphite and a newer package GrafanaGraphite can draw data from stores other than Whisper the native datastoragecomponent of the Graphite package but this isnt necessarily a welltrodden pathAs a databaseagnostic package Grafana does quite well at accommodating foreigndata stores including all those listed in the previous section At last count more than back ends were supported Grafana originally started out as an attempt to improve graphing for Graphite so its quite comfortable in a Graphite environment tooBoth Graphite and Grafana present a dashboardlike graphing interface that cangenerate insightprovoking and managementpleasing visualizations You can usethem to display anything from lowlevel system metrics to businesslevel indicatorsBakeoffs usually give the nod to Grafana for its superior UI and prettier graphsExhibit C on the next page shows a simple Grafana dashboardExhibit B Grafana dashboard exampleCommercial monitoring platformsHundreds of companies sell monitoring software and new competitors enter themarket every week If you are looking for a commercial solution you should at leastconsider the options listed in Table Table Popular commercial monitoring platformsPlatform URL CommentsDatadog datadoghqcom Cloudbased application monitoring platformHuge list of supported systems apps and servicesLibrato libratocom Plug and Play with existing open source pluginsMonitus monitusnet Ecommerce platform monitoringPingdom pingdomcom SaaSbased monitoring platformaSignalFx signalfxcom SaaS platform with long list of cloud integrationsSolarWinds solarwindscom Network monitoring stalwartSysdig Cloud sysdigcom Specialty Docker monitoring and alertingEasy to correlate events across servicesZenoss zenosscom Incredibly complex alternative to Icingaa No software install required Good fit for web apps onlyWhether their systems reside in the cloud on a data center hypervisor or in a closetmost businesses should not be building their own monitoring stack Outsourcingis cheaper and more reliable Hence consider Datadog Librato SignalFx or SysdigCloud if you need a monitoring stack for a common set of applications or serversWhen investigating commercial monitoring platforms you often first consider priceBut dont forget to research the operational details as well How easy is it to integrate into your configuration management system How does the system deploy new plugins or checks to your hosts Arethey pushed or pulled Does it integrate well with your existing notification platform if you have one Does your environment allow the type of external connectivity neededto facilitate a cloudbased monitoring solutionThese are just a few of the questions you should be asking when researching platforms In the end the best platform for your site is one that is easily configurablemeets your budget and is easily adopted by your usersHosted monitoring platformsIf youre not interested in setting up and maintaining your own network monitoringtools you might want to consider a hosted cloud solution Many free and commercial options exist but a popular one is StatusCake statuscakecom An externalproviders ability to see the internal details of your network is limited but hostedoptions work well for validating the health of publicfacing services and web sitesA hosted monitoring provider can also liberate you from the constraints of yourorganizations normal Internet connection If you rely on your upstream networkto transport notifications from an internal monitoring systemas most sites ultimately doyou may want to ensure that your upstream network is itself monitoredand instrumented so that staff can be marshaled in the event of trouble Data collectionThe previous sections reviewed a variety of packages that can serve as a sites centralmonitoring engine However selecting and deploying one of these systems is onlythe first part of the setup process You must now make sure that the data and eventsyoure interested in monitoring make their way into the central monitoring platformThe details of this instrumentation process depend on the systems you want to monitor and the philosophy of your monitoring platform In many cases youll need towrite some simple glue scripts to convert status information into a form that yourmonitoring platform can understand Some platforms such as Icinga come with awide variety of plugins that harvest standard metrics from commonly monitoredsystems Others such as Graphite and InfluxDB make no real provision for datainput at all and must be supplemented by a front end that handles this roleIn the following sections we first look at StatsD a generalpurpose data collectionfront end then review some tools and techniques for instrumenting some commonly monitored systemsStatsD generic data submission protocolStatsD was written by engineers at Etsy as a way to track anything and everythingwithin their own environment Its a UDPbased frontend proxy that dumps anydata you throw at it into a monitoring platform for consumption calculation anddisplay StatsDs superpower is its ability to ingest and perform calculations on arbitrary statisticsEtsys StatsD daemon was written in Nodejs But these days StatsD refers moreto the protocol than to any one of the many software packages that implement itTruth be told even Etsys version is not the original it was inspired by a similarlynamed project at Flickr Implementations have been written in many differentlanguages but we focus on the Etsy release hereStatsD depends on Nodejs so make sure that package has been installed andconfigured appropriately before you move on to installing StatsD The Etsy implementation isnt included in most OS vendors package repositories though otherversions of StatsD often are make sure you dont confuse them Its easiest to clonethe Etsy version directly from GitHub git clone httpsgithubcometsystatsdStatsD is incredibly modular and can feed the incoming data to a variety of backends and clients Lets look at a simple example that uses Graphite as the back endTo ensure that Graphite and StatsD communicate correctly you must modify Carbon Graphites storage component Edit etccarbonstorageschemasconf andadd a stanza similar to the followingstatspattern statsretentions shmindminyThis configuration tells Carbon to keep hours of data at second intervalsCarbon summarizes expiring data at minute intervals and keeps that summaryinformation for an additional days Similarly data at minute granularity ismaintained for a full year Theres nothing magic about these choices youll needto determine whats appropriate for your organizations retention needs and thedata being collectedThe exact definition of what it means to summarize timeseries data varies according to the type of data If youre counting network errors for example you probablywant to summarize by adding up values If youre looking at metrics that representload or utilization you probably want an average You might also need to specifyappropriate ways of handling missing dataThese policies are specified in the file etccarbonstorageaggregationconf Ifyou dont already have a working Graphite installation you might find Graphitessample configuration useful as a starting pointusrsharedocgraphitecarbonexamplesstorageaggregationconfexampleBelow are some reasonable defaults to include in the storageaggregationconf fileminpattern lowerxFilesFactor aggregationMethod minmaxpattern upperdxFilesFactor aggregationMethod maxsumpattern sumxFilesFactor aggregationMethod sumcountpattern countxFilesFactor aggregationMethod sumcountlegacypattern statscountsxFilesFactor aggregationMethod sumdefaultaveragepattern xFilesFactor aggregationMethod averageNote that every configuration block has a regular expression pattern that attemptsto match the names of data series Blocks are read in order and the first matchingblock becomes the controlling specification for each data series For example aseries named samplecount would match the pattern for the count block Thevalues would be rolled up by adding up data points aggregationMethod sumThe xFilesFactor setting determines the minimum number of samples neededto meaningfully downsample your metric Its expressed as a number between and that represents the percentage of nonnull values that must exist at the moregranular layer in order for the rollup layer to have a nonnull value For examplethe xFilesFactor setting for min and max above is so even a single datavalue will satisfy this criterion given our settings in the storageschemaconf fileThe default is If the numbers arent thoughtfully set youll get inaccurate ormissing dataWe can send some test data to StatsD with netcat nc echo samplecountc nc u w statsdadmincom This command submits a value of as a count metric as indicated by the c to thesamplecount data set The packet goes to port on statsdadmincom this isthe port on which statsd listens by default If this datum shows up in your Graphitedashboard youre ready to collect all kinds of monitoring data through one of themany StatsD clients See the StatsD GitHub wiki page githubcometsystatsdwikifor a list of some of the clients that can communicate with StatsD Or write yourown The protocol is simple and the possibilities are endlessData harvesting from command outputIf you can investigate something from the command line you can track it in yourmonitoring platform All you need is a few lines of scripting glue to extract the datanuggets youre interested in which you then massage into a format your monitoring platform can acceptFor example uptime shows the length of time that the system has been up thenumber of loggedin users and the load averages over the last and minutes uptime up days users load average As a human you can parse the output at a glance and see that the current load average is If you want to write a script to check that value regularly or to feed itto another monitoring process you can use text manipulation commands to isolate the desired value uptime perl anFs e print FHere we use Perl to split the output wherever theres a sequence of spaces and commas and to print the contents of the tenth field the minute load average VoilAlthough Perl has been eclipsed by modern languages like Python and Ruby inmost domains it remains the king of quickanddirty text wrangling Its probablynot worth learning Perl solely for this use but Perls ability to phrase sophisticatedtext transformations as oneline commands does come in handyWe can easily expand this oneliner into a short script that determines the loadaverage and submits it to StatsDusrbinenv perluse NetStatsduse SysHostnameNetStatsdHOST statsdadmincomloadavg split s uptimeNetStatsdgaugehostname loadAverage loadavgCompare this script with our oneline StatsD test command from page andour oneline parsing of uptime output above Here Perl has to run the uptimecommand and process its output as a string so that portion looks somewhat different from its oneliner equivalent The oneliner relies on Perls autosplit modeInstead of using nc to handle the network transmission of data to StatsD we use asimple StatsD wrapper that we downloaded from the CPAN archive Thats generally the preferred approach libraries are less brittle than hacks and they clarifythe codes intentMany commands can generate more than one output format Check the man pagefor the command to see what options are available before you attempt to parse itsoutput Some formats are much easier to deal with than othersA few commands support an output format that specifically facilitates parsingOthers have configurable output systems in which you ask for only the fields thatyou really want Yet another common option is a flag that suppresses descriptiveheader lines in the output Network monitoringNetwork status monitoring has traditionally been many sites first foray into thewider world of monitoring and dashboards so its the first of several types of monitoring that we look at in a bit more depth In subsequent sections we also look atOSlevel monitoring application and service monitoring and security monitoringThe basic unit of network monitoring is the network ping also known as an ICMPEcho Request packet We discuss the technical details more thoroughly starting onpage along with the ping and ping commands which initiate pings fromthe command lineThe concept is simple you send an echo request packet to another host on the network and that hosts IP implementation returns a packet to you in response If youreceive a response to your probe you know that all the network gateways and devices that lie between you and the target host are operational You also know that thetarget host is powered on and that its kernel is up and running However becausepings are handled within the TCPIP protocol stack they dont guarantee anythingabout the state of higherlevel software that might be running on the target hostPings dont impose much overhead on the network so its OK to send them frequently say every ten seconds Design your pinging strategy thoughtfully so thatit covers all important gateways and networks Keep in mind that if a ping cant getthrough a gateway neither can monitoring data that reports the failure of pingsYoull want at least one set of pings to originate on the central monitoring host itselfNetwork gateways arent required to answer ping packets so pings might be droppedby a busy gateway Even a properly functioning network loses a packet now and The Comprehensive Perl Archive Network cpanorgthen Ergo dont set off alarms at the first sign of trouble It makes sense to collectping data as binary event records got throughdidnt get through and roll it upinto aggregate measures of percentage packet loss over longer termsYou might also find it interesting to measure throughput between two points onthe network That can be done with iPerf see page for detailsMost network devices support the Simple Network Management Protocol SNMPan industrystandard way of naming and collecting operational data AlthoughSNMP has metastasized far beyond its networking roots we consider it obsoletefor purposes other than basic network monitoringSNMP is a rather large topic of its own so we defer further discussion of it untillater in this chapter See SNMP the Simple Network Management Protocol startingon page for details Systems monitoringSince the kernel controls a systems CPU memory IO and devices most of theinteresting systemlevel state information you might want to monitor lives somewhere inside the kernel Whether youre investigating a particular system by handor setting up an automated monitoring platform you need the right tools to extractand expose this state information Most kernels define formal channels throughwhich such information is exportedUnfortunately kernels are like other types of software error checking instrumentation and debugging features are often something of an afterthought Althoughrecent years have brought improvements in transparency identifying and understanding the exact parameter you might want to monitor can be challenging andsometimes impossibleA particular value can often be obtained in more than one way In the case of loadaverages for example you can read the values directly from procloadavg on Linux systems or with sysctl n vmloadavg on FreeBSD Load averages are also included in the output of the uptime w sar and top commands though top wouldbe a poor choice for noninteractive use Its generally easiest and most efficient toaccess values directly from the kernel through sysctl or proc if you canMonitoring platforms such as Nagios and Icinga include a rich set of communitydeveloped monitoring plugins that you can use to get your hands on commonlymonitored elements They too are often simply scripts that run commands andparse the resulting output but they come already tested and debugged and theyoften work on multiple platforms If you cant find a plugin that yields the valueyoure interested in you can write your ownSee page for moreinformation aboutthe proc filesystemCommands for systems monitoringTable lists some command that are commonly used in monitoring Many ofthese commands yield wildly different output depending on the commandlineoptions you supply so check the man pages for detailsTable Commands that yield commonly monitored parametersCommand Available informationdf Free and used disk space and inodesdu Directory sizesfree Free used and swap virtual memoryiostat Disk performance and throughputlsof Open files and network portsmpstat Perprocessor utilization on multiprocessor systemsvmstat Process CPU and memory statisticsThe Swiss Army knife of commandline data extraction is sar short for systemactivity report This command has a sordid history having originally been introduced in System V UNIX in the s The primary attraction of this command isthat it has been implemented on a wide variety of systems so it enhances the portability of both scripts and sysadmins Sadly the BSD port is no longer maintainedThe example below requests reports every two seconds for a period of one minuteie reports The DEV argument is a literal keyword not a placeholder for adevice or interface name sar n DEV IFACE rxpcks txpcks rxbyts txbyts rxcmps txcmps rxmcsts lo eth eth This example is from a Linux machine with two network interfaces The output includes both instantaneous and average readings of interface utilization in units ofboth bytes and packets The second interface eth is clearly not in usecollectd generalized system data harvesterAs the work of system administration has evolved from wrangling individual systems to managing fleets of virtualized instances simple commandline tools havestarted to create a lot of friction in the monitoring world Although writing scriptsto collect and analyze parameters is a utilitarian and flexible approach maintainingthe consistency of that code base across multiple systems quickly becomes cumbersome Modern tools such as collectd sysdig and dtrace offer a more scalableapproach to collecting this type of data Oldschool sysadmins are often identifiable by their fluency in sarCollection of system statistics should be a continuous process and the UNIX solution to an ongoing task is to create a daemon to handle it Enter collectd the systemstatistics collection daemonThis popular and mature tool runs on both Linux and FreeBSD Typically collectdruns on the local system collects metrics at specified intervals and stores the resulting values You can also configure collectd to run in clientserver mode whereone or more collectd instances aggregate data from a group of other serversSpecification of the metrics to be collected and the destinations to which they aresaved is flexible over plugins are available to suit your exact needs Oncecollectd is running it can be queried by a platform such as Icinga or Nagios forinstantaneous monitoring or can forward data to a platform such as Graphite orInfluxDB for timeseries analysisAn example collectd configuration file is shown below etccollectdcollectdconfHostname clientadmincomFQDNLookup falseInterval LoadPlugin syslogPlugin syslogLogLevel infoPluginLoadPlugin cpuLoadPlugin dfLoadPlugin diskLoadPlugin interfaceLoadPlugin loadLoadPlugin memoryLoadPlugin processesLoadPlugin rrdtoolPlugin rrdtoolDataDir varlibcollectdrrdPluginThis basic configuration collects a variety of interesting system statistics every seconds and writes RRDtoolcompatible data files in varlibcollectdrrdsysdig and dtrace execution tracerssysdig Linux and dtrace BSD comprehensively instrument both kernel and userprocess activity They include components that are inserted into the kernel itselfexposing not only deep kernel parameters but also perprocess system calls andother performance statistics These tools are sometimes described as Wiresharkfor the kernel and processesSee page formore informationabout WiresharkBoth of these tools are complex However they are well worth tackling A weekendspent learning either one will give you amazing new superpowers and ensure yourstatus as an Alist guest on the sysadmin cocktail circuitsysdig is containeraware so it confers extraordinary visibility into environmentswhere tools such as Docker and LXC are in use sysdig is distributed as open sourceand you can integrate it with other monitoring tools such as Nagios or Icinga Thedevelopers also offer a commercial monitoring service Sysdig Cloud that has fullmonitoring and alerting capability Application monitoringAt the top of the software ziggurat we find the holy grail application monitoringThis type of monitoring is rather vaguely defined but the general idea is that it attempts to validate the status and performance of particular pieces of software ratherthan systems or networks as a whole In many cases application monitoring canreach into those systems and profile their internal operationsTo make sure youre monitoring the right things you need business units anddevelopers to join the party and tell you more about their interests and concernsIf you have a web site that runs on the LAMP stack for example youll probablywant to make sure youre monitoring page load times flagging critical PHP errorskeeping tabs on the MySQL database and monitoring for specific issues such asexcessive connection attemptsAlthough monitoring for this layer can be complex this domain is also where monitoring gets sexy Imagine monitoring and pulling into your beautiful Grafana dashboard the number of widgets youve sold in the past hour or the average length oftime that an item stays in a shopping cart If you show your application developersand process owners that level of functionality you usually get immediate buyinto add more monitoring and may even get some help implementing it Eventuallythis layer of monitoring becomes invaluable to the business and you start to beviewed as the champion of monitoring metrics and data analysisApplicationlevel monitoring can yield additional insight into other events withinyour environment For example if widget sales decrease quickly that might be anindication that one of your advertisement networks is downLog monitoringIn its most basic form log monitoring involves grepping through log files to findinteresting data youd like to monitor pulling out that data and processing it intoa form thats usable for analysis display and alerting Since log messages consistof freeform text implementation of this pipeline can range in complexity fromtrivial to challengingSee Chapter for more information about Dockerand containersLogs are typically best managed with a comprehensive aggregation system designedfor that purpose We address such systems in the section Management of logs at scalewhich starts on page Although these systems focus primarily on centralizinglog data and making it easy to search and review most aggregation systems alsosupport threshold alarm and reporting functionalityIf you need automated log review for a few specific purposes and are reluctant tocommit to a more general log management solution we recommend a couple ofsmallerscale tools logwatch and OSSEClogwatch is a flexible batchoriented log summarizer Its primary use is to createdaily summaries of events reported in the logs You can run logwatch more oftenthan once a day but it isnt designed for realtime monitoring For that you wouldprobably want to take a look at OSSEC which we discuss on page OSSEC ispromoted as a security tool but its architecture is general enough that its usefulfor other kinds of monitoring as wellSupervisor Munin a simple option for limited domainsAn allsinging alldancing platform such as Icinga or Prometheus might be overkillfor your needs or your environment What if youre only interested in monitoringone particular application process and dont want the headache of a fullfledgedmonitoring platform Consider combining Munin and Supervisor Theyre easy toinstall require little configuration and work well togetherSupervisor and its server process supervisord help you monitor processes and generate events or notifications when the processes exit or throw an exception The systemis similar in spirit to Upstart or to the processmanagement portions of systemdAs mentioned on page Munin is a general monitoring platform with particularstrengths in application monitoring Its written in Perl and requires an agent knownas a Munin Node to be running on all the systems you want to monitor Setting upa new Node is easy just install the muninnode package edit muninnodeconfto point to the master machine and youre good to goMunin defaults to creating RRDtool graphs with the data that it collects so its anice way to get some graphical feedback without much configuration More than plugins are distributed with Munin and nearly others are available as contributed libraries Its likely that you can find an existing plugin that meets yourneeds If not its easy to write a new script for muninnode to executeCommercial application monitoring toolsIf you search Google for application monitoring tool youll discover many pagesof commercial offerings to evaluate For goldstar due diligence youll also needto scrub through layers of recent discussions regarding application performancemonitoring APMYoull see many references to DevOps in these venues and for good reason application monitoring and APM are key tenets of DevOps They supply quantitativemetrics that teams can use to decide which areas of the stack would most benefitfrom efforts to improve performance and stabilityWe think New Relic newreliccom and AppDynamics appdynamicscom arestandouts in this area These systems capabilities overlap in many ways but AppDynamics typically targets more a full stack monitoring solution whereas NewRelic deals more with profiling behavior inside the application layer itselfRegardless of how you monitor your applications its crucial to keep the development team involved in the process Theyll help ensure that all important metricsare being monitored Close cooperation on monitoring fosters the relationshipbetween teams and limits duplication of effort Security monitoringSecurity monitoring is a universe of its own This area of operational practice issometimes known as security operations or SecOpsDozens of open source and commercial tools and services can be enlisted to helpmonitor an environments security Third parties sometimes called managed securityservice providers MSSPs render outsourced services Despite all these optionssecurity breaches remain common and often go undetected for months or yearsPerhaps the most important thing to know about security monitoring is that noautomated tool or service is enough You must implement a comprehensive security program that includes standards for user behavior data storage and incidentresponse procedures just to name a few elements Chapter Security coversthese basicsTwo core security functions should be integrated with your automated continuousmonitoring strategy system integrity verification and intrusion detectionSystem integrity verificationSystem integrity verification often called file integrity monitoring or FIM is thevalidation of the current state of the system against a knowngood baseline Mostoften this validation compares the contents of the system files kernel executablecommands config files with a cryptographically sound checksum such as SHAIf the checksum value of the file in the running system is different from that of thebaseline version a sysadmin is notified Of course regular maintenance activities It always sounds attractive to outsource security operations then it becomes someone elses problemto make sure your environment is secure But think of it this way would you be comfortable payingsomeone to watch your cashfilled wallet sit on a table with other wallets in a busy train station If so an MSSP might be a good fit for you Acceptable hashing algorithms change over time For example MD is no longer considered cryptographically secure and should no longer be usedSee page formore informationabout DevOpssuch as planned changes updates and patches must be taken into account not allchanges are suspiciousThe most commonly deployed FIM platforms are Tripwire and OSSEC the latteris described in more detail starting on page The Linux version of AIDE alsoincludes file integrity monitoring but unfortunately the FreeBSD version lacksthis componentSimpler is often better A great barebones FIM option is mtree which is native toFreeBSD and has recently been ported to Linux mtree is an easy way to monitorfile state and content changes and its easily integrated into your monitoring scriptsHeres an example of a quick script that uses mtreebinbashif eq thenecho mtreechecksh bvecho b create baselineecho v verify against baselineexitfi seedKEY baseline directoryDIRusrlocallibmtreecheckif b thenrm rf DIRmtreecd DIRmtreec K sha s KEY p sbin mtreesbinfiif v thencd DIRmtree s KEY p sbin mtreesbin mail s hostname mtree integrity check danadmincomfiWith the b flag this script creates and stores the baseline When run again withthe v flag it validates the current contents of sbin against the baselineAs with so many aspects of system administration setting up a FIM platform andoperating it over time are wildly different propositions Youll need a defined processin place to maintain the FIM data and respond to FIM alerts We suggest feedinginformation from your FIM platform into your monitoring and alerting infrastructure so that it is not sidelined or ignoredIntrusion detection monitoringTwo common forms of intrusion detection systems IDSs are in use hostbasedHIDS and networkbased NIDS NIDS systems examine the traffic transiting thenetwork and attempt to identify unexpected or suspicious patterns The most common NIDS system is based on Snort and is covered in detail starting on page HIDS systems run as a set of processes on each system They typically keep tabson a variety of things including network connections file modification times andchecksums daemon and applications logs use of elevated privileges and otherclues that may signal the operation of tools designed to facilitate unauthorized access root kits A HIDS is not a onestop solution for security but its a valuablecomponent of a comprehensive approachThe two most popular open source HIDS platforms are OSSEC Open Source SECurity and AIDE the Advanced Intrusion Detection Environment In our experience OSSEC is handsdown the better choice Although AIDE is a nice FIMplatform on Linux OSSEC includes a broader array of functions It can even beused in a clientserver mode that supports nonUNIX clients such as MicrosoftWindows and a variety of network infrastructure devicesMuch like FIM alerts HIDS data is only as useful as the attention thats paid to itHIDS is not a set it and forget it subsystem and you will need to integrate HIDSalerts with your overall monitoring system The most effective strategy weve foundfor addressing this issue is to autoopen tickets for HIDS alerts in your trouble ticketing system You can then add a monitoring check that alerts on any unresolvedHIDS tickets SNMP the Simple Network Management ProtocolYears ago the networking industry decided it would be helpful to create a standardprotocol for the collection of monitoring data Hence the Simple Network Management Protocol aka SNMPDespite its name SNMP is actually quite complex It defines a hierarchical namespace of management data and methods for reading and writing that data on eachnetwork device SNMP also defines a way for managed servers and devices agentsto send event notification messages traps to management stationsBefore we delve into the arcana of SNMP we should note that the terminology associated with it is some of the most wretched technobabble to be found in the networking arena In many cases the standard names for SNMP concepts and objectsactively lead you away from an understanding of whats going onThat said the protocol itself is simple most of SNMPs complexity lies above theprotocol layer in the conventions for constructing the namespace and in the unnecessarily baroque vocabulary that surrounds SNMP like a protective shell Aslong as you dont think too hard about its internal mechanics SNMP is easy to useSNMP was designed to be implementable by dedicated network hardware such asrouters in which context it remains a plausible option SNMP was later extendedto include monitoring of servers and desktop systems but its fitness for this purpose has always been questionable Today much better alternatives eg collectdsee page are availableWe suggest that you approach SNMP as a lowlevel data collection protocol for usewith specialpurpose devices that dont support anything else Get data out of theSNMP world as quickly as possible and turn it over to a generalpurpose monitoring platform for storage and processing SNMP can be an interesting neighborhoodto visit but you wouldnt want to live thereSNMP organizationSNMP data is arranged in a standardized hierarchy The naming hierarchy is madeup of Management Information Bases MIBs structured text files that describe thedata accessible through SNMP MIBs contain descriptions of specific data variableswhich are referred to with names known as object identifiers or OIDs All currentSNMPcapable devices support the structure for MIBII defined in RFC Buteach vendor can and does extend that MIB further to add more data and metricsOIDs exist within a hierarchical namespace where the nodes are numbered rather than named However for ease of reference nodes also have conventional textnames The separator for pathname components is a dot For example the OIDthat refers to the uptime of a device is This OID is also known bythe humanreadable though not necessarily humanunderstandable without additional documentation nameisoorgdodinternetmgmtmibsystemsysUpTimeTable presents a sampling of OID nodes that might be interesting to monitorfor assessing network availabilityIn addition to the universally supported MIBII there are MIBs for various kindsof hardware interfaces and protocols MIBs for individual vendors MIBs for different snmpd server implementations and MIBs for particular hardware productsA MIB is only a schema for naming management data To be useful a MIB mustbe backed up with agentside code that maps between the SNMP namespace andthe devices actual stateSNMP agents that run on UNIX Linux or Windows come with builtin supportfor MIBII Most can be extended to support supplemental MIBs and to interfacewith scripts that do the actual work of fetching and storing these MIBs associated data Youll see lots of software like this thats left over from a bygone era whenSNMP was the new hotness But its all smoke and no fire you shouldnt be runningan SNMP agent on a UNIX system at all these days except perhaps to answer themost basic queries about network configuration An OID is just a fancy way of naming a specific managed piece of informationTable Selected OIDs from MIBIIOIDa Type ContentssystemsysDescr string System info vendor model OS type etcinterfacesifNumber int Number of network interfaces presentinterfacesifTable table Table of infobits about each interfaceipipForwarding int if system is a gateway otherwise ipipAddrTable table Table of IP addressing data masks etcicmpicmpInRedirects int Number of ICMP redirects receivedicmpicmpInEchos int Number of pings receivedtcptcpInErrs int Number of TCP errors receiveda Relative to isoorgdodinternetmgmtmibSNMP protocol operationsOnly four basic SNMP operations exist get getnext set and trapGet and set are the basic operations for reading and writing data to the node identified by a specific OID Getnext steps through a MIB hierarchy and can read thecontents of tables as wellA trap is an unsolicited asynchronous notification from server agent to clientmanager that reports the occurrence of an interesting event or condition Severalstandard traps are defined including Ive just come up notifications reports offailure or recovery of a network link and announcements of various routing andauthentication problems The mechanism by which the destinations of trap messages are specified depends on the implementation of the agentSince SNMP messages can potentially modify configuration information somesecurity mechanism is needed The simplest version of SNMP security uses theconcept of an SNMP community string which is really just a horribly obfuscated way of saying password Theres usually one community string for readonlyaccess and another that allows writing These days it makes a lot more sense to setup the SNMPv management framework which allows for more security includingauthorization and access control for individual usersNetSNMP tools for serversOn Linux and FreeBSD the most common package that implements SNMP is calledNetSNMP It includes an agent snmpd some commandline tools a server forreceiving traps and even a library for developing SNMPaware applicationsThese days NetSNMP is primarily of interest because of its commands and librariesrather than its agent It has been ported to many different UNIXlike systems so it Many systems come with the default community string set to public Never leave this default inplace set real passwords for both the readonly and readwrite community stringsacts as a consistent platform on top of which you can write scripts So most distributions just separate out the NetSNMP agent into a package of its own making iteasier to install only the commandsOn Debian and Ubuntu the NetSNMP packages are called snmp and snmpd Install only the commands with aptget install snmpOn Red Hat and CentOS the analogous packages are netsnmp and netsnmptoolsInstall the commands with yum install netsnmptoolsOn Linux configuration information goes in etcsnmp take note of the snmpdconffile and snmpd directory in that location Run systemctl start snmpd to start upthe agent daemonOn FreeBSD everything is included in one package pkg install netsnmp Configuration information goes in usrlocaletcsnmp which youll have to create byhand You can start the agent by hand with service snmpd start or putsnmpdenableYESin etcrcconf to start it at boot timeOn all systems where you need to run the SNMP agent youll need to ensure thatUDP port is not blocked by a firewallThe commands that come with NetSNMP can familiarize you with SNMP andtheyre also great for oneoff checks of specific OIDs Table lists the mostcommonly used toolsTable Commandline tools in the NetSNMP packageCommand Functionsnmpdelta Monitors changes in SNMP variables over timesnmpdf Monitors disk space on a remote host through SNMPsnmpget Gets the value of an SNMP variable from an agentsnmpgetnext Gets the next variable in sequencesnmpset Sets an SNMP variable on an agentsnmptable Gets a table of SNMP variablessnmptranslate Searches for and describes OIDs in the MIB hierarchysnmptrap Generates a trap alertsnmpwalk Traverses a MIB starting at a particular OIDBasic SNMP checks generally use some combination of snmpget and snmpdeltaOther programs are helpful when you want to identify new OIDs to monitor fromyour fancy enterprise management tool For example snmpwalk starts at a specified OID or at the beginning of the MIB by default and repeatedly makes getRHELnext calls to the agent This process dumps a complete list of available OIDs andtheir associated valuesFor example heres a truncated sample snmpwalk of the host tuva a Linux system Thecommunity string is secretcommunity and v specifies simple authentication snmpwalk c secretcommunity v tuvaSNMPvMIBsysDescr STRING Linux tuvaatrustcom ELsmp SNMPvMIBsysUpTime Timeticks SNMPvMIBsysName STRING tuvaatrustcomIFMIBifDescr STRING loIFMIBifDescr STRING ethIFMIBifDescr STRING ethIFMIBifType INTEGER softwareLoopbackIFMIBifType INTEGER ethernetCsmacdIFMIBifType INTEGER ethernetCsmacdIFMIBifPhysAddress STRINGIFMIBifPhysAddress STRING defIFMIBifPhysAddress STRING defIFMIBifInOctets Counter IFMIBifInOctets Counter IFMIBifInOctets Counter IFMIBifInUcastPkts Counter IFMIBifInUcastPkts Counter IFMIBifInUcastPkts Counter In this example general information about the system is followed by statisticsabout the hosts network interfaces lo eth and eth Depending on the MIBssupported by the agent you are managing a complete dump can run to hundredsof lines In fact a full install on an Ubuntu system configured to serve every MIBspits out over linesIf you looked up the MIBs for the latest version of NetSNMP on an Ubuntu systemyou would see that the fiveminute load average OID is Ifyou were interested in seeing the fiveminute load average for the local host configured with a community string of public you would run snmpget v c c public localhost iso STRING Many useful SNMPrelated Perl Ruby and Python modules are available fromthese languages respective module repositories Although you can write scripts interms of NetSNMP commands its usually easier and cleaner to make use of nativemodules that are customized for your language of choice Check out mibdepotcom or install the snmpmibsdownloader package Tips and tricks for monitoringOver the years weve picked up a few tips on how to maximize the effectiveness ofmonitoring These are the main ideas Avoid monitoring burnout Ensure that sysadmins who receive notifications outside of regular work hours get regular breaks This goal is bestachieved with a rotation system in which teams of two or more individualsare on call for a day or a week then the next team takes over Failure toheed this advice results in crabby sysadmins who hate their jobs Define what circumstances really require attention and make surethis information is clearly communicated to your monitoring team youroncall teams and the customers or business units you support The merefact that youre monitoring something doesnt mean that administratorsshould be mustered at am when the value goes out of bounds Manyissues should be addressed during normal business hours Eliminate monitoring noise If false positives or notifications for noncriticalservices are being generated make time to stop and fix them Otherwiselike the boy who cried wolf all notifications will eventually fail to receivethe necessary attention Create run books for everything Any common restart reset or correctiveprocedure should be documented in a form that allows a responder whois not intimately familiar with the system in question to take appropriateaction The costs of not having such documentation are that problemswill not be fixed quickly mistakes will be made and additional staff willbe rousted to handle emergencies Wikis are great for maintaining thistype of documentation Monitor the monitoring platform This one will seem obvious once youvemissed a critical outage because the monitoring platform was also downLearn from our mistakes and make sure something is watching yourwatchful eyes Miss an outage because of something that wasnt monitored Make sureit gets added so you catch the problem next time Finally and perhaps most importantly no server or service goes into production without first being added to the monitoring system No exceptions Recommended readingHecht James Rethinking Monitoring for Container Operations Great details onstrategy and philosophy for monitoring containers Find it athttpthenewstackiomonitoringresetcontainersTurnbull JamesThe Art of Monitoring Seattle WA Amazon Digital Services Dixon Jason Monitoring with Graphite Tracking Dynamic Host and ApplicationMetrics at Scale Sebastopol CA OReilly Media Performance analysis and tuning are often treated as a form of system administration witchcraft Theyre not really witchcraft but they do qualify as both scienceand art The science part involves making careful quantitative measurements andapplying the scientific method The art part relates to the need to balance resources in a practical levelheaded way since optimizing for one application or user canresult in other applications or users suffering As with so many things in life youwill often find that its impossible to make everyone happyFolks often assert that todays performance problems are somehow wildly different from those of previous decades That claim is inaccurate Its true that systemshave become more complex but the baseline determinants of performance and thehighlevel abstractions for measuring and managing it remain the same as alwaysUnfortunately improvements in system performance correlate strongly with thecommunitys ability to create new applications that suck up all available resourcesAn added complexity of recent years is the many layers of abstraction that often sitbetween your servers and the physical infrastructure of the cloud Its often impossible to know exactly what hardware is providing storage or CPU cycles to your server Performance AnalysisThe magic and the challenge of the cloud are two aspects of the same form Despitepopular belief you do not get to ignore performance considerations just becauseyour servers are virtual In fact the billing models used by cloud providers createan even more direct link between operational efficiency and server costs Knowinghow to measure and evaluate performance has become more important than everThis chapter focuses on the performance of systems that are used as servers Desktopsystems and laptops typically do not experience the same types of performanceissues that servers do The answer to the question of how to improve performanceon a desktop machine is almost always Upgrade the hardware Users like thisanswer because it means they get fancy new systems more often Performance tuning philosophyOne way that UNIX and Linux differ from other mainstream operating systemsis that copious data are available to characterize their inner workings Detailedinformation is generated by every level of the system and administrators controla variety of tunable parameters Source code is usually available for review if youhave trouble identifying the cause of a performance problem despite the availableinstrumentation For these reasons UNIX and Linux are typically the operatingsystems of choice at performanceconscious sitesEven so performance tuning isnt easy Users and administrators alike often thinkthat if they only knew the right magic their systems would be twice as fast Butthats rarely trueOne common fantasy involves tweaking the kernel variables that control the pagingsystem and the buffer pools These days kernels are pretuned to achieve reasonablethough admittedly not optimal performance under a variety of load conditionsIf you try to optimize the system on the basis of one particular measure of performance eg buffer utilization the chances are high that you will distort the systemsbehavior relative to other performance metrics and load conditionsThe most serious performance issues often lie within applications and have little todo with the underlying operating system This chapter discusses systemlevel performance tuning and mostly leaves applicationlevel tuning to others As a systemadministrator you need to be mindful that application developers are people tooHow many times have you said or thought that It must be a network problemIn a similar way application developers often initially assume that any issues mustoriginate in a subsystem that is someone elses responsibilityGiven the complexity of modern applications some problems can only be resolvedthrough collaboration among application developers system administrators serverengineers DBAs storage administrators and network architects In this chapterwe help you determine what data and information to take back to these other folksto help them solve a performance problemif indeed the problem lies in theirarea This approach is far more productive than just saying Everything looks fineits not my problemIn all cases take everything you read on the Internet with a tablespoon of salt Inthe area of system performance you will see superficially convincing arguments onall sorts of topics However most of the proponents of these theories do not havethe knowledge discipline and time required to design valid experiments Popularsupport means very little for every harebrained proposal you can expect to see aGreek chorus of I increased the size of my buffer cache by a factor of ten just likeJoe said and the system feels much much faster RightHere are some rules to keep in mind Collect and review historical information about your system If the system was performing fine a week ago an examination of the aspects ofthe system that have changed may well lead you to a smoking gun Keepbaselines and trends in your hip pocket to pull out in an emergency Asa first step review log files to see if an underlying hardware problem hasdeveloped Familiarize yourself with the trend collection and analysis tools discussedin Chapter Monitoring These tools are critical for performanceassessment Tune your system in a way that lets you compare the current results tothe systems previous baseline Dont intentionally overload your systems or your network The kernelgives each process the illusion of infinite resources But once of thesystems resources are in use the kernel has to work hard to maintainthat illusion delaying processes and often consuming a sizable fractionof the resources itself As in particle physics the more information you collect with system monitoring utilities the more you affect the system you are observing It is bestto rely on something simple and lightweight that runs in the backgroundeg sar or vmstat for routine observation If those feelers show something significant you can investigate further with other tools When making changes change only one thing at a time and documenteach change that you make Observe record and ponder the results before changing anything else Always make sure you have a rollback plan in case your magic fix actuallymakes things worse Ways to improve performanceHere are some specific things you can do to improve performance Ensure that the system has enough memory As we see in the next sectionmemory size has a major influence on performance If youre running asystem in the cloud the amount of memory allocated to an instance isusually easy to adjust though its often bundled with other resource allocations into a fullsystem profile Eliminate storage resources dependence on mechanical operations wherepossible Solid state disk drives SSDs are widely available and can yieldbig performance boosts because they dont require the physical movementof a disk or armature to read bits SSDs are easily installed or in the caseof cloud environments chosen in place of existing oldschool disk drives If you are using UNIX or Linux as a web server or as some other type ofnetwork application server you may want to spread traffic among several systems with a physical or virtual load balancer Such an appliancebalances the load according to one of several userselectable algorithmssuch as most responsive server or round robinThese load balancers also add useful redundancy should a server go downTheyre really quite necessary if your site must handle unexpected trafficspikes Doublecheck the configuration of the system and of individual applications Many applications can be tuned to yield tremendous performanceimprovements eg by spreading data across disks by not performingDNS lookups on the fly or by running multiple instances of a server Correct problems of usage both those caused by real work too manyservices running at once inefficient programming practices batch jobsrun at excessive priority and large jobs run at inappropriate times of dayand those caused by the system such as unwanted daemons Organize hard disks and filesystems so that load is evenly balanced maximizing IO throughput For specific applications such as databases youcan use a fancy multidisk technology such as striped RAID to optimizedata transfers Consult your database vendor for recommendations ForLinux systems ensure that youve selected the appropriate Linux IOscheduler for your disk see page for detailsRemember that different types of applications and databases responddifferently to being spread across multiple disks RAID comes in manyforms take time to determine which form if any is appropriate for yourparticular application Monitor your network to be sure that it is not saturated with traffic andthat the error rate is low A wealth of network information is availablethrough the netstat FreeBSD and ss Linux commands Identify cases where the system is fundamentally inadequate to satisfy thedemands being made of it You cannot tune your way out of these situationsThese steps are listed in rough order of effectiveness Adding memory convertingto SSDs and balancing traffic across multiple servers can often make a huge difference in performance The effectiveness of the other measures ranges from noticeable to noneAnalysis and optimization of software data structures and algorithms almost alwayslead to significant performance gains But unless you have a substantial base of localsoftware this level of design is usually out of your control Factors that affect performancePerceived performance is determined by the basic capacities of the systems resources and by the efficiency with which those resources are allocated and shared Theexact definition of a resource is rather vague It can include such items as cachedcontexts on the CPU chip and entries in the address table of the memory controller However to a first approximation only the following four resources have mucheffect on performance CPU utilization and stolen cycles see below Memory Storage IO Network IOIf resources are still left after active processes have taken what they want the systems performance is about as good as it can beIf there are not enough resources to go around processes must take turns A processthat does not have immediate access to the resources it needs has to wait arounddoing nothing The amount of time spent waiting is one of the basic measures ofperformance degradationHistorically CPU utilization was one of the easiest resources to measure because aconstant amount of processing power was always available Nowadays some virtualized or cloud environments may allocate CPU more dynamically A process thatsusing more than of the allocated CPU is entirely CPU bound and is consumingessentially all of the systems available computing powerMany people assume that CPU resources are the most important factor affectinga systems overall performance Given infinite amounts of all other resources orcertain types of applications eg numerical simulations a faster CPU or moreCPU cores does make a dramatic difference But in the everyday world CPU isactually relatively unimportantDisk bandwidth is a common performance bottleneck Because traditional harddisks are mechanical systems it takes many milliseconds to locate a disk block fetchits contents and wake up the process thats waiting for it Delays of this magnitudeovershadow every other source of performance degradation Each disk access causes a stall worth millions of CPU instructions Solid state drives SSDs are one toolyou can use to address this problemBecause of virtual memory disk bandwidth and memory can be directly relatedif the demand for physical memory is greater than the supply Situations in whichphysical memory becomes scarce often result in memory pages being written to diskso that they can be reclaimed and reused for another purpose In these situationsusing memory is just as expensive as using the disk Avoid this trap when performance is important make sure that every system has adequate physical memoryNetwork bandwidth resembles disk bandwidth in many ways because of the latencies involved in network communication However networks are atypical in thatthey involve entire communities rather than individual computers They are alsoparticularly susceptible to hardware problems and overloaded servers Stolen CPU cyclesThe promise of the cloud and of virtualization more generally is that your serveralways has the resources it needs In reality much of this bounty is created withsmoke and mirrors Even in a largescale virtualization environment resource contention can have a noticeable effect on a virtual servers performanceCPU is the resource most commonly affected There are two ways by which CPUcycles can be stolen from your virtual machine The hypervisor running your VM enforces a CPU quota based on howmuch CPU power you have contracted for Shortfalls can be addressed byallocation of more resources at the hypervisor level or by your purchasinga larger instance size from the cloud provider The physical hardware is oversubscribed and there are not enough physicalCPU cycles available to meet the current needs of all VM instances eventhough these instances may all be under their CPU quotas On a cloudprovider fixing this issue may be as simple as restarting your instance sothat it gets reassigned to new physical hardware In a data center of yourown the solution may require upgrading your virtualization environmentwith more resourcesAlthough CPU stealing can happen to any operating system running on a virtualized platform Linux gives you some visibility into this phenomenon with the stmetric stolen in top vmstat and mpstatHeres an example from toptop up days user load average Tasks total running sleeping stopped zombieCpu us sy ni id wa hi si stIn this example of the time the system is ready to do work but is unableto run because CPU is being diverted away from the VM by the hypervisor Thistime spent waiting translates directly to reduced throughput Monitor this metriccarefully on virtual servers to ensure that your workloads arent being unintentionally starved of CPU Analysis of performance problemsIt can be difficult to isolate performance problems in a complex system As a sysadmin you often receive anecdotal problem reports that suggest a particular causeor fix eg The web server has gotten painfully sluggish because of all those damnAJAX calls Take note of this information but dont assume that its accurate orreliable do your own investigationRigorous transparent application of the scientific method helps you reach conclusions that you and others in your organization can rely on Such an approach letsothers evaluate your results increases your credibility and raises the likelihoodthat your suggested changes will actually fix the problemBeing scientific doesnt mean that you have to gather all the relevant data yourselfExternal information usually helps a lot Dont spend hours doing experiments related to issues that can just as easily be looked up in a FAQWe suggest the following five steps Formulate the question Pose a specific question in a defined functionalarea or state a tentative conclusion or recommendation that you areconsidering Be specific about the type of technology the componentsinvolved the alternatives you are considering and the outcomes of interest Gather and classify evidence Conduct a systematic search of documentation knowledge bases known issues blogs white papers forums andother resources to locate external evidence related to your question Onyour own systems capture telemetry data and where necessary or possible instrument specific system and application areas of interest Critically appraise the data Review each data source for relevance andcritique it for validity Abstract key information and note the quality ofthe sources Summarize the evidence both narratively and graphically Combine findingsfrom multiple sources into a narrative prcis and if possible a graphicrepresentation Data that appears equivocal in numeric form can oftenbecome decisive once charted Develop a conclusion statement State your conclusions ie the answer toyour question concisely Assign a grade to indicate the overall strengthor weakness of the evidence that supports your conclusions System performance checkupEnough generalitieslets look at some specific tools and areas of interest Beforeyou take measurements you need to know what youre looking atTaking stock of your equipmentStart your inquiry with an inventory of your physical or virtual hardware especiallyCPU and memory resources This inventory can help you interpret the informationpresented by other tools and can help you set realistic expectations regarding theupper bounds on performanceOn Linux systems the proc filesystem is the place to find an overview of the hardware your operating system thinks you have More detailed hardware informationcan be found in sys see page Table shows some of the key files Seepage for general information about procTable Sources of hardware information on LinuxFile Contentsproccpuinfo CPU type and descriptionprocmeminfo Memory size and usageprocdiskstats Disk devices and usage statisticsFour lines in proccpuinfo help you identify the systems exact CPU vendoridcpu family model and model name Some of the values are cryptic its best tolook up the decoding onlineThe exact info contained in proccpuinfo varies by system and processor but heresa representative example cat proccpuinfoprocessor vendorid GenuineIntelcpu family model model name IntelR XeonR CPU E GHzstepping cpu MHz cache size KBphysical id cpu cores siblings The file contains one entry for each processor core seen by the OS The data varyslightly by kernel version The processor value uniquely identifies each core physicalid values are unique per CPU socket and core id values not shown above areunique per core within a CPU socket Cores that support hyperthreading duplicationof CPU contexts without duplication of other processing features are identified byan ht in the flags field not shown above If hyperthreading is actually in use thesiblings field for each core shows how many contexts are available on a given coreAnother command to run for information on both FreeBSD and Linux is dmidecodeIt dumps the systems Desktop Management Interface DMI aka SMBIOS dataThe most useful option is t type Table shows the valid typesTable Type values for dmidecode tValue Description System information Base board Information Chassis information Processor information Cache information Port connector information System slot information OEM strings System configuration options BIOS language information Physical memory array Memory device Memory array mapped address System boot information IPMI device informationThe example below shows typical information sudo dmidecode t dmidecode SMBIOS presentHandle x DMI type bytesProcessor InformationSocket Designation PGA Type Central ProcessorFamily CeleronManufacturer GenuineIntelID FF F Signature Type Family Model Stepping Bits of network configuration information are scattered about the system ifconfiga FreeBSD and ip a Linux are the best sources of IP and MAC information foreach configured interfaceGathering performance dataMost performance analysis tools tell you whats going on at a particular point However the number and character of loads probably change throughout the day Besure to gather a crosssection of data before taking action The best informationon system performance often becomes clear only after a long period a month ormore of data collection It is particularly important to collect data during periodsof peak use Resource limitations and system misconfigurations are often visibleonly when the machine is under heavy load See Chapter Monitoring formore information about collecting and analyzing dataAnalyzing CPU usageYou will probably want to gather three kinds of CPU data overall utilization loadaverages and perprocess CPU consumption Overall utilization can help identifysystems on which the CPUs speed is itself the bottleneck Load averages profileoverall system performance Perprocess CPU consumption data can identify specific processes that are hogging resourcesYou can obtain summary information with the vmstat command vmstat takestwo arguments the number of seconds to monitor the system for each line of output and the number of reports to print If you dont specify the number of reportsvmstat runs until you press ControlC For example vmstat procs memory swap io system cpu r b swpd free buff cache si so bi bo in cs us sy id wa Although exact columns vary among systems CPU utilization statistics are fairlyconsistent across platforms User time system kernel time idle time and timewaiting for IO are shown in the us sy id and wa columns on the far right CPUnumbers that are heavy on user time generally indicate computation and highsystem numbers indicate that processes are making a lot of system calls or are performing lots of IOA rule of thumb for generalpurpose compute servers that has served us well overthe years is this the system should spend approximately of its nonidle time inuser space and in system space the overall idle percentage should be nonzeroIf you are dedicating a server to a single CPUintensive application the majorityof time should be spent in user spaceThe cs column shows context switches per interval that is the number of times thatthe kernel changed which process was running The number of interrupts per interval usually generated by hardware devices or components of the kernel is shownin the in column Extremely high cs or in values typically indicate a misbehavingor misconfigured hardware device The other columns are useful for memory anddisk analysis which we discuss later in this chapterLongterm averages of the CPU statistics let you determine whether there is fundamentally enough CPU power to go around If the CPU usually spends part ofits time in the idle state there are cycles to spare Upgrading to a faster CPU wontdo much to improve the overall throughput of the system although it may speedup individual operationsAs you can see from this example the CPU generally flipflops back and forth between heavy use and idleness Therefore be sure to observe these numbers as anaverage over time The smaller the monitoring interval the less consistent the resultsOn multiprocessor machines most tools present an average of processor statisticsacross all processors On Linux the mpstat command generates vmstatlike outputfor each individual processor The P flag lets you specify a specific processor toreport on mpstat is useful for debugging software that supports symmetric multiprocessingits also enlightening to see how inefficiently your system uses multiple processors Heres an example that shows the status of each of four processorslinux mpstat P ALL PM CPU user nice sys iowait irq soft idle intrs PM PM PM PM On a workstation with only one user the CPU generally spends most of its timeidle Then when you render a web page or switch windows the CPU is used heavily for a short period In this situation information about longterm average CPUusage is not meaningfulThe second CPU statistic thats useful for characterizing the burden on your systemis the load average which represents the average number of runnable processesIt gives you a good idea of how many pieces the CPU pie is being divided into Theload average is obtained with the uptime command uptimeam up days users load average Three values are given corresponding to the and minute averages In generalthe higher the load average the more important the systems aggregate performancebecomes If there is only one runnable process that process is usually bound by asingle resource commonly disk bandwidth or CPU The peak demand for thatone resource becomes the determining factor in performanceWhen more processes share the system loads may or may not be more evenly distributed If the processes on the system all consume a mixture of CPU disk andmemory the performance of the system is less likely to be dominated by constraintson a single resource In this situation it becomes most important to look at averagemeasures of consumption such as total CPU utilizationThe system load average is an excellent metric to track as part of a system baselineIf you know your systems load average on a normal day and it is in that same rangeon a bad day this is a hint that you should look elsewhere such as the networkfor performance problems A load average above the expected norm suggests thatyou should look at the processes running on the system itselfAnother way to view CPU usage is to run the ps aux command to see how muchof the CPU each process is using On a busy system at least of the CPU isoften consumed by just one or two processes Deferring the execution of the CPUhogs or reducing their priority makes the CPU more available to other processesAn excellent alternative to ps is a program called top It presents about the sameinformation as ps but in a live regularly updated format that shows the status ofthe system over timeUnderstanding how the system manages memoryThe kernel manages memory in units called pages that are usually KiB or largerIt allocates virtual pages to processes as they request memory Each virtual pageis mapped to real storage either to RAM or to backing store on disk The kerneluses a page table to keep track of the mappings between these madeup virtualpages and real pages of memoryThe kernel can effectively allocate as much memory as processes ask for by augmenting real RAM with swap space Since processes expect their virtual pages to map toreal memory the kernel may have to constantly shuffle pages between RAM andswap as different pages are accessed This activity is known as pagingThe kernel tries to manage the systems memory so pages that have been recently accessed are kept in memory and less active pages are paged out to disk Thisscheme is known as an LRU system since the least recently used pages are the onesthat get shunted to diskIt would be inefficient for the kernel to keep track of all memory references so ituses a cachelike algorithm to decide which pages to keep in memory The exactalgorithm varies by system but the concept is similar across platforms This systemis cheaper than a true LRU system and produces comparable results Refreshing tops output too rapidly can itself be quite a CPU hog so be judicious in your use of top Ages ago a second process known as swapping could occur in which all of a processs pages werepushed out to disk at the same time Today demand paging is used in all casesSee page formore informationabout prioritiesSee page for moreinformation about topWhen memory is low the kernel tries to guess which pages on the inactive list wereleast recently used If those pages have been modified by a process they are considered dirty and must be paged out to disk before the memory can be reusedPages that have been laundered in this fashion or that were never dirty to beginwith are clean and can be recycled for use elsewhereWhen a process refers to a page on the inactive list the kernel returns the pagesmemory mapping to the page table resets the pages age and transfers the pagefrom the inactive list to the active list Pages that have been written to disk must bepaged in before they can be reactivated if the page in memory has been remappedA soft fault occurs when a process references an inmemory inactive page anda hard fault results from a reference to a nonresident pagedout page In otherwords a hard fault requires a page to be read from disk and a soft fault does notThe kernel tries to stay ahead of the systems demand for memory so there is notnecessarily a onetoone correspondence between pageout events and page allocations by running processes The goal of the system is to keep enough free memory handy that processes dont have to actually wait for a pageout each time theymake a new allocation If paging increases dramatically when the system is busy itwould probably benefit from more RAMYou can tune the kernels swappiness parameter procsysvmswappiness to tellthe kernel how to balance between reclaiming swapbacked and filebacked pagesSetting swappiness to zero focuses reclamations entirely on filebacked pages aswappiness of makes an equal balance between the two By default the swappiness parameter has a value of If you find yourself tempted to modify thisparameter its probably time to buy more RAM for the systemIf the kernel is unable to reclaim pages Linux uses an outofmemory killer tohandle this condition The killer function selects and kills a process to free up memory Although the kernel attempts to kill off the least important user process onyour system running out of memory is always something to avoid In this situationits likely that a substantial portion of the systems resources are being devoted tomemory housekeeping rather than to useful workAnalyzing memory usageTwo numbers summarize memory activity the total amount of active virtual memory and the current paging rate The first number tells you the total demand formemory and the second suggests the proportion of that memory that is activelyused Your goal is to reduce activity or increase memory until paging remains at anacceptable level Occasional paging is inevitable dont try to eliminate it completelyRun swapon s to determine the amount of paging swap space thats currently in uselinux swapon sFilename Type Size Used Prioritydevhdb partition devhda partition swapon reports usage in kilobytes The sizes quoted by these programs do not include the contents of core memory so you have to compute the total amount ofvirtual memory yourselfVM size of real memory amount of swap space usedOn FreeBSD systems paging statistics obtained with vmstat look like thisfreebsd vmstat procs memory page disks faultsr b w avm fre flt re pi po fr sr da cd in sy M G M G M G M G M G CPU information has been removed from this example Under the procs headingare shown the number of processes that are immediately runnable blocked onIO and runnable but swapped If the value in the w column is ever nonzero it islikely that the systems memory is pitifully inadequate relative to the current loadUnder the memory heading you can see both the active virtual memory avm andfree virtual memory fre The columns under the page heading give informationabout paging activity All columns represent average values per second Table shows their meaningsTable Decoding guide for FreeBSD vmstat paging statisticsColumn Meaningflt Total number of page faultsre Number of pages reclaimed rescued from the free listpi Number of kilobytes paged inpo Number of kilobytes paged outfr Number of kilobytes placed on the free listsr Number of pages scanned by the clock algorithmOn Linux systems paging statistics obtained with vmstat look like thislinux vmstat procs memory swap io system cpu r b swpd free buff cache si so bi bo in cs us sy id wa st As in the FreeBSD output the number of processes that are immediately runnableand that are blocked on IO are shown under the procs heading Paging statisticsare condensed to two columns si and so which represent pages swapped in andout respectivelyAny apparent inconsistencies among the memoryrelated columns are for the mostpart illusory Some columns count pages and others count kilobytes All values arerounded averages Furthermore some are averages of scalar quantities and othersare average deltasUse the si and so fields to evaluate the systems swapping behavior A pagein sirepresents a page being recovered from the swap area A pageout so representsdata being written to the swap area after being forcibly ejected by the kernelIf your system has a constant stream of pageouts its likely that you would benefitfrom more physical memory But if paging happens only occasionally and does notproduce annoying hiccups or user complaints you can ignore it If your systemfalls somewhere in the middle further analysis should depend on whether you aretrying to optimize for interactive performance eg a workstation or for a moreserverlike workloadAnalyzing disk IOYou can monitor disk performance with the iostat command Like vmstat it acceptsoptional arguments to specify an interval in seconds and a repetition count and itsfirst line of output is a summary since boot iostat output on Linux looks like thislinux iostatDevice tps kBreads kBwrtns kBread kBwrtnsda dm dm dm Each hard disk has the columns tps kBreads kBwrtns kBread and kBwrtnindicating transfers per second kilobytes read per second kilobytes written persecond total kilobytes read and total kilobytes writtenThe cost of seeking is the most important factor affecting mechanical disk drive performance To a first approximation the rotational speed of the disk and the speedof the bus to which the disk is connected have relatively little impact Modern mechanical disks can transfer hundreds of megabytes of data per second if they areread from contiguous sectors but they can only perform about to seeksper second If you transfer one sector per seek you can easily realize less than of the drives peak throughput SSD disks have a significant advantage over theirmechanical predecessors because their performance is not tied to platter rotationor head movementWhether youre using mechanical or SSD disks you should put filesystems thatare used together on separate disks to maximize performance Although bus architectures and device drivers influence efficiency most computers can managemultiple disks independently thereby increasing throughput For example its often worthwhile to segregate frequently accessed web server data and web serverlogs onto different disksIts especially important to split the paging swap area among several disks if possible since paging tends to slow down the entire system Many systems can use bothdedicated swap partitions and swap files on a formatted filesystemThe lsof command which lists open files and the fuser command which shows theprocesses that are using a filesystem can be helpful for isolating disk IO performance issues These commands show interactions between processes and filesystemssome of which may be unintended For example if an application is writing its logto the same device used for database logs a disk bottleneck may resultfio testing storage subsystem performancefio githubcomaxboefio is available for both Linux and FreeBSD Use it to test theperformance of the storage subsystem Its particularly helpful in large environmentswhere shared storage resources such as a Storage Area Network are deployed Ifyou find yourself in a situation where storage performance is a concern its oftenvaluable to determine quantitative values for the following Throughput in IO operations per second IOPS read write and mixed Average latency read and write Maximum latency peak read or write latencyAs part of the fio distribution config fio files for common tests such as these areincluded in the examples subdirectory Heres an example of a simple readwrite test fio readwritefioReadWriteTest g rwrw bsKKKKKK engposixaio depthfioStarting threadJobs f M done MBMBKB sKK iops eta ms read ioMB bwKBs iops runt msec slat usec min max avg stdev clat usec min max avg stdev lat usec min max avg stdev clat percentiles usec th th th th th th th th th th th th th th th th th See page formore informationabout lsof and fuser READ ioMB aggrbKBs minbBs maxbKBsmintmsec maxtmsec WRITE ioMB aggrbKBs minbKBs maxbKBsmintmsec maxtmsecAs with so many performancerelated metrics there is no universally correct value for any of these measures Its best to establish a benchmark make adjustmentsand remeasuresar collecting and reporting statistics over timeThe sar command is a performance monitoring tool that has lingered through multiple UNIX and Linux epochs despite its somewhat obscure commandline syntaxThe original command had its roots in early ATT UNIXAt first glance sar seems to display much the same information as vmstat andiostat However theres one important difference sar can report on historical aswell as current dataWithout options the sar command reports CPU utilization for the day at minuteintervals since midnight as shown below This historical data collection is madepossible by the sal script which is part of the sar toolset and must be set up torun from cron at periodic intervals sar stores the data it collects underneath thevarlog directory in a binary formatlinux sarLinux generic nuerbull x CPU CPU user nice system iowait steal idle all all In addition to CPU information sar can also report on metrics such as disk andnetwork activity Use sar d for a summary of this days disk activity or sar n DEVfor network interface statistics sar A reports all available informationsar has some limitations but its a good bet for quickanddirty historical information If youre serious about making a longterm commitment to performancemonitoring we suggest that you set up a data collection and graphing platformsuch as GrafanaChoosing a Linux IO schedulerLinux systems use an IO scheduling algorithm to mediate among processes competing to perform disk IO The IO scheduler massages the order and timing ofdisk requests to achieve the best possible overall IO performance for a given application or situationThe Linux packagethat contains saris called sysstatSee page formore informationabout GrafanaThree different scheduling algorithms are available in current Linux kernels Completely Fair Queuing This is the default algorithm and is usually thebest choice for mechanical hard disks on generalpurpose servers It triesto evenly distribute access to IO bandwidth If nothing else the algorithm surely deserves an award for marketing who could ever say no toa completely fair scheduler Deadline This algorithm tries to minimize the latency for each requestIt reorders requests to increase performance NOOP This algorithm implements a simple FIFO queue It assumes thatIO requests have already been optimized or reordered by the driver orthat they will be optimized or reordered by the device as might be doneby an intelligent controller This option may be the best choice in someSAN environments and is the best choice for SSD drives because SSDdrives dont have variable retrieval latenciesYou can view or set the algorithm in use for any particular device through the filesysblockdiskqueuescheduler The active scheduler is enclosed in brackets cat sysblocksdaqueueschedulernoop deadline cfq sudo sh c echo noop sysblocksdaqueuescheduler cat sysblocksdaqueueschedulernoop deadline cfqBy determining which scheduling algorithm is most appropriate for your environment you may need to run trials with each scheduler you may be able to improveIO performanceUnfortunately the scheduling algorithm does not persist across reboots when set inthis manner You can set it for all devices at boot time with the elevatoralgorithmkernel argument That configuration is usually set in the GRUB boot loaders configuration file grubconfperf profiling Linux systems in detailLinux kernel versions and higher include a perfevents interface that affordsuserlevel access to the kernels performance metric event stream The perf command is a powerful integrated system profiler that reads and analyzes informationfrom this stream All components of a system can be profiled hardware kernelmodules the kernel itself shared libraries and applicationsTo get started with perf youll need to get a full set of the linuxtools packages sudo aptget install linuxtoolscommon linuxtoolsgenericlinuxtoolsuname rSee page formore informationabout GRUBOnce youve installed the software check out the tutorial at googlfmt for examples and use cases This is a deep link into perfwikikernelorgThe TLDR path to getting started is to try perf top which is a toplike display ofsystemwide CPU use Of course the simple example below only scratches thesurface of perfs capabilities sudo perf topSamples K of event cpuclock Event count approx Overhead Shared Object Symbol kernel k xfffdb kernel k finishtaskswitch kernel k entrySYSCALLafterswapgs kernel k strhashbufsigned kernel k halfmdtransform find xa kernel k exthtreestoredirent libcso strlen kernel k dlookuprcu find xf kernel k copyusergenericunrolled kernel k kfree kernel k rawspinlock find xfa find xa find ftsread kernel k kmalloc kernel k extreaddir libcso malloc libcso fcntl kernel k extcheckdirentryThe Overhead column shows the percentage of the time the CPU was in the corresponding function when it was sampled The Shared Object column is the component eg the kernel shared library or process in which the function residesand the Symbol column is the name of the function in cases where symbol information hasnt been stripped Help My server just got really slowIn previous sections weve talked mostly about issues that relate to the averageperformance of a system Solutions to these longterm concerns generally take theform of configuration adjustments or upgradesHowever you will find that even properly configured systems are sometimes moresluggish than usual Luckily transient problems are often easy to diagnose Mostof the time they are caused by a greedy process that is simply consuming so muchCPU power disk or network bandwidth that other processes are affected On occasion malicious processes hog available resources to intentionally slow a systemor network a scheme known as a denial of service DOS attackThe first step in diagnosis is to run ps auxww or top to look for obvious runawayprocesses Any process thats using more than of the CPU is likely to be atfault If no single process is getting an inordinate share of the CPU check to seehow many processes are getting at least If you snag more than two or threedont count ps itself the load average is likely to be quite high This is in itself acause of poor performance Check the load average with uptime and use vmstator top to check whether the CPU is ever idleIf no CPU contention is evident run vmstat to see how much paging is going onAll disk activity is interesting a lot of pageouts may indicate contention for memoryand disk traffic in the absence of paging may mean that a process is monopolizingthe disk by constantly reading or writing filesTheres no direct way to tie disk operations to processes but ps can narrow downthe possible suspects for you Any process that is generating disk traffic must beusing some amount of CPU time You can usually make an educated guess aboutwhich of the active processes is the true culprit Use kill STOP to suspend theprocess and test your theorySuppose you do find that a particular process is at faultwhat should you do Usually nothing Some operations just require a lot of resources and are bound to slowdown the system It doesnt necessarily mean that theyre illegitimate It is sometimesuseful however to renice an obtrusive process that is CPUboundSometimes application tuning can dramatically reduce a programs demand forCPU resources this effect is especially visible with custom network server softwaresuch as web applicationsProcesses that are disk or memory hogs often cant be dealt with so easily renicegenerally does not help You do have the option of killing or stopping such processesbut we recommend against this if the situation does not constitute an emergencyAs with CPU pigs you can use the lowtech solution of asking the owner to runthe process laterLinux has a handy option for dealing with processes that consume excessive diskbandwidth in the form of the ionice command This command sets a processs IOscheduling class at least one of the available classes supports numeric IO prioritieswhich can be set through ionice as well The most useful invocation to remember is ionice c p pid which allows the named process to perform IO only if noother processes wants to A large virtual address space or resident set used to be a suspicious sign but shared libraries havemade these numbers less useful ps is not very smart about separating systemwide shared libraryoverhead from the address spaces of individual processes Many processes wrongly appear to havetens or hundreds of megabytes of active memoryThe kernel allows a process to restrict its own use of physical memory by calling thesetrlimit system call This facility is also available in the shell through the builtinulimit command limits on FreeBSD For example the command ulimit m causes all subsequent commands that the user runs to have their use of physicalmemory limited to MB This feature is roughly equivalent to renice for memorybound processesIf a runaway process doesnt seem to be the source of poor performance investigatetwo other possible causes The first is an overloaded network Many programs areso intimately bound up with the network that its hard to tell where system performance ends and network performance beginsSome network overloading problems are hard to diagnose because they come andgo very quickly For example if every machine on the network runs a networkrelated program out of cron at a particular time each day there will often be a briefbut dramatic glitch Every machine on the net will hang for five seconds and thenthe problem will disappear as quickly as it cameServerrelated delays are another possible cause of performance crises UNIX andLinux systems are constantly consulting remote servers for NFS Kerberos DNS andany of a dozen other facilities If a server is dead or some other problem makes theserver expensive to communicate with the effects ripple back through client systemsFor example on a busy system some process may use the gethostent library routine every few seconds or so If a DNS glitch makes this routine take two secondsto complete you will likely perceive a difference in overall performance DNS forward and reverse lookup configuration problems are responsible for a surprisingnumber of server performance issues Recommended readingDrepper Ulrich What Every Programmer Should Know about Memory You canfind this article online at lwnnetArticlesEzolt Phillip G Optimizing Linux Performance Upper Saddle River NJ Prentice Hall PTR Gregg Brendan Systems Performance Enterprise and the Cloud Upper SaddleRiver NJ Prentice Hall PTR Koziol Prabhat and Quincey Koziol High Performance Parallel IO LondonCRC Press More granular resource management can be achieved through the Classbased Kernel Resource Management functionality see ckrmsourceforgenetA service is only as reliable as the data center that houses it For those with handson experience thats just common senseProponents of cloud computing sometimes seem to imply that the cloud can magically break the chains that join the physical and virtual worlds But although cloudproviders offer a variety of services that help boost resilience and availability everycloud resource ultimately lives somewhere in mundane realityUnderstanding where your data actually lives is an important part of being a systemadministrator If you are involved in selecting third party cloud providers evaluatevendors and their facilities quantitatively You might also find yourself in positionswhere security data sovereignty or political concerns dictate that you build andmaintain your own data centerA data center is composed of A physically safe and secure space Racks that hold computer networking and storage devices Electric power and standby power sufficient to operate the installed devices At least to a first approximation Of course its possible to distribute a service among multiple datacenters thereby limiting the impact of a failure in any one center Data Center Basics Cooling to keep the devices within their operating temperature ranges Network connectivity throughout the data center and to places beyondenterprise network partners vendors Internet Onsite operational staff to support the equipment and infrastructureCertain aspects of data centerssuch as their physical layout power and coolingwere traditionally designed and maintained by facilities or physical plant staffHowever the fastmoving pace of IT technology and the increasingly low tolerancefor downtime have forced a shotgun marriage of IT and facilities staff as partnersin the planning and operation of data centers RacksThe days of the traditional raisedfloor data centerin which power cooling network connections and telecommunications lines are all hidden underneath thefloorare over Have you ever tried to trace a cable that runs under the floor ofone of these labyrinths Our experience is that although it looks nice through glassa classic raisedfloor machine room is really just a hidden rats nest Today youshould use a raised floor to hide electrical power feeds to distribute cooled air andfor nothing else Network cabling both copper and fiber should be routed throughoverhead raceways designed specifically for this purposeIn a dedicated data center storing equipment in racks as opposed to say settingit on tables or on the floor is the only maintainable professional choice The beststorage schemes use racks that are interconnected with an overhead track systemthrough which cables can be routed This approach confers that irresistible hightech feel without sacrificing organization or maintainabilityThe best overhead track system that we know of is manufactured by ChatsworthProducts You can construct homes for both shelfmounted and rackmountedservers from standard singlerail telco racks Two backtoback telco racksmake a hightechlooking traditional rack for situations in which you need to attach rack hardware both in front of and in back of equipment Chatsworth providesthe racks cable races and cable management doodads as well as all the hardwarenecessary to mount them in your building Since the cables lie in visible tracks theyare easy to trace and you will naturally be motivated to keep them tidy PowerSeveral strategies may be needed to provide a data center with clean stable faulttolerant power Common options include Uninterruptible power supplies UPSs UPSs provide power whenthe normal longterm power source eg the commercial power gridbecomes unavailable Depending on size and capacity a UPS can provide Electrical feeds are often overhead these days tooanywhere from a few minutes to a couple of hours of power UPSs alonecannot support a site in the event of a longterm outage Onsite power generation If the commercial grid is unavailable onsitestandby generators can provide longterm power Generators are usuallyfueled by diesel LP gas or natural gas and can support the site as long asfuel is available It is customary to store at least hours of fuel onsiteand to arrange to buy fuel from multiple providers Redundant power feeds In some locations it may be possible to obtainmore than one power feed from the commercial power grid possibly fromdifferent power generatorsIn all cases servers and network infrastructure equipment should at least be puton uninterruptible power supplies Good UPSs have an Ethernet or USB interfacethat can be attached either to the machine to which they supply power or to a centralized monitoring infrastructure that can elicit a higherlevel response Such connections let the UPS warn computers or operators that power has failed and that aclean shutdown should be performed before the batteries run outUPSs are available in various sizes and capacities but even the largest ones cannotprovide longterm backup power If your facility must operate on standby powerfor longer than a UPS can handle you need a local generator in addition to a UPSA large selection of standby power generators is available ranging in capacity from kW to more than kW The gold standard is the family of generators madeby Cummins Onan powercumminscom Most organizations select diesel astheir fuel type If youre in a cold climate make sure you fill the tank with wintermix diesel or substitute Jet A aircraft fuel to prevent gelling Diesel is chemicallystable but can grow algae so consider adding an algicide to diesel that you plan tostore for an extended periodGenerators and the infrastructure to support them are expensive but they cansave money in some ways too If you install a standby generator your UPSs needonly be large enough to cover the short gap between the power going out and yourgenerator coming onlineIf UPSs or generators are part of your power strategy it is extremely important tohave a periodic test plan in place We recommend that you test all componentsof your standby power system at least every months In addition you or yourvendor should perform preventive maintenance on standby power componentsat least annuallyRack power requirementsPlanning the power for a data center is a difficult challenge Typically the opportunity to build a new data center or to significantly remodel an existing one comesup only every decade or so So its important to look far down the road when planning power systemsSee page for moreinformation aboutshutdown proceduresMost architects are biased toward calculating the amount of power needed in a datacenter by multiplying the centers square footage by a magic number This approachproves to be ineffective in most realworld cases because the size of the data centeralone tells you little about the types of equipment it might eventually house Ourrecommendation is to use a perrack power consumption model and to ignore theamount of floor spaceHistorically data centers have been designed to provide between kW and kWto each rack But now that server manufacturers have started squeezing servers intoU of rack space and building blade server chassis that hold or more blades thepower needed to support a full rack of modern gear has skyrocketedOne approach to solving the power density problem is to put only a handful of Uservers in each rack leaving the rest of the rack empty Although this techniqueeliminates the need to provide more power to the rack its a prodigious waste ofspace A better strategy is to develop a realistic projection of the power that mightbe needed by each rack and to provision power accordinglyEquipment varies in its power requirements and its hard to predict exactly whatthe future will hold A good approach is to create a system of power consumptiontiers that allocates the same amount of power to all racks in a particular tier Thisscheme is useful not only for meeting current equipment needs but also for planning future use Table outlines some basic starting points for tier definitionsTable Powertier estimates for racks in a data centerPower tier WattsrackInsanely high density or custom kWUltra high density kWVery high density eg blade servers kWHigh density eg U servers kWStorage equipment kWNetwork switching equipment kWNormal density kWOnce youve defined your power tiers estimate your need for racks in each tier Onthe floor plan put racks from the same tier together Such zoning concentrates thehighpower racks and lets you plan cooling resources accordinglykVA vs kWOne of the many common disconnects between IT folks facilities folks and UPSengineers is that each of these groups uses different units for power The amount ofpower a UPS can provide is typically labeled in kVA kilovoltamperes But computer equipment and the electrical engineers that support your data center usuallyexpress power in watts W or kilowatts kW You might remember from fourthgrade science class that watts volts amps Unfortunately your fourth gradeteacher probably failed to mention that watts is a vector value which for AC powerincludes a power factor pf in addition to volts and ampsIf you are designing a bottlefilling line at a brewery that involves lots of large motors and other heavy equipment ignore this section and hire a qualified engineerto determine the correct power factor for use in your calculations But for modernday computer equipment you can cheat and use a constant for a probablygood enough conversion between kVA and kWkVA kW A final point to note is that when estimating the amount of power you need in a datacenter or to size a UPS you should measure devices actual power consumptionrather than relying on manufacturers stated values as shown on equipment labelsLabel values typically represent the maximum possible power consumption andare therefore misleadingEnergy efficiencyEnergy efficiency has become a popular operational metric for evaluating data centers The industry has standardized on a simple ratio known as the power usageeffectiveness PUE as a way of expressing a plants overall efficiencyA hypothetically perfect data center would have a PUE of that is it would consume exactly the amount of power needed by IT gear with no overhead Of coursethis goal is unreachable in practical terms Equipment generates heat that must beremoved human operators need lighting and other environmental accommodations etc The higher the PUE value the less energy efficient and more expensivea data center is to operateModernday data centers that are reasonably energy efficient generally have a PUEratio of or less For reference data centers from a decade ago typically had PUEratios in the range Google which has made energy efficiency a focus regularly publishes its PUE ratios and as of has achieved an average PUE of across its data centersMeteringYou get what you measure If you are serious about energy efficiency its importantto understand which devices are actually consuming the most energy Althoughthe PUE ratio gives you a general impression of the amount of energy consumed asnonIT overhead it says very little about the power efficiency of the actual serversSee page forsome additionaltips on measuringpower consumptionTotal power consumed by facilityTotal power consumed by IT equipment PUE In fact replacing servers with more powerefficient models will increase the PUErather than decreasing itIts up to the data center administrator to select components that use the minimumamount of energy One obvious enabling technology is power consumption metering at the aisle rack and device level Select or build data centers that can easilyprovide this critical usage dataCostOnce upon a time the cost of power was more or less the same across data centersin different locations These days the hyperscale cloud industry Amazon GoogleMicrosoft and others sends data center designers hunting for potential cost efficiencies in every corner of the world One successful strategy has been to locate largedata centers near sources of inexpensive power such as hydroelectric power plantsWhen deciding whether to operate your own data center be sure to factor the costof power into your assessment Chances are that the big guys have a builtin costadvantage in this aspect of operations and others Widespread fiber and bandwidth availability have largely rendered obsolete the traditional advice to locateyour data center near your teamRemote controlYou might occasionally find yourself needing to regularly powercycle a server because of a kernel or hardware glitch Or perhaps you have nonLinux servers inyour data center that are more prone to this type of problem In either case youcan consider installing a system that lets you powercycle problem servers by remote controlA reasonable family of solutions is manufactured by American Power ConversionAPC Their remotely manageable products are conceptually similar to powerstrips except that they can be controlled by a web browser that reaches the powerdistribution unit through a builtin Ethernet port Cooling and environmentJust like humans computers work better and live longer if theyre happy in theirenvironment Maintenance of a safe operating temperature is a prerequisite forthis happinessThe American Society of Heating Refrigerating and Airconditioning EngineersASHRAE traditionally recommended data center temperatures measured at serverinlets in the range of to F to C To support organizations attemptsto reduce energy consumption ASHRAE released guidance in that suggestsa more lenient temperature range to to F to C Although thisrange seems unhelpfully broad it does suggest that todays hardware can flourishin a wide range of environmentsCooling load estimationTemperature maintenance starts with an accurate estimate of your cooling loadTraditional textbook models for data center cooling even those from the scan be off from the realities of todays highdensity blade server chassis by up toan order of magnitude Hence we have found that its a good idea to doublecheckthe cooling load estimates produced by your HVAC folksYou need to determine the heat load contributed by the following components Roof walls and windows Electronic gear Light fixtures Operators peopleOf these only the first should be left to your HVAC folks The other componentscan be assessed by the HVAC team but you should do your own calculations aswell Make sure that any discrepancies between your results and those of the HVACteam are fully explained before construction startsRoof walls and windowsYour roof walls and windows dont forget solar load contribute to your environments cooling load HVAC engineers usually have a lot of experience with theseelements and should be able to give you good estimatesElectronic gearYou can estimate the heat load produced by your servers and other electronic gearby determining their power consumption In practical terms all electric power thatis consumed eventually ends up as heatAs when planning for powerhandling capacity direct measurement of power consumption is by far the best way to obtain this information Your friendly neighborhood electrician can help or you can purchase an inexpensive meter and do ityourself The Kill A Watt meter made by P is a popular choice at around butits limited to small loads amps that plug in to a standard wall outlet For largerloads or nonstandard connectors use a clampon ammeter such as the Fluke also known as a current clamp to make these measurementsMost equipment is labeled with its maximum power consumption in watts Howevertypical consumption tends to be significantly less than the maximumYou can convert power consumption to the standard heat unit BTUH British thermal units per hour by multiplying by BTUHwatt For example if you wantedto build a data center that would house servers rated at watts each the calculation would beLight fixturesAs with electronic gear you can estimate light fixture heat load from power consumption Typical office light fixtures contain four watt fluorescent tubes If yournew data center had six of these fixtures the calculation would beOperatorsAt one time or another humans will need to enter the data center to service something Allow BTUH for each occupant To allow for four humans in the datacenter at the same timeTotal heat loadOnce you have calculated the heat load for each component sum the results to determine your total heat load For this example lets assume that our HVAC engineerestimated the load from the roof walls and windows to be BTUHCooling system capacity is typically expressed in tons You can convert BTUH totons by dividing by BTUHton You should also allow at least a slopfactor to account for errors and future growthSee how your estimate matches up with the one from your HVAC folksHot aisles and cold aislesYou can dramatically reduce your data centers cooling difficulties by putting somethought into its physical layout The most common and effective strategy is to alternate hot and cold aisles servers BTUH wattsserver BTUHwatt xtures BTUH wattsxture BTUHwatt humans BTUH BTUHhuman BTUH for roof walls and windows BTUH for servers and other electronic gear BTUH for light xtures BTUH for operators BTUH total BTUH tons of cooling required ton BTUHFacilities that have a raised floor and are cooled by a traditional CRAC computerroom air conditioner unit are often set up so that cool air enters the space underthe floor rises up through holes in the perforated floor tiles cools the equipmentand then rises to the top of the room as warm air where it is sucked into return airducts Traditionally racks and perforated tiles have been placed randomly aboutthe data center a configuration that results in relatively even temperature distribution The result is an environment that is comfortable for humans but not reallyoptimized for computersA better strategy is to lay out alternating hot and cold aisles between racks Coldaisles have perforated cooling tiles and hot aisles do not Racks are arranged sothat equipment draws in air from a cold aisle and exhausts it to a hot aisle the exhaust sides of two adjacent racks are therefore back to back Exhibit A illustratesthis basic conceptExhibit A Hot and cold aisles raised floorHotaisle ColdaisleColdaisleHotaisleThis arrangement optimizes the flow of cooling so that air inlets always breathe coolair rather than another servers hot exhaust Properly implemented the alternating row strategy results in aisles that are noticeably cold and hot You can measureyour cooling success with an infrared thermometer such as the Fluke which isan indispensable tool of the modern system administrator This pointandshoot device instantly measures the temperature of anything you aim it at up to sixfeet away Dont take it out to the barsIf you must run cabling under the floor run power under cold aisles and networkcabling under hot aislesFacilities without a raised floor can use inrow cooling units such as those manufactured by APC apccom These units are skinny and sit between racks ExhibitB on the next page shows how this system worksExhibit B Hot and cold aisles with inrow cooling birdseye viewHot aisle containmentCOOLERRack Rack RackCOOLERRack Rack Rack COOLERCOOLERRack Rack Rack RackCOOLERRack RackBoth CRAC and inrow cooling units need a way to dissipate heat outside the datacenter This requirement is typically satisfied with a loop of liquid refrigerant suchas chilled water PuronRA or R that carries the heat outdoors We omittedthe refrigerant loops from Exhibit A and Exhibit B for simplicity but most installations will require themHumidityAccording to the ASHRAE guidelines data center humidity should be keptbetween and If the humidity is too low static electricity becomes a problem Recent testing has shown that there is little operational difference between and the previous standard of so the minimum humidity standard wasadjusted accordinglyIf humidity is too high condensation can form on circuit boards and cause shortcircuits and oxidationDepending on your geographic location you might need either humidification ordehumidification equipment to maintain a proper level of humidityEnvironmental monitoringIf you are supporting a missioncritical computing environment its a good idea tomonitor the temperature and other environmental factors such as noise and power in the data center even when you are not there It can be disappointing to arriveon Monday morning and find a pool of melted plastic on your data center floorFortunately automated data center monitors can watch the goods while you areaway We use and recommend the Sensaphone product family These inexpensiveboxes monitor environmental variables such as temperature noise and power andthey phone or text you when they detect a problem Data center reliability tiersThe Uptime Institute is a commercial entity that certifies data centers They havedeveloped a fourtier system for classifying the reliability of data centers whichwe summarize in Table In this table N means that you have just enough ofsomething eg UPSs or generators to meet normal needs N means that youhave one spare N means that each device has its own spareTable Uptime Institute availability classification systemTier Generators UPSs Power feeds HVAC Availability None N Single N N N a Single N N N a Dual switchable N N N Dual simultaneous N a With redundant componentsCenters in the highest tier must be compartmentalized which means that groupsof systems are powered and cooled in such a way that the failure of one group hasno effect on other groupsEven availability may look pretty good at first glance but it works out tonearly hours of downtime per year availability corresponds to minutes of downtime per yearOf course no amount of redundant power or cooling will keep an applicationavailable if its administered poorly or is improperly architected The data center isa foundational building block necessary but not sufficient to ensure overall availability from the end users perspectiveYou can learn more about the Uptime Institutes certification standards which include certification of design construction and operational phases from their website uptimeinstituteorg In some cases organizations use the concept of these tierswithout paying the Uptime Institutes hefty certification fees The important part isnot the framed plaque but the use of a common vocabulary and assessment methodology to compare data centers Data center securityPerhaps it goes without saying but the physical security of a data center is at leastas important as its environmental attributes Make sure that threats of both naturaleg fire flood earthquake and human eg competitors and criminals originhave been carefully considered A layered approach to security is the best way toensure that a single mistake or lapse will not lead to a catastrophic outcomeLocationWhenever possible data centers should not be located in areas that are prone toforest fires tornadoes hurricanes earthquakes or floods For similar reasons itsadvisable to avoid manmade hazard zones such as airports freeways refineriesand tank farmsBecause the data center you select or build will likely be your home for a longtime its worthwhile to invest some time in researching the available risk data whenmaking a site selection The US Geological Survey usgsgov publishes statisticssuch as earthquake probability and the Uptime Institute produces a composite mapof data center location risksPerimeterTo reduce the risk of a targeted attack a data center should be surrounded by afence that is at least feet from the building on all sides Access to the inside ofthe fence perimeter should be controlled by a security guard or a multifactor badgeaccess system Vehicles allowed within the fence perimeter should not be permittedwithin feet of the buildingContinuous video monitoring must cover of the external perimeter including all gates access driveways parking lots and roofsBuildings should be unmarked No signage should indicate what company thebuilding belongs to or mention that it houses a data centerFacility accessAccess to the data center itself should be controlled by a security guard and a multifactor badge system possibly one that incorporates biometric factors Ideallyauthorized parties should be enrolled in the physical accesscontrol system beforetheir first visit to the data center If this is not possible onsite security guardsshould follow a vetting process that includes confirming each individuals identityand authorized actionsOne of the trickiest situations in training security guards is properly handling theappearance of vendors who claim they have come to fix some part of the infrastructure such as the air conditioning Make no mistake unless the guard canconfirm that someone authorized or requested this vendor visit such visitors mustbe turned awayRack accessLarge data centers are often shared with other parties This is a costeffective approach but it comes with the added responsibility of securing each rack or cageof racks This is another case in which a multifactor access control system suchas a card reader plus a fingerprint reader is needed to ensure that only authorizedparties have access to your equipment Each rack should also be individually monitored by video ToolsA welloutfitted sysadmin is an effective sysadmin Having a dedicated tool box isan important key to minimizing downtime in an emergency Table lists someitems to keep in your tool box or at least within easy reachTable A system administrators tool boxGeneral toolsHex Allen wrench kit Ballpeen hammer ozScissors Electricians knife or Swiss Army knifeSmall LED flashlight Phillipshead screwdrivers and Socket wrench kit Pliers both flatneedlenose and regularStud finder Ridgid SeeSnake micro inspection cameraTape measure Slothead screwdrivers and Torx wrench kit Teensy tiny jewelers screwdriversTweezersComputerrelated specialty itemsPC screw kit Cable ties and their Velcro cousinsInfrared thermometer Digital multimeter DMMRJ end crimper Portable network analyzerlaptopSCSI terminators Spare Category and A RJ crossover cablesSpare power cord Spare RJ connectors solid core and strandedStatic grounding strap Wire stripper with an integrated wire cutterMiscellaneousQTips Telescoping magnetic pickup wandCellular telephone Firstaid kit including ibuprofen and acetaminophenElectrical tape Home phone and pager s of oncall support staffCan of compressed air List of emergency maintenance contacts aDentists mirror Sixpack of good microbrew beer suggested minimuma And maintenance contract numbers if applicable Recommended readingASHRAE Inc ASHRAE Thermal Guidelines for Data Processing Environments rdedition Atlanta GA ASHRAE Inc Telecommunications Infrastructure Standard for Data Centers ANSITIAEIA A variety of useful information and standards related to energy efficiency can befound at the Center of Expertise for Energy Efficiency in Data Centers web site atdatacenterslblgovDuring the past four decades the role of information technology in business anddaily life has changed dramatically Its hard to imagine a world without the instantgratification of Internet searchFor most of this period the predominant philosophy of IT management was toincrease stability by minimizing change In many cases hundreds or thousands ofusers depended on a single system If a failure occurred hardware often had to beexpress shipped for repair or hours of downtime were needed to reinstall softwareand restore state IT teams lived in fear that something would break and that theywouldnt be able to fix itChange minimization has undesirable side effects IT departments often becamestuck in the past and failed to keep pace with business needs Technical debt accumulated in the form of systems and applications in desperate need of upgrade orreplacement that everyone was afraid to touch for fear of breaking something ITstaff became the butt of jokes and the least popular folks everywhere from boardrooms to holiday partiesThankfully those times are behind us The advents of cloud infrastructure virtualization automation tools and broadband communication have greatly reducedthe need for oneoff systems Such servers have been replaced by armies of clones Methodology Policyand Politicsthat are managed as battalions In turn these technical factors have enabled theevolution of a service philosophy known as DevOps which lets IT organizationsdrive and encourage change rather than resisting it The DevOps name is a portmanteau of development and operations the two traditional disciplines it combinesAn IT organization is more than a group of technical folks who set up WiFi hotspots and computers From a strategic perspective IT is a collection of people androles that use technology to accelerate and support the organizations mission Neverforget the golden rule of system administration enterprise needs drive IT activitiesnot the other way aroundIn this chapter we discuss the nontechnical aspects of running a successful ITorganization that uses DevOps as its overarching schema Most of the topics andideas presented in this chapter are not specific to any particular environment Theyapply equally to a parttime system administrator or to a large group of fulltimeprofessionals Like green vegetables theyre good for you no matter what size mealyoure preparing The grand unified theory DevOpsSystem administration and other operational roles within IT have traditionally beenseparate from domains such as application development and project managementThe theory was that app developers were specialists who would push products forwardwith new features and enhancements Meanwhile the stolid and changeresistantoperations team would provide management of the production environmentThis arrangement usually creates tremendous internal conflict and ultimately failsto meet the needs of the business and its clientsExhibit A Courtesy of Dave RothThe DevOps approach mingles developers programmers application analystsapplication owners project managers with IT operations staff system and networkadministrators security monitors data center staff database administrators in atightly integrated way This philosophy is rooted in the belief that working togetheras a collaborative team breaks down barriers reduces finger pointing and producesbetter results Exhibit B summarizes a few of the main conceptsExhibit B What is DevOpsDevOps IS NOT DevOps ISA philosophyA personAn environment A job title A teamA toolDevOps is a relatively new development in IT management The early s broughtchange to the development side of the house which moved from waterfall release cycles to agile approaches that featured iterative development This systemincreased the speed at which products features and fixes could be created but deployment of those enhancements often stalled because the operations side wasntprepared to move as quickly as the development side Hitching up the developmentand operations groups allowed everyone to accelerate down the road at the samepace and DevOps was bornDevOps is CLAMSThe tenets of DevOps philosophy are most easily described with the acronymCLAMS Culture Lean Automation Measurement and SharingCulturePeople are the ultimate drivers of any successful team so the cultural aspects ofDevOps are the most important Although DevOps has its own canon of culturaltips and tricks the main goal is to get everyone working together and focused onthe overall pictureUnder DevOps all disciplines work together to support a common business driverproduct objective community etc through all phases of its life cycle Achievingthis goal may ultimately require changes in reporting structure no more isolatedapplication development groups seating layout and even job responsibilities Thesedays good system administrators occasionally write code often automation or deployment scripts and good application developers regularly examine and manageinfrastructure performance metricsHere are some typical features of a DevOps culture Both developers Dev and operations Ops have simultaneouseveryone gets paged oncall responsibility for the complete environment This rule has the wonderful side effect that root causes can be addressed wherever they occur No application or service can launch without automated testing and monitoring being in place at both the system and application level This ruleseals in functionality and creates a contract between Dev and Ops Likewise Dev and Ops must sign off on any launch before it happens All production environments are mirrored by identical development environments This rule creates a runway for testing and reduces accidentsin production Dev teams do regular code reviews to which Ops is invited Code architecture and functionality are no longer just Dev functions Likewise Opsperforms regular infrastructure reviews in which Dev is involved Dev mustbe aware ofand contribute todecisions about underlying infrastructure Dev and Ops have regular joint standup meetings In general meetingsshould be minimized but joint standups serve as a useful stopgap tofoster communication Dev and Ops should all sit in a common chat room dedicated to discussionof both strategic architecture direction sizing and operational issuesThis communication channel is often known as ChatOps and severalamazing platforms are available to support it Check out HipChat SlackMatterMost and Zulip to name a fewA successful DevOps culture pushes Dev and Ops so close that their scopes interpenetrate and everyone learns to be comfortable with that The optimal level ofoverlap is probably higher than most people would naturally prefer in the absenceof cultural indoctrination Team members must learn to respond gracefully to queries and feedback about their work from colleagues who may be formally trainedin other disciplinesLeanThe easiest way to explain the lean aspect of DevOps is to note that if you schedulea recurring weekly meeting at your organization to discuss your DevOps implementation plan you have instantly failed The first six weeks or so of a shared oncall model is painful Then suddenly it turns around Trust usDevOps is about realtime interaction and communication among people processesand systems Use realtime tools like ChatOps to communicate wherever possibleand focus on solving component problems one at a time Always ask what canwe do today to make progress on an issue Avoid the temptation to boil the oceanAutomationAutomation is the most universally recognized aspect of DevOps The two goldenrules of automation are these If you need to perform a task more than twice it should be automated Dont automate what you dont understandAutomation brings many advantages It prevents staff from being trapped performing mundane tasks Staff brainpower and creativity can be used to solve new and more difficult challenges It reduces the risk of human error It captures infrastructure in the form of code allowing versions and outcomes to be tracked It facilitates evolution while also reducing risk If a change fails automatedrollback is well should be easy It facilitates the use of virtualized or cloud resources to achieve scale andredundancy Need more Spin some up Need less Kill them offTools are instrumental in the quest for automation Systems such as Ansible SaltPuppet and Chef covered in Chapter Configuration Management are frontand center Continuous integration tools such as Jenkins and Bamboo see page help manage repeatable or triggered tasks Packaging and launch utilities suchas Packer and Terraform automate lowlevel infrastructure tasksDepending on your environment you might need one some or all of thesetools New tools and enhancements are being developed rapidly so focus on findingthe tool that is a good fit for the particular function or process you are automatingas opposed to picking a tool and then looking for the questions it answers Mostimportantly reevaluate your tool set every year or twoYour automation strategy should include at least the following elements Automated setup of new machines This is not just OS installation Italso includes all the additional software and local configuration necessaryto allow a machine to enter production Its inevitable that your site willneed to support more than one type of configuration so include multiplemachine types in your plans from the beginning Automated configuration management Configuration changes shouldbe entered in the configuration base and applied automatically to all machines of the same type This rule helps keep the environment consistent Automated promotion of code Propagation of new functionality fromthe development environment to the test environment and from the testenvironment into production should be automated Testing itself shouldbe automated with clear criteria for evaluation and promotion Systematic patching and updating of existing machines When you identify a problem with your setup you need a standardized and easy way todeploy updates to all affected machines Because servers are not turned onall the time even if they are supposed to be your update scheme mustcorrectly handle machines that are not online when an update is initiatedYou can check for updates at boot time or update on a regular schedulesee Periodic processes starting on page for more informationMeasurementThe ability to scale virtualized or cloud infrastructure see Chapter Cloud Computing has pushed the world of instrumentation and measurement to new heightsTodays gold standard is the collection of subsecond measurements throughoutthe entire service stack business application database subsystems servers network and so on Several DevOpsy tools such as Graphite Grafana ELK theElasticsearch Logstash Kibana stack plus monitoring platforms like Icingaand Zenoss support these effortsHaving measurement data and doing something useful with it are two differentthings however A mature DevOps shop ensures that metrics from the environment are visible and evangelized to all interested parties both inside and outside ofIT DevOps sets nominal targets for each metric and chases down any anomaliesto determine their causeSharingCollaborative work and shared development of capabilities lie at the heart of asuccessful DevOps effort Staff should be encouraged and incentivized to sharetheir work both internally lunchandlearn presentations team showandtellwiki howto articles and externally Meetups white papers conferences Theseefforts extend the silobusting philosophy beyond the local workgroup and helpeveryone learn and growSystem administration in a DevOps worldSystem administrators have always been the jacksandjillsofalltrades of the ITworld and that remains true under the broader DevOps umbrella The system administrator role oversees systems and infrastructure typically including primaryresponsibility for these areas Building configuring automating and deploying system infrastructure Ensuring that the operating system and major subsystems are securepatched and up to dateSee Chapter formore informationabout monitoring Deploying supporting and evangelizing DevOps technologies for continuous integration continuous deployment monitoring measurementcontainerization virtualization and ChatOps platforms Coaching other team members on infrastructure and security best practices Monitoring and maintaining infrastructure physical virtual or cloud toensure that it meets performance and availability requirements Responding to user resource or enhancement requests Fixing problems with systems and infrastructure as they occur Planning for the future expansion of systems infrastructure and capacity Advocating cooperative interactions among team members Managing various outside vendors cloud colocation disaster recoverydata retention connectivity physical plant hardware service Managing the life cycle of infrastructure components Maintaining an emergency stash of ibuprofen tequila andor chocolateto be shared with other team members on those notasfresh daysThis is just a subset of the breadth covered by a successful system administratorThe role is part drill sergeant part mother hen part EMT and part glue that keepseverything running smoothlyAbove all remember that DevOps is founded on overcoming ones normal territorial impulses If you find yourself at war with other team members take a stepback and remember that you are most effective if you are seen as a hero who helpsmake everyone else successful Ticketing and task management systemsA ticketing and task management system lies at the heart of every functioning ITgroup As with all things DevOps having one ticketing system that spans all ITdisciplines is critical In particular enhancement requests issue management andsoftware bug tracking should all be part of the same systemA good ticketing system helps staff avoid two of the most common workflow pitfalls Tasks that fall through the cracks because everyone thinks they are beingtaken care of by someone else Resources that are wasted through duplication of effort when multiplepeople or groups work on the same problem without coordinationCommon functions of ticketing systemsA ticket system accepts requests through various interfaces email and web beingthe most common and tracks them from submission to solution Managers canassign tickets to groups or to individual staff members Staff can query the systemto see the queue of pending tickets and perhaps resolve some of them Users canfind out the status of requests and see who is working on them Managers can extract highlevel information such as The number of open tickets The average time to close a ticket The productivity of staff members The percentage of unresolved rotting tickets Workload distribution by time to solutionThe request history stored in the ticket system becomes a history of the problemswith your IT infrastructure and also the solutions to those problems If that history is easily searchable it becomes an invaluable resource for the sysadmin staffResolved tickets can be provided to novice staff members and trainees insertedinto a FAQ system or made searchable for later discovery New staff members canbenefit from receiving copies of closed tickets because those tickets include not onlytechnical information but also examples of the tone and communication style thatare appropriate for use with customersLike all documents your ticketing systems historical data can potentially be usedagainst your organization in court Follow the document retention guidelines setup by your legal departmentMost request tracking systems automatically confirm new requests and assign thema tracking number that submitters can use to follow up or inquire about the requests status The automated response message should clearly state that it is just aconfirmation It should be followed promptly by a message from a real person thatexplains the plan for dealing with the problem or requestTicket ownershipWork can be shared but in our experience responsibility is less amenable to distribution Every task should have a single welldefined owner That person need notbe a supervisor or manager just someone willing to act as a coordinatorsomeonewilling to say I take responsibility for making sure this task gets doneAn important side effect of this approach is that it is implicitly clear who implemented what or who made what changes This transparency becomes important ifyou want to figure out why something was done in a certain way or why somethingis suddenly working differently or not working anymoreBeing responsible for a task should not equate to being a scapegoat if problemsarise If your organization defines responsibility as blameworthiness you may findthat the number of available project owners quickly dwindles Your goal in assigning ownership is simply to remove ambiguity about who should be addressing eachproblem Dont punish staff members for requesting helpFrom a customers point of view a good assignment system is one that routes problems to a person who is knowledgeable and can solve them quickly and completelyBut from a managerial perspective assignments must occasionally be challenging tothe assignee so that staff members continue to grow and learn in the course of doingtheir jobs Your task is to balance reliance on staff members strengths against theneed to challenge them all while keeping both customers and staff members happyLarger tasks can be anything up to and including fullblown software engineeringprojects These tasks may require the use of formal project management and software engineering tools We dont describe those tools here nevertheless theyreimportant and should not be overlookedSometimes sysadmins know that a particular task needs to be done but they dontdo it because the task is unpleasant A sysadmin who points out a neglected unassigned or unpopular task is likely to receive that task as an assignment This situation creates a conflict of interest because it motivates sysadmins to remain silentregarding such situations Dont let that happen at your site give your sysadminsan avenue for pointing out problems You can allow them to open up tickets without assigning an owner or associating themselves to the issue or you can create anemail alias to which issues can be sentUser acceptance of ticketing systemsReceiving a prompt response from a real person is a critical determinant of customer satisfaction even if the personal response contains no more informationthan the automated response For most problems it is far more important to letthe submitter know that the ticket has been reviewed by a real person than it is tofix the problem immediately Users understand that administrators receive manyrequests and theyre willing to wait a fair and reasonable time for your attentionBut theyre not willing to be ignoredThe mechanism through which users submit tickets affects their perception ofthe system Make sure you understand your organizations culture and your userspreferences Do they want a web interface A custom application An email aliasMaybe theyre only willing to make phone callsIts also important that administrators take the time to make sure they understandwhat users are actually requesting This point sounds obvious but think back to thelast five times you emailed a customer service or tech support alias Wed bet therewere at least a couple of cases in which the response seemed to have nothing to dowith the questionnot because those companies were especially incompetent butbecause accurately parsing tickets is harder than it looksOnce youve read enough of a ticket to develop an impression of what the customer is asking about the rest of the ticket starts to look like blah blah blah Fightthis Clients hate waiting for a ticket to find its way to a human only to learn thatthe request has been misinterpreted and must be resubmitted or restated Back tosquare oneTickets are often vague or inaccurate because the submitter does not have the technical background needed to describe the problem in the way that a sysadmin wouldThat doesnt stop users from making their own guesses as to whats wrong howeverSometimes these guesses are perfectly correct Other times you must first decodethe ticket to determine what the user thinks the problem is then trace back alongthe users train of thought to intuit the underlying problemSample ticketing systemsThe following tables summarize the characteristics of several wellknown ticketingsystems Table shows open source systems and Table shows commercialsystemsTable Open source ticket systemsName Input a Lang Backendb Web siteDouble Choco Latte W PHP MP githubcomgnuedcldclMantis WE PHP M mantisbtorgOTRS WE Perl DMOP otrsorgRT Request Tracker WE Perl M bestpracticalcomOSTicket WE PHP M osticketcomBugzilla WE Perl MOP bugzillaorga Input types W web E emailb Back end D DB M MySQL O Oracle P PostgreSQLTable shows some of the commercial alternatives for request managementSince the web sites for commercial offerings are mostly marketing hype detailssuch as the implementation language and back end are not listedSome of the commercial offerings are so complex that they need a person or twodedicated to maintaining configuring and keeping them running Others such asJira and ServiceNow are available as a software as a service productTicket dispatchingIn a large group even one with an awesome ticketing system one problem stillremains to be solved it is inefficient for several people to divide their attention between the task they are working on right now and the request queue especially ifrequests come in by email to a personal mailbox We have experimented with twosolutions to this problemOur first try was to assign halfday shifts of trouble queue duty to staff membersin our sysadmin group The person on duty would try to answer as many of the incoming queries as possible during a shift The problem with this approach was thatnot everybody had the skills to answer all questions and fix all problems Answerswere sometimes inappropriate because the person on duty was new and was notfamiliar with the customers their environments or the specific support contractsthey were covered by The result was that the more senior people had to keep aneye on things and so were not able to concentrate on their own work In the endthe quality of service was worse and nothing was really gainedAfter this experience we created a dispatcher role that rotates monthly among agroup of senior administrators The dispatcher checks the ticketing system for newentries and farms out tasks to specific staff members If necessary the dispatchercontacts users to extract any additional information that is necessary for prioritizing requests The dispatcher uses a homegrown database of staff skills to decidewho on the support team has the appropriate skills and time to address a giventicket The dispatcher also makes sure that requests are resolved in a timely manner Local documentation maintenanceJust as most people accept the health benefits of exercise and leafy green vegetableseveryone appreciates good documentation and has a vague idea that its importantUnfortunately that doesnt necessarily mean that theyll write or update documentation without prodding Why should we care really Documentation reduces the likelihood of a single point of failure Its wonderful to have tools that deploy workstations in no time and distributepatches with a single command but these tools are nearly worthless if nodocumentation exists and the expert is on vacation or has quit Documentation aids reproducibility When practices and procedures arenot stored in institutional memory they are unlikely to be followed consistently When administrators cant find information about how to dosomething they have to wing itTable Commercial ticket systemsName Scale Web siteEMC Ionix Infra Huge infracorpcomsolutionsHEAT Medium ticomixcomJira Any atlassiancomRemedy now BMC Huge remedycomServiceDesk Huge cacomusservicedeskaspxServiceNow Any servicenowcomTrackIt Medium trackitcom Documentation saves time It doesnt feel like youre saving time as youwrite it but after spending a few days resolving a problem that has beentackled before but whose solution has been forgotten most administrators are convinced that the time is well spent Finally and most importantly documentation enhances the intelligibilityof the system and allows subsequent modifications to be made in a manner thats consistent with the way the system is supposed to work Whenmodifications are made on the basis of only partial understanding theyoften dont quite conform to the architecture Entropy increases over timeand even the administrators that work on the system come to see it as adisorderly collection of hacks The end result is often the desire to scrapeverything and start again from scratchLocal documentation should be kept in a welldefined spot such as an internal wikior a thirdparty service such as Google Drive Once you have convinced your administrators to document configurations and administration practices its important to protect this documentation as well A malicious user can do a lot of damageby tampering with your organizations documentation Make sure that people whoneed the documentation can find it and read it make it searchable and that everyone who maintains the documentation can change it But balance accessibilitywith the need for protectionInfrastructure as codeAnother important form of documentation is known as infrastructure as code Itcan take a variety of forms but is most commonly seen in the form of configurationdefinitions such as Puppet modules or Ansible playbooks that can then be storedand tracked in a version control system such as Git The system and its changes arewell documented in the configuration files and the environment can be built andcompared against the standard on a regular basis This approach ensures that thedocumentation and the environment always match and are up to date solving themost common problem of traditional documentation See Chapter Configuration Management for more informationDocumentation standardsIf you must document elements manually our experience suggests that the easiestand most effective way to maintain documentation is to standardize on short lightweight documents Instead of writing a system management handbook for yourorganization write many onepage documents each of which covers a single topicStart with the big picture and then break it down into pieces that contain additionalinformation If you have to go into more detail somewhere write an additional onepage document that focuses on steps that are particularly difficult or complicatedThis approach has several advantages Higher management is probably only interested in the general setup ofyour environment That is all thats needed to answer questions from aboveor to conduct a managerial discussion Dont pour on too many details oryou will just tempt your boss to interfere in them The same holds true for customers A new employee or someone taking on new duties within your organization needs an overview of the infrastructure to become productive Itsnot helpful to bury such people in information Its more efficient to use the right document than to browse through alarge document You can index pages to make them easy to find The less time administrators have to spend looking for information the better Its easier to keep documentation current when you can do that by updating a single pageThis last point is particularly important Keeping documentation up to date is a hugechallenge documentation is often is the first thing to be dropped when time is shortWe have found that a couple of specific approaches keep the documentation flowingFirst set the expectation that documentation be concise relevant and unpolishedCut to the chase the important thing is to get the information down Nothingmakes the documentation sphincter snap shut faster than the prospect of writinga dissertation on design theory Ask for too much documentation and you mightnot get any Consider developing a simple form or template for your sysadmins touse A standard structure helps avoid blankpage anxiety and guides sysadmins torecord pertinent information rather than fluffSecond integrate documentation into processes Comments in configuration filesare some of the best documentation of all Theyre always right where you need themand maintaining them takes virtually no time at all Most standard configurationfiles allow comments and even those that arent particularly comment friendly canoften have some extra information sneaked into themLocally built tools can require documentation as part of their standard configurationinformation For example a tool that sets up a new computer can require information about the computers owner location support status and billing informationeven if these facts arent directly relevant to the machines software configurationDocumentation should not create information redundancies For example if youmaintain a sitewide master list of systems there should be no other place wherethis information is updated by hand Not only is it a waste of your time to make updates in multiple locations but inconsistencies are also certain to creep in over timeWhen this information is required in other contexts and configuration files writeSee page formore informationabout crona script that obtains it from or updates the master configuration If you cannotcompletely eliminate redundancies at least be clear about which source is authoritative And write tools to catch inconsistencies perhaps run regularly from cronThe advent of tools such as wikis blogs and other simple knowledge managementsystems has made it much easier to keep track of IT documentation Set up a single location where all your documents can be found and updated Dont forget tokeep it organized however One wiki page with child pages all in one list iscumbersome and difficult to use Be sure to include a search function to get themost out of your system Environment separationOrganizations that write and deploy their own software need separate developmenttest and production environments so that releases can be staged into general usethrough a structured process Separate that is but identical make sure that whendevelopment systems are updated the changes propagate to the test and productionenvironments as well Of course the configuration updates themselves should besubject to the same kind of structured release control as the code Configurationchanges include everything from OS patches to application updates and administrative changesHistorically it has been standard practice to protect the production environmentby enforcing role separation throughout the promotion process For example thedevelopers who have administrative privileges in the development environmentare not the same people who have administrative and promotion privileges in other environments The fear was that a disgruntled developer with code promotionpermissions could conceivably insert malicious code at the development stage andthen promote it through to production By distributing approval and promotionduties to other people multiple people would need to collude or make mistakesbefore problems could find their way into production systemsUnfortunately the anticipated benefits of such draconian measures are rarely realized Code promoters often dont have the skills or time to review code changesat a level that would actually catch intentional mischief Instead of helping thesystem creates a false sense of protection introduces unnecessary roadblocks andwastes resourcesIn the DevOps era this problem is solved in a different way Rather than separateroles the preferred approach is to track all changes as code in a repository suchas Git that has an immutable audit trail Any undesirable change is traceable backto the human that introduced it so strict role separation is unnecessary Becauseconfiguration changes are applied in an automated way across each environmentidentical changes can be evaluated in lower environments such as dev or test before they are promoted to production to ensure that no unintended consequences In many cases this statement applies as well to sites that run complex offtheshelf software such asERP or financial systemsmanifest themselves If problems are discovered reversion is as easy as identifyingthe problematic commit and temporarily bypassing itIn a perfect world neither developers nor ops staff would have administrative privileges in the production environment Instead all changes would be made throughan automated tracked process that has appropriate privileges of its own Althoughthis is a worthy aspirational goal our experience has been that it is not yet realisticfor most organizations Work toward this utopian fantasy but dont get trapped by it Disaster managementYour organization depends on a working IT environment Not only are you responsible for daytoday operations but you should also have plans in place to deal withany reasonably foreseeable eventuality Preparation for such largescale problemsinfluences both your overall game plan and the way that you define daily operationsIn this section we look at various kinds of disasters the data you need to recovergracefully and the important elements of recovery plansRisk assessmentBefore designing a disaster recovery plan its a good idea to pull together a riskassessment to help you understand what assets you have what risks they face andwhat mitigation steps you already have in place The NIST special publication details an extensive risk assessment process You can download it from nistgovPart of the risk assessment process is to make an explicit written catalog of the potential disasters you want to protect against Disasters are not all the same and youmay need several different plans to cover the full range of possibilities For examplesome common threat categories are Malicious users both external and internal Floods Fires Earthquakes Hurricanes and tornadoes Electrical storms and power spikes Power failures both short and longterm Extreme heat or failure of cooling equipment ISPTelecomCloud outage Device hardware failures dead servers fried hard disks Terrorism Zombie apocalypse Network device failures routers switches cables Accidental user errors deleted or damaged files and databases lost configuration information lost passwords etc Historically about half of security breaches originate with insiders Internal misbehavior continues tobe the disaster of highest likelihood at most sitesFor each potential threat consider and write down all the possible implicationsof that eventOnce you understand the threats prioritize the services within your IT environmentBuild a table that lists your IT services and assigns a priority to each For examplea software as a service company might rate its external web site as a toppriorityservice while an office with a simple informational external web site might notworry about the sites fate during a disasterRecovery planningMore and more organizations are designing their critical systems to automaticallyfail over to secondary servers in the case of problems This is a great idea if you havelittle or no tolerance for services being down However dont fall prey to the beliefthat because you are mirroring your data you do not need offline backups Evenif your data centers are miles apart it is certainly possible that you could lose bothof them Make sure you include data backups in your disaster planningCloud computing is often an essential element of disaster planning Through services such as Amazons EC you can get a remote site set up and functioning withinminutes without having to pay for dedicated hardware You pay only for what youuse when you use itA disaster recovery plan should include the following sections derived from theNIST disaster recovery standard Introduction purpose and scope of the document Concept of operations system description recovery objectives information classification line of succession responsibilities Notification and activation notification procedures damage assessmentprocedures plan activation Recovery the sequence of events and procedures required to recoverlost systems Return to normal operation concurrent processing reconstituted systemtesting return to normal operation plan deactivationWe are accustomed to communicating and accessing documents through the network However these facilities may be unavailable or compromised after an incident Store all relevant contacts and procedures offline Know where to get recentbackups and how to make use of them without reference to online dataIn all disaster scenarios you will need access to both online and offline copies ofessential information The online copies should if possible be kept in a selfsufficient environment one that has a rich complement of tools has key sysadmins Malicious hackers and ransomware can easily destroy an organization that does not maintainreadonly offline backupsRead more aboutcloud computingin Chapter environments runs its own name server has a complete local etchosts file hasno filesharing dependencies and so onHeres a list of handy data to keep in the disaster support environment An outline of the recovery procedure who to call what to say Service contract phone numbers and customer numbers Key local phone numbers police fire staff boss Cloud vendor login information Inventory of backup media and the backup schedule that produced them Network maps Software serial numbers licensing data and passwords Copies of software installation media can be kept as ISO files Copies of your systems service manuals Vendor contact information Administrative passwords Data on hardware software and cloud environment configurations OSversions patch levels partition tables and the like Startup instructions for systems that need to be brought back online ina particular orderStaffing for a disasterYour disaster recovery plan should document who will be in charge in the eventof a catastrophic incident Set up a chain of command and keep the names andphone numbers of the principals offline We keep a little laminated card with important names and phone numbers printed in microscopic type Handyand itfits in your walletThe best person to put in charge may be a sysadmin from the trenches not the ITdirector who is usually a poor choice for this roleThe person in charge must be someone who has the authority and decisiveness tomake tough decisions in the context of minimal information eg a decision todisconnect an entire department from the network The ability to make such decisions communicate them in a sensible way and lead the staff through the crisisare probably more important than having theoretical insight into system and network managementAn important but sometimes unspoken assumption made in most disaster plans isthat sysadmin staff will be available to deal with the situation Unfortunately peopleget sick go on vacation leave for other jobs and in stressful times may even turnhostile Consider what youd do if you needed extra emergency help Not havingenough sysadmins around can sometimes constitute an emergency in its own rightif your systems are fragile or your users unsophisticatedYou might try forming a sort of NATO pact with a local consulting company thathas sharable system administration talent Of course you must be willing to shareback when your buddies have a problem Most importantly dont operate close tothe wire in your daily routine Hire enough system administrators and dont expectthem to work hour daysSecurity incidentsSystem security is covered in detail in Chapter However its worth mentioninghere as well because security considerations impact the vast majority of administrative tasks No aspect of your sites management strategy can be designed withoutdue regard for securityFor the most part Chapter concentrates on ways of preventing security incidents from occurring However thinking about how you might recover from asecurityrelated incident is an equally important part of security planningHaving your web site hijacked is a particularly embarrassing type of breakin Forthe sysadmin at a webhosting company a hijacking can be a calamitous event especially when it involves sites that handle credit card data Phone calls stream infrom customers from the media from the company VIPs who just saw the newsof the hijacking on CNN Who will take the calls What should that person sayWho is in charge What role does each person play If you are in a highvisibilitybusiness its definitely worth thinking through this type of scenario coming upwith some preplanned answers and perhaps even having a practice session to workout the detailsSites that accept credit card data have legal requirements to deal with after a hijacking Make sure your organizations legal department is involved in security incidentplanning and make sure you have relevant contact names and phone numbers tocall in a time of crisisWhen CNN or Reddit announces that your web site is down the same effect thatmakes highway traffic slow down to look at an accident on the side of the road causesyour Internet traffic to increase enormously often to the point of breaking whateverit was that you just fixed If your web site cannot handle an increase in traffic of or more consider having your loadbalancing device route excess connections to aserver that presents a page that simply says Sorry we are too busy to handle yourrequest right now Of course forwardthinking capacity planning that includesautoscaling into the cloud see Chapter might avoid this situation altogetherDevelop a complete incident handling guide to take the guesswork out of managingsecurity problems See page for more details on security incident management IT policies and proceduresComprehensive IT policies and procedures serve as the groundwork for a modernIT organization Policies set standards for users and administrators and foster consistency for everyone involved More and more policies require acknowledgementin the form of a signature or other proof that the user has agreed to abide by theircontents Although this may seem excessive to some it is actually a great way toprotect administrators in the long runThe ISOIEC standard is a good basis for constructing your policy setIt interleaves general IT policies with other important elements such as IT securityand the role of the Human Resources department In the next few sections we discuss the ISOIEC framework and highlight some of its most importantand useful elementsThe difference between policies and proceduresPolicies and procedures are two distinct things but they are often confused and thewords are sometimes even used interchangeably This sloppiness creates confusionhowever To be safe think of them this way Policies are documents that define requirements or rules The requirementsare usually specified at a relatively high level An example of a policy mightbe that incremental backups must be performed daily with total backupsbeing completed each week Procedures are documents that describe how a requirement or rule willbe met So the procedure associated with the policy above might saysomething like Incremental backups are performed with Backup Execsoftware which is installed on the server backupsThis distinction is important because your policies should not change often Youshould review them annually and maybe change one or two pieces Procedureson the other hand evolve continuously as you change your architecture systemsand configurationsSome policy decisions are dictated by the software you are running or by the policies of external groups such as ISPs Some policies are mandatory if the privacyof your users data is to be protected We call these topics nonnegotiable policyIn particular we believe that IP addresses hostnames UIDs GIDs and usernamesshould all be managed sitewide Some sites multinational corporations for example are clearly too large to implement this policy but if you can swing it sitewidemanagement makes things a lot simpler We know of a company that enforces sitewide management for users and machines so the threshold at whichan organization becomes too big for sitewide management must be pretty highOther important issues have a larger scope than just your local IT group Handling of security breakins Filesystem export controls Password selection criteria Removal of logins for cause Copyrighted material eg MPs and movies Software piracyPolicy best practicesSeveral policy frameworks are available and they cover roughly the same territories The following topics are examples of those that are typically included in anIT policy set Information security policy External party connectivity agreements Asset management policy Data classification system Human Resources security policy Physical security policy Access control policies Security standards for development maintenance and new systems Incident management policy Business continuity management disaster recovery Data retention standards Protection of user privacy Regulatory compliance policyProceduresProcedures in the form of checklists or recipes can codify existing practice Theyare useful both for new sysadmins and for old hands Better yet are proceduresthat include executable scripts or are captured in a configuration managementtool such as Ansible Salt Puppet or Chef Over the long term most proceduresshould be automatedSeveral benefits accrue from standard procedures Tasks are always done in the same way Checklists reduce the likelihood of errors or forgotten steps Its faster for the sysadmin to work from a recipe Changes are selfdocumenting Written procedures provide a measurable standard of correctnessHere are some common tasks for which you might want to set up procedures Adding a host Adding a user Localizing a machine Setting up backups and snapshots for a new machine Securing a new machine Removing an old machine Restarting a complicated piece of software Reviving a web site that is not responding or not serving data Upgrading the operating system Patching software Installing a software package Upgrading critical software Backing up and restoring files Expiring old backups Performing emergency shutdownsMany issues sit squarely between policy and procedure For example Who can have an account on your network What happens when they leaveThe resolutions of such issues need to be written down so that you can stay consistent and avoid falling prey to the wellknown fouryearolds ploy of Mommysaid no lets go ask Daddy Service level agreementsFor the IT organization to keep users happy and meet the needs of the enterprisethe exact details of the service being provided must be negotiated agreed to anddocumented in service level agreements or SLAs A good SLA is a tool that setsappropriate expectations and serves as a reference when questions arise But remember IT provides solutions not roadblocksWhen something is broken users want to know when its going to be fixed Thatsit They dont really care which hard disk or generator broke or why leave that information for your managerial reportsFrom a users perspective no news is good news The system either works or itdoesnt and if the latter it doesnt matter why Our customers are happiest whenthey dont even notice that we exist Sad but trueAn SLA helps align end users and support staff A wellwritten SLA addresses eachof the issues discussed in the following sectionsScope and descriptions of servicesThis section is the foundation of the SLA because it describes what the organizationcan expect from IT Write it in terms that can be understood by nontechnical staffSome example services might be Email Chat Internet and web access File servers Business applications AuthenticationThe standards that IT will adhere to when providing these services must also be defined For example an availability section would define the hours of operation theagreedon maintenance windows and the expectations regarding the times at whichIT staff will be available to provide live support One organization might decidethat regular support should be available from am to pm on weekdaysbut that emergency support must be available Another organization mightdecide that it needs standard live support available at all timesHere is a list of issues to consider when documenting your standards Response time Service and response times during weekends and offhours House calls support for environments at home Weird unique or proprietary hardware Upgrade policy ancient hardware software etc Supported operating systems Supported cloud platforms Standard configurations Data retention Specialpurpose softwareWhen considering service standards keep in mind that many users will want tocustomize their environments or even their systems if the software is not naileddown to prevent this The stereotypical IT response is to forbid all user modifications but although this policy makes things easier for IT it isnt necessarily the bestpolicy for the organizationAddress this issue in your SLAs and try to standardize on a few specific configurations Otherwise your goals of easy maintenance and scaling to grow with the organization will meet some serious impediments Encourage your creative OShackingemployees to suggest modifications that they need for their work and be diligentand generous in incorporating these suggestions into your standard configurationsIf you dont your users will work hard to subvert your rulesQueue prioritization policiesIn addition to knowing what services are provided users must also know aboutthe priority scheme used to manage the work queue Priority schemes always havewiggle room but try to design one that covers most situations with few or no exceptions Some priorityrelated variables are listed below The importance of the service to the overall organization The security impact of the situation has there been a breach The service level the customer has paid or contracted for The number of users affected The importance of any relevant deadline The loudness of the affected users squeaky wheels The importance of the affected users this is a tricky one but lets be honestsome people in your organization have more pull than othersAlthough all these factors will influence your rankings we recommend a simpleset of rules together with some common sense to deal with the exceptions We usethe following basic priorities Many people cannot work One person cannot work Requests for improvementsIf two or more requests have top priority and the requests cannot be worked onin parallel we decide which problem to tackle first by assessing the severity of theissues eg email not working makes almost everybody unhappy whereas the temporary unavailability of a web service might hinder only a few people Queues atlower priorities are usually handled in a FIFO mannerConformance measurementsAn SLA needs to define how the organization will measure your success at fulfilling the terms of the agreement Targets and goals allow the staff to work towarda common outcome and can lay the groundwork for cooperation throughout theorganization Of course you must make sure you have tools in place to measurethe agreedon metricsAt a minimum you should track the following metrics for your IT infrastructure Percentage or number of projects completed on time and on budget Percentage or number of SLA elements fulfilled Uptime percentage by system eg email available through Q Percentage or number of tickets that were satisfactorily resolved Average time to ticket resolution Time to provision a new system Percentage or number of security incidents handled according to the documented incident handling process Compliance regulations and standardsIT auditing and governance are big issues today Regulations and quasistandardsfor specifying measuring and certifying compliance have spawned myriad acronyms SOX ITIL COBIT and ISO just to name a few Unfortunately thisalphabet soup is leaving something of a bad taste in system administrators mouthsand quality software to implement all the controls deemed necessary by recent legislation is currently lackingSome of the major advisory standards guidelines industry frameworks and legalrequirements that might apply to system administrators are listed below The legislative requirements are largely specific to the United StatesTypically the standard you must use is mandated by your organization type or thedata you handle In jurisdictions outside the United States you will need to identify the applicable regulations The CJIS Criminal Justice Information Systems standard applies toorganizations that track criminal information and integrate that information with the FBIs databases Its requirements can be found onlineat fbigovhqcjisdcjishtm COBIT is a voluntary framework for information management that attempts to codify industry best practices It is developed jointly by the Information Systems Audit and Control Association ISACA and the ITGovernance Institute ITGI see isacaorg for details COBITs mission isto research develop publicize and promote an authoritative uptodateinternational set of generally accepted information technology controlobjectives for daytoday use by business managers and auditorsThe first edition of the framework was published in and we arenow at version published in This latest iteration was stronglyinfluenced by the requirements of the SarbanesOxley Act It includes highlevel processes categorized into five domains Align Plan andOrganize APO Build Acquire and Implement BAI Deliver Serviceand Support DSS Monitor Evaluate and Assess MEA and EvaluateDirect and Monitor EDM COPPA the Childrens Online Privacy Protection Act regulates organizations that collect or store information about children under the ageof Parental permission is required to gather certain information seeftcgov for details FERPA the Family Educational Rights and Privacy Act applies to allinstitutions that are recipients of federal aid administered by the Secretaryof Education This regulation protects student information and accordsstudents specific rights with respect to their data For details search forFERPA at edgov FISMA the Federal Information Security Management Act appliesto all government agencies and their contractors Its a large and rathervague set of requirements that seek to enforce compliance with a varietyof IT security publications from NIST the National Institute of Standardsand Technology Whether or not your organization falls under the mandate of FISMA the NIST documents are worth reviewing See nistgovfor more information The FTCs Safe Harbor framework bridges the gap between the US andEU approaches to privacy legislation and defines a way for US organizations that interface with European companies to demonstrate their datasecurity See exportgovsafeharbor GLBA the GrammLeachBliley Act regulates financial institutionsuse of consumers private information If youve been wondering whythe worlds banks credit card issuers brokerages and insurers have beenpelting you with privacy notices thats the GrammLeachBliley Act atwork See ftcgov for details Currently the best GLBA information is inthe business section of the Tips Advice portion of the web site Theshortcut googlvv currently works as a deep link HIPAA the Health Insurance Portability and Accountability Act applies to organizations that transmit or store protected health informationaka PHI It is a broad standard that was originally intended to combatwaste fraud and abuse in health care delivery and health insurance butit is now used to measure and improve the security of health informationas well See hhsgovocrprivacyindexhtml ISO and ISO are a voluntary and informativecollection of securityrelated best practices for IT organizations See isoorg CIP Critical Infrastructure Protection is a family of standards fromthe North American Electric Reliability Corporation NERC which promote the hardening of infrastructure systems such as power telephoneand financial grids against risks from natural disasters and terrorism Ina textbook demonstration of the Nietzschean concept of organizational will to power it turns out that most of the economy falls into one ofNERCs critical infrastructure and key resource CIKR sectors andis therefore richly in need of CIP guidance Organizations within thesesectors should be evaluating their systems and protecting them as appropriate See nerccom The Payment Card Industry Data Security Standard PCI DSS wascreated by a consortium of payment brands including American ExpressDiscover MasterCard JCB and Visa It covers the management of payment card data and is relevant for any organization that accepts creditcard payments The standard comes in two flavors a selfassessment forsmaller organizations and a thirdparty audit for organizations that process more transactions See pcisecuritystandardsorg The FTCs Red Flag Rules require anyone who extends credit to consumers ie any organization that sends out bills to implement a formal program to prevent and detect identity theft The rules require credit issuersto develop heuristics for identifying suspicious account activity hencered flag Search for red flag at ftcgov for details In the s and early s the Information Technology Infrastructure Library ITIL was a de facto standard for organizations seeking acomprehensive IT service management solution Many large organizationsdeployed a formal ITIL program complete with project managers for eachprocess managers for the project managers and reporting for managersof the project managers In most cases the results were not favorable Theheavy process focus combined with siloed functions resulted in intractableIT constipation This red tape created opportunities for lean startups tosteal market share from wellestablished companies thus sending manycareer IT practitioners out to pasture We hope weve seen the last of ITILSome say DevOps is the antiITIL methodology Last but certainly not least the IT general controls ITGC portion ofthe SarbanesOxley Act SOX applies to all public companies and isdesigned to protect shareholders from accounting errors and fraudulentpractices See secgovSome of these standards contain good advice even for organizations that are notrequired to adhere to them It might be worth breezing through a few of them justto see if they contain any best practices you might want to adopt If you have noother constraints check out NERC CIP and NIST they are our favoriteswith regard to thoroughness and applicability to a broad range of situationsThe National Institute for Standards and Technology NIST publishes a host ofstandards that are useful to administrators and technologists The two most commonly used ones are mentioned below but if you are ever bored and looking forstandards you might check out their web site You will not be disappointedNIST Recommended Security Controls for Federal Information Systems andOrganizations describes how to assess the security of information systems If yourorganization has developed an inhouse application that holds sensitive information NIST can help you make sure you have truly secured it Beware however embarking on a NIST compliance journey is not for the faint of heartYou are likely to end up with a document that is close to pages long and thatincludes excruciating detailsNIST Contingency Planning Guide for Information Technology Systems isNISTs disaster recovery bible It is directed at government agencies but any organization can benefit from it Following the NIST planning process takestime but it forces you to answer important questions such as Which systems are If you plan to do business with a US government agency you may be required to complete a NIST assessment whether you want to or notthe most critical How long can we survive without these systems and Howare we going to recover if our primary data center is lost Legal issuesThe US federal government and several states have enacted laws regarding computer crime At the federal level two pieces of legislation date from the early sand three are more recent The Electronic Communications Privacy Act The Computer Fraud and Abuse Act The No Electronic Theft Act The Digital Millennium Copyright Act The Email Privacy Act The Cybersecurity Act of Some major issues in the legal arena are these liability of sysadmins network providers and public clouds peertopeer filesharing networks copyright issues andprivacy issues The topics in this section comment on these issues and a variety ofother legal debacles related to system administrationPrivacyPrivacy has always been difficult to safeguard and with the rise of the Internet itis in more danger than ever Medical records have been repeatedly disclosed frompoorly protected systems stolen laptops and misplaced backup tapes Databasesfull of credit card numbers are routinely compromised and sold on the black market Web sites purporting to offer antivirus software actually install spyware whenused Fake email arrives almost daily appearing to be from your bank and allegingthat problems with your account require you to verify your account dataTechnical measures can never protect against these attacks because they target yoursites most vulnerable weakness its users Your best defense is a welleducated userbase To a first approximation no legitimate email or web site will ever Suggest that you have won a prize Request that you verify account information or passwords Ask you to forward a piece of email Ask you to install software you have not explicitly searched for or Inform you of a virus or other security problemUsers who have a basic understanding of these dangers are more likely to makesensible choices when a popup window claims they have won a free MacBook Usually a close inspection of the email would reveal that the data would go to a hacker in eastern Europe or Asia and not to your bank This type of attack is called phishingPolicy enforcementLog files might prove to you conclusively that person X did bad thing Y but to acourt it is all just hearsay evidence Protect yourself with written policies Log filessometimes include time stamps which are useful but not necessarily admissibleas evidence unless you can also prove that the computer was running the NetworkTime Protocol NTP to keep its clock synced to a reference standardYou may need a security policy to prosecute someone for misuse That policy shouldinclude a statement such as this Unauthorized use of computing systems may involve not only transgression of organizational policy but also a violation of stateand federal laws Unauthorized use is a crime and may involve criminal and civilpenalties it will be prosecuted to the full extent of the lawWe advise you to display a splash screen that advises users of your snooping policyYou might say something like Activity may be monitored in the event of a real orsuspected security incidentTo ensure that users see the notification at least once include it in the startup filesyou give to new users If you require the use of SSH to log in and you should youcan configure etcsshsshdconfig so that SSH always shows the splash screenBe sure to specify that through the act of using their accounts users acknowledgeyour written policy Explain where users can get additional copies of policy documents and post key documents on an appropriate web page Also include the specific penalty for noncompliance eg deletion of the accountIn addition to displaying the splash screen have users sign a policy agreementbefore giving them access to your systems Craft the acceptable use agreement inconjunction with your legal department If you dont have signed agreements fromcurrent employees make a sweep to obtain them then make signing the agreementa standard part of the induction process for new hiresYou might also consider periodically offering training sessions on information security This is a great opportunity to educate users about important issues such asphishing scams when its OK to install software and when its not password security and any other points that affect your environmentControl liabilityService providers ISP cloud etc typically have an appropriate use policy AUPdictated by their upstream providers and required of their downstream customersThis flow down of liability assigns responsibility for users actions to the usersthemselves not to the service provider or the service providers upstream providerSuch policies have been used to attempt spam control and to protect service providers in cases where customers have stored illegal or copyrighted material in theiraccounts Check the laws in your area your mileage may varyYour policies should explicitly state that users are not to use organizational resourcesfor illegal activities However thats not really enoughyou also need to disciplineusers if you find out they are misbehaving Organizations that know about violationsbut do not act on them are complicit and can be prosecuted Unenforced or inconsistent policies are worse than none from both a practical and legal point of viewBecause of the risk of being found complicit in user activities some sites limit thedata that they log the length of time for which log files are kept and the amountof log file history kept on backup tapes Some software packages help with the implementation of this policy by including levels of logging that help the sysadmindebug problems but that do not violate users privacy However always be awareof what kind of logging might be required by local laws or by any regulatory standards that apply to youSoftware licensesMany sites have paid for K copies of a software package and have N copies in dailyuse where K N Getting caught in this situation could be damaging to the companyprobably more damaging than the cost of those NminusK other licenses Other sites have received a demo copy of an expensive software package andhacked it reset the date on the machine found a license key etc to make it continue working after the expiration of the demo period How do you as a sysadmindeal with requests to violate license agreements and make copies of software onunlicensed machines What do you do when you find that machines for which youare responsible are running pirated softwareIts a tough call Management will often not back you up in your requests that unlicensed copies of software be either removed or paid for Often it is a sysadmin whosigns the agreement to remove the demo copies after a certain date but a managerwho makes the decision not to remove themWe are aware of several cases in which a sysadmins immediate manager would notdeal with the situation and told the sysadmin not to rock the boat The admin thenwrote a memo to the boss asking for correction of the situation and documentingthe number of copies of the software that were licensed and the number that werein use The admin quoted a few phrases from the license agreement and carboncopied the president of the company and his bosss managers In one case this procedure worked and the sysadmins manager was let go In another case the sysadmin quit when even higher management refused to do the right thing No matterwhat you do in such a situation get things in writing Ask for a written reply if allyou get is spoken words write a short memo documenting your understanding ofyour instructions and send it to the person in charge Organizations conferences and other resourcesMany UNIX and Linux support groupsboth general and vendorspecifichelpyou network with other people who are running the same software Table briefly lists a few such organizations but many other national and regional groupsare not listed hereTable UNIX and Linux organizations of interest to system administratorsName What it isFSF The Free Software Foundation sponsor of GNUUSENIX UNIXLinux user group quite technically orientedaLOPSA The League of Professional System AdministratorsSANS Sponsors sysadmin and security conferencesSAGEAU Australian sysadmins who hold yearly conferences in OzLinux Foundation Nonprofit Linux consortium produces LinuxCon among othersLinuxFest Northwest Grassroots conference with great contenta Wellknown parent organization of the LISA special interest group which was retired in FSF the Free Software Foundation sponsors the GNU Project GNUs Not Unixa recursive acronym The free in the FSFs name is the free of free speech andnot that of free beer The FSF is also the origin of the GNU Public License whichnow exists in several versions and covers many of the free software packages usedon UNIX and Linux systemsUSENIX an organization of users of Linux UNIX and other open source operating systems holds one general conference and several specialized smaller conferences or workshops each year The Annual Technical Conference ATC is apotpourri of indepth UNIX and Linux topics and is a great place for networkingwith the communityThe League of Professional System Administrators LOPSA has a fairly complexand somewhat sordid history It was originally associated with USENIX and wasintended to assume the mantle of USENIXs system administration special interestgroup SAGE Unfortunately LOPSA and USENIX parted on less than amicableterms and are now separate organizationsToday LOPSA sponsors a variety of sysadminrelated networking mentorship andeducational programs including events such as System Administrator AppreciationDay on the last Friday of July The customary gift for this holiday is bottle of scotchSANS offers courses and seminars in the security space and also founded a certification program the Global Information Assurance Certification GIAC whichoperates somewhat independently Certifications are available in a variety of generaland specific skill areas such as system administration coding incident handlingand forensics See giacorg for detailsMany local areas have their own regional UNIX Linux or open systems user groupsMeetupcom is an excellent resource for finding relevant groups in your area Localgroups usually have regular meetings workshops with local or visiting speakersand often dinner together before or after the meetings Theyre a good way to network with other sysadmins Recommended readingBrooks Frederick P Jr The Mythical ManMonth Essays on Software Engineering nd Edition Reading MA AddisonWesley Kim Gene Kevin Behr and George Spafford The Phoenix Project A NovelAbout IT DevOps and Helping Your Business Win Revised Edition Scottsdale AZIT Revolution Press Kim Gene et al The DevOps Handbook How to Create WorldClass Agility Reliability and Security in Technology Organizations Scottsdale AZ IT RevolutionPress Limoncelli Thomas A Time Management for System Administrators SebastopolCA OReilly Media Machiavelli Niccol The Prince Available online from gutenbergorgMorris Kief Infrastructure as Code Managing Servers in the Cloud Sebastopol CAOReilly Media This book is a wellwritten foot overview of DevOpsand largescale tools for system administration in the cloud It includes few specifics about configuration management per se but its helpful for understandinghow configuration management integrates into the larger scheme of DevOps andstructured administrationThe site itlnistgov is the landing page for the NIST Information Technology Laboratory and includes lots of information about standardsThe web site of the Electronic Frontier Foundation efforg is a great place to findcommentary on the latest issues in privacy cryptography and legislation Alwaysinteresting readingSANS hosts a collection of security policy templates at sansorgresourcespoliciesWe have alphabetized files under their last components And in most cases only the lastcomponent is listed For example to find index entries relating to the etcmailaliases filelook under aliases Our friendly vendors have forced our hand by hiding standard files innew and inventive directories on each systemIndexSymbols directory entry directory entry shebang syntax Password Directory Server IEEE standards see IEEEstandardsAA DNS records AA Access Agent AAAA DNS records acceptance tests accept command acceptredirects parameter accept router Exim acceptsourceroute parameter Access Agent AA access control access control lists see ACLsaccessdb feature sendmail etcmailaccess file access points wireless accounts see user accountsACLsDNS Exim filesystem Active Directory see Microsoft Active Directoryaddressessee also IPsee also IPvbroadcast Ethernet aka MAC loopback multicast adduser command etcadduserconf file Adleman Leonard varadm directory administrative privileges see rootaccountAdobe InDesigncrash URL frequently usedexperiences with AES Advanced Encryption Standard AFR Annual Failure Rate AfriNIC AgileBits AIDE Advanced Intrusion Detection Environment air conditioning see coolingair plenums wiring AIX Akamai Technologies algorithms cryptographic aliasdatabase parameter Postfix etcmailaliases file aliases email see also emailsee also Eximsee also Postfixsee also sendmailfiles as alias source files mailing to hashed database loops mailing lists postmaster programs mailing to aliasmaps parameter Postfix Allman Eric varcronallowdeny file Almquist shell Alpine Linux alwaysadddomain featuresendmail Amazon EC Container Registry Amazon Linux Amazon Web Services see AWSAMD American Power ConversionAPC American Registry for InternetNumbers ARIN AMP Anixter Annual Failure Rate AFR Ansible access options client in AWS comments on comparison to Salt and Docker example groups client groups dynamic iteration and Jinja passwords playbooks play elements pros and cons recommendations configuration base requirements client roles securing client connectionssecurity setup state parameters tasks templates variables ansiblecfg file ansible command etcansible directory ansiblegalaxy command ansibleplaybook command ansiblevault command Anvin H Peter anycast apacheconf file varlogapache files Apache Cassandra apachectl command Apache Directory Studio Apache HTTP Server Apache Software Foundation Apache Spark Apache Traffic Server Apache Zookeeper APC American Power Conversion apex zone DNS APIs Application Programminginterfaces APM Application PerformanceMonitoring APNIC AppArmor etcapparmord directory AppDynamics appendfile transport Exim application monitoring Application Programming Interfaces APIs Appropriate Use Policy AUPapropos command APT Advanced Package Tool aptcache command apt command varlogapt file aptget command aptmirror command Arch Linux ARIN American Registry for Internet Numbers ARP Address Resolution Protocol ARPANET arp command Artifactory as a service ASHRAE temperature range ash shell Assmann Claus ATT attack surface ATT UNIX System V auditd daemon auditing user access authconfig command varlogauthlog file AUTHMECHANISMS option sendmail sshauthorizedkeys file etcautodirect file autofs filesystem etcautomaster file etcautomaster file automation code promotion configuration managementmachine setup patching scripts upgrades automountautomatic mounts direct maps executable maps indirect maps on Linux master maps replicated filesystems visibility automountd daemon automount utility autonegotiation Ethernet etcautonet file autonomous system AS autoreply transport Exim autounmountd daemon availability availability zones cloud Avatier Identity Management SuiteAIMS AWS and Ansible booting alternate kernel CDN CloudFormation CloudWatch CodeDeploy console log EBS EC EC Container Service Elastic Beanstalk Elastic File System EFS emergency mode eventbased computing firewall IAM instance store Lambda load balancing NACLs and NFS PaaS pricing quick start AWS continuedRDS Redshift reserved instance security groups shutting down systems singleuser mode SQS stopping instances subnets swap space and Terraform VPC VPN aws CLI tool BBackblaze backing store backup dataneed for plan and security strategy bad blocks disk BADRCPTTHROTTLE featuresendmail Bairavasundaram et al Bamboo Barracuda bashprofile file bashrc file etcbashrc file bash shell see also sh shellcommand editing environment variablespipes quoting redirection search path variables Basic InputOutput System BIOS see also UEFIbc command BCP Best Current Practice BCPL Basic Combined Programming Language beer suggested minimum Belden Cable Bell Labs Berkeley see University of California at BerkeleyBerkeley Fast File System Berkeley Internet Name Domainsystem see BINDBerkeley Software Design IncBSDI BGP Border Gateway Protocol bgpd daemon bhyve BIND see also DNSsee also namedsee also name serversACLs AXFR zone transfer chrooted environment components configuration examplesdebugging delv command DNSSEC dnsseckeygen command dnssecsignzone commanddoc command drill command example configuration forwarding zone forwardonly server IXFR zone transfer namedconf file nsupdate program rndc command etcrndcconf file rndcconfgen command etcrndckey file security features TSIGTKEY zone signing zone transfers bin directory bin directory usrbin directory BINDIRECTORY variable EximBIOS Basic InputOutput System see also UEFIBitBucket Black Box Corporation blacklistrecipients featuresendmail blacklists sendmail block device files block size disk block storage Blowfish hashing algorithm bluegreen deployment boot directory bootinginitial processes logging and NFS filesystems PXE boot loader GRUB loader password varlogbootlog file bootstrappingdrive selection failures firmware fsck and process overview singleuser mode startup scripts tasks bootbootxefi bootstrap efibootbootxefi bootstrapBostic Keith botnets Bourneagain shell see bashBourne shell Bourne Stephen break the glass broadcastdomain packets ping storm Bro network intrusion detectionsystem Brouwer Andries E Bryant Bill BSDCan conference bsdinstall utility BSD UNIX btrfs command Btrfs filesystem and Docker setup shallow copies snapshots subvolumes volumes vs ZFS BugTraq Bugzilla building wiring Burgess Mark bus errors BUS signal Ccablesbase Category coating colorcoding Ethernet fiber wiring standard UTP CA Certificate Authority cache poisoning DNS cache web server cachingonly name server Cacti CAIDA Cooperative Associationfor Internet Data Analysis camcontrol command Canaday Rudd cancel command Canonical Ltd canonical name CNAME DNSrecords Capistrano Carbon Card Rmy Carrier Pigeon Internet ProtocolCPIP CAS CBK Common Body of Knowledge ccTLDs country code Top LevelDomains CDN Content Delivery NetworkCenter for Internet Security CentOS Linux Ceph CERT Certificate Authority CA Certificate Signing Request CSRcfdisk command CFEngine chage command chain of trust DNSSEC channels wireless character device files ChatOps chat platforms Chatsworth Products chattr command checkclientaccess optionPostfix Check Point checksum network Chef chfn command chgrp command Childrens Online Privacy Protection Act COPPA chkrootkit command chmod command chown command Christiansen Tom CIA triad CICD Continuous Integrationand Continuous Deliveryartifact auditability automation bluegreen deployment build and containers delivery deployment environments essential concepts example feature flags integration pipeline release release candidate repository organization and revision control CICD continuedstages testing zero downtime CIDR Classless InterDomainRouting CIFS see SMB Server MessageBlockCIP Critical Infrastructure Protection CISA Certified Information Systems Auditor CIS Center for Internet Security Cisco Adaptive Security Appliance Cisco IronPort Cisco routers Cisco Systems CISSP Certified InformationSystems Security Professional CJIS Criminal Justice InformationSystems ClamAV CLAMS acronym C language cleanup daemon varspoolclientmqueue directory clone system call CloudBees cloud computing access to automation availability zones backup data booting alternate kernelscost control CPU stolen cycles and DevOps foundations of fundamentals IaaS identity and authorization images instances management layers networking PaaS platforms public private and hybridcloud computing continuedreasons for regions SaaS serverless storage virtual private servers web hosting CloudFlare CloudFront cloud hosting providers varlogcloudinitlog file CNAME DNS records cn LDAP attribute Coarse Wavelength Division Multiplexing CWDM Coax cable Cobbler COBIT code coverage code promotion collectd command etccollectdcollectdconf fileCommon Body of KnowledgeCBK Common Criteria common name LDAP etcpamdcommonpasswdfile compat directory Computer Fraud and Abuse Actconcentrators see Ethernet hubsconferences system administration confidentiality data etcselinuxconfig file configuration managementarchitecture best practices dangers of dependency managementelements of language comparison platforms overview popular systems rosetta stone usreximconfigure file CONFIGUREFILE variable EximconfLOGLEVEL option sendmailcongestion control algorithmsTCP conncontrol feature sendmail ConnectionRateThrottle optionsendmail CONNECTIONRATETHROTTLE option sendmail containers as build artifacts capabilities and CICD clustering control groups core concepts images management softwarenamespaces networking utility of vs virtualization Content Delivery Network CDNContinuous Integration and Delivery see CICDcontrol CONT signal coolingcalculating load data center inrow COPPA Childrens Online PrivacyProtection Act Corbato Fernando CoreOS Linux country code domains ccTLDsCourion CPAN Comprehensive Perl Archive Network CPUanalyzing usage stolen cycles utilization proccpuinfo file CRAC Computer Room Air Conditioner varcrash directory Critical Infrastructure ProtectionCIP etccronallowdeny file cron daemon log file varlogcron file crontab command crontab file cryptography DiffieHellman key exchangeDNSSEC public key symmetric key C shell cshrc file csh shell CSMACD protocol CSR Certificate Signing RequestCSRG Computer Systems Research Group Cummins Onan CUPS Common UNIX PrintingSystem see also printingautoconfiguration filters instances logging network printing queue restarting cupsconfig command etccupscupsdconf file cupsd daemon cupsdisable command cupsenable command curl command cut command CVS CyberArk Cybersecurity Act of DDA Delivery Agent varlogdaemonlog file daemons DARPA Defense Advanced Research Project Agency dash shell DataBase Administrators DBAsdata centersee also coolingavailability components cooling load generators hot aisle humidity inrow cooling location power rack density rack power requirementsracks raised floor redundant power reliability tiers security temperature range toolbox track system UPSs Datadog Data Loss Prevention DLP DBAN Dariks Boot and NukeDBAs DataBase Administrators dc LDAP attribute DDoS Distributed DenialofService attack debconf utility DebianGNU Linux debianinstaller script debt technical varlogdebug files debugging see troubleshootingdefault route DefCon conference defense in depth DeHaan Michael DELAYLA option sendmail deleting accounts Delivery Agent DA deluser command delv command denial of service DOS attackDNS email Dennis Jack Deraison Renaud DES hashing algorithm etcdevdconf file devd daemon dev directory development environment devfs filesystem device drivers device files block vs character creation of for disks management of device names ephemeral DevOps see also CICDautomation ChatOps chat platforms CICD CLAMS acronym and cloud computing code promotion and configuration management culture environment separation infrastructure as code lean tenet measurement philosophy paging sharing system administrator role tenets DevOpsDays conference devtmpfs filesystem df command dhclient command etcdhclientconf file dhcpdconf file dhcpd daemon DHCP Dynamic Host Configuration Protocol dhcrelay daemon DiffieHellmanMerkle key exchange dig command Digital Millennium Copyright ActDMCA DigitalOcean booting alternate kernel networking quick start recovery kernel directories deleting search bit disaster recovery in the cloud list of data to keep handy planning risk assessment staffing standards threats who to put in charge devdisk directory diskpart command diskssee also filesystemsaddition of bad block managementblock size comparison of HDD and SSDdevice files elevator algorithm failure rate filesystems formatting hardware hardware attachment hardware interfaces HDD hotpluggable hybrid logical volumes LVM management layers monitoring with SMARTnaming standards device partitions performance physical volumes RAID reliability HDD reliability SSD resizing filesystems disks continuedrewritability limit SSD scheme partitioning snapshots speeds tradeoffs of types usage usb drive mounting volume groups warranties procdiskstats file distancevector protocols distinguished name LDAP Distributed Denial of Service attack DDoS DIX Ethernet II framing DKIM DomainKeys IdentifiedMail DKIM DomainKeys IdentifiedMail DNS records DLP Data Loss Prevention DMARC Domainbased MessageAuthentication Reporting andConformance DNS records DMCA Digital Millennium Copyright Act dmesg command varlogdmesg file dmidecode command dmsetup command DMZ DeMilitarized Zone dn LDAP attribute DNSsee also BINDsee also name serverssee also resource records DNSsee also zones DNSapex zone architecture Berkeley Internet Name Domain BIND daemonbogus TLD cache poisoning caching and CDNs cloudbased configuration database debugging delv command dynamic updates EDNS protocol DNS continuedefficiency forward zones inaddrarpa zone iparpa IPv support key rollover DNSSEC KSKs keysigning keyslame delegations lookups namedconf file nameserver directive name server market share name servers name server types namespace negative caching NOERROR status etcnsswitchconf file NXDOMAIN status open resolvers primary objective private addresses queries fromquery record types recursive servers registering a domain name etcresolvconf file resolver configuration resource records see resourcerecords DNSreverse mapping root server hints root servers round robin secondlevel domain name security SERVFAIL status service providers splattercast split DNS subdomains TTL time to live UDP packet size ZSK ZoneSigning KeysDNSKEY DNS records dnslookup driver Exim Dnsmasq DNSSEC dnsseckeygen command dnssecsignzone command doc command Docker architecture base images bridge network client setup debugging Dockerfile docker group Docker Hub filesystem image building installation interactive shell logging logs namespaces networking options overlays network registries repository images rules of thumb running containers security storage drivers subcommands Swarm and systemd TLS volume containers volumes dockercfg file docker command dockerd daemon varlibdocker directory DOCKERHOST environment variable Docker Hub Docker Inc varrundockersock socket Docker Swarm documentation BCPs FYIs local man pages packagespecific RFCs standards STDs systemspecific DomainKeys Identified Mailsee DKIMDOMAIN macro sendmail do not fragment flag DontBlameSendmail optionsendmail DONTBLAMESENDMAIL optionsendmail dot files DOUBLEBOUNCEADDRESS optionsendmail Double Choco Latte double colon notation dpkg command varlogdpkglog file dpkgquery command DR see disaster recoverydrill command drivers see device driversDrone Dropbear DS DNS records dtrace command dtrace tool du command dumpon command Duo Eecho command EDITOR environment variable editors text effective GID effective UID efibootmgr command EFI Extensible Firmware Interface EFI System Partition ESP EGID Effective GID Eich Brendan EIGRP Enhanced Interior Gateway Routing Protocol Elastic Elastic Beanstalk Elastic Load Balancer ELB Elasticsearch Electronic Communications Privacy Act Electronic Frontier Foundationelevator algorithm ELK Elasticsearch Logstash Kibana stack emacs file emailsee also aliases emailsee also Eximsee also MX DNS recordssee also Postfixsee also sendmailaccess agents aliases architecture body bounces components configuration delivery agents encryption envelope forgery headers loops Maildir format mailerdaemon Mail Submission Agent MSAmarketshare mbox format message structure MX records postmaster privacy spam see spamtransport agents User Agents UA Email Privacy Act EMC Ionix Infra emergencytarget target encryption see cryptographyenvironmental monitoring environment separation environment variables ephemeral storage equipment racks escrow password ESMTP protocol ESP EFI System Partition etc directory Ethernet addressing autonegotiation broadcast domain Ethernet continuedbroadcast storms cable characteristics collisions congestion CSMACD protocol framing hubs jumbo frames loops MTU OUIs packet types power over PoE Routers signaling speeds standards switches topology trunking VLANs ethtool command Etsy EUID Effective UID event logging example systems exec system call execute bit exicyclog command exigrep command exilog command Exim see also emailaccess control lists ACLsaliases file authentication blacklists commandline flags configuration configuration variables content scanning debugging forward file global options installation of lists logging macros options panic log retry configuration file section Exim continuedrewrite configuration file section routers security transports utilities virus protection eximcheckaccess command exim command eximdbmbuild command eximdumpdb command eximfixdb command eximlock command eximon command eximstats command eximtidydb command EXIMUSER variable Exim exinext command exipick command exiqgrep command exiqsumm command exiwhat command expect language export command exportfs command etcexports file exports NFS EXPOSEDUSER macro sendmailext filesytem FFaaS Functions as a Service Fabric Fabry Robert FailBan varlogfaillog file Family Educational Rights and Privacy Act FERPA FASTSPLIT option sendmail FAT File Allocation Table fcntl systems call fdisk command FEATURE macro sendmail Federal Information Security Management Act FISMA Fedora Linux Feigenbaum Barry FERPA Family EducationalRights and Privacy Act Ferraiolo David fetch command fiber opticalcolorcoding multimode singlemode standards types Fielding Roy file attributes ACLs change time changing colorcoding of device files displaying with ls encoding execute bit flags group permission inode number linux bonus mnemonic syntax NFS owner permission permission bits setuidsetgid bits sticky bit file command file descriptors File Integrity Monitoring FIMfilenamescontrol characters in globbing length limitation of pathnames pattern matching spaces in filessee also device filessee also directoriessee also file attributessee also filenamesaccess control of block device character device default permissions deleting directory hard link link count local domain socket files continuedmodes see file attributesnamed pipe regular renaming revision control symbolic link types of file sharing Filesystem Hierarchy Standardfilesystems see also partitions diskACLs automatic mounting Btrfs checking and repairing components of copyonwrite error detection ext inodes journaling lazy unmounting lostfound directory mounting NFS organization of pathnames performance polymorphism processes using relation to other layersreplicated resizing root SMB superblock terminology UFS union unmounting XFS ZFS filesystem UID file transfer secure FIM File Integrity Monitoringfind command fio command firewalls packetfiltering safety of service filtering stateful inspection firmware system FISMA Federal Information Security Management Act flock system call Fluke Fluke meter Fontana Richard fork system call formatting disk forward file FORWARD chain iptables forwarder DNS forward file forward zone DNS Fowler Martin Fox Brian FQDNs Fully Qualified DomainNames frame network FreeBSD and Active Directory adding users and Kerberos antivirus autonegotiation boot messages building kernel configuration kernel default route device management disk addition recipe firewall fstab file idle timeout installation jails kernel panics loadable modules kernel location of kernel source logging logical volume managementlog rotation NAT network hardware networking network parameters paging statistics FreeBSD continuedparameters kernel partitioning disk printing proc filesystem removing users router use as a security of shadow passwords software managementtracing versions kernel virtualization VPN freebsdupdate command free command Free Software Foundation FSFfsck command FSF Free Software Foundationetcfstab file on FreeBSD on Linux fully qualified domain namesFQDNs Functions as a Service FaaS fuser command FYI For Your Information Ggcloud cli tool gconf file gconfpath file GCP Google Cloud Platform App Engine BigQuery booting alternate kernel Cloud Functions networking pricing quick start GECOS field gem command General Electric generators standby generic toplevel domains gTLDsgeom command GeoTrust getent command getfacl command getpwnam library call getpwuid library call getty process GIAC Global Information Assurance Certification gibi prefix GIDs see group IDsgiga prefix Git git command gitconfig file GitHub repository GitLab Gi unit GLBA GrammLeachBliley Actglobbing GlusterFS go command Go language Google Google App Engine Google Authenticator Google Cloud Platform see GCPGoogle Compute Engine GCEbooting failures serial console Google Container Registry Google Deployment Manager Google Gmail Google G Suite Google Wifi gpart command gparted command gpasswd command gpg command GPG encryption GPT GUID Partition Table Grafana GrammLeachBliley Act GLBA graphicaltarget target Graphite Gravitational Graylog greetpause feature sendmailgrep command groupadd command groupdel command etcgroup file group IDs see also groupsmapping names to pseudogroups real effective and saved groupmod command group permission bits groupssee also etcgroup filesee also group IDsadding docker GID GIDs group IDs passwords vs RBAC growfs command GRUB boot loader boot password command line commands options singleuser mode grubcfg config file grubmkconfig utility efiubuntugrubxefi bootstrap etcgshadow file gTLDs Generic TopLevel Domains GUID globally unique identifier GUID Partition Table GPT GVinum Hhi command HO HTTP server halt command halting the system haproxycfg file HAProxy load balancer configuration of health checks statistics sticky sessions TLS terminaton Hard Disk Drive HDD hard links hash cryptographic HashiCorp Hazel Philip HDD Hard Disk Drive hdparm command head command header packet Health Insurance Portability andAccountability Act see HIPAAHealth Insurance Portabilityand Accountability ActHEAT Hein Trent R Heroku HGST HIDS Hostbased Intrusion Detection System hier man page HIPAA Health Insurance Portability and Accountability Act HipChat historyof Linux of Sun Microsystems now Oracle America of system administratorsof UNIX Hitachi home directory home directory user homemailbox option Postfix host command hosting recommendations hostname fully qualified hostname command etchostname file etchosts file hot aisle cooling HPUX htop command htpasswd command htpasswd file httpdconf file varloghttpd file httpd server applications server modulesauthentication basic configuration log file httpd server continuedlogging multiprocessing modules TLS configuration virtual hosts HTTP protocol authentication basic headers keepalive load balancers over TLS port request methods responses security of servers SNI Server Name Indicationversions of HTTPS protocol port hubs Ethernet humidity HUP signal HVAC see coolingHVM Hardware Virtual Machinehybrid cloud hypervisors IIaaS Infrastructure as a ServiceIAM Identity and Access Management IANA Internet Assigned Numbers Authority IBM SoftLayer IBM T J Watson Research Center ICANN Internet Corporation forAssigned Names and Numbers Icinga icmpechoignorebroadcastsparameter ICMP Internet Control MessageProtocol redirects id command sshidecdsapub file identity management see IAMidle timeout FreeBSD IDS Intrusion Detection SystemIEC units IEEE framing IEEE standardsQ x ab af an ba bs bt u ac b g n IETF Internet Engineering TaskForce ifconfig command ifdown command ifup command IGF Internet Governance Forum IMAP protocol IMAPS protocol inaddrarpa DNS records inaddrarpa zone incident handling usrinclude directory InfluxDB Information Systems Audit andControl Association ISACAInformation Technology Infrastructure Library ITIL Infrastructure as a Service IaaSinfrastructure as code init process bootstrapping and flavors of modes run levels startup scripts vs systemd init scripts Innotek GmbH inode file attribute INPUT chain iptables inrow cooling INSECURE magazine integration tests integrity data Intel etcnetworkinterfaces file interfaces network see networksInternational Computer ScienceInstitute ICSI International Organization forStandardization ISO Internetdocumentation governance of history registries standards Internet Assigned Numbers Authority IANA Internet Corporation for AssignedNames and Numbers ICANN Internet of Things IoT Internet Printing Protocol IPPInternet protocol see IPInternet Systems ConsortiumISC INT signal ioctl system call iostat command IoT Internet of Things IP see also routingaddress assignment address classes addresses allocation address anycast ARP broadcast broadcast ping broadcast storm CIDR configuration configuring debugging default route DHCP directed broadcast encapsulation faster than light FTL IP continuedfirewalls forwarding fragmentation IPv vs IPv layers masquerading see NATMTU multicast netmask packet size packet sniffers packet structure ports private addresses privileged ports reassembly redirects routing security services source routing spoofing stack subnetting timetolive field TTL transmission via avian carriers unicast uRFP VPN TCPIP see IPiparpa zone ipcalc tool ip command iperf tool etcipfipfconf file IPFilter ipforward parameter ipfw command IPP Internet Printing ProtocolIPsec protocol iptables command IPv see IPprocsysnetipv directory IPv see also IPaddressing address notation automatic host numbering debugging IPv continuedDNS support double colon fragmentation linklocal unicast MTU Neighbor Discovery ND penetration of prefixes scenic routing for SLAAC tunneling vs IPv procsysnetipv directory ISC International InformationSystems Security CertificationConsortium ISO standard ISO standard ISOC Internet Society ISO International Organizationfor Standardization ISP Internet Service Provider IT Governance Institute ITGIITIL Information Technology Infrastructure Library iwconfig command iwlist command JJacobson Van Java language JavaScript JavaScript Object Notation JSON Jenkins build agents build context build master build trigger and code repositories concepts distributed builds and Docker Jenkinsfile file job Pipeline project Jenkins Enterprise Jenkinsfile file Jinja Jira Jodies Krischan John the Ripper Jolitz Bill JOSSO journalctl command etcsystemdjournaldconf file journald process journaling filesystem Joy Bill jq command JSON JavaScript Object Notation jumbo frames Ethernet Juniper Networks Kks see KubernetesKali Linux Kaminsky Dan Karels Mike kde directory kdestroy command kdump process KeePass Kerberos and Active Directory and NFS and NTP kernelarguments booting booting alternate in cloudbuilding FreeBSD building Linux errors functions of initialization of loadable modules loading location of options panics selection of kernel continuedsingleuser mode source location FreeBSD source location Linux tuning FreeBSD tuning Linux version numbers bootkernel directory varlogkernlog file kgdb command Kibana kibi prefix Kickstart killall command Kill A Watt meter kill command KILL signal kilo prefix kinit command Ki unit kldload command kldstat command kldunload command klist command devkmsg device knife command sshknownhosts file Kolstad Rob Korn shell etckrbconf file Krebs Brian kscfg file ksh shell kubectl tool Kubernetes Kuhn Rick kVA unit conversion KVM guest installation kW unit conversion LLACNIC Lambda lame delegations DNS LAN Local Area NetworkLarge Installation System Administration LISA conferencelast command varloglastlog file layer switches LDAP alternatives to attributes coverting passwd and groupfile data structure use with Exim LDIF querying uses for use with sendmail etcopenldapldapconf file LDAPDEFAULTSPEC optionsendmail LDAP Lightweight Directory Access Protocol ldaprouting feature sendmailldapsearch command LDIF LDAP Data InterchangeFormat League of Professional System Administrators LOPSA least privilege principle of LEDE Lerdorf Rasmus Lets Encrypt lib directory lib directory usrlib directory libpcap format Librato licenses software Lightweight Directory Access Protocol see LDAPlimits command link layer linkshard symbolic linkstate protocols lint tool Linuxaccount attributes and Active Directory adding users antivirus as a firewall autonegotiation boot messages building kernel bulk account creation Linux continuedconfiguration kernel default route device driver adding device management device mapper disk addition recipe distributions errors kernel filesystem creating firewall fstab file history of iptables and Kerberos loadable modules kernellocation of kernel source logging logical volume managementlog rotation LXC NAT network configuration network hardware networking NetworkManager network tuning parameters kernel partitioning disk PAT performance checkuppluggable congestion controlalgorithms printing profiling performanceRAID reasons to choose router use as a scheduler IO securityenhanced security of shadow passwords stateful firewall swap space TCPIP options tracing versions kernel vga modes virtualization and viruses Linux continuedVPN and ZFS LinuxCon conference LinuxFest Northwest Linux Foundation Linux installationsee also system administrationautomating with debianinstaller automating with KickstartCentOS Debian netbooting with Cobbler preseeding Red Hat Ubuntu via PXE Linux Mint Linux package managementAPT highlevel management systems repositories RHN Lions John LISA conference LLC Link Layer Control layer lmtp daemon LMTP Local Mail Transfer Protocol ln command loadable modulesin FreeBSD in Linux procloadavg device load balancingarchitecture in AWS equalization with HAProxy http with NGINX partitioning round robin and security servers loader bootstrap commands configuration bootloaderconf configurationfile bootdefaultsloaderconf fileLocal Area Network LANlocal daemon usrlocal directory etcmaillocalhostnames filelocalinterfaces option Eximlocalization software guidelines limiting active releases organization testing update structure Local Mail Transfer ProtocolLMTP locate command lockd daemon lockf system call locking accounts varlog directory varspooleximlog directory logfilepath option Exim logger command logging see also syslogarchitecture at scale boottime Docker for sudo growth httpd kernel locations management policies rotation of files Loggly logical volume management see also Btrfs filesystemsee also LVMsee also ZFS filesystemlogin command loginconf file etcloginconf file logind process etcpamdlogin file login file logins see user accountslogin shell user log monitoring logos example system etclogrotateconf file logrotate utility logselector option Exim devlog socket Logstash logwatch tool loopback address LOPSA League of ProfessionalSystem Administrators lostfound directory lpadmin command lpc command lp command lpinfo command lpmove command lpoption command lpoptions command lppasswd command lpq command lpr command lprm command lpstat command lsattr command lsblk command ls command LSM Linux Security Modules lsmod command lsof command lsusb command lvchange command lvcreate command lvdisplay command LVM logical volume management architecture configuration phases relation to other layerssnapshots vs RAID lvresize command LWN Linux Weekly News LXC Lynis audit tool Mm command MAC Mandatory Access ControlMAC Media Access Control address MAC Media Access Control layer binmail command mail see emailmailboxcommand option Postfix mailboxtransport option Postfix Maildir format email Maildrop varspoolpostfixmaildrop directory MAILER macro sendmail varlogmail files MAILHUB macro sendmail mailq command mailspooldirectory optionPostfix Mail Submission Agent MSA Mail Transport Agent MTA Mail User Agent MUA maincf file Postfix major numbers device make command makemap command makewhatis command malware man command Mandatory Access Control MACmandb command usrshareman directory man pages sections updating keywords db MANPATH environment variable Mantis manualroute driver Exim devmapper device Marathon Mascheck Sven MASQUERADEAS macro sendmail Master Boot Record MBR MasterCard mastercf file postfix etcsaltmaster fiile etcmasterpasswd file Matsumoto Yukihiro Matz MatterMost MaxDaemonChildren option sendmail MAXDAEMONCHILDREN optionsendmail MaxMessageSize option sendmail MAXMESSAGESIZE option sendmail MAXMIMEHEADERLENGTH option sendmail MAXRCPTSPERMESSAGE featuresendmail MAXRCPTSPERMESSAGE optionsendmail mbox format email MBR Master Boot Record McAfee SaaS Email ProtectionMcCarthy John MCICACHESIZE option sendmail MCICACHETIMEOUT optionsendmail McIlroy Doug McKusick Kirk MD hashing algorithm mdadm command mdadmconf file md RAID system procmdstat file Mean Time Between FailuresMTBF mebi prefix media directory mega prefix memcached daemon procmeminfo file memorymanagement paging usage analyzing Mercurial Mesos varlogmessages file Metasploit Metcalfe Bob MFA MultiFactor Authentication mfsBSD project MIB Management InformationBase microscripts Microsoft Active Directory as a nameserver and FreeBSD and Linux Microsoft Azure Microsoft Office miitool command Miller Todd MINFREEBLOCKS option sendmail etcsaltminion file minor numbers device MINQUEUEAGE option sendmail Mirai botnet MIT Mi unit mkdir command mkfsbtrfs command mkfs command mkisofs command mknod command mkswap command mnt directory Moby project modcache caching module httpdmodified EUI algorithm modpassenger module httpdmodperl module httpd modphp module httpd modprobe command etcmodprobeconf file modproxyfcgi module httpdbootmodules directory libmodules directory modwsgi module httpd Monitorama conference monitoring application burnout charting platforms command output harvestingcommercial platformsculture monitoring continueddashboards data collection data types environmental events graphing historical data historic trends instrumentation intrusion detection log network noise notifications overview platforms push notifications and quality of life realtime metrics realtime platforms run books security SNMP systems temperature timeseries platformstips and tricks what to monitor Monitus Mosh mount command mountd daemon mounting filesystem mountntfs command mountsmbfs command Mozilla Foundation MPLS Multiprotocol LabelSwitching mpstat command varspoolmqueue directory MSA Mail Submission Agent varspooleximmsglog file MSSP Managed Security ServiceProvider MTA Mail Transport Agent MTBF Mean Time Between Failures mtree command mtr tool MTU Maximum Transfer Unit MUA mail user agent multicast IP multicast routing Multics MultiFactor Authentication MFAmultimode fiber MultipleInput MultipleOutputMIMO wireless multiuser mode multiusertarget target Munin muninnode command muninnodeconf file Murdock Ian MX DNS records mylocaldelivery clause Eximmyremotedelivery clause EximNNagios namedsee also BINDsee also DNSsee also name serversINCLUDE directive ORIGIN directive TTL directive acl statement allowquerycache clauseallowquery clause allowtransfer clause allowupdate clause alsonotify statement avoidvudpports clauseavoidvudpports clauseblackhole clause channel clause clientsperquery option controls statement for rndcdatasize option debug levels named continueddirectory statement dnssecenable option dnssecmustbesecure option dnssecvalidation optiondynamic updates ednsudpsize option error messages files option forwarders clause forward option freeze hostname statement include statement keydirectory statement key TSIG statement lamettl option localhost zone logging logging clause log messages masters statement maxacachesize option maxcachesize option maxcachettl option maxclientsperquery option maxjournalsize option maxncachettl option maxudpsize option notify statement options statement performance tuning querysource clause querysourcev clause recursion option recursiveclients option reload rootcache file search directive serverid statement server statement slave servers configuring split DNS statisticschannel statement tcpclients option updatepolicy clause usevudpports clause usevudpports clause version statement named continuedview statement zones configuring zone statement zonestatistics option zone transfer permissions zone transfers namedconf file named pipes name serverssee also BINDsee also DNSsee also namedauthoritative caching cachingonly delegation forwarder master nonauthoritative nonrecursive primary recursive root servers secondary slave stub switch file Name Service Switch NSS namespaces Linux namespaces process nano command Napieraa Edward Tomasz National Security Agency NSA NAT Network Address Translation nc command ND Neighbor Discovery protocol negative caching DNS Neighbor Discovery protocolND Nemeth Evi xlxli NERC CIP NERC North American ElectricReliability Corporation Nessus vulnerability scannernetcat command net command Netlink sockets NetSNMP package netstat command nettools package network administrators network booting networkd process etcsysconfignetwork file Network File System see NFSnetworkingbroadcast storm congestion control algorithmsdefault route and Docker packet sniffers troubleshooting tuning Network Intrusion Detection System NIDS NetworkManager network monitoring network operations center NOCnetworkssee also Ethernetsee also IPsee also IPvsee also routingsee also TCPIPsee also wireless networksarchitecture congestion design issues documentation expansion firewalls maintenance management packet contents port scanning softwaredefined SDN subnetting success factors wireless networkscripts directory Network Time Protocol NTP Neumann Peter newaliases command Newark Electronics newfs command newgrp command New Relic newsyslog utility newusers command NFS ACLs approach automatic mounting and AWS clientside dedicated servers drawbacks of exports hard vs soft mounts history of identity mapping and Kerberos on Linux locking file mounting at boot time mount options nobody account on FreeBSD performance ports protocol versions pseudofilesystem remote procedure calls root access RPC security serverside statefulness statistics transport protocols vs SMB nfsd daemon nfsstat command nfsuserd daemon nginxconf file nginx daemon NGINX HTTP server configuration installation of load balancing master process signals TLS virtual hosts worker process nice command niceness process NIDS Network Intrusion Detection System NIS Network Information Service NIST SP series standards NLnet Labs DNSSEC tools nmap port scanner nmbd daemon nobody account NOC network operations center Nodejs No Electronic Theft Act nohup command varrunnologin file binnologin shell nonrepudiation North American Electric Reliability Corporation NERC NoSQL database NSA National Security Agency NS DNS records NSEC DNS records NSEC DNS records nslookup command NSS Name Service Switch nsswitchconf file ntpd daemon NTP Network Time Protocoldevnull file OobjectClass LDAP attribute object stores office wiring OID Object Identifier SNMPo LDAP attribute OM fiber OM fiber OM fiber OpenLDAP open resolvers DNS OpenSolaris OpenSSH openssl selection OpenStack openSUSE Linux OpenVPN Open Web Application SecurityProject OWASP OpenWrt opt directory optical fiber etcnetworkoptions file Oracle Oracle Identity Management Oracle Linux Oracle VirtualBox OReilly Media OReilly series books OReilly Tim organizational unit LDAP orphaned processes OS fiber OSCON conference ospfd daemon ospfd daemon OSPF Open Shortest Path Firstprotocol OSSEC Open Source SECurity OSS mailing list OSTicket OSTYPE macro sendmail OTRS OUI Organizationally UniqueIdentifier ou LDAP attribute outofmemory killer oven easybake OWASP Open Web ApplicationSecurity Project owner permission bits ownership file PPaaS Platform as a Service package management packages see software packagesPacker packer command packet encapsulation packet filtering packetfiltering firewalls packet forwarding packetsbroadcast multicast unicast packet sniffers Padl Software pagedaemon process page size page table PAM Pluggable AuthenticationModules panic kernel Papertrail paravirtualization parted command partitions disk see also filesystemsrelation to other layersscheme PARTLABEL option PARTUUID option passphrase passwd command etcpasswd file passwords aging alternatives to break the glass change interval changing cracking escrow expiration hashes management obsolescence of root strength vaults PATH environment variable path MTU discovery pathnames PATH shell variable PAT Port Address Translation payload packet PCI DSS Payment Card IndustryData Security Standard PCIe Peripheral Component Interconnect Express interfacePCRE PerlCompatible RegularExpression Peek Mark penetration testing application perf command perfevents interface performance see also performance analysistoolsanalysis methdologyBIND common issues CPU disk disk bandwidth kernel variables memory network NFS nice command philosophy tuning resources that affect troubleshooting tuning rules performance analysis toolsfio command iostat command mpstat command perf command ps command sar command top command uptime command vmstat command performance tests periodic processes Perl permission bits file permitmynetworks option Postfix PGP Pretty Good Privacy pgrep command philosophy IT management phishing PHP language phpLDAPadmin physical volumes pickup daemon pidof command PID Process ID srvpillar directory ping command ping command Pingdom pip command pipe daemon pkg command pkill command PKI Public Key InfrastructurePlatform as a Service PaaS Pluggable Authentication Modulessee PAMpolicy appropriate use best practices enforcement standards POPS protocol portmap daemon portmaster command portsnap utility postalias command postcat command postconf command Postel Jon xliPostels Law xliPostfix see also emailaccess control aliases file architecture configuration debugging encryption forward file lookup tables null client configuration programs queues receiving mail security sending mail softbouncing utilities virtual domains postfix command postmap command POSTROUTING chain iptables postsuper command power factor powerofftarget target Power over Ethernet PoE power requirementsblade servers network equipment storage equipment PowerShell Desired State Configuration PPID Parent PID Pratt Ian PREROUTING chain iptables preseedcfg file Pretty Good Privacy PGP encryption preventative maintenance printenv command PRINTER environment variableprintf command printing see also CUPSarchitecture debugging Internet Printing ProtocolIPP network printers service shutoff privacy and sendmail PRIVACYFLAGS option sendmail privacy legislation EU PrivacyOption variable sendmail private addresses private cloud privileged ports proc directory procedures processes components of control terminal EGID EUID GID init see init processlife cycle monitoring of namespaces niceness nice value open files orphaned ownership processes continuedperiodic PID PPID priorities runaway spontaneous starting and stopping states tracing UID uninterruptible zombie proc filesystem procfs filesystem procmail command production environment etcprofiled directory etcprofile file profile file Prometheus proxy cache web server proxy HTTP ps command pseudoaccounts pseudodevices pseudogroups pseudorandom number generators PTR DNS records public cloud public key authentication Public Key Infrastructure PKIPUE Power Usage EffectivenessPuppet and Docker Purdue pvchange command pvck command pvcreate command pvdisplay command PVH ParaVirtualized HardwarePVHVM ParaVirtualized HVMpw command pwconv utility PXELINUX PXE Preboot eXecution Environment Python language best practices commandline argumentsdata types dictionaries files indentation input validation lists loops numbers package management strings tuples variables versions virtual environments vs Ruby QQCon conference QEMU PC emulator qmgr daemon qshape command quad A DNS records Quagga quayio QUEUELA option sendmail QUIT signal RRabbitMQ rack density rack power racks equipment Rackspace RAID Redundant Array of Inexpensive Disks disk failure recovery levels RAID drawbacks RAID write hole scrubbing software vs hardware vs LVM RAIDZ see ZFS filesystemRails web development platformRainerScript Rake RancherOS Linux devrandom device random number generation ransomware Rapid ratecontrol feature sendmail RBAC RoleBased Access Control etcdefaultsrcconf file etcrcconf file etcrcd directory etcrcdrclocal script etcrc script real GID realm command realmd daemon real UID reboot command reboot procedures reboottarget target recipientdelimiter optionPostfix Red Flag Rule Red Hat Enterprise Linuxsee RHELRed Hat Inc Red Hat Network RHN redirect driver Exim redirect feature sendmail Redis Redundant Array of InexpensiveDisks see RAID Redundant Array of Inexpensive DisksReed Darren REFUSELA option sendmail regexes see regular expressionsregions cloud regular expressions captures examples of failure modes literal characters matching process reInvent conference reject command rejectunauthdestination option Postfix RELAYDOMAIN feature sendmailetcmailrelaydomains file relayentiredomain featuresendmail relayhostsonly feature sendmail etcpostfixrelayingaccess filerelease release candidate Remedy removing accounts rendezvous addresses multicastrenice command rescue mode rescuetarget target resizefs command resizing filesystem etcresolvconf file resource records DNS A AAAA CNAME DKIM DMARC DNSKEY DS MX NS NSEC NSEC PTR quad A RRSIG SOA special characters in SPF SRV TXT types REST Representational StateTransfer etcpostfixrestrictedrecipientsfile reverse proxy web server reverse zone DNS revision control RFC addresses RFC Request for Comments RHEL RHN Red Hat Network Richards Martin ripd daemon RIPE DNSSEC tools RIPE NCC ripngd daemon RIPng Routing Information Protocol next generation RIP Routing Information Protocol risk assessment Ritchie Dennis Rivest Ron RJ wiring standard rm command rmdir command rmuser command rndc command etcrndcconf file rndcconfgen command etcrndckey file RoleBased Access ControlRBAC root account see also RBACbest practices disabling management of user ID rootcache file root directory root filesystem rootkits root server hints DNS root servers DNS root shell rotation log file round robin DNS route command routed daemon routing network adding Cisco routers configuration cost metrics daemons default default routes deleting distancevector linkstate multicast next hop protocols routing network continuedredirects static strategy table tables RPC rpcidmapd daemon rpm command RRDtool RRSIG DNS records RSA conference RSA public key cryptosystem rsync command etcrsyslogconf file rsyslogconf file rsyslogd daemon architecture configuration legacy configuration optionsmessage properties versions RT Request Tracker Ruby language as a filter best practices blocks environment managementhashes installation of package management regular expressions symbols vs Python runaway processes run directory varrun directory run levels init rvm environment managerSSaaS Software as a Service Safari Books Online Safe Harbor SAGEAU SailPoint IdentityIQ Salt comments on comparison to Ansibledebugging dependencies and Docker documentation environments formulas functions globbing highstates installation of and Jinja matching minion parameters pillars ports network pros and cons security setup minions sls files state binding minions state IDs states variables minions salt command srvsalt directory saltkey command saltmaster daemon saltminion daemon Salt Open SaltStack Samba see also SMB Server MessageBlockand AD browsing shares character sets configuring shares debugging group permissions installation of local authentication logging mounting shares security varlogsamba file SAML Security Assertion MarkupLanguage SANS SysAdmin Audit NetworkSecurity Institute SarbanesOxley Act SOX sar command SAS Serial Attached SCSI interface formatting disk SATA serial ATA interface formatting disk secure erase Satellite Server savecore command saved GID saved UID sbin directory usrsbin directory SCALE conference Schneier Bruce Schroeder Bianca scientific method scp command scripting see also bashsee also Perlsee also Python languageautomation choosing a language error messages useful languages microscripts philosophy style SCSI Small Computer System Interface SDN SoftwareDefined Networking search path secondlevel domain name SecOps Secret Server varlogsecure file Secure Hash Algorithm SHASHA SHA Secure Sockets Layer SSL securitysee also cryptographyaccess control aging password and Ansible architecture attack response attack surface auditing security continuedauthentication public keyand automation AWS and backups basic measures Blowfish hash boot loader botnets broadcast ping buffer overflows certifications chain of trust DNSSEC CIA triad configuration errors credit card data of credit cards see PCI DSSPayment Card Industry DataSecurity Standarddata loss prevention DLPDDoS defense in depth deleting accounts demoting data DES hash disk erasure DMZ DNS DNSSEC and Docker elements of encryption event logging of Exim file integrity monitoring file transfer secure firewall Linux or UNIX as afirewalls group logins handling attacks hash cryptographichome directory permissionsand HTTP ICMP redirects incident handling incident hotline insider abuse intrusion detection security continuedIoT IP forwarding and IP networking IPsec Kerberos least privilege and load balancing locking accounts malware MD hash monitoring multifactor authentication NFS of open source software open vs closed operating systems packet filtering passphrase password cracking password expiration password hashes passwords passwords obsolescence ofpassword strength patching schedule penetration testing phishing port scanning of Postfix power tools privileged ports and random numbers removing accounts root account root account disabling rootkits and Salt and Samba of search paths selfassessments SELinux of sendmail SHA hash shell secure ssh and SNMP social engineering source routing sources of compromisesources of information onspear phishing security continuedstandards sudo command of syslog messages system accounts nonrootof TCPIP TrustedBSD unnecessary services updates software vigilance viruses VPN vs convenience vulnerabilities softwarevulnerability scanningof wireless networks worms Security Assertion Markup Language SAML SecurityFocus security incidents SecuritySpace Seeley Donn segmentation violation segment TCP SEGV signal Selenium etcselinux directory SELinux SecurityEnhanced Linux Sender ID Sender Policy Framework SPF sendmail see also aliases emailsee also emailsee also spamblacklists and chroot command line flags configuration daemon mode databases directory locations and LDAP load average limit logging m and masquerading open relay permissions sendmail continuedprivacy queue monitoring queue processing queues rate and connection limits sample configuration security and the service switch file starting testing and debuggingsendmailcf file Sensu Server Fault serverless cloud server mode ServiceDesk Service Level Agreement SLAServiceNow etcservices file setfacl command setgid bit setgid execution setrlimit system call setuid execution setuidsetgid bits sfdisk command sftp command sftpserver command sgformat command SHA SHA SHA SecureHash Algorithm SHA hashing algorithm etcshadow file shadow passwords Shamir Adi Shapiro Greg usrshare directory shares NFS sharing a filesystem see NFSshell root shell scripting Shibboleth showmount command shred command sh shell see also basharithmetic commandline argumentssh shell continuedcomparison operators control flow execution of file evaluation functions globbing IO loops scripting shutdown command shutdown procedures Shuttleworth Mark Siemon SignalFx signals BUS caught blocked or ignored CONT HUP INT KILL list of important QUIT SEGV sending STOP TERM tracing TSTP USR USR WINCH Silicon Graphics Inc Simple Mail Transport Protocolsee SMTPsinglemode fiber singleuser mode cloud instances FreeBSD Linux remounting the root filesystem Site Reliability Engineer SRE etcskel directory SLAAC StateLess Address AutoConfiguration Slack Slackware Linux etcopenldapslapdconf file slapd daemon SLA Service Level Agreementslices see partitions diskslurpd daemon smartctl command smartd daemon SMARTHOST macro sendmail SMART selfmonitoring analysis and reporting technologyusrlocaletcsmbconf file smbclient command etcsambasmbconf file smbd daemon smbpasswd command SMB Server Message Blockhistory of vs NFS smbstatus command SMIME email encryption SmokePing smrsh command SMS notifications SMTP authentication commands debugging error codes status messages smtpd daemon smtpdrecipientrestrictionsoption Postfix smtpdrestrictions optionsPostfix smtpdsaslauthenable optionPostfix smtpdtls options Postfix smtp transport Exim smurf attacks snapshots volume SNI Server Name Indication snmpdconf file snmpd daemon snmpdelta command snmpdf command etcsnmp directory snmpget command snmpgetnext command snmpset command SNMP Simple Network Management Protocol agents community string SNMP continuedgraphing MIB organization protocol operations traps snmptable command snmptranslate command snmptrap command snmpwalk command Snort network intrusion detectionsystem Snowden Edward SOAP Simple Object Access Protocol SOA Start of Authority DNS records social coding sockets local domain socket system call softbounce option Postfix softwaresee also software packagesinstallation from source codeinstalling from a web script package management Software as a Service SaaS SoftwareDefined NetworkingSDN software delivery software packagessee also softwarelocalization management Solaris SolarWinds Sony sort command etcaptsourceslist file SOX SarbanesOxley Act spamsee also emailblacklists cloudbased services open relay Sender ID and sendmail SPF Sender Policy Framework spear phishing spectrum allocation wireless SPF Sender Policy Framework SPF Sender Policy FrameworkDNS records splattercast split DNS varspool directory SPOOLDIRECTORY variable EximSpotify Squid caching server usrsrc directory SRE Site Reliability Engineer srv directory SRV DNS records ss command SSD Solid State Disk SSH agent aliases host client connection multiplexing essentials file transfer keys password authentication disabling port port forwarding server SSHFP verification sshadd command sshagent command sshagent daemon ssh command sshconfig file sshdconfig file sshd daemon etcssh directory ssh directory SSHD solid state hybrid driveSSHFP DNS record SSHFP host key verification sshkeygen command sshkeyscan command SSIDs wireless SSL Secure Sockets Layer SSO Single SignOn see also LDAPaccount management for applications concepts elements of and Kerberos LDAP PAM and SaaS SAML sssdconf file sssd daemon Stack Overflow staging environment standard error standard input standard output standardssee also IEEE standardsCJIS Criminal Justice Information Systems COBIT Common Criteria contingency planning Critical Infrastructure Protection CIP Family Educational Rights andPrivacy Act FERPA Federal Information SecurityManagement Act FISMAFISMA GrammLeachBliley ActGLBA Health Insurance Portabilityand Accountability Act HIPAA IEEE Information Technology Infrastructure Library ITILISO ISO NERC CIP NIST SP NIST SP NIST SP series OWASP standards continuedPayment Card Industry DataSecurity Standard PCI DSSPCI DSS Red Flag Rule RJ wiring Safe Harbor SarbanesOxley Act SOXsecurity TIAEIAA TIAEIAB wireless wiring standard services Stanford Law School STARTTLS extension sendmailstartup scripts statd daemon stateful inspection firewalls State University of New YorkSUNY Buffalo static code analysis static routes StatsD StatusCake STDERR file descriptor STDIN file descriptor STDOUT file descriptor STD Standard sticky bit STOP signal storageblock ephemeral layers of object storage management see disksetccarbonstorageschemasconf file strace command Stuxnet worm subdomains DNS submitcf file subnetting Subversion su command sudo command xli configuration example configuration sitewide pros and cons sudo command continuedusing with Ansible using without password using with Salt vs advanced access control without a control terminal etcsudoers file Sumo Logic Sun Microsystems superblock filesystem superuser see root accountSupervisor supervisord daemon SUSE Linux swapctl command swapon command procsysvmswappiness parameter swap space Swarm Sweet Michael switches Ethernet symbolic links sync system call etcsysconfig directory sysctl command etcsysctlconf file Sysdig Cloud sysdig tool sys directory usrsrcsys directory sysfs filesystem syslog see also log filessee also loggingactions and DNS logging facility names messages security severity levels and systemd journal etcsyslogconf file syslogd daemon varlogsyslog file system administrationadjacent disciplines conferences essential tasks GUI tools system administration continuedkeeping current metrics prioritization resources for reading about service descriptionssystem administratorand CICD common tasks distinguishing characteristicshistory localization guidelinesprofessional attributes of responsibilities role in DevOps roles tool box SYSTEMALIASESFILE variableExim system calls tracing systemconfigkickstart tool systemctl command systemd daemon and init scripts caveats dependencies and Docker execution order journal logging management of targets timers unit files unit statuses vs init systemdjournald daemon systemdjournalremote tool System V UNIX Ttail command tape magnetic targets systemd task management TBERD line analyzer tcpdump tool TCPIP see also IPsee also IPvsee also networkingconnection reuse Fast Open TFO tcsh shell technical debt tee command Teleport telinit command temperaturedata center effect on hard disks TERM signal Terraform terraform command bintest command testingacceptance infrastructure integration performance software localization static code analysis unit testparm command TFO TCP Fast Open The Open Group Thompson Ken ThoughtWorks threads threat categories disaster Thycotic TIAEIAA wiring ticketgranting ticket Kerberosticketing ticketing systems timetolive field IP TLS Transport Layer Security tmp directory usrtmp directory vartmp directory tmp filesystem token ring toolbox TO options sendmail top command Torvalds Linus Townsend Jennine traceroute command TrackIt Transport Layer Security TLS Tridgell Andrew Tripwire Troan Erik Troposphere troubleshootingsee also performanceAmazon Web Services AWSinstances BIND booting cloud systems DigitalOcean instances DNS Docker Exim Google Compute EngineGCE instances HTTP connections kernel network performance Postfix printing Salt Samba sendmail SMTP syslog TLS servers web caching Trump Donald J trunks Ethernet truss command TrustedBSD tshark command Tso Theodore TSTP signal TTL timetolive DNS tugboat cli tool tunefs command tunefs command Tweedie Stephen Twofish TXT DNS records typographical conventions UUA mail User Agent UBER Uncorrectable Bit ErrorRate Ubiquiti Ubuntu Linux udevadm command etcudevudevconf file udevd daemon UDP User Datagram ProtocolUEFI Unified Extensible Firmware Interface bootstrap path UFS filesystem ufw command UIDs see user IDsulimit command umask command umask default umount command uname command Uncorrectable Bit Error RateUBER unicast IP unicast packets unicast Reverse Path ForwardinguRPF Uniform Resource LocatorsURLs uninterruptible power suppliesuniq command United Nations units unit tests Universal Plug and Play UPnPUniversal Serial Bus USB interface University of California at Berkeley University of Cambridge University of Colorado at Boulderxl University of Maryland University of Utah UNIXsee also FreeBSDas a firewall history of origin of name philosophy reasons to choose UNIX continuedsecurity of and viruses unlink system call unshielded twisted pair see UTPcablesunsolicited commercial emailsee spamupdatedb command updates software uptime command Uptime Institute The devurandom device URI Uniform Resource Identifier URLs Uniform Resource Locators URN Uniform Resource NameuRPF unicast Reverse Path Forwarding USB drive mounting USB Universal Serial Bus interface US Department of Defense usecwfile feature sendmailUSENIX Association user accounts adding attributes centralized managementdefaults deleting encrypted passwords GECOS field GID home directory identity management idle timeout locking login name login shell nobody password algorithm password expiration password quality passwords password setting password strength pseudoaccounts user accounts continuedRBAC removing shadow passwords startup files UID umask useradd command etcdefaultuseradd file userdel command userdellocal script user IDs mapping to names real effective and saved usermod command usernames see user accountsUSR signal USR signal usr directory UTP cables UUID VVagrant van Rossum Guido var directory variables environment Varnish caching server vault password VAX Velocity conference vendors we like Venema Wietse VeriSign Veritas Verizon Data Breach InvestigationsReport vgchange command vgck command vgcreate command vgdisplay command vgextend command vgscan command Viavi vi command vigr command viminfo file vimrc file vipw command virsh command virtinstall command virtmanager package virtualalias options PostfixVirtualBox virtualenv package virtual hosts web server in httpd in NGINX virtualization see also KVMsee also Xencontainerization on FreeBSD full hardwareassisted HVM Hardware Virtual Machine hypervisors images on Linux live migration paravirtualization provisioning PVH ParaVirtualized Hardware PVHVM ParaVirtualizedHVM QEMU type vs type vs containers virtualmailbox options Postfix Virtual Private Network VPN virtual private servers virtusertable feature sendmailetcmailvirtuserable file viruses virus scanningsee also emailVisa visudo command Vixie Paul VLANs trunking wireless vmstat command VMware VMware ESXi VMware Identity Manager VMware vCloud Air VMware Workstation VMWorld conference volume groups relations to other layersVPN Virtual Private Network vtysh daemon vulnerabilities software vulnerability scanning WWall Larry Watson Robert wc command web hosting APIs architecture build vs buy cache in the cloud components proxy server reverse proxy serverless server types static content TCP connection reuse virtual hosts WellKnown Service WKS portswget command wheel group whereis command which command Whisper WiFi networks WiFi Protected Access WPAWINCH signal Windows Defender Wired Equivalent Privacy WEPwireless networks access points APs channels frequency spectrum security SSIDs topology VLANs Wireshark wiring building wisdom Evis tenets of xliWorld Wide Web Consortium worms WPA see WiFi Protected Accesswpasupplicant command write hole RAID varlogwtmp file XX directory service Xen see also virtualizationcomponents dom guest installation overhead virtual block devices VBDsetcxen directory varlogxen files XenServer XFS filesystem xfsgrowfs command xl tool XML Extensible Markup Language varlogXorgnlog file XORP eXtensible Open RouterPlatform YYAML Ylnen Tatu yum command varlogyumlog file Zzebra daemon Zenoss devzero file zero downtime deploymentzfs command ZFS filesystem clones disk addition and Docker ZFS filesystem continuedinheritance property and Linux properties filesystem RAID raw volumes snapshots spare disks storage pool vs Btrfs Zimmermann Phil Zix zombie processes zones DNS forward forwarding localhost master reverse signing slave transfers zpool command Zulip In the modern age most folks have at least a vague idea what system administratorsdo work tirelessly to meet the needs of their users and organizations plan and implement a robust computing environment and pull proverbial rabbits out of manydifferent hats Although sysadmins are often viewed as underpaid and underappreciated most users can at least identify their friendly local sysadminin manycases more quickly than they can name their bosss bossIt wasnt always this way Over the last years and the year history of thisbook the role of the system administrator has evolved handinhand with UNIXand Linux A full understanding of system administration requires an understanding of how we got here and of some of the historical influences that have shapedour landscape Join us as we reflect on the many wonderful yearsThe dawn of computing system operators The first commercial computer the IBM was completed in Before the all computers had been oneoffs In a redesigned version of the wasannounced as the IBM It had words of magnetic core memory and threeA Brief History of SystemAdministrationWith Dr Peter H Salus technology historianindex registers It used bit words as opposed to the s bit words and didfloatingpoint arithmetic It executed instructions every secondBut the was more than just an update it was incompatible with the Although deliveries were not to begin until late the operators of the eighteens in existence the predecessors of modern system administrators were alreadyfretful How would they survive this upgrade and what pitfalls lay aheadIBM itself had no solution to the upgrade and compatibility problem It had hosted atraining class for customers of the in August but there were no textbooksSeveral people who had attended the training class continued to meet informallyand discuss their experiences with the system IBM encouraged the operators tomeet to discuss their problems and to share their solutions IBM funded the meetings and made available to the members a library of computer programs Thisgroup known as SHARE is still the place years later where IBM customersmeet to exchange informationFrom singlepurpose to time sharing Early computing hardware was physically large and extraordinarily expensive Thesefacts encouraged buyers to think of their computer systems as tools dedicated tosome single specific mission whatever mission was large enough and concreteenough to justify the expense and inconvenience of the computerIf a computer were a singlepurpose toollets say a sawthen the staff that maintained that computer would be the operators of the saw Early system operators wereviewed more as folks that cut lumber than as folks that provide whats necessaryto build a house The transition from system operator to system administrator didnot start until computers began to be seen as multipurpose tools The advent oftime sharing was a major reason for this change in viewpointJohn McCarthy had begun thinking about time sharing in the mids But itwas only at MIT in that he Jack Dennis and Fernando Corbato talkedseriously about permitting each user of a computer to behave as though he werein sole control of a computerIn MIT General Electric and Bell Labs embarked on a project to build anambitious timesharing system called Multics the Multiplexed Information andComputing Service Five years later Multics was over budget and far behind schedule Bell Labs pulled out of the projectUNIX is born Bell Labs abandonment of the Multics project left several researchers in MurrayHill NJ with nothing to work on Three of themKen Thompson Rudd Canaday Although SHARE was originally a vendorsponsored organization today it is independentand Dennis Ritchiehad liked certain aspects of Multics but hadnt been happywith the size and the complexity of the system and they often gathered in front ofa whiteboard to delve into design philosophy The Labs had Multics running on itsGE and Thompson continued to work on it just for fun Doug McIlroy themanager of the group said When Multics began to work the very first place itworked was here Three people could overload itIn the summer of Thompson became a temporary bachelor for a month whenhis wife Bonnie took their yearold son to meet his relatives on the West CoastThompson recalled I allocated a week each to the operating system the shell theeditor and the assemblerit was totally rewritten in a form that looked like anoperating system with tools that were sort of known you know assembler editorshellif not maintaining itself right on the verge of maintaining itself to totallysever the GECOS connectionessentially one person for a monthSteve Bourne who joined Bell Labs the next year described the castoff PDP usedby Ritchie and Thompson The PDP provided only an assembler and a loaderOne user at a time could use the computerThe environment was crude and partsof a singleuser UNIX system were soon forthcomingThe assembler and rudimentary operating system kernel were written and crossassembled for the PDPon GECOS The term UNICS was apparently coined by Peter Neumann an inveterate punster in The original UNIX was a singleuser system obviously anemasculated Multics But although there were aspects of UNICSUNIX that wereinfluenced by Multics there were also as Dennis Ritchie said profound differencesWe were a bit oppressed by the big system mentality he said Ken wanted to dosomething simple Presumably as important as anything was the fact that our meanswere much smaller We could get only small machines with none of the fancy Multics hardware So UNIX wasnt quite a reaction against MulticsMultics wasntthere for us anymore but we liked the feel of interactive computing that it offeredKen had some ideas about how to do a system that he had to work outMulticscolored the UNIX approach but it didnt dominate itKen and Denniss toy system didnt stay simple for long By user commandsincluded as the assembler cal a simple calendar tool cat catenate and printchdir change working directory chmod change mode chown change ownercmp compare two files cp copy file date dc desk calculator du summarizedisk usage ed editor and over two dozen others Most of these commands arestill in useBy February there were UNIX installations Two big innovations had occurred The first was a new programming language C based on B which wasitself a cutdown version of Martin Richards BCPL Basic Combined Programming Language The other innovation was the idea of a pipeA pipe is a simple concept a standardized way of connecting the output of oneprogram to the input of another The Dartmouth TimeSharing System had com GECOS was the General Electric Comprehensive Operating Systemmunication files which anticipated pipes but their use was far more specific Thenotion of pipes as a general facility was Doug McIlroys The implementation wasKen Thompsons at McIlroys insistence It was one of the only places where I verynearly exerted managerial control over UNIX Doug saidIts easy to say cat into grep into or who into cat into grep and so on McIlroyremarked Its easy to say and it was clear from the start that it would be something youd like to say But there are all these side parameters And from time totime Id say How about making something like this And one day I came up witha syntax for the shell that went along with piping and Ken said Im going to do itIn an a orgy of rewriting Thompson updated all the UNIX programs in one nightThe next morning there were oneliners This was the real beginning of the powerof UNIXnot from the individual programs but from the relationships amongthem UNIX now had a language of its own as well as a philosophy Write programs that do one thing and do it well Write programs to work together Write programs that handle text streams as a universal interfaceA generalpurpose timesharing OS had been born but it was trapped inside BellLabs UNIX offered the promise of easily and seamlessly sharing computing resources among projects groups and organizations But before this multipurposetool could be used by the world it had to escape and multiply Katy bar the doorUNIX hits the big time In October the ACM held its Symposium on Operating Systems PrinciplesSOSP in the auditorium at IBMs new TJ Watson Research Center in YorktownHeights NY Ken and Dennis submitted a paper and on a beautiful autumn daydrove up the Hudson Valley to deliver it Thompson made the actual presentationAbout people were in the audience and the talk was a smash hitOver the next six months the number of UNIX installations tripled When thepaper was published in the July issue of the Communications of the ACM theresponse was overwhelming Research labs and universities saw shared UNIX systems as a potential solution to their growing need for computing resourcesAccording to the terms of a antitrust settlement the activities of ATT parentof Bell Labs were restricted to running the national telephone system and to special projects undertaken on behalf of the federal government Thus ATT couldnot sell UNIX as a product and Bell Labs had to license its technology to others Inresponse to requests Ken Thompson began shipping copies of the UNIX sourcecode According to legend each package included a personal note signed love kenOne person who received a tape from Ken was Professor Robert Fabry of the University of California at Berkeley By January the seed of Berkeley UNIX hadbeen plantedOther computer scientists around the world also took an interest in UNIX In John Lions on the faculty of the University of New South Wales in Australiapublished a detailed commentary on a version of the kernel called V This effortbecame the first serious documentation of the UNIX system and helped others tounderstand and expand on Ken and Denniss workStudents at Berkeley enhanced the version of UNIX they had received from Bell Labsto meet their needs The first Berkeley tape BSD short for st Berkeley SoftwareDistribution included a Pascal system and the vi editor for the PDP The student behind the release was a grad student named Bill Joy BSD came the next yearand BSD the first Berkeley release for the DEC VAX was distributed in late In Professor Fabry struck a deal with the Defense Advanced Research ProjectAgency DARPA to continue the development of UNIX This arrangement led tothe formation of the Computer Systems Research Group CSRG at Berkeley Latethe next year BSD was released It became quite popular largely because it was theonly version of UNIX that ran on the DEC VAX the commodity computingplatform of the time Another big advancement of BSD was the introduction ofTCPIP sockets the generalized networking abstraction that spawned the Internetand is now used by most modern operating systems By the mids most major universities and research institutions were running at least one UNIX systemIn Bill Joy took the BSD tape with him to start Sun Microsystems now partof Oracle America and the Sun operating system SunOS In the courtordered divestiture of ATT began One unanticipated side effect of the divestiturewas that ATT was now free to begin selling UNIX as a product They releasedATT UNIX System V a wellrecognized albeit awkward commercial implementation of UNIXNow that Berkeley ATT Sun and other UNIX distributions were available to awide variety of organizations the foundation was laid for a general computing infrastructure built on UNIX technology The same system that was used by the astronomy department to calculate star distances could be used by the applied mathdepartment to calculate Mandelbrot sets And that same system was simultaneouslyproviding email to the entire universityThe rise of system administratorsThe management of generalpurpose computing systems demanded a different set ofskills than those required just two decades earlier Gone were the days of the systemoperator who focused on getting a single computer system to perform a specializedtask System administrators came into their own in the early s as people whoran UNIX systems to meet the needs of a broad array of applications and usersBecause UNIX was popular at universities and because those environments included lots of students who were eager to learn the latest technology universitieswere early leaders in the development of organized system administration groupsUniversities such as Purdue the University of Utah the University of Coloradothe University of Maryland and the State University of New York SUNY Buffalobecame hotbeds of system administrationSystem administrators also developed an array of their own processes standardsbest practices and tools such as sudo Most of these products were built out ofnecessity without them unstable systems and unhappy users were the resultEvi Nemeth became known as the mother of system administration by recruiting undergraduates to work as system administrators to support the EngineeringCollege at the University of Colorado Her close ties with folks at Berkeley theUniversity of Utah and SUNY Buffalo created a system administration community that shared tips and tools Her crew often called the munchkins or Evi slavesattended USENIX and other conferences and worked as onsite staff in exchangefor the opportunity to absorb information at the conferenceIt was clear early on that system administrators had to be rabid jacks of all tradesA system administrator might start a typical day in the s by using a wirewraptool to fix an interrupt jumper on a VAX backplane Midmorning tasks might include sucking spilled toner out of a malfunctioning firstgeneration laser printerLunch hour could be spent helping a grad student debug a new kernel driver andthe afternoon might consist of writing backup tapes and hassling users to clean uptheir home directories to make space in the filesystem A system administrator wasand is a fixeverything takenoprisoners guardian angelThe s were also a time of unreliable hardware Rather than living on a singlesilicon chip the CPUs of the s were made up of several hundred chips all ofthem prone to failure It was the system administrators job to isolate failed hardware and get it replaced quickly Unfortunately these were also the days beforeit was common to FedEx parts on a whim so finding the right part from a localsource was often a challengeIn one case our beloved VAX was down leaving the entire campus withoutemail We knew there was a business down the street that packaged VAXes to beshipped to the then coldwar Soviet Union for research purposes Desperate weshowed up at their warehouse with a huge wad of cash in our pocket and after aboutan hour of negotiation we escaped with the necessary board At the time someoneremarked that it felt more comfortable to buy drugs than VAX parts in BoulderSystem administration documentation and trainingAs more individuals began to identify themselves as system administratorsandas it became clear that one might make a decent living as a sysadminrequests fordocumentation and training became more common In response folks like TimOReilly and his team then called OReilly and Associates now OReilly Mediabegan to publish UNIX documentation that was based on handson experienceand written in a straightforward wayAs a vehicle for inperson interaction the USENIX Association held its first conference focused on system administration in This Large Installation SystemAdministration LISA conference catered mostly to a west coast crowd Three yearslater the SANS SysAdmin Audit Network Security Institute was established tomeet the needs of the east coast Today both the LISA and SANS conferences servethe entire US region and both are still going strongIn we published the first edition of this book then titled UNIX System Administration Handbook It was quickly embraced by the community perhaps because ofthe lack of alternatives At the time UNIX was so unfamiliar to our publisher thattheir production department replaced all instances of the string etc with and soon resulting in filenames such as and so onpasswd We took advantage of thesituation to seize total control of the bits from cover to cover but the publisher isadmittedly much more UNIX savvy today Our year relationship with this samepublisher has yielded a few other good stories but well omit them out of fear ofsouring our otherwise amicable relationshipUNIX hugged to near death Linux is born By late it seemed that UNIX was well on its way to world domination It wasunquestionably the operating system of choice for research and scientific computingand it had been adopted by mainstream businesses such as Taco Bell and McDonalds Berkeleys CSRG group then consisting of Kirk McKusick Mike Karels KeithBostic and many others had just released BSDReno a pun on an earlier release that added support for the CCI Power code named Tahoe processorCommercial releases of UNIX such as SunOS were also thriving their success driven in part by the advent of the Internet and the first glimmers of ecommerce PChardware had become a commodity It was reasonably reliable inexpensive andrelatively highperformance Although versions of UNIX that ran on PCs did existall the good options were commercial and closed source The field was ripe for anopen source PC UNIXIn a group of developers that had worked together on the BSD releases DonnSeeley Mike Karels Bill Jolitz and Trent R Hein together with a few other BSDadvocates founded Berkeley Software Design Inc BSDI Under the leadership ofRob Kolstad BSDI provided binaries and source code for a fully functional commercial version of BSD UNIX on the PC platform Among other things this project proved that inexpensive PC hardware could be used for production computingBSDI fueled explosive growth in the early Internet as it became the operating systemof choice for early Internet service providers ISPsIn an effort to recapture the genie that had escaped from its bottle in ATT in filed a lawsuit against BSDI and the Regents of the University of California alleging code copying and theft of trade secrets It took ATTs lawyers over two yearsto identify the offending code When all was said and done the lawsuit was settledand three files out of more than were removed from the BSD code baseSee Chapter for more pointers tosysadmin resourcesUnfortunately this twoyear period of uncertainty had a devastating effect on theentire UNIX world BSD and nonBSD versions alike Many companies jumped shipto Microsoft Windows fearful that they would end up at the mercy of ATT as ithugged its child to neardeath By the time the dust cleared BSDI and the CSRGwere both mortally wounded The BSD era was coming to an endMeanwhile Linus Torvalds a Helsinki college student had been playing with Minixand began writing his own UNIX clone By a variety of Linux distributionsincluding SuSE and Yggdrasil Linux had emerged saw the establishmentof Red Hat and Linux ProMultiple factors have contributed to the phenomenal success of Linux The strongcommunity support enjoyed by the system and its vast catalog of software fromthe GNU archive make Linux quite a powerhouse It works well in production environments and some folks argue that you can build a more reliable and performant system on top of Linux than you can on top of any other operating systemIts also interesting to consider that part of Linuxs success may relate to the goldenopportunity created for it by ATTs action against BSDI and Berkeley That illtimed lawsuit struck fear into the hearts of UNIX advocates right at the dawn ofecommerce and the start of the Internet bubbleBut who cares right What remained constant through all these crazy changeswas the need for system administrators A UNIX system administrators skill setis directly applicable to Linux and most system administrators guided their usersgracefully through the turbulent seas of the s Thats another important characteristic of a good system administrator calm during a stormA world of windows Microsoft first released Windows NT in The release of a server version ofWindows which had a popular user interface generated considerable excitementjust as ATT was busy convincing the world that it might be out to fleece everyonefor license fees As a result many organizations adopted Windows as their preferredplatform for shared computing during the late s Without question the Microsoft platform has come a long way and for some organizations it is the best optionUnfortunately UNIX Linux and Windows administrators initially approachedthis marketplace competition in an adversarial stance Less filling vs tastes greatarguments erupted in organizations around the world Many UNIX and Linuxsystem administrators started learning Windows convinced theyd be put out topasture if they didnt After all Windows was on the horizon By the close ofthe millennium the future of UNIX looked grim Minix is a PCbased UNIX clone developed by Andrew S Tanenbaum a professor at the Free University in Amsterdam Just for the record Windows is indeed less fillingUNIX and Linux thrive As the Internet bubble burst everyone scrambled to identify what was real andwhat had been only a venturecapitalfueled mirage As the smoke drifted away itbecame clear that many organizations with successful technology strategies wereusing UNIX or Linux along with Windows rather than one or the other It wasnta war anymoreA number of evaluations showed that the total cost of ownership TCO of a Linuxserver was significantly lower than that of a Windows server As the impact of the economic crash hit TCO became more important than ever The world againsteered toward open source versions of UNIX and LinuxUNIX and Linux in the hyperscale cloud presentLinux and PCbased UNIX variants such as FreeBSD have continued to expandtheir market share with Linux being the only operating system whose market shareon servers is growing Not to be left out Apples current fullsize operating systemmacOS is also a variant of UNIXMuch of the recent growth in UNIX and Linux has occurred in the context of virtualization and cloud computing See Chapter Virtualization and Chapter Cloud Computing for more details about these technologiesThe ability to create virtual infrastructure and entire virtual data centers by makingAPI calls has fundamentally shifted the course of the river once again Gone are thedays of managing physical servers by hand Scaling infrastructure no longer meansslapping down a credit card and waiting for boxes to appear on the loading dockThanks to services such as Google GCP Amazon AWS and Microsoft Azure theera of the hyperscale cloud has arrived Standardization tools and automation arenot just novelties but intrinsic attributes of every computing environmentToday competent management of fleets of servers requires extensive knowledgeand skill System administrators must be disciplined professionals They must knowhow to build and scale infrastructure how to work collaboratively with peers in aDevOps environment how to code simple automation and monitoring scripts andhow to remain calm when a thousand servers are down at onceUNIX and Linux tomorrowWhere are we headed next The lean modular paradigm that has served UNIX sowell over the last few decades is also one foundation of the upandcoming Internet Even Apples iPhone runs a cousin of UNIX and Googles Android operating system derives from theLinux kernel One thing hasnt changed whiskey is still a close friend to many system administratorsof Things IoT The Brookings Institution estimates that billion small distributed IoT devices will exist by see brookgsbNwbyaIts tempting to think of these devices as we thought of the nonnetworked consumer appliances eg toaster ovens or blenders of yesteryear plug them in usethem for a few years and if they break throw them in the landfill They dont needmanagement or central administration rightIn fact nothing could be further from the truth Many of these devices handle sensitive data eg audio streamed from a microphone in your living room or perform missioncritical functions such as controlling the temperature of your houseSome of these devices run embedded software derived from the OSS world Butregardless of whats inside the devices themselves the majority report back to amother ship in the cloud that runsyou guessed itUNIX or Linux In the earlymarketshare land grab many devices have already been deployed without muchthought to security or to how the ecosystem will operate in the futureThe IoT craze isnt limited to the consumer market Modern commercial buildingsare riddled with networked devices and sensors for lighting HVAC physical security and video just to name a few These devices often pop up on the networkwithout coordination from the IT or Information Security departments Theyrethen forgotten without any plan for ongoing management patching or monitoringSize doesnt matter when it comes to networked systems System administratorsneed to advocate for the security performance and availability of IoT devices andtheir supporting infrastructure regardless of size location or functionSystem administrators hold the worlds computing infrastructure together solvethe hairy problems of efficiency scalability and automation and provide experttechnology leadership to users and managers alikeWe are system administrators Hear us roarRecommended readingMcKusick Marshall Kirk Keith Bostic Michael J Karels and John SQuarterman The Design and Implementation of the BSD Operating Systemnd Edition Reading MA AddisonWesley Salus Peter H A Quarter Century of UNIX Reading MA AddisonWesley Salus Peter H Casting the Net From ARPANET to Internet and Beyond ReadingMA AddisonWesley Salus Peter H The Daemon the Gnu and the Penguin Marysville WA ReedMedia Services This book was also serialized at wwwgroklawnet Dont really do that You should recycle everything